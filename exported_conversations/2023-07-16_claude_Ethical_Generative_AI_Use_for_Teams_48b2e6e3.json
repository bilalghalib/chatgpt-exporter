{
  "id": "48b2e6e3-3902-4103-8247-2b2c63cdb2ba",
  "source": "claude",
  "title": "Ethical Generative AI Use for Teams",
  "created_at": "2023-07-16T07:45:01.140043Z",
  "updated_at": "2023-07-16T07:59:29.201000Z",
  "summary": "",
  "message_count": 10,
  "messages": [
    {
      "id": "4743868a-ea82-49cd-895c-31b9ed145ba9",
      "role": "human",
      "content": "Can yo usummarize this document and give me 3 slides worth of content to share about ethical generative AI use with teams:\n\n**EBOOK**\n\n**Build Responsible Generative AI Applications: Introducing the RAFT Framework**\n\n**Introduction: Great Power,**\n\n**Great Responsibility**\n\nRecent excitement around Generative AI — in particular Large Language Models (LLMs) — means organizations are pushing forward with the use of AI at an unprecedented pace. There has arguably never been a more pivotal time in the history of AI.\n\nAt the same time, it’s important to stay grounded. The truth is that flaws within AI systems and the data they are built on can present — and have presented, even before the rise of Generative AI — real risks.\n\nMore than ever before, organizations need to think about building AI systems in a responsible and governed manner.\n\nIn this ebook, we will deep dive into those aforementioned risks plus introduce the RAFT (Reliable, Accountable, Fair, and Transparent) framework for Responsible AI, showing how it can be applied to both traditional and Generative AI systems.\n\n**EBOOK 2 DATAIKU**\n\n**Understanding Values for Responsible AI**\n\nWhether your business is scaling the use of AI across the organization, interested in experimenting with the latest developments in Generative AI, or simply looking to make sense of forthcoming regulation, it’s important to have a set of guardrails defined for the use of AI at your organization.\n\nVarious standards organizations and governments have proposed frameworks for AI values — see Table 1. These are a good starting point, but in the next section, we’ll take it one step further with a more specific, robust, and tested framework.\n\n**TABLE 1: MAPPING OF AI RMF TAXONOMY TO AI POLICY DOCUMENTS**\n\n_Source : NIST AI RMF V2_\n\n**EBOOK 3 DATAIKU**\n\n**The RAFT Framework for Responsible AI**\n\nTo support the scale of safe AI, Dataiku offers a number of built-in features for responsible design, deployment, and governance. These features align with a value framework similar to those proposed in Table 1, which any organization can use as a starting point for their own Responsible AI programs.\n\n**TABLE 2: RAFT FRAMEWORK BASICS**\n\n_Note: See page 11 for the full framework._\n\nThese values make up Dataiku’s baseline approach to Responsible AI — we call it RAFT for Reliable, Accountable, Fair, and Transparent. The values outlined in the RAFT framework are crucial for the development of AI and analytics, and they cover both traditional and new methods in Generative AI.\n\nTo effectively execute on these principles requires understanding the potential risks and impacts of technology. In the rest of this ebook, we will cover the specific risks of Generative AI and broader approaches to Responsible AI practices.\n\nOur understanding of the risks and necessary steps to reduce potential harm from AI is meant as a starting point for organizations looking to build best practices in governed and Responsible AI. In addition to building best practices, these guidelines can support organizations in their readiness efforts toward upcoming regulations, such as the EU AI Act.\n\nAs the field of AI continuously evolves, so do our approaches to safe and scaled uses of new technology. We encourage readers to take the suggestions and recommendations here and adapt and expand them as needed per industry and regulatory standards.\n\n**EBOOK 4 DATAIKU**\n\n**Risks to Using**\n\n**Generative AI in the Enterprise**\n\nAI systems are inherently socio-technical, which means “they are influenced by societal dynamics and human behavior.”1 Acknowledging the way AI impacts society and vice versa allows us to better anticipate potential negative consequences and proactively reduce or prevent those harms before they occur.\n\nIn addition to the socio-technical risks from Generative AI, there are also emerging legal considerations around privacy and copyright infringements. Below, we list a number of risks that may arise in the use of Generative AI in the enterprise. These risks are common across various types of Generative AI technology but will surface in different ways across use cases:\n\n**RISKS FROM GENERATIVE AI AND LLMS**\n\n**Toxicity:** Toxic, obscene, or otherwise inappropriate outputs.\n\n**Polarity:** Unfair positive or negative attitudes to certain individuals or groups.\n\n**Discrimination:** Model performance is less robust for certain social groups.\n\n**Human-Computer Interactions2:** Over-reliance on the outputs of AI due to perceived sentience or blind trust in an automated system.\n\n**Disinformation:** Presenting factually incorrect answers or information. **Data Privacy:** Input data shared back to 3rd-party model providers and possibly\n\nshared as future outputs to non-authorized users.\n\n**Model Security:** Ability for a user to circumvent security protocols intended to prevent social-technical harms or gain access to unauthorized data.\n\n**Copyright Infringements:** Redistribution of copyrighted material, presented as original content.\n\n_1\\. https://www.nist.gov/news-events/news/2023/01/nist-risk-management-framework-aims-improve- trustworthiness-artificial#_\n\n_2\\. “Ethical and Social Risks of harm from Language Models”, DeepMind 2021, https://arxiv.org/abs/2112.04359_\n\n**EBOOK 5 DATAIKU**\n\nThe potential harms listed here are not exclusive to language models, but they are heightened by the use of natural language processing (NLP) techniques to analyze, categorize, or generate text in a variety of business contexts.\n\nUnderstanding and addressing these risks before implementing an LLM or other Generative AI techniques into an AI system is crucial to ensure the responsible and governed use of the latest technology. By thinking through potential harms, designers, developers, and deployers of LLMs can set thresholds for risk and incorporate mitigation strategies in each stage of the AI lifecycle.\n\n**Risks of Generative AI by Context**\n\nBeyond the inherent, high-level risks associated with Generative AI technology, businesses should consider the context in which the system will be deployed.\n\nA baseline approach is to assess the use case across two dimensions:\n\n1\\. Target of analysis, which focuses on the type of data or documents that the model will make use of to generate output. Corporate or business documents include items as invoices, legal agreements, or system documentation. Individual or personal data can include traditional tabular information about a person’s specific characteristics, as well as call center transcriptions or text written by an end user. Academic or domain-specific texts are typically used in industry research and analysis, such as medical publications, manufacturing research, or legal codes.\n\n**EBOOK 6 DATAIKU**\n\n2\\. Delivery method, which looks at how the output of a model is delivered to end users. In the first instance, the output is shared as a report, recommendation, or suggested actions in response to a single query. This is different from the next category of virtual assistants, chatbots, etc., that respond in a human-like way to end users’ queries. The final category is automated processes, such as sending mass marketing emails or robotic process automation (RPA).\n\nEach category within these two dimensions will carry different risk tradeoffs and strategies to prevent harm to the business, clients, and broader society.\n\n**Target of Analysis Delivery Method**\n\n**Corporate or Business Documents**\n\nWhile social risks from the use of these documents are lower, it is important to ensure the documents are up to date and relevant for the question at hand. This should include designing the input parsing strategy to leverage the correct documents and content to answer a given query. Prevent leakage of sensitive or copyright material into the model output — users should not be able to circumvent model parameters to gain access to unauthorized documents.\n\n**Output Shared as a Report, Recommendation, or Suggestion Actions**\n\nGenerally this type of delivery ensures that end users have control over how the output is shared or deployed down the line. It provides an opportunity for review of outputs before action is taken, allowing the end user a chance to review explanations and provide feedback.\n\n**Individual or Personal Data**\n\nPersonal data on customers, users, or patients should be protected from unauthorized access. Models that make use of personal information to draw conclusions about individuals should be tested for fairness across subpopulations of interest, language output should not be toxic or reinforce stereotypes about groups of people, and end users should know that their data is being used to train and deploy models (with the ability to opt out from this usage). Take care that personal information or details that are not relevant to the specific use case are not used in the model or shared back to the end user.\n\n**Virtual Assistant**\n\nVirtual assistant delivery includes chatbots, text to voice responses, and question answering. These interactions should be clearly marked as a computer/ model and not a human agent. Extra precautions should be taken to make it clear the model is not a sentient agent and could potentially provide incorrect responses. Additionally, generative text models should be developed with guardrails around the type of language and discussion permissible by the model and end users to prevent toxic or harmful conversation from occurring.\n\n**Academic or Domain-Specific Texts**\n\nTexts should be carefully cultivated to ensure fit with the intended use case. Model outputs should be able to provide citations to real texts in the corpus to support any generated answers or recommendations.\n\n**Automated Process**\n\nResults of a Generative AI model may be passed directly to an end user without review. Automated processes are typically used to scale AI use, meaning real-time human intervention is not possible. Regular review of model outputs, quality, and the ability to pause automated processes when necessary is critical to catch any potential harms. Ensure you clearly document accountability over stopping an automated process.\n\n**EBOOK 7 DATAIKU**\n\n**Concerns Based on Expected Audience**\n\nOne important dimension of Generative AI use cases we have not yet covered is the type of audience or expected reach for model outputs. The audience for model outputs are usually business users, consumers, individuals or some combination of both groups.\n\nHowever, no matter the expected audience for model outputs, there are core criteria that should be met in the deployment of any AI system — these four criteria further the goals of reliability and transparency as detailed in the RAFT principles and support broader trust in AI systems:\n\n**EBOOK 8 DATAIKU**\n\n**Assessing Potential Impacts of AI**\n\nBefore deploying an AI system —generative or not — it’s important to assess the potential impact on individuals or groups once a solution is in use. While specific impacts or unintended consequences will vary from use case to use case, Dataiku suggests two broad dimensions that can be used to understand impact from a deployed AI system. These impacts and potential risks are based on standards such as the NIST Risk Framework or the EU AI Act and are meant to guide our customers as they implement AI systems.\n\nDepending on the use case, an AI pipeline may have more than one type of model providing output. For example, the next best offer project3 uses both a traditional recommendation model and an LLM to help write the text of an email to a customer.\n\nIn such an instance, it is important to assess impact and apply Responsible AI principles to both sets of models in the project. Potential bias or poor performance from a recommendation model will have different impacts than bias or toxicity from a language model.\n\nThe risk scoring for unintended consequences is based on two variables:\n\n1\\. Whether the risk could materialize as a harm to individuals and groups directly because of the solution’s implementation or indirectly because of some constellation of factors that are difficult to qualify at the time of deployment.\n\n_3\\. https://www.dataiku.com/solutions/catalog/llm-enhanced-next-best-offer/_\n\n**EBOOK 9 DATAIKU**\n\n2\\. Whether the risk could materialize as a harm immediately or over a longer period of time.\n\nThis results in two larger guiding questions about the nature of an AI system :\n\n**Q1 : Does the output of this project lead to direct impact on individuals or groups? Q2 : Is the impact felt immediately or observed over time?**\n\nPutting these two variables together, we qualify the solution’s Responsible AI considerations in one of the following categories :\n\n**Putting Principles into Practice**\n\nHow do we move from defining principles, potential risks, and impacts of AI to implementing best practices and guardrails in the development of AI systems?\n\nLet’s return to the RAFT framework, which provides a baseline set of values for safe AI and that can serve as a starting point for your organization’s own indicators for Responsible AI. We encourage the governance, ethics and compliance teams at your organization to adapt the framework to accommodate specific industry requirements, local regulations or additional business values.\n\nAs with assessing impact, it is necessary to apply the principles from the RAFT framework to all models (both traditional and generative) used in an AI system.\n\n**EBOOK 10 DATAIKU**\n\nNote: Specific methods to assess and manage the bias of language models or other Generative AI are still in development. When building or fine tuning a model, developers should use diverse and representative datasets and check model performance against risks like polarity, toxicity, or other unfair behavior against sensitive groups.\n\n**EBOOK DATAIKU**\n\n**Conclusion: An Evolving World & Ongoing**\n\n**Considerations**\n\nIt’s only the beginning for Generative AI, which means we’re only at the beginning of our understanding of the extent of the opportunity — as well as the risks — it presents.\n\nIn addition to the RAFT guidelines for Responsible AI, Dataiku is proud to offer a comprehensive training on how to implement Responsible AI in practice. These courses are available for anyone who wishes to gain hands-on experience with measuring bias, building fairer models, and creating compelling reporting on data science pipelines with Dataiku. The training covers core concepts in Responsible AI through the lens of classification models, but much of the teachings around bias, explainability and reliability can be applied to Generative AI models as well.\n\nHere are some additional considerations to keep top of mind moving forward when it comes to leveraging Generative AI in the enterprise.\n\n**Access the Responsible AI training on Dataiku Academy**\n\n**EBOOK 12 DATAIKU**\n\n**Third-Party Models**\n\nIn many cases today, businesses will rely entirely on pre-trained, third-party models to support a Generative AI use case. In this situation, risks arise from the inability to know how the model was trained, not having confirmation that it is not biased or unreliable, and having no control over how input data is shared back to the third-party provider for retraining the model.\n\nThe use of third-party models should be closely monitored across the organization, and prompts should be regularly reviewed to ensure private data is not supplied as inputs to the model. Additionally, education on how to work with the third-party model (plus test the reliability of outputs) should be available for users.\n\n**Environmental Impacts**\n\nAn additional area of concern is that of the financial and environmental cost of training and deploying LLMs. Given the numerous pre-trained models available for commercial and open-source use, many organizations do not need to build an LLM from scratch. Instead, many businesses opt to fine tune existing models on specific data, which is far less costly and requires less compute resources.\n\nFine tuning an existing model is therefore a benefit to businesses who wish to avoid the pitfalls from using third-party models and reduce the cost of training a new model. However, it is important to note that using a generative model in a production environment requires the use of GPUs, which will incur higher computational and environmental costs.\n\n**EBOOK 13 DATAIKU**\n\n**Everyday AI, Extraordinary People**\n\n**Elastic Architecture Built for the Cloud**\n\n**Machine Learning Visualization Data Preparation**\n\n**DataOps Governance & MLOps Applications**\n\nDataiku is the platform for Everyday AI, enabling data experts and domain experts to work together to build AI into their daily operations. Together, they design, develop and deploy new AI capabilities, at all scales and in all industries.\n\n**Name**\n\nNatural lang.\n\nBraund, Mr. Owen Harris Moran, Mr. James\n\n**Sex Age**\n\nHeikkinen, Miss. L\n\nmale 22 male 38 female 26\n\na\n\nin\n\na\n\n**ove** rows containing Mr. Futrelle, Mrs. Jacques Heath\n\nMcCarthy, Mr. Rob\n\n**l**\n\nHewlett, Mrs (Mar\n\ny\n\n**R**\n\ne\n\ny\n\nD\n\nK\n\n**e**\n\nr\n\n**m**\n\ne\n\n)\n\n29\n\nGender\n\nInteger\n\nfemale 35 **Keep only** rows containing Mr.\n\nAllen, Mr. William Henry\n\nmale 35 male\n\n**S**\n\n**it** column on Mr. t\n\n**p**\n\n**R**\n\n**e**\n\n**p**\n\n**la**\n\n**ce**\n\n... **Remove** rows equal to Moran, Mr. James\n\n**Keep only** rows equal to Moran, Mr. James **Clear** cells equal to Moran, Mr. James\n\nin\n\ngc\n\nM\n\no\n\nr\n\n.b\n\nm\n\nFilter on Moran, Mr. James Filter on Mr. Toggle row highlight Show complete value\n\n**©2023 dataiku | dataiku.com**\n\n**Authors**\n\n**JACOB BESWICK**\n\nJacob Beswick is Dataiku’s Director for AI Governance Solutions and Responsible AI. Jacob leads a team that works with organizations globally to develop their approaches to AI and analytics governance, ensuring that their objectives — for example, compliance, risk management, or responsible development and use — can be embedded in Dataiku’s platform. Prior to joining Dataiku, Jacob served in the UK Civil Service, where he led a portfolio covering AI adoption, governance and regulation. As part of this, he represented the UK at the European Commission, drafted what has become the UK’s proposals on AI regulation, and created BridgeAI, which is now being delivered by Innovate UK.\n\n**TRIVENI GANDHI**\n\nTriveni is a Jill-of-all-trades data scientist, thought leader, and advocate for the responsible use of AI who likes to find simple solutions to complicated problems. As Responsible AI Lead at Dataiku, she builds and implements custom solutions to support the responsible and safe scaling of\n\nAI. This includes hands-on training to put RAI principles into practice as well as change management programs that help business leaders translate ethical values into actionable indicators. Triveni also supports field product teams with innovative methods to govern and oversee complex AI problems — often serving as a technical bridge between subject matter experts and engineers. Triveni\n\nis the winner of VentureBeats Women in AI Awards in the category of Responsibility and Ethics in AI and was listed as 100 Brilliant Women in AI Ethics in 2022. She holds a Ph.D in Political Science from Cornell University.\n\n**EBOOK**\n\n**15 DATAIKU**\n\n**©2023 DATAIKU | DATAIKU.COM**",
      "created_at": "2023-07-16T07:45:34.856372Z",
      "sequence_order": 0,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-07-16T07:45:34.856372Z",
            "stop_timestamp": "2023-07-16T07:45:34.856372Z",
            "flags": null,
            "type": "text",
            "text": "Can yo usummarize this document and give me 3 slides worth of content to share about ethical generative AI use with teams:\n\n**EBOOK**\n\n**Build Responsible Generative AI Applications: Introducing the RAFT Framework**\n\n**Introduction: Great Power,**\n\n**Great Responsibility**\n\nRecent excitement around Generative AI — in particular Large Language Models (LLMs) — means organizations are pushing forward with the use of AI at an unprecedented pace. There has arguably never been a more pivotal time in the history of AI.\n\nAt the same time, it’s important to stay grounded. The truth is that flaws within AI systems and the data they are built on can present — and have presented, even before the rise of Generative AI — real risks.\n\nMore than ever before, organizations need to think about building AI systems in a responsible and governed manner.\n\nIn this ebook, we will deep dive into those aforementioned risks plus introduce the RAFT (Reliable, Accountable, Fair, and Transparent) framework for Responsible AI, showing how it can be applied to both traditional and Generative AI systems.\n\n**EBOOK 2 DATAIKU**\n\n**Understanding Values for Responsible AI**\n\nWhether your business is scaling the use of AI across the organization, interested in experimenting with the latest developments in Generative AI, or simply looking to make sense of forthcoming regulation, it’s important to have a set of guardrails defined for the use of AI at your organization.\n\nVarious standards organizations and governments have proposed frameworks for AI values — see Table 1. These are a good starting point, but in the next section, we’ll take it one step further with a more specific, robust, and tested framework.\n\n**TABLE 1: MAPPING OF AI RMF TAXONOMY TO AI POLICY DOCUMENTS**\n\n_Source : NIST AI RMF V2_\n\n**EBOOK 3 DATAIKU**\n\n**The RAFT Framework for Responsible AI**\n\nTo support the scale of safe AI, Dataiku offers a number of built-in features for responsible design, deployment, and governance. These features align with a value framework similar to those proposed in Table 1, which any organization can use as a starting point for their own Responsible AI programs.\n\n**TABLE 2: RAFT FRAMEWORK BASICS**\n\n_Note: See page 11 for the full framework._\n\nThese values make up Dataiku’s baseline approach to Responsible AI — we call it RAFT for Reliable, Accountable, Fair, and Transparent. The values outlined in the RAFT framework are crucial for the development of AI and analytics, and they cover both traditional and new methods in Generative AI.\n\nTo effectively execute on these principles requires understanding the potential risks and impacts of technology. In the rest of this ebook, we will cover the specific risks of Generative AI and broader approaches to Responsible AI practices.\n\nOur understanding of the risks and necessary steps to reduce potential harm from AI is meant as a starting point for organizations looking to build best practices in governed and Responsible AI. In addition to building best practices, these guidelines can support organizations in their readiness efforts toward upcoming regulations, such as the EU AI Act.\n\nAs the field of AI continuously evolves, so do our approaches to safe and scaled uses of new technology. We encourage readers to take the suggestions and recommendations here and adapt and expand them as needed per industry and regulatory standards.\n\n**EBOOK 4 DATAIKU**\n\n**Risks to Using**\n\n**Generative AI in the Enterprise**\n\nAI systems are inherently socio-technical, which means “they are influenced by societal dynamics and human behavior.”1 Acknowledging the way AI impacts society and vice versa allows us to better anticipate potential negative consequences and proactively reduce or prevent those harms before they occur.\n\nIn addition to the socio-technical risks from Generative AI, there are also emerging legal considerations around privacy and copyright infringements. Below, we list a number of risks that may arise in the use of Generative AI in the enterprise. These risks are common across various types of Generative AI technology but will surface in different ways across use cases:\n\n**RISKS FROM GENERATIVE AI AND LLMS**\n\n**Toxicity:** Toxic, obscene, or otherwise inappropriate outputs.\n\n**Polarity:** Unfair positive or negative attitudes to certain individuals or groups.\n\n**Discrimination:** Model performance is less robust for certain social groups.\n\n**Human-Computer Interactions2:** Over-reliance on the outputs of AI due to perceived sentience or blind trust in an automated system.\n\n**Disinformation:** Presenting factually incorrect answers or information. **Data Privacy:** Input data shared back to 3rd-party model providers and possibly\n\nshared as future outputs to non-authorized users.\n\n**Model Security:** Ability for a user to circumvent security protocols intended to prevent social-technical harms or gain access to unauthorized data.\n\n**Copyright Infringements:** Redistribution of copyrighted material, presented as original content.\n\n_1\\. https://www.nist.gov/news-events/news/2023/01/nist-risk-management-framework-aims-improve- trustworthiness-artificial#_\n\n_2\\. “Ethical and Social Risks of harm from Language Models”, DeepMind 2021, https://arxiv.org/abs/2112.04359_\n\n**EBOOK 5 DATAIKU**\n\nThe potential harms listed here are not exclusive to language models, but they are heightened by the use of natural language processing (NLP) techniques to analyze, categorize, or generate text in a variety of business contexts.\n\nUnderstanding and addressing these risks before implementing an LLM or other Generative AI techniques into an AI system is crucial to ensure the responsible and governed use of the latest technology. By thinking through potential harms, designers, developers, and deployers of LLMs can set thresholds for risk and incorporate mitigation strategies in each stage of the AI lifecycle.\n\n**Risks of Generative AI by Context**\n\nBeyond the inherent, high-level risks associated with Generative AI technology, businesses should consider the context in which the system will be deployed.\n\nA baseline approach is to assess the use case across two dimensions:\n\n1\\. Target of analysis, which focuses on the type of data or documents that the model will make use of to generate output. Corporate or business documents include items as invoices, legal agreements, or system documentation. Individual or personal data can include traditional tabular information about a person’s specific characteristics, as well as call center transcriptions or text written by an end user. Academic or domain-specific texts are typically used in industry research and analysis, such as medical publications, manufacturing research, or legal codes.\n\n**EBOOK 6 DATAIKU**\n\n2\\. Delivery method, which looks at how the output of a model is delivered to end users. In the first instance, the output is shared as a report, recommendation, or suggested actions in response to a single query. This is different from the next category of virtual assistants, chatbots, etc., that respond in a human-like way to end users’ queries. The final category is automated processes, such as sending mass marketing emails or robotic process automation (RPA).\n\nEach category within these two dimensions will carry different risk tradeoffs and strategies to prevent harm to the business, clients, and broader society.\n\n**Target of Analysis Delivery Method**\n\n**Corporate or Business Documents**\n\nWhile social risks from the use of these documents are lower, it is important to ensure the documents are up to date and relevant for the question at hand. This should include designing the input parsing strategy to leverage the correct documents and content to answer a given query. Prevent leakage of sensitive or copyright material into the model output — users should not be able to circumvent model parameters to gain access to unauthorized documents.\n\n**Output Shared as a Report, Recommendation, or Suggestion Actions**\n\nGenerally this type of delivery ensures that end users have control over how the output is shared or deployed down the line. It provides an opportunity for review of outputs before action is taken, allowing the end user a chance to review explanations and provide feedback.\n\n**Individual or Personal Data**\n\nPersonal data on customers, users, or patients should be protected from unauthorized access. Models that make use of personal information to draw conclusions about individuals should be tested for fairness across subpopulations of interest, language output should not be toxic or reinforce stereotypes about groups of people, and end users should know that their data is being used to train and deploy models (with the ability to opt out from this usage). Take care that personal information or details that are not relevant to the specific use case are not used in the model or shared back to the end user.\n\n**Virtual Assistant**\n\nVirtual assistant delivery includes chatbots, text to voice responses, and question answering. These interactions should be clearly marked as a computer/ model and not a human agent. Extra precautions should be taken to make it clear the model is not a sentient agent and could potentially provide incorrect responses. Additionally, generative text models should be developed with guardrails around the type of language and discussion permissible by the model and end users to prevent toxic or harmful conversation from occurring.\n\n**Academic or Domain-Specific Texts**\n\nTexts should be carefully cultivated to ensure fit with the intended use case. Model outputs should be able to provide citations to real texts in the corpus to support any generated answers or recommendations.\n\n**Automated Process**\n\nResults of a Generative AI model may be passed directly to an end user without review. Automated processes are typically used to scale AI use, meaning real-time human intervention is not possible. Regular review of model outputs, quality, and the ability to pause automated processes when necessary is critical to catch any potential harms. Ensure you clearly document accountability over stopping an automated process.\n\n**EBOOK 7 DATAIKU**\n\n**Concerns Based on Expected Audience**\n\nOne important dimension of Generative AI use cases we have not yet covered is the type of audience or expected reach for model outputs. The audience for model outputs are usually business users, consumers, individuals or some combination of both groups.\n\nHowever, no matter the expected audience for model outputs, there are core criteria that should be met in the deployment of any AI system — these four criteria further the goals of reliability and transparency as detailed in the RAFT principles and support broader trust in AI systems:\n\n**EBOOK 8 DATAIKU**\n\n**Assessing Potential Impacts of AI**\n\nBefore deploying an AI system —generative or not — it’s important to assess the potential impact on individuals or groups once a solution is in use. While specific impacts or unintended consequences will vary from use case to use case, Dataiku suggests two broad dimensions that can be used to understand impact from a deployed AI system. These impacts and potential risks are based on standards such as the NIST Risk Framework or the EU AI Act and are meant to guide our customers as they implement AI systems.\n\nDepending on the use case, an AI pipeline may have more than one type of model providing output. For example, the next best offer project3 uses both a traditional recommendation model and an LLM to help write the text of an email to a customer.\n\nIn such an instance, it is important to assess impact and apply Responsible AI principles to both sets of models in the project. Potential bias or poor performance from a recommendation model will have different impacts than bias or toxicity from a language model.\n\nThe risk scoring for unintended consequences is based on two variables:\n\n1\\. Whether the risk could materialize as a harm to individuals and groups directly because of the solution’s implementation or indirectly because of some constellation of factors that are difficult to qualify at the time of deployment.\n\n_3\\. https://www.dataiku.com/solutions/catalog/llm-enhanced-next-best-offer/_\n\n**EBOOK 9 DATAIKU**\n\n2\\. Whether the risk could materialize as a harm immediately or over a longer period of time.\n\nThis results in two larger guiding questions about the nature of an AI system :\n\n**Q1 : Does the output of this project lead to direct impact on individuals or groups? Q2 : Is the impact felt immediately or observed over time?**\n\nPutting these two variables together, we qualify the solution’s Responsible AI considerations in one of the following categories :\n\n**Putting Principles into Practice**\n\nHow do we move from defining principles, potential risks, and impacts of AI to implementing best practices and guardrails in the development of AI systems?\n\nLet’s return to the RAFT framework, which provides a baseline set of values for safe AI and that can serve as a starting point for your organization’s own indicators for Responsible AI. We encourage the governance, ethics and compliance teams at your organization to adapt the framework to accommodate specific industry requirements, local regulations or additional business values.\n\nAs with assessing impact, it is necessary to apply the principles from the RAFT framework to all models (both traditional and generative) used in an AI system.\n\n**EBOOK 10 DATAIKU**\n\nNote: Specific methods to assess and manage the bias of language models or other Generative AI are still in development. When building or fine tuning a model, developers should use diverse and representative datasets and check model performance against risks like polarity, toxicity, or other unfair behavior against sensitive groups.\n\n**EBOOK DATAIKU**\n\n**Conclusion: An Evolving World & Ongoing**\n\n**Considerations**\n\nIt’s only the beginning for Generative AI, which means we’re only at the beginning of our understanding of the extent of the opportunity — as well as the risks — it presents.\n\nIn addition to the RAFT guidelines for Responsible AI, Dataiku is proud to offer a comprehensive training on how to implement Responsible AI in practice. These courses are available for anyone who wishes to gain hands-on experience with measuring bias, building fairer models, and creating compelling reporting on data science pipelines with Dataiku. The training covers core concepts in Responsible AI through the lens of classification models, but much of the teachings around bias, explainability and reliability can be applied to Generative AI models as well.\n\nHere are some additional considerations to keep top of mind moving forward when it comes to leveraging Generative AI in the enterprise.\n\n**Access the Responsible AI training on Dataiku Academy**\n\n**EBOOK 12 DATAIKU**\n\n**Third-Party Models**\n\nIn many cases today, businesses will rely entirely on pre-trained, third-party models to support a Generative AI use case. In this situation, risks arise from the inability to know how the model was trained, not having confirmation that it is not biased or unreliable, and having no control over how input data is shared back to the third-party provider for retraining the model.\n\nThe use of third-party models should be closely monitored across the organization, and prompts should be regularly reviewed to ensure private data is not supplied as inputs to the model. Additionally, education on how to work with the third-party model (plus test the reliability of outputs) should be available for users.\n\n**Environmental Impacts**\n\nAn additional area of concern is that of the financial and environmental cost of training and deploying LLMs. Given the numerous pre-trained models available for commercial and open-source use, many organizations do not need to build an LLM from scratch. Instead, many businesses opt to fine tune existing models on specific data, which is far less costly and requires less compute resources.\n\nFine tuning an existing model is therefore a benefit to businesses who wish to avoid the pitfalls from using third-party models and reduce the cost of training a new model. However, it is important to note that using a generative model in a production environment requires the use of GPUs, which will incur higher computational and environmental costs.\n\n**EBOOK 13 DATAIKU**\n\n**Everyday AI, Extraordinary People**\n\n**Elastic Architecture Built for the Cloud**\n\n**Machine Learning Visualization Data Preparation**\n\n**DataOps Governance & MLOps Applications**\n\nDataiku is the platform for Everyday AI, enabling data experts and domain experts to work together to build AI into their daily operations. Together, they design, develop and deploy new AI capabilities, at all scales and in all industries.\n\n**Name**\n\nNatural lang.\n\nBraund, Mr. Owen Harris Moran, Mr. James\n\n**Sex Age**\n\nHeikkinen, Miss. L\n\nmale 22 male 38 female 26\n\na\n\nin\n\na\n\n**ove** rows containing Mr. Futrelle, Mrs. Jacques Heath\n\nMcCarthy, Mr. Rob\n\n**l**\n\nHewlett, Mrs (Mar\n\ny\n\n**R**\n\ne\n\ny\n\nD\n\nK\n\n**e**\n\nr\n\n**m**\n\ne\n\n)\n\n29\n\nGender\n\nInteger\n\nfemale 35 **Keep only** rows containing Mr.\n\nAllen, Mr. William Henry\n\nmale 35 male\n\n**S**\n\n**it** column on Mr. t\n\n**p**\n\n**R**\n\n**e**\n\n**p**\n\n**la**\n\n**ce**\n\n... **Remove** rows equal to Moran, Mr. James\n\n**Keep only** rows equal to Moran, Mr. James **Clear** cells equal to Moran, Mr. James\n\nin\n\ngc\n\nM\n\no\n\nr\n\n.b\n\nm\n\nFilter on Moran, Mr. James Filter on Mr. Toggle row highlight Show complete value\n\n**©2023 dataiku | dataiku.com**\n\n**Authors**\n\n**JACOB BESWICK**\n\nJacob Beswick is Dataiku’s Director for AI Governance Solutions and Responsible AI. Jacob leads a team that works with organizations globally to develop their approaches to AI and analytics governance, ensuring that their objectives — for example, compliance, risk management, or responsible development and use — can be embedded in Dataiku’s platform. Prior to joining Dataiku, Jacob served in the UK Civil Service, where he led a portfolio covering AI adoption, governance and regulation. As part of this, he represented the UK at the European Commission, drafted what has become the UK’s proposals on AI regulation, and created BridgeAI, which is now being delivered by Innovate UK.\n\n**TRIVENI GANDHI**\n\nTriveni is a Jill-of-all-trades data scientist, thought leader, and advocate for the responsible use of AI who likes to find simple solutions to complicated problems. As Responsible AI Lead at Dataiku, she builds and implements custom solutions to support the responsible and safe scaling of\n\nAI. This includes hands-on training to put RAI principles into practice as well as change management programs that help business leaders translate ethical values into actionable indicators. Triveni also supports field product teams with innovative methods to govern and oversee complex AI problems — often serving as a technical bridge between subject matter experts and engineers. Triveni\n\nis the winner of VentureBeats Women in AI Awards in the category of Responsibility and Ethics in AI and was listed as 100 Brilliant Women in AI Ethics in 2022. She holds a Ph.D in Political Science from Cornell University.\n\n**EBOOK**\n\n**15 DATAIKU**\n\n**©2023 DATAIKU | DATAIKU.COM**",
            "citations": []
          }
        ],
        "updated_at": "2023-07-16T07:45:34.856372Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "7b39a747-2651-4b7b-a161-5824eb4a102f",
      "role": "assistant",
      "content": "Here are 3 potential slides summarizing key points about ethical generative AI use from the documents:\n\nSlide 1 Title: Build Responsible Generative AI\n\n- Generative AI like large language models (LLMs) enable new capabilities, but also present risks around toxicity, bias, privacy, security, and more.  \n\n- Organizations need a framework like RAFT (Reliable, Accountable, Fair, Transparent) to ensure responsible and governed AI development.\n\nSlide 2 Title: Assess Potential Impacts\n\n- Evaluate if AI outputs directly impact individuals/groups or indirectly over time.\n\n- Use this to gauge level of monitoring needed and quantify risks.\n\nSlide 3 Title: Put Principles Into Practice \n\n- Apply RAFT values across the model development lifecycle.\n\n- Audit models for fairness, explain outputs to users, ensure reliability of data inputs.\n\n- Adapt principles like RAFT for your industry and local regulations.",
      "created_at": "2023-07-16T07:45:34.937382Z",
      "sequence_order": 1,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-07-16T07:45:34.937382Z",
            "stop_timestamp": "2023-07-16T07:45:34.937382Z",
            "flags": null,
            "type": "text",
            "text": "Here are 3 potential slides summarizing key points about ethical generative AI use from the documents:\n\nSlide 1 Title: Build Responsible Generative AI\n\n- Generative AI like large language models (LLMs) enable new capabilities, but also present risks around toxicity, bias, privacy, security, and more.  \n\n- Organizations need a framework like RAFT (Reliable, Accountable, Fair, Transparent) to ensure responsible and governed AI development.\n\nSlide 2 Title: Assess Potential Impacts\n\n- Evaluate if AI outputs directly impact individuals/groups or indirectly over time.\n\n- Use this to gauge level of monitoring needed and quantify risks.\n\nSlide 3 Title: Put Principles Into Practice \n\n- Apply RAFT values across the model development lifecycle.\n\n- Audit models for fairness, explain outputs to users, ensure reliability of data inputs.\n\n- Adapt principles like RAFT for your industry and local regulations.",
            "citations": []
          }
        ],
        "updated_at": "2023-07-16T07:45:34.937382Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "8572c74a-e785-4531-b4a1-412519371035",
      "role": "human",
      "content": "Can yo ugive me a slide just about reliability for businesses wanting to use generative AI ?",
      "created_at": "2023-07-16T07:47:01.440937Z",
      "sequence_order": 2,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-07-16T07:47:01.440937Z",
            "stop_timestamp": "2023-07-16T07:47:01.440937Z",
            "flags": null,
            "type": "text",
            "text": "Can yo ugive me a slide just about reliability for businesses wanting to use generative AI ?",
            "citations": []
          }
        ],
        "updated_at": "2023-07-16T07:47:01.440937Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "e34e5f58-4bf9-4f43-8385-48d82d349ac2",
      "role": "assistant",
      "content": "Here is a potential slide focused on reliability for businesses using generative AI:\n\nTitle: Ensure Reliable Generative AI\n\n- Test model outputs to ensure accuracy and relevance to business use case. \n\n- Leverage high-quality, representative data inputs.\n\n- Validate model does not create false information or \"hallucinate\" details.\n\n- Monitor model performance over time and retrain when errors increase.\n\n- Clearly explain model limitations and uncertainties to end users.",
      "created_at": "2023-07-16T07:47:01.494363Z",
      "sequence_order": 3,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-07-16T07:47:01.494363Z",
            "stop_timestamp": "2023-07-16T07:47:01.494363Z",
            "flags": null,
            "type": "text",
            "text": "Here is a potential slide focused on reliability for businesses using generative AI:\n\nTitle: Ensure Reliable Generative AI\n\n- Test model outputs to ensure accuracy and relevance to business use case. \n\n- Leverage high-quality, representative data inputs.\n\n- Validate model does not create false information or \"hallucinate\" details.\n\n- Monitor model performance over time and retrain when errors increase.\n\n- Clearly explain model limitations and uncertainties to end users.",
            "citations": []
          }
        ],
        "updated_at": "2023-07-16T07:47:01.494363Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "7d5249a8-38af-43b3-98fa-785f5fcb2609",
      "role": "human",
      "content": "Can you give me a slide for accountability?",
      "created_at": "2023-07-16T07:52:42.605173Z",
      "sequence_order": 4,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-07-16T07:52:42.605173Z",
            "stop_timestamp": "2023-07-16T07:52:42.605173Z",
            "flags": null,
            "type": "text",
            "text": "Can you give me a slide for accountability?",
            "citations": []
          }
        ],
        "updated_at": "2023-07-16T07:52:42.605173Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "93dc43b4-7675-451e-ab27-a21979f81f91",
      "role": "assistant",
      "content": "Here is a potential slide on accountability for generative AI:\n\nTitle: Ensure Accountable Generative AI\n\n- Document who is responsible for model development, training, deployment. \n\n- Assign accountability for monitoring risks and harms from model use.\n\n- Build explainability into models so users understand outputs.\n\n- Log model inputs and outputs to enable auditing. \n\n- Have processes to pause automated AI if harms detected.\n\n- Enable users to provide feedback on model quality.",
      "created_at": "2023-07-16T07:52:42.670152Z",
      "sequence_order": 5,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-07-16T07:52:42.670152Z",
            "stop_timestamp": "2023-07-16T07:52:42.670152Z",
            "flags": null,
            "type": "text",
            "text": "Here is a potential slide on accountability for generative AI:\n\nTitle: Ensure Accountable Generative AI\n\n- Document who is responsible for model development, training, deployment. \n\n- Assign accountability for monitoring risks and harms from model use.\n\n- Build explainability into models so users understand outputs.\n\n- Log model inputs and outputs to enable auditing. \n\n- Have processes to pause automated AI if harms detected.\n\n- Enable users to provide feedback on model quality.",
            "citations": []
          }
        ],
        "updated_at": "2023-07-16T07:52:42.670152Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "35246d0e-a287-4362-b61e-9f8e40e8e305",
      "role": "human",
      "content": "Slide for fairness",
      "created_at": "2023-07-16T07:54:51.051872Z",
      "sequence_order": 6,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-07-16T07:54:51.051872Z",
            "stop_timestamp": "2023-07-16T07:54:51.051872Z",
            "flags": null,
            "type": "text",
            "text": "Slide for fairness",
            "citations": []
          }
        ],
        "updated_at": "2023-07-16T07:54:51.051872Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "dc35ffc3-f494-49c5-b1bd-833e0c7653f2",
      "role": "assistant",
      "content": "Here is a slide focused on fairness in generative AI:\n\nTitle: Ensure Fair Generative AI \n\n- Test model performance across diverse user groups.\n\n- Mitigate bias in training data through techniques like data augmentation.\n\n- Leverage techniques like adversarial debiasing during model training.\n\n- Deploy safeguards against toxic, biased or inappropriate language output.\n\n- Enable opt-out for users who don't want their data used.\n\n- Audit model outputs regularly for fairness issues.",
      "created_at": "2023-07-16T07:54:51.116873Z",
      "sequence_order": 7,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-07-16T07:54:51.116873Z",
            "stop_timestamp": "2023-07-16T07:54:51.116873Z",
            "flags": null,
            "type": "text",
            "text": "Here is a slide focused on fairness in generative AI:\n\nTitle: Ensure Fair Generative AI \n\n- Test model performance across diverse user groups.\n\n- Mitigate bias in training data through techniques like data augmentation.\n\n- Leverage techniques like adversarial debiasing during model training.\n\n- Deploy safeguards against toxic, biased or inappropriate language output.\n\n- Enable opt-out for users who don't want their data used.\n\n- Audit model outputs regularly for fairness issues.",
            "citations": []
          }
        ],
        "updated_at": "2023-07-16T07:54:51.116873Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "45b93364-735d-4e23-865e-6b3ebda5a5f7",
      "role": "human",
      "content": "slide for transparency",
      "created_at": "2023-07-16T07:59:29.091413Z",
      "sequence_order": 8,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-07-16T07:59:29.091413Z",
            "stop_timestamp": "2023-07-16T07:59:29.091413Z",
            "flags": null,
            "type": "text",
            "text": "slide for transparency",
            "citations": []
          }
        ],
        "updated_at": "2023-07-16T07:59:29.091413Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "fad81330-9a0f-4afb-a75a-05df81debe2f",
      "role": "assistant",
      "content": "Here is a potential slide on transparency for generative AI:\n\nTitle: Ensure Transparent Generative AI\n\n- Provide explanations of model outputs to end users. \n\n- Document data sources and other inputs used to train models.\n\n- Disclose limitations and uncertainties of model capabilities.\n\n- Enable inspection of training datasets and model parameters. \n\n- Log model performance data like accuracy metrics over time.\n\n- Label AI content clearly so users know it is machine-generated.",
      "created_at": "2023-07-16T07:59:29.201000Z",
      "sequence_order": 9,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-07-16T07:59:29.201000Z",
            "stop_timestamp": "2023-07-16T07:59:29.201000Z",
            "flags": null,
            "type": "text",
            "text": "Here is a potential slide on transparency for generative AI:\n\nTitle: Ensure Transparent Generative AI\n\n- Provide explanations of model outputs to end users. \n\n- Document data sources and other inputs used to train models.\n\n- Disclose limitations and uncertainties of model capabilities.\n\n- Enable inspection of training datasets and model parameters. \n\n- Log model performance data like accuracy metrics over time.\n\n- Label AI content clearly so users know it is machine-generated.",
            "citations": []
          }
        ],
        "updated_at": "2023-07-16T07:59:29.201000Z",
        "attachments": [],
        "files": []
      }
    }
  ]
}