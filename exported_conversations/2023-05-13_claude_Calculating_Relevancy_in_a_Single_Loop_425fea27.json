{
  "id": "425fea27-0d7e-480d-a85d-21a1b74155bc",
  "source": "claude",
  "title": "Calculating Relevancy in a Single Loop",
  "created_at": "2023-05-13T10:06:23.453643Z",
  "updated_at": "2023-05-14T09:28:25.328630Z",
  "summary": "",
  "message_count": 62,
  "messages": [
    {
      "id": "4073458e-1c60-44e8-989f-992eb6ad79ad",
      "role": "human",
      "content": "why does the code keep calculating relevancy in a long loop over all the agents multiple times. Shouldn't it just do it once after\n\nafter a single message from the user? come up with 3 reasons and pick the most likely to fix:\n\nparliament.py\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\ndef calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = None\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nif relevancy\\_score is None:\n\nprint(\"Maximum number of attempts reached. Unable to retrieve a valid relevancy score.\")\n\nrelevancy\\_score = 0 # Return 0 if no valid score obtained after max attempts\n\nelse:\n\nprint(f\"Relevancy score for {self.name}: {relevancy\\_score}%\")\n\nreturn relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def try\\_to\\_speak(self):\n\nif self.should\\_speak() and not self.is\\_speaking:\n\nawait self.speak()\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nawait self.try\\_to\\_speak()\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.try\\_to\\_speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nsorted\\_agents = self.get\\_sorted\\_agents\\_based\\_on\\_relevancy()\n\nfor agent in sorted\\_agents\\[:3\\]: # Add the N most relevant agents\n\nself.speaking\\_queue.add(agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nfor agent in self.agents.values():\n\nif agent.tagged and not agent.is\\_speaking:\n\nself.add\\_to\\_speaking\\_queue(agent)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nself.trigger\\_next\\_speaker()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\": agent.calculate\\_relevancy(self.chat\\_history)\n\n})\n\nreturn agents\\_info\n\nthe server code is server.py:\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = self.parliament.get\\_agents\\_info() # Assuming that this method returns agents' information\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nawait self.update\\_queue(websocket)\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue.to\\_json()}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue.to\\_json()}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()\n\nhere's the chat.js\n\nimport styles from './Chat.module.css';\n\nimport { debounce } from 'lodash';\n\nimport { useState, useEffect, useRef, useCallback } from 'react';\n\nimport { ListGroup, Form, Button, Spinner, ListGroupItem } from 'react-bootstrap';\n\nimport { FaTimes, FaReply } from 'react-icons/fa'; // Importing cross and reply icons\n\nimport { MentionsInput, Mention } from 'react-mentions';\n\nfunction Message({ message, sender, handleReply }) {\n\nreturn (\n\n<ListGroup.Item className={styles.messageItem}>\n\n<div className={styles.flexCenter}>\n\n<strong>{sender}:</strong>\n\n<Button\n\nvariant=\"link\"\n\nstyle={{ padding: 0, color: '#007bff', textDecoration: 'none' }}\n\nonClick={() => handleReply(sender)}\n\n\\>\n\n<FaReply />\n\n</Button>\n\n</div>\n\n{message.text}\n\n</ListGroup.Item>\n\n);\n\n}\n\nfunction AgentItem({ agent, handleClear, speakingQueue, handleReply }) {\n\nconst queueNumber = speakingQueue.indexOf(agent.name);\n\nreturn (\n\n<ListGroup.Item className={styles.agentItem}>\n\n<div>\n\n{agent.name}\n\n<span style={{ marginLeft: '5px' }}>{agent.relevancy}%</span> {/\\* Add this line \\*/}\n\n</div>\n\n{queueNumber !== -1 && <span> Queue: {queueNumber + 1}</span>}\n\n<FaReply\n\nstyle={{ color: 'blue', cursor: 'pointer', marginRight: '5px' }}\n\nonClick={() => handleReply(agent.name)}\n\n/>\n\n<FaTimes style={{ color: 'red', cursor: 'pointer' }} onClick={() => handleClear(agent.name)} />\n\n</ListGroup.Item>\n\n);\n\n}\n\nfunction Chat({ parliament, websocket }) {\n\nconst \\[messages, setMessages\\] = useState(\\[\\]);\n\nconst \\[messageInput, setMessageInput\\] = useState('');\n\nconst \\[isSending, setIsSending\\] = useState(false);\n\nconst \\[agents, setAgents\\] = useState(parliament.agents);\n\nconst \\[speakingQueue, setSpeakingQueue\\] = useState(\\[\\]); // Add this line\n\nconst chatContainer = useRef(null);\n\nconst \\[isTyping, setIsTyping\\] = useState(false);\n\nconst \\[typingTimeoutId, setTypingTimeoutId\\] = useState(null);\n\nconst debouncedSendTypingStatus = useCallback(debounce(sendTypingStatus, 300), \\[sendTypingStatus\\]);\n\nconst defaultStyle = {\n\nfontWeight: 'bold',\n\ncolor: 'black',\n\nfontSize: '16px',\n\npadding: '10px',\n\nborderRadius: '4px',\n\nborder: '1px solid #ccc',\n\nwidth: '100%',\n\nboxSizing: 'border-box',\n\noutline: 'none',\n\nresize: 'none',\n\n};\n\nfunction handleError(error) {\n\nconsole.error(\"Websocket error: \", error);\n\n}\n\nuseEffect(() => {\n\nif (websocket) {\n\nwebsocket.onmessage = handleReceiveMessage;\n\nwebsocket.onerror = handleError;\n\n}\n\nsetInitialMessages();\n\n}, \\[parliament, websocket\\]);\n\nuseEffect(() => {\n\nsendTypingStatus();\n\n}, \\[isTyping\\]);\n\nuseEffect(() => {\n\nchatContainer.current.scrollTop = chatContainer.current.scrollHeight;\n\n}, \\[messages\\]);\n\nfunction setInitialMessages() {\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages,\n\n{\n\nsender: 'System',\n\ntext: `Parliament is in session, the topic of discussion is: ${parliament.topic}`,\n\n},\n\n{\n\nsender: 'System',\n\ntext: \\`The parliament members are ${parliament.agents\n\n.map((agent) => agent.name)\n\n.join(', ')}, and ${parliament.username}\\`,\n\n},\n\n\\]);\n\n}\n\nfunction handleReceiveMessage(event) {\n\nconst data = JSON.parse(event.data);\n\nconst { action, message, sender, updated\\_agents, queue } = data;\n\nif (action === \"receive\\_message\") {\n\nconsole.log(\"data received\")\n\nconsole.log(data)\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== parliament.username || message.text !== \"typing...\"),\n\n{ sender, text: message },\n\n\\]);\n\n} else if (action === \"typing\" && sender !== parliament.username) {\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n\n{ sender, text: \"typing...\" },\n\n\\]);\n\n} else if (action === \"update\\_queue\") {\n\nsetSpeakingQueue(queue);\n\n}\n\nif (updated\\_agents) {\n\nsetAgents(updated\\_agents);\n\n}\n\n}\n\nfunction handleClear(agentName) {\n\n// Send a clear event to the server\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'clear\\_agent',\n\nagent\\_name: agentName,\n\n}),\n\n);\n\n}\n\nfunction sendTypingStatus() {\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'typing',\n\nsender: parliament.username,\n\nis\\_typing: isTyping,\n\n}),\n\n);\n\n}\n\nasync function sendMessage() {\n\nif (!messageInput.trim() || isSending) return;\n\nsetIsSending(true);\n\ntry {\n\nconsole.log(\\`\\[${parliament.username}\\] Sending message to server...\\`);\n\nconsole.log(messageInput.slice(0, 50) + '...');\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'send\\_message',\n\nsender: parliament.username,\n\nmessage: messageInput,\n\n}),\n\n);\n\nconsole.log(\\`\\[${parliament.username}\\] Message sent!\\`);\n\nsetMessageInput('');\n\nsetIsTyping(false);\n\n// Call broadcast\\_message here\n\nconsole.log(\\`\\[${parliament.username}\\] Sending broadcast message to server...\\`);\n\nconsole.log(messageInput.slice(0, 50) + '...');\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'broadcast\\_message',\n\nmessage: messageInput,\n\nsender: parliament.username,\n\n}),\n\n);\n\nconsole.log(\\`\\[${parliament.username}\\] Broadcast message sent!\\`);\n\n} catch (error) {\n\nconsole.log(\\`\\[${parliament.username}\\] Failed to send message:\\`, error);\n\n} finally {\n\nsetIsSending(false);\n\n}\n\n}\n\nfunction handleReply(sender) {\n\nsetMessageInput(\\`@${sender} \\`);\n\n}\n\nconst agentNames = agents.map(agent => ({ id: agent.name, display: agent.name }));\n\nreturn (\n\n<div style={{ display: 'flex', gap: '1em' }}>\n\n<div style={{ width: '70%' }}>\n\n<h1>Parliament Chat</h1>\n\n<ListGroup\n\nref={chatContainer}\n\nstyle={{ height: '400px', overflowY: 'scroll' }}>\n\n{messages.map((message, index) => (\n\n<Message\n\nkey={index}\n\nmessage={message}\n\nsender={message.sender}\n\nhandleReply={handleReply}\n\nuser={parliament.username}\n\n/>\n\n))}\n\n</ListGroup>\n\n<Form>\n\n<Form.Group controlId=\"messageInput\">\n\n<MentionsInput\n\nautoFocus\n\nvalue={messageInput}\n\nonChange={(event, value, plainTextValue) => {\n\nsetMessageInput(plainTextValue);\n\nsetIsTyping(true);\n\ndebouncedSendTypingStatus();\n\nclearTimeout(typingTimeoutId);\n\nsetTypingTimeoutId(setTimeout(() => setIsTyping(false), 2000));\n\n}}\n\nonKeyPress={(e) => {\n\nif (e.key === 'Enter') {\n\nsendMessage();\n\ne.preventDefault();\n\n}\n\n}}\n\nplaceholder=\"Type your message...\"\n\nstyle={defaultStyle} // styles for the input field\n\n\\>\n\n<Mention\n\ntrigger=\"@\"\n\ndata={agentNames}\n\ndisplayTransform={(id, display) => `@${display}`}\n\nclassName={styles.mentionStyle} // use your CSS module to style the mentions\n\n/>\n\n</MentionsInput>\n\n</Form.Group>\n\n</Form>\n\n<Button onClick={sendMessage} disabled={isSending}>\n\n{isSending ? <Spinner animation=\"border\" size=\"sm\" /> : 'Send Message'}\n\n</Button>\n\n</div>\n\n<div style={{ width: '20%', marginTop: '25px' }}> {/\\* You can adjust the marginTop as needed \\*/}\n\n<h2 style={{ fontSize: '1em' }}>Agents</h2>\n\n<ListGroup>\n\n{agents\n\n.sort((a, b) => {\n\nconst aQueueNumber = speakingQueue.indexOf(a.name);\n\nconst bQueueNumber = speakingQueue.indexOf(b.name);\n\n// If both agents are in the queue, compare their positions\n\nif (aQueueNumber !== -1 && bQueueNumber !== -1) {\n\nreturn aQueueNumber - bQueueNumber;\n\n}\n\n// If only one of the agents is in the queue, that agent comes first\n\nif (aQueueNumber !== -1) {\n\nreturn -1;\n\n}\n\nif (bQueueNumber !== -1) {\n\nreturn 1;\n\n}\n\n// If neither agent is in the queue, compare their names alphabetically\n\nreturn a.name.localeCompare(b.name);\n\n})\n\n.map((agent, index) => (\n\n<AgentItem\n\nkey={index}\n\nagent={agent}\n\nhandleClear={handleClear}\n\nspeakingQueue={speakingQueue}\n\nhandleReply={handleReply} // Add this line\n\n/>\n\n))}\n\n</ListGroup>\n\n</div>\n\n</div>\n\n);\n\n}\n\nexport default Chat;",
      "created_at": "2023-05-13T10:07:05.680223Z",
      "sequence_order": 0,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:07:05.680223Z",
            "stop_timestamp": "2023-05-13T10:07:05.680223Z",
            "flags": null,
            "type": "text",
            "text": "why does the code keep calculating relevancy in a long loop over all the agents multiple times. Shouldn't it just do it once after\n\nafter a single message from the user? come up with 3 reasons and pick the most likely to fix:\n\nparliament.py\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\ndef calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = None\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nif relevancy\\_score is None:\n\nprint(\"Maximum number of attempts reached. Unable to retrieve a valid relevancy score.\")\n\nrelevancy\\_score = 0 # Return 0 if no valid score obtained after max attempts\n\nelse:\n\nprint(f\"Relevancy score for {self.name}: {relevancy\\_score}%\")\n\nreturn relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def try\\_to\\_speak(self):\n\nif self.should\\_speak() and not self.is\\_speaking:\n\nawait self.speak()\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nawait self.try\\_to\\_speak()\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.try\\_to\\_speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nsorted\\_agents = self.get\\_sorted\\_agents\\_based\\_on\\_relevancy()\n\nfor agent in sorted\\_agents\\[:3\\]: # Add the N most relevant agents\n\nself.speaking\\_queue.add(agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nfor agent in self.agents.values():\n\nif agent.tagged and not agent.is\\_speaking:\n\nself.add\\_to\\_speaking\\_queue(agent)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nself.trigger\\_next\\_speaker()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\": agent.calculate\\_relevancy(self.chat\\_history)\n\n})\n\nreturn agents\\_info\n\nthe server code is server.py:\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = self.parliament.get\\_agents\\_info() # Assuming that this method returns agents' information\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nawait self.update\\_queue(websocket)\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue.to\\_json()}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue.to\\_json()}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()\n\nhere's the chat.js\n\nimport styles from './Chat.module.css';\n\nimport { debounce } from 'lodash';\n\nimport { useState, useEffect, useRef, useCallback } from 'react';\n\nimport { ListGroup, Form, Button, Spinner, ListGroupItem } from 'react-bootstrap';\n\nimport { FaTimes, FaReply } from 'react-icons/fa'; // Importing cross and reply icons\n\nimport { MentionsInput, Mention } from 'react-mentions';\n\nfunction Message({ message, sender, handleReply }) {\n\nreturn (\n\n<ListGroup.Item className={styles.messageItem}>\n\n<div className={styles.flexCenter}>\n\n<strong>{sender}:</strong>\n\n<Button\n\nvariant=\"link\"\n\nstyle={{ padding: 0, color: '#007bff', textDecoration: 'none' }}\n\nonClick={() => handleReply(sender)}\n\n\\>\n\n<FaReply />\n\n</Button>\n\n</div>\n\n{message.text}\n\n</ListGroup.Item>\n\n);\n\n}\n\nfunction AgentItem({ agent, handleClear, speakingQueue, handleReply }) {\n\nconst queueNumber = speakingQueue.indexOf(agent.name);\n\nreturn (\n\n<ListGroup.Item className={styles.agentItem}>\n\n<div>\n\n{agent.name}\n\n<span style={{ marginLeft: '5px' }}>{agent.relevancy}%</span> {/\\* Add this line \\*/}\n\n</div>\n\n{queueNumber !== -1 && <span> Queue: {queueNumber + 1}</span>}\n\n<FaReply\n\nstyle={{ color: 'blue', cursor: 'pointer', marginRight: '5px' }}\n\nonClick={() => handleReply(agent.name)}\n\n/>\n\n<FaTimes style={{ color: 'red', cursor: 'pointer' }} onClick={() => handleClear(agent.name)} />\n\n</ListGroup.Item>\n\n);\n\n}\n\nfunction Chat({ parliament, websocket }) {\n\nconst \\[messages, setMessages\\] = useState(\\[\\]);\n\nconst \\[messageInput, setMessageInput\\] = useState('');\n\nconst \\[isSending, setIsSending\\] = useState(false);\n\nconst \\[agents, setAgents\\] = useState(parliament.agents);\n\nconst \\[speakingQueue, setSpeakingQueue\\] = useState(\\[\\]); // Add this line\n\nconst chatContainer = useRef(null);\n\nconst \\[isTyping, setIsTyping\\] = useState(false);\n\nconst \\[typingTimeoutId, setTypingTimeoutId\\] = useState(null);\n\nconst debouncedSendTypingStatus = useCallback(debounce(sendTypingStatus, 300), \\[sendTypingStatus\\]);\n\nconst defaultStyle = {\n\nfontWeight: 'bold',\n\ncolor: 'black',\n\nfontSize: '16px',\n\npadding: '10px',\n\nborderRadius: '4px',\n\nborder: '1px solid #ccc',\n\nwidth: '100%',\n\nboxSizing: 'border-box',\n\noutline: 'none',\n\nresize: 'none',\n\n};\n\nfunction handleError(error) {\n\nconsole.error(\"Websocket error: \", error);\n\n}\n\nuseEffect(() => {\n\nif (websocket) {\n\nwebsocket.onmessage = handleReceiveMessage;\n\nwebsocket.onerror = handleError;\n\n}\n\nsetInitialMessages();\n\n}, \\[parliament, websocket\\]);\n\nuseEffect(() => {\n\nsendTypingStatus();\n\n}, \\[isTyping\\]);\n\nuseEffect(() => {\n\nchatContainer.current.scrollTop = chatContainer.current.scrollHeight;\n\n}, \\[messages\\]);\n\nfunction setInitialMessages() {\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages,\n\n{\n\nsender: 'System',\n\ntext: `Parliament is in session, the topic of discussion is: ${parliament.topic}`,\n\n},\n\n{\n\nsender: 'System',\n\ntext: \\`The parliament members are ${parliament.agents\n\n.map((agent) => agent.name)\n\n.join(', ')}, and ${parliament.username}\\`,\n\n},\n\n\\]);\n\n}\n\nfunction handleReceiveMessage(event) {\n\nconst data = JSON.parse(event.data);\n\nconst { action, message, sender, updated\\_agents, queue } = data;\n\nif (action === \"receive\\_message\") {\n\nconsole.log(\"data received\")\n\nconsole.log(data)\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== parliament.username || message.text !== \"typing...\"),\n\n{ sender, text: message },\n\n\\]);\n\n} else if (action === \"typing\" && sender !== parliament.username) {\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n\n{ sender, text: \"typing...\" },\n\n\\]);\n\n} else if (action === \"update\\_queue\") {\n\nsetSpeakingQueue(queue);\n\n}\n\nif (updated\\_agents) {\n\nsetAgents(updated\\_agents);\n\n}\n\n}\n\nfunction handleClear(agentName) {\n\n// Send a clear event to the server\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'clear\\_agent',\n\nagent\\_name: agentName,\n\n}),\n\n);\n\n}\n\nfunction sendTypingStatus() {\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'typing',\n\nsender: parliament.username,\n\nis\\_typing: isTyping,\n\n}),\n\n);\n\n}\n\nasync function sendMessage() {\n\nif (!messageInput.trim() || isSending) return;\n\nsetIsSending(true);\n\ntry {\n\nconsole.log(\\`\\[${parliament.username}\\] Sending message to server...\\`);\n\nconsole.log(messageInput.slice(0, 50) + '...');\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'send\\_message',\n\nsender: parliament.username,\n\nmessage: messageInput,\n\n}),\n\n);\n\nconsole.log(\\`\\[${parliament.username}\\] Message sent!\\`);\n\nsetMessageInput('');\n\nsetIsTyping(false);\n\n// Call broadcast\\_message here\n\nconsole.log(\\`\\[${parliament.username}\\] Sending broadcast message to server...\\`);\n\nconsole.log(messageInput.slice(0, 50) + '...');\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'broadcast\\_message',\n\nmessage: messageInput,\n\nsender: parliament.username,\n\n}),\n\n);\n\nconsole.log(\\`\\[${parliament.username}\\] Broadcast message sent!\\`);\n\n} catch (error) {\n\nconsole.log(\\`\\[${parliament.username}\\] Failed to send message:\\`, error);\n\n} finally {\n\nsetIsSending(false);\n\n}\n\n}\n\nfunction handleReply(sender) {\n\nsetMessageInput(\\`@${sender} \\`);\n\n}\n\nconst agentNames = agents.map(agent => ({ id: agent.name, display: agent.name }));\n\nreturn (\n\n<div style={{ display: 'flex', gap: '1em' }}>\n\n<div style={{ width: '70%' }}>\n\n<h1>Parliament Chat</h1>\n\n<ListGroup\n\nref={chatContainer}\n\nstyle={{ height: '400px', overflowY: 'scroll' }}>\n\n{messages.map((message, index) => (\n\n<Message\n\nkey={index}\n\nmessage={message}\n\nsender={message.sender}\n\nhandleReply={handleReply}\n\nuser={parliament.username}\n\n/>\n\n))}\n\n</ListGroup>\n\n<Form>\n\n<Form.Group controlId=\"messageInput\">\n\n<MentionsInput\n\nautoFocus\n\nvalue={messageInput}\n\nonChange={(event, value, plainTextValue) => {\n\nsetMessageInput(plainTextValue);\n\nsetIsTyping(true);\n\ndebouncedSendTypingStatus();\n\nclearTimeout(typingTimeoutId);\n\nsetTypingTimeoutId(setTimeout(() => setIsTyping(false), 2000));\n\n}}\n\nonKeyPress={(e) => {\n\nif (e.key === 'Enter') {\n\nsendMessage();\n\ne.preventDefault();\n\n}\n\n}}\n\nplaceholder=\"Type your message...\"\n\nstyle={defaultStyle} // styles for the input field\n\n\\>\n\n<Mention\n\ntrigger=\"@\"\n\ndata={agentNames}\n\ndisplayTransform={(id, display) => `@${display}`}\n\nclassName={styles.mentionStyle} // use your CSS module to style the mentions\n\n/>\n\n</MentionsInput>\n\n</Form.Group>\n\n</Form>\n\n<Button onClick={sendMessage} disabled={isSending}>\n\n{isSending ? <Spinner animation=\"border\" size=\"sm\" /> : 'Send Message'}\n\n</Button>\n\n</div>\n\n<div style={{ width: '20%', marginTop: '25px' }}> {/\\* You can adjust the marginTop as needed \\*/}\n\n<h2 style={{ fontSize: '1em' }}>Agents</h2>\n\n<ListGroup>\n\n{agents\n\n.sort((a, b) => {\n\nconst aQueueNumber = speakingQueue.indexOf(a.name);\n\nconst bQueueNumber = speakingQueue.indexOf(b.name);\n\n// If both agents are in the queue, compare their positions\n\nif (aQueueNumber !== -1 && bQueueNumber !== -1) {\n\nreturn aQueueNumber - bQueueNumber;\n\n}\n\n// If only one of the agents is in the queue, that agent comes first\n\nif (aQueueNumber !== -1) {\n\nreturn -1;\n\n}\n\nif (bQueueNumber !== -1) {\n\nreturn 1;\n\n}\n\n// If neither agent is in the queue, compare their names alphabetically\n\nreturn a.name.localeCompare(b.name);\n\n})\n\n.map((agent, index) => (\n\n<AgentItem\n\nkey={index}\n\nagent={agent}\n\nhandleClear={handleClear}\n\nspeakingQueue={speakingQueue}\n\nhandleReply={handleReply} // Add this line\n\n/>\n\n))}\n\n</ListGroup>\n\n</div>\n\n</div>\n\n);\n\n}\n\nexport default Chat;",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:07:05.680223Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "57a4f8e4-2f95-4b60-b68b-8b9e209fb3c5",
      "role": "assistant",
      "content": " There are a few likely reasons the code keeps calculating relevancy in a loop:\n\n1. To keep the speaking queue updated with the most relevant agents: The code is calculating relevancy for all agents and adding the most relevant ones to the speaking queue. This ensures the queue stays up to date with the agents most relevant to the current conversation.\n\n2. To handle mentions: The code may be recalculating relevancy after a mention to add the mentioned agent(s) to the front of the speaking queue. \n\n3. For debugging/logging purposes: The code may be printing the relevancy scores for all agents to the console for debugging, requiring it to recalculate for each agent.\n\nOf these options, the most likely reasons are:\n\n1. To keep the speaking queue updated: This is a core part of the system's functionality, to determine who speaks next based on relevancy. Recalculating periodically is important for this.\n\n2. To handle mentions: Mentions directly impact the speaking queue by bringing the mentioned agent to the front. Recalculating relevancy for all agents after a mention makes sense in this case.\n\nOption 3, for debugging/logging, is less likely to be a primary driver for the relevancy calculation loop. The logging can likely be made more efficient or removed in a production system.\n\nIn summary, the two most important reasons for frequently recalculating relevancy are:\n\n1. Updating the speaking queue with the most relevant agents \n2. Handling mentions by adjusting the queue \n\nThese drive the core logic of the system, while logging is more peripheral. I would consider optimizing or removing the logging, but keeping the queue updates and mention handling logic.",
      "created_at": "2023-05-13T10:07:34.323683Z",
      "sequence_order": 1,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:07:34.323683Z",
            "stop_timestamp": "2023-05-13T10:07:34.323683Z",
            "flags": null,
            "type": "text",
            "text": " There are a few likely reasons the code keeps calculating relevancy in a loop:\n\n1. To keep the speaking queue updated with the most relevant agents: The code is calculating relevancy for all agents and adding the most relevant ones to the speaking queue. This ensures the queue stays up to date with the agents most relevant to the current conversation.\n\n2. To handle mentions: The code may be recalculating relevancy after a mention to add the mentioned agent(s) to the front of the speaking queue. \n\n3. For debugging/logging purposes: The code may be printing the relevancy scores for all agents to the console for debugging, requiring it to recalculate for each agent.\n\nOf these options, the most likely reasons are:\n\n1. To keep the speaking queue updated: This is a core part of the system's functionality, to determine who speaks next based on relevancy. Recalculating periodically is important for this.\n\n2. To handle mentions: Mentions directly impact the speaking queue by bringing the mentioned agent to the front. Recalculating relevancy for all agents after a mention makes sense in this case.\n\nOption 3, for debugging/logging, is less likely to be a primary driver for the relevancy calculation loop. The logging can likely be made more efficient or removed in a production system.\n\nIn summary, the two most important reasons for frequently recalculating relevancy are:\n\n1. Updating the speaking queue with the most relevant agents \n2. Handling mentions by adjusting the queue \n\nThese drive the core logic of the system, while logging is more peripheral. I would consider optimizing or removing the logging, but keeping the queue updates and mention handling logic.",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:07:34.323683Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "ef010ca5-5415-46d6-b60d-a7538f50ce7c",
      "role": "human",
      "content": "we don't need to update relevancy after each mention, so let's remove that\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = None\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nif self.relevancy\\_score is None:\n\nprint(\"Maximum number of attempts reached. Unable to retrieve a valid relevancy score.\")\n\nself.relevancy\\_score = 0 # Return 0 if no valid score obtained after max attempts\n\nelse:\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nreturn self.relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def try\\_to\\_speak(self):\n\nif self.should\\_speak() and not self.is\\_speaking:\n\nawait self.speak()\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nawait self.try\\_to\\_speak()\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.try\\_to\\_speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nsorted\\_agents = self.get\\_sorted\\_agents\\_based\\_on\\_relevancy()\n\nfor agent in sorted\\_agents\\[:3\\]: # Add the N most relevant agents\n\nself.speaking\\_queue.add(agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nfor agent in self.agents.values():\n\nif agent.tagged and not agent.is\\_speaking:\n\nself.add\\_to\\_speaking\\_queue(agent)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nself.trigger\\_next\\_speaker()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\": agent.calculate\\_relevancy(self.chat\\_history)\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-13T10:14:21.901761Z",
      "sequence_order": 2,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:14:21.901761Z",
            "stop_timestamp": "2023-05-13T10:14:21.901761Z",
            "flags": null,
            "type": "text",
            "text": "we don't need to update relevancy after each mention, so let's remove that\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = None\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nif self.relevancy\\_score is None:\n\nprint(\"Maximum number of attempts reached. Unable to retrieve a valid relevancy score.\")\n\nself.relevancy\\_score = 0 # Return 0 if no valid score obtained after max attempts\n\nelse:\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nreturn self.relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def try\\_to\\_speak(self):\n\nif self.should\\_speak() and not self.is\\_speaking:\n\nawait self.speak()\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nawait self.try\\_to\\_speak()\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.try\\_to\\_speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nsorted\\_agents = self.get\\_sorted\\_agents\\_based\\_on\\_relevancy()\n\nfor agent in sorted\\_agents\\[:3\\]: # Add the N most relevant agents\n\nself.speaking\\_queue.add(agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nfor agent in self.agents.values():\n\nif agent.tagged and not agent.is\\_speaking:\n\nself.add\\_to\\_speaking\\_queue(agent)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nself.trigger\\_next\\_speaker()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\": agent.calculate\\_relevancy(self.chat\\_history)\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:14:21.901761Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "572d3583-73b9-495c-be7c-c860154d51c4",
      "role": "assistant",
      "content": " Here are the main changes I would make:\n\n1. Remove the call to calculate_relevancy() after a mention. Since we are handling mentions by directly adding the mentioned agent to the front of the speaking queue, we do not need to recalculate relevancy.\n\n2. Add a check in calculate_relevancy() to use a cached relevancy score if the context has not changed. This will prevent unnecessary calls to the OpenAI API. The logic would look something like this:\n\n```py\ndef calculate_relevancy(self, chat_context: str) -> int:\n    if self.relevancy_score is None or self.initial_context != chat_context:\n        # Calculate relevancy and cache the score...\n    else:\n        print(\"Using cached relevancy score...\")\n    return self.relevancy_score \n```\n\n3. Remove the call to get_sorted_agents_based_on_relevancy(). Since we are only adding the single most relevant agent to the queue, we do not need to sort all agents by relevancy.\n\n4. Consider setting a maximum relevancy_score, e.g. 100, to avoid unnecessarily high scores. Agents with the max score could be added to the queue, but their scores would not increase beyond 100.\n\n5. Add logging to indicate when a cached relevancy score is being used vs a new score is calculated. This can help debug any issues, and ensure scores are being cached properly.\n\nThe updated code would look something like this:\n```py\ndef calculate_relevancy(self, chat_context: str) -> int:\n    if self.relevancy_score is None or self.initial_context != chat_context:\n        print(\"Calculating relevancy...\")\n        # ...Calculate and cache relevancy score...\n        print(f\"Relevancy score for {self.name}: {self.relevancy_score}%\")\n    else:\n        print(\"Using cached relevancy score...\")\n    return min(self.relevancy_score, 100) # Cap maximum score at 100\n\ndef update_speaking_queue(self):\n    \"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n    for agent in self.agents.values():\n        chat_context = self.chat_history.truncated_chat_history\n        relevancy_score = agent.calculate_relevancy(chat_context)\n\n    # Add the most relevant agent to the speaking queue\n    most_relevant_agent = ...\n    if most_relevant_agent:\n        self.speaking_queue.enqueue(most_relevant_agent.name)\n```\n\nThis should help optimize the relevancy calculation logic and avoid unnecessary API calls. Let me know if you have any other questions!",
      "created_at": "2023-05-13T10:14:46.438958Z",
      "sequence_order": 3,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:14:46.438958Z",
            "stop_timestamp": "2023-05-13T10:14:46.438958Z",
            "flags": null,
            "type": "text",
            "text": " Here are the main changes I would make:\n\n1. Remove the call to calculate_relevancy() after a mention. Since we are handling mentions by directly adding the mentioned agent to the front of the speaking queue, we do not need to recalculate relevancy.\n\n2. Add a check in calculate_relevancy() to use a cached relevancy score if the context has not changed. This will prevent unnecessary calls to the OpenAI API. The logic would look something like this:\n\n```py\ndef calculate_relevancy(self, chat_context: str) -> int:\n    if self.relevancy_score is None or self.initial_context != chat_context:\n        # Calculate relevancy and cache the score...\n    else:\n        print(\"Using cached relevancy score...\")\n    return self.relevancy_score \n```\n\n3. Remove the call to get_sorted_agents_based_on_relevancy(). Since we are only adding the single most relevant agent to the queue, we do not need to sort all agents by relevancy.\n\n4. Consider setting a maximum relevancy_score, e.g. 100, to avoid unnecessarily high scores. Agents with the max score could be added to the queue, but their scores would not increase beyond 100.\n\n5. Add logging to indicate when a cached relevancy score is being used vs a new score is calculated. This can help debug any issues, and ensure scores are being cached properly.\n\nThe updated code would look something like this:\n```py\ndef calculate_relevancy(self, chat_context: str) -> int:\n    if self.relevancy_score is None or self.initial_context != chat_context:\n        print(\"Calculating relevancy...\")\n        # ...Calculate and cache relevancy score...\n        print(f\"Relevancy score for {self.name}: {self.relevancy_score}%\")\n    else:\n        print(\"Using cached relevancy score...\")\n    return min(self.relevancy_score, 100) # Cap maximum score at 100\n\ndef update_speaking_queue(self):\n    \"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n    for agent in self.agents.values():\n        chat_context = self.chat_history.truncated_chat_history\n        relevancy_score = agent.calculate_relevancy(chat_context)\n\n    # Add the most relevant agent to the speaking queue\n    most_relevant_agent = ...\n    if most_relevant_agent:\n        self.speaking_queue.enqueue(most_relevant_agent.name)\n```\n\nThis should help optimize the relevancy calculation logic and avoid unnecessary API calls. Let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:14:46.438958Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "cc4cfba5-1a13-4418-8c11-9f65aaa8fa0d",
      "role": "human",
      "content": "can you give me the update speaking queue updated?\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nsorted\\_agents = self.get\\_sorted\\_agents\\_based\\_on\\_relevancy()\n\nfor agent in sorted\\_agents\\[:3\\]: # Add the N most relevant agents\n\nself.speaking\\_queue.add(agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nfor agent in self.agents.values():\n\nif agent.tagged and not agent.is\\_speaking:\n\nself.add\\_to\\_speaking\\_queue(agent)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure",
      "created_at": "2023-05-13T10:18:42.414179Z",
      "sequence_order": 4,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:18:42.414179Z",
            "stop_timestamp": "2023-05-13T10:18:42.414179Z",
            "flags": null,
            "type": "text",
            "text": "can you give me the update speaking queue updated?\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nsorted\\_agents = self.get\\_sorted\\_agents\\_based\\_on\\_relevancy()\n\nfor agent in sorted\\_agents\\[:3\\]: # Add the N most relevant agents\n\nself.speaking\\_queue.add(agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nfor agent in self.agents.values():\n\nif agent.tagged and not agent.is\\_speaking:\n\nself.add\\_to\\_speaking\\_queue(agent)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:18:42.414179Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "768d852d-0818-419b-bfb4-1a2faae0c421",
      "role": "assistant",
      "content": " Here is the updated update_speaking_queue() method:\n\n```py\ndef update_speaking_queue(self):\n    \"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n    \n    most_relevant_agent = None\n    highest_relevancy_score = -1\n\n    for agent in self.agents.values():\n        chat_context = self.chat_history.truncated_chat_history\n        relevancy_score = agent.calculate_relevancy(chat_context) \n        if relevancy_score > highest_relevancy_score:\n            highest_relevancy_score = relevancy_score\n            most_relevant_agent = agent\n\n    # Add the most relevant agent to the speaking queue\n    if most_relevant_agent:\n        self.speaking_queue.enqueue(most_relevant_agent.name)\n\n    self.add_tagged_agents_to_queue()\n    self.add_high_pressure_agents_to_queue()\n```\n\nThe main changes are:\n\n1. Remove the call to get_sorted_agents_based_on_relevancy() since we only need the single most relevant agent.\n\n2. Move the logic to find the most relevant agent into its own loop. This avoids calling calculate_relevancy() multiple times for each agent.\n\n3. Only add the most relevant agent to the queue. The other top N agents are no longer added.\n\n4. Keep the existing logic to add tagged and high-pressure agents to the queue.\n\nThis optimized method will call calculate_relevancy() once for each agent to find the single most relevant agent, and add them to the front of the queue. The other agents are not directly added to the queue based on relevancy.\n\nLet me know if you have any other questions!",
      "created_at": "2023-05-13T10:19:04.001242Z",
      "sequence_order": 5,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:19:04.001242Z",
            "stop_timestamp": "2023-05-13T10:19:04.001242Z",
            "flags": null,
            "type": "text",
            "text": " Here is the updated update_speaking_queue() method:\n\n```py\ndef update_speaking_queue(self):\n    \"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n    \n    most_relevant_agent = None\n    highest_relevancy_score = -1\n\n    for agent in self.agents.values():\n        chat_context = self.chat_history.truncated_chat_history\n        relevancy_score = agent.calculate_relevancy(chat_context) \n        if relevancy_score > highest_relevancy_score:\n            highest_relevancy_score = relevancy_score\n            most_relevant_agent = agent\n\n    # Add the most relevant agent to the speaking queue\n    if most_relevant_agent:\n        self.speaking_queue.enqueue(most_relevant_agent.name)\n\n    self.add_tagged_agents_to_queue()\n    self.add_high_pressure_agents_to_queue()\n```\n\nThe main changes are:\n\n1. Remove the call to get_sorted_agents_based_on_relevancy() since we only need the single most relevant agent.\n\n2. Move the logic to find the most relevant agent into its own loop. This avoids calling calculate_relevancy() multiple times for each agent.\n\n3. Only add the most relevant agent to the queue. The other top N agents are no longer added.\n\n4. Keep the existing logic to add tagged and high-pressure agents to the queue.\n\nThis optimized method will call calculate_relevancy() once for each agent to find the single most relevant agent, and add them to the front of the queue. The other agents are not directly added to the queue based on relevancy.\n\nLet me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:19:04.001242Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "29f31e86-a88c-4415-a8c6-c212551fa2fa",
      "role": "human",
      "content": "do we not need to self.trigger\\_next\\_speaker() at the end?",
      "created_at": "2023-05-13T10:24:35.421985Z",
      "sequence_order": 6,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:24:35.421985Z",
            "stop_timestamp": "2023-05-13T10:24:35.421985Z",
            "flags": null,
            "type": "text",
            "text": "do we not need to self.trigger\\_next\\_speaker() at the end?",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:24:35.421985Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "85a57a7c-d3b4-4161-8c62-bc29095a3463",
      "role": "assistant",
      "content": " You're right, we need to call self.trigger_next_speaker() at the end of update_speaking_queue() to actually trigger the next speaker to talk. Without that call, the queue will be updated but no new speaker will be selected.\n\nThe full updated method should be:\n```py\ndef update_speaking_queue(self):\n    \"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n    \n    most_relevant_agent = None\n    highest_relevancy_score = -1\n\n    for agent in self.agents.values():\n        chat_context = self.chat_history.truncated_chat_history\n        relevancy_score = agent.calculate_relevancy(chat_context)  \n        if relevancy_score > highest_relevancy_score:\n            highest_relevancy_score = relevancy_score\n            most_relevant_agent = agent\n\n    # Add the most relevant agent to the speaking queue\n    if most_relevant_agent:\n        self.speaking_queue.enqueue(most_relevant_agent.name)\n\n    self.add_tagged_agents_to_queue()\n    self.add_high_pressure_agents_to_queue()\n    \n    self.trigger_next_speaker()  # Trigger the next speaker \n```\n\nAdding self.trigger_next_speaker() at the end will resolve the issue and properly select the next agent to speak after updating the queue.\n\nThe full method handles:\n\n1. Finding the most relevant agent and adding them to the front of the queue\n2. Adding any tagged or high-pressure agents to the queue\n3. Triggering the next speaker to actually talk based on the updated queue.\n\nLet me know if you have any other questions!",
      "created_at": "2023-05-13T10:25:02.160083Z",
      "sequence_order": 7,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:25:02.160083Z",
            "stop_timestamp": "2023-05-13T10:25:02.160083Z",
            "flags": null,
            "type": "text",
            "text": " You're right, we need to call self.trigger_next_speaker() at the end of update_speaking_queue() to actually trigger the next speaker to talk. Without that call, the queue will be updated but no new speaker will be selected.\n\nThe full updated method should be:\n```py\ndef update_speaking_queue(self):\n    \"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n    \n    most_relevant_agent = None\n    highest_relevancy_score = -1\n\n    for agent in self.agents.values():\n        chat_context = self.chat_history.truncated_chat_history\n        relevancy_score = agent.calculate_relevancy(chat_context)  \n        if relevancy_score > highest_relevancy_score:\n            highest_relevancy_score = relevancy_score\n            most_relevant_agent = agent\n\n    # Add the most relevant agent to the speaking queue\n    if most_relevant_agent:\n        self.speaking_queue.enqueue(most_relevant_agent.name)\n\n    self.add_tagged_agents_to_queue()\n    self.add_high_pressure_agents_to_queue()\n    \n    self.trigger_next_speaker()  # Trigger the next speaker \n```\n\nAdding self.trigger_next_speaker() at the end will resolve the issue and properly select the next agent to speak after updating the queue.\n\nThe full method handles:\n\n1. Finding the most relevant agent and adding them to the front of the queue\n2. Adding any tagged or high-pressure agents to the queue\n3. Triggering the next speaker to actually talk based on the updated queue.\n\nLet me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:25:02.160083Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "4ebbbd85-0d33-4ba5-a43c-c15a5807eee4",
      "role": "human",
      "content": "After an agent speaks, why doesn't their \"agent is typing\" go away from the chat list?\n\nChat.js:\n\nimport styles from './Chat.module.css';\n\nimport { debounce } from 'lodash';\n\nimport { useState, useEffect, useRef, useCallback } from 'react';\n\nimport { ListGroup, Form, Button, Spinner, ListGroupItem } from 'react-bootstrap';\n\nimport { FaTimes, FaReply } from 'react-icons/fa'; // Importing cross and reply icons\n\nimport { MentionsInput, Mention } from 'react-mentions';\n\nfunction Message({ message, sender, handleReply }) {\n\nreturn (\n\n<ListGroup.Item className={styles.messageItem}>\n\n<div className={styles.flexCenter}>\n\n<strong>{sender}:</strong>\n\n<Button\n\nvariant=\"link\"\n\nstyle={{ padding: 0, color: '#007bff', textDecoration: 'none' }}\n\nonClick={() => handleReply(sender)}\n\n\\>\n\n<FaReply />\n\n</Button>\n\n</div>\n\n{message.text}\n\n</ListGroup.Item>\n\n);\n\n}\n\nfunction AgentItem({ agent, handleClear, speakingQueue, handleReply }) {\n\nconst queueNumber = speakingQueue.indexOf(agent.name);\n\nreturn (\n\n<ListGroup.Item className={styles.agentItem}>\n\n<div>\n\n{agent.name}\n\n<span style={{ marginLeft: '5px' }}>{agent.relevancy}%</span> {/\\* Add this line \\*/}\n\n</div>\n\n{queueNumber !== -1 && <span> Queue: {queueNumber + 1}</span>}\n\n<FaReply\n\nstyle={{ color: 'blue', cursor: 'pointer', marginRight: '5px' }}\n\nonClick={() => handleReply(agent.name)}\n\n/>\n\n<FaTimes style={{ color: 'red', cursor: 'pointer' }} onClick={() => handleClear(agent.name)} />\n\n</ListGroup.Item>\n\n);\n\n}\n\nfunction Chat({ parliament, websocket }) {\n\nconst \\[messages, setMessages\\] = useState(\\[\\]);\n\nconst \\[messageInput, setMessageInput\\] = useState('');\n\nconst \\[isSending, setIsSending\\] = useState(false);\n\nconst \\[agents, setAgents\\] = useState(parliament.agents);\n\nconst \\[speakingQueue, setSpeakingQueue\\] = useState(\\[\\]); // Add this line\n\nconst chatContainer = useRef(null);\n\nconst \\[isTyping, setIsTyping\\] = useState(false);\n\nconst \\[typingTimeoutId, setTypingTimeoutId\\] = useState(null);\n\nconst debouncedSendTypingStatus = useCallback(debounce(sendTypingStatus, 300), \\[sendTypingStatus\\]);\n\nconst defaultStyle = {\n\nfontWeight: 'bold',\n\ncolor: 'black',\n\nfontSize: '16px',\n\npadding: '10px',\n\nborderRadius: '4px',\n\nborder: '1px solid #ccc',\n\nwidth: '100%',\n\nboxSizing: 'border-box',\n\noutline: 'none',\n\nresize: 'none',\n\n};\n\nfunction handleError(error) {\n\nconsole.error(\"Websocket error: \", error);\n\n}\n\nuseEffect(() => {\n\nif (websocket) {\n\nwebsocket.onmessage = handleReceiveMessage;\n\nwebsocket.onerror = handleError;\n\n}\n\nsetInitialMessages();\n\n}, \\[parliament, websocket\\]);\n\nuseEffect(() => {\n\nsendTypingStatus();\n\n}, \\[isTyping\\]);\n\nuseEffect(() => {\n\nchatContainer.current.scrollTop = chatContainer.current.scrollHeight;\n\n}, \\[messages\\]);\n\nfunction setInitialMessages() {\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages,\n\n{\n\nsender: 'System',\n\ntext: `Parliament is in session, the topic of discussion is: ${parliament.topic}`,\n\n},\n\n{\n\nsender: 'System',\n\ntext: \\`The parliament members are ${parliament.agents\n\n.map((agent) => agent.name)\n\n.join(', ')}, and ${parliament.username}\\`,\n\n},\n\n\\]);\n\n}\n\nfunction handleReceiveMessage(event) {\n\nconst data = JSON.parse(event.data);\n\nconst { action, message, sender, updated\\_agents, queue } = data;\n\nif (action === \"receive\\_message\") {\n\nconsole.log(\"data received\")\n\nconsole.log(data)\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== parliament.username || message.text !== \"typing...\"),\n\n{ sender, text: message },\n\n\\]);\n\n} else if (action === \"typing\" && sender !== parliament.username) {\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n\n{ sender, text: \"typing...\" },\n\n\\]);\n\n} else if (action === \"update\\_queue\") {\n\nsetSpeakingQueue(queue);\n\n}\n\nif (updated\\_agents) {\n\nsetAgents(updated\\_agents);\n\n}\n\n}\n\nfunction handleClear(agentName) {\n\n// Send a clear event to the server\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'clear\\_agent',\n\nagent\\_name: agentName,\n\n}),\n\n);\n\n}\n\nfunction sendTypingStatus() {\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'typing',\n\nsender: parliament.username,\n\nis\\_typing: isTyping,\n\n}),\n\n);\n\n}\n\nasync function sendMessage() {\n\nif (!messageInput.trim() || isSending) return;\n\nsetIsSending(true);\n\ntry {\n\nconsole.log(\\`\\[${parliament.username}\\] Sending message to server...\\`);\n\nconsole.log(messageInput.slice(0, 50) + '...');\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'send\\_message',\n\nsender: parliament.username,\n\nmessage: messageInput,\n\n}),\n\n);\n\nconsole.log(\\`\\[${parliament.username}\\] Message sent!\\`);\n\nsetMessageInput('');\n\nsetIsTyping(false);\n\n// Call broadcast\\_message here\n\nconsole.log(\\`\\[${parliament.username}\\] Sending broadcast message to server...\\`);\n\nconsole.log(messageInput.slice(0, 50) + '...');\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'broadcast\\_message',\n\nmessage: messageInput,\n\nsender: parliament.username,\n\n}),\n\n);\n\nconsole.log(\\`\\[${parliament.username}\\] Broadcast message sent!\\`);\n\n} catch (error) {\n\nconsole.log(\\`\\[${parliament.username}\\] Failed to send message:\\`, error);\n\n} finally {\n\nsetIsSending(false);\n\n}\n\n}\n\nfunction handleReply(sender) {\n\nsetMessageInput(\\`@${sender} \\`);\n\n}\n\nconst agentNames = agents.map(agent => ({ id: agent.name, display: agent.name }));\n\nreturn (\n\n<div style={{ display: 'flex', gap: '1em' }}>\n\n<div style={{ width: '70%' }}>\n\n<h1>Parliament Chat</h1>\n\n<ListGroup\n\nref={chatContainer}\n\nstyle={{ height: '400px', overflowY: 'scroll' }}>\n\n{messages.map((message, index) => (\n\n<Message\n\nkey={index}\n\nmessage={message}\n\nsender={message.sender}\n\nhandleReply={handleReply}\n\nuser={parliament.username}\n\n/>\n\n))}\n\n</ListGroup>\n\n<Form>\n\n<Form.Group controlId=\"messageInput\">\n\n<MentionsInput\n\nautoFocus\n\nvalue={messageInput}\n\nonChange={(event, value, plainTextValue) => {\n\nsetMessageInput(plainTextValue);\n\nsetIsTyping(true);\n\ndebouncedSendTypingStatus();\n\nclearTimeout(typingTimeoutId);\n\nsetTypingTimeoutId(setTimeout(() => setIsTyping(false), 2000));\n\n}}\n\nonKeyPress={(e) => {\n\nif (e.key === 'Enter') {\n\nsendMessage();\n\ne.preventDefault();\n\n}\n\n}}\n\nplaceholder=\"Type your message...\"\n\nstyle={defaultStyle} // styles for the input field\n\n\\>\n\n<Mention\n\ntrigger=\"@\"\n\ndata={agentNames}\n\ndisplayTransform={(id, display) => `@${display}`}\n\nclassName={styles.mentionStyle} // use your CSS module to style the mentions\n\n/>\n\n</MentionsInput>\n\n</Form.Group>\n\n</Form>\n\n<Button onClick={sendMessage} disabled={isSending}>\n\n{isSending ? <Spinner animation=\"border\" size=\"sm\" /> : 'Send Message'}\n\n</Button>\n\n</div>\n\n<div style={{ width: '20%', marginTop: '25px' }}> {/\\* You can adjust the marginTop as needed \\*/}\n\n<h2 style={{ fontSize: '1em' }}>Agents</h2>\n\n<ListGroup>\n\n{agents\n\n.sort((a, b) => {\n\nconst aQueueNumber = speakingQueue.indexOf(a.name);\n\nconst bQueueNumber = speakingQueue.indexOf(b.name);\n\n// If both agents are in the queue, compare their positions\n\nif (aQueueNumber !== -1 && bQueueNumber !== -1) {\n\nreturn aQueueNumber - bQueueNumber;\n\n}\n\n// If only one of the agents is in the queue, that agent comes first\n\nif (aQueueNumber !== -1) {\n\nreturn -1;\n\n}\n\nif (bQueueNumber !== -1) {\n\nreturn 1;\n\n}\n\n// If neither agent is in the queue, compare their names alphabetically\n\nreturn a.name.localeCompare(b.name);\n\n})\n\n.map((agent, index) => (\n\n<AgentItem\n\nkey={index}\n\nagent={agent}\n\nhandleClear={handleClear}\n\nspeakingQueue={speakingQueue}\n\nhandleReply={handleReply} // Add this line\n\n/>\n\n))}\n\n</ListGroup>\n\n</div>\n\n</div>\n\n);\n\n}\n\nexport default Chat;\n\nparliament.py:\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def try\\_to\\_speak(self):\n\nif self.should\\_speak() and not self.is\\_speaking:\n\nawait self.speak()\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nawait self.try\\_to\\_speak()\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.try\\_to\\_speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\": agent.calculate\\_relevancy(self.chat\\_history)\n\n})\n\nreturn agents\\_info\n\nServer.py:\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = self.parliament.get\\_agents\\_info() # Assuming that this method returns agents' information\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nawait self.update\\_queue(websocket)\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue.to\\_json()}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue.to\\_json()}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()",
      "created_at": "2023-05-13T10:31:01.721377Z",
      "sequence_order": 8,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:31:01.721377Z",
            "stop_timestamp": "2023-05-13T10:31:01.721377Z",
            "flags": null,
            "type": "text",
            "text": "After an agent speaks, why doesn't their \"agent is typing\" go away from the chat list?\n\nChat.js:\n\nimport styles from './Chat.module.css';\n\nimport { debounce } from 'lodash';\n\nimport { useState, useEffect, useRef, useCallback } from 'react';\n\nimport { ListGroup, Form, Button, Spinner, ListGroupItem } from 'react-bootstrap';\n\nimport { FaTimes, FaReply } from 'react-icons/fa'; // Importing cross and reply icons\n\nimport { MentionsInput, Mention } from 'react-mentions';\n\nfunction Message({ message, sender, handleReply }) {\n\nreturn (\n\n<ListGroup.Item className={styles.messageItem}>\n\n<div className={styles.flexCenter}>\n\n<strong>{sender}:</strong>\n\n<Button\n\nvariant=\"link\"\n\nstyle={{ padding: 0, color: '#007bff', textDecoration: 'none' }}\n\nonClick={() => handleReply(sender)}\n\n\\>\n\n<FaReply />\n\n</Button>\n\n</div>\n\n{message.text}\n\n</ListGroup.Item>\n\n);\n\n}\n\nfunction AgentItem({ agent, handleClear, speakingQueue, handleReply }) {\n\nconst queueNumber = speakingQueue.indexOf(agent.name);\n\nreturn (\n\n<ListGroup.Item className={styles.agentItem}>\n\n<div>\n\n{agent.name}\n\n<span style={{ marginLeft: '5px' }}>{agent.relevancy}%</span> {/\\* Add this line \\*/}\n\n</div>\n\n{queueNumber !== -1 && <span> Queue: {queueNumber + 1}</span>}\n\n<FaReply\n\nstyle={{ color: 'blue', cursor: 'pointer', marginRight: '5px' }}\n\nonClick={() => handleReply(agent.name)}\n\n/>\n\n<FaTimes style={{ color: 'red', cursor: 'pointer' }} onClick={() => handleClear(agent.name)} />\n\n</ListGroup.Item>\n\n);\n\n}\n\nfunction Chat({ parliament, websocket }) {\n\nconst \\[messages, setMessages\\] = useState(\\[\\]);\n\nconst \\[messageInput, setMessageInput\\] = useState('');\n\nconst \\[isSending, setIsSending\\] = useState(false);\n\nconst \\[agents, setAgents\\] = useState(parliament.agents);\n\nconst \\[speakingQueue, setSpeakingQueue\\] = useState(\\[\\]); // Add this line\n\nconst chatContainer = useRef(null);\n\nconst \\[isTyping, setIsTyping\\] = useState(false);\n\nconst \\[typingTimeoutId, setTypingTimeoutId\\] = useState(null);\n\nconst debouncedSendTypingStatus = useCallback(debounce(sendTypingStatus, 300), \\[sendTypingStatus\\]);\n\nconst defaultStyle = {\n\nfontWeight: 'bold',\n\ncolor: 'black',\n\nfontSize: '16px',\n\npadding: '10px',\n\nborderRadius: '4px',\n\nborder: '1px solid #ccc',\n\nwidth: '100%',\n\nboxSizing: 'border-box',\n\noutline: 'none',\n\nresize: 'none',\n\n};\n\nfunction handleError(error) {\n\nconsole.error(\"Websocket error: \", error);\n\n}\n\nuseEffect(() => {\n\nif (websocket) {\n\nwebsocket.onmessage = handleReceiveMessage;\n\nwebsocket.onerror = handleError;\n\n}\n\nsetInitialMessages();\n\n}, \\[parliament, websocket\\]);\n\nuseEffect(() => {\n\nsendTypingStatus();\n\n}, \\[isTyping\\]);\n\nuseEffect(() => {\n\nchatContainer.current.scrollTop = chatContainer.current.scrollHeight;\n\n}, \\[messages\\]);\n\nfunction setInitialMessages() {\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages,\n\n{\n\nsender: 'System',\n\ntext: `Parliament is in session, the topic of discussion is: ${parliament.topic}`,\n\n},\n\n{\n\nsender: 'System',\n\ntext: \\`The parliament members are ${parliament.agents\n\n.map((agent) => agent.name)\n\n.join(', ')}, and ${parliament.username}\\`,\n\n},\n\n\\]);\n\n}\n\nfunction handleReceiveMessage(event) {\n\nconst data = JSON.parse(event.data);\n\nconst { action, message, sender, updated\\_agents, queue } = data;\n\nif (action === \"receive\\_message\") {\n\nconsole.log(\"data received\")\n\nconsole.log(data)\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== parliament.username || message.text !== \"typing...\"),\n\n{ sender, text: message },\n\n\\]);\n\n} else if (action === \"typing\" && sender !== parliament.username) {\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n\n{ sender, text: \"typing...\" },\n\n\\]);\n\n} else if (action === \"update\\_queue\") {\n\nsetSpeakingQueue(queue);\n\n}\n\nif (updated\\_agents) {\n\nsetAgents(updated\\_agents);\n\n}\n\n}\n\nfunction handleClear(agentName) {\n\n// Send a clear event to the server\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'clear\\_agent',\n\nagent\\_name: agentName,\n\n}),\n\n);\n\n}\n\nfunction sendTypingStatus() {\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'typing',\n\nsender: parliament.username,\n\nis\\_typing: isTyping,\n\n}),\n\n);\n\n}\n\nasync function sendMessage() {\n\nif (!messageInput.trim() || isSending) return;\n\nsetIsSending(true);\n\ntry {\n\nconsole.log(\\`\\[${parliament.username}\\] Sending message to server...\\`);\n\nconsole.log(messageInput.slice(0, 50) + '...');\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'send\\_message',\n\nsender: parliament.username,\n\nmessage: messageInput,\n\n}),\n\n);\n\nconsole.log(\\`\\[${parliament.username}\\] Message sent!\\`);\n\nsetMessageInput('');\n\nsetIsTyping(false);\n\n// Call broadcast\\_message here\n\nconsole.log(\\`\\[${parliament.username}\\] Sending broadcast message to server...\\`);\n\nconsole.log(messageInput.slice(0, 50) + '...');\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'broadcast\\_message',\n\nmessage: messageInput,\n\nsender: parliament.username,\n\n}),\n\n);\n\nconsole.log(\\`\\[${parliament.username}\\] Broadcast message sent!\\`);\n\n} catch (error) {\n\nconsole.log(\\`\\[${parliament.username}\\] Failed to send message:\\`, error);\n\n} finally {\n\nsetIsSending(false);\n\n}\n\n}\n\nfunction handleReply(sender) {\n\nsetMessageInput(\\`@${sender} \\`);\n\n}\n\nconst agentNames = agents.map(agent => ({ id: agent.name, display: agent.name }));\n\nreturn (\n\n<div style={{ display: 'flex', gap: '1em' }}>\n\n<div style={{ width: '70%' }}>\n\n<h1>Parliament Chat</h1>\n\n<ListGroup\n\nref={chatContainer}\n\nstyle={{ height: '400px', overflowY: 'scroll' }}>\n\n{messages.map((message, index) => (\n\n<Message\n\nkey={index}\n\nmessage={message}\n\nsender={message.sender}\n\nhandleReply={handleReply}\n\nuser={parliament.username}\n\n/>\n\n))}\n\n</ListGroup>\n\n<Form>\n\n<Form.Group controlId=\"messageInput\">\n\n<MentionsInput\n\nautoFocus\n\nvalue={messageInput}\n\nonChange={(event, value, plainTextValue) => {\n\nsetMessageInput(plainTextValue);\n\nsetIsTyping(true);\n\ndebouncedSendTypingStatus();\n\nclearTimeout(typingTimeoutId);\n\nsetTypingTimeoutId(setTimeout(() => setIsTyping(false), 2000));\n\n}}\n\nonKeyPress={(e) => {\n\nif (e.key === 'Enter') {\n\nsendMessage();\n\ne.preventDefault();\n\n}\n\n}}\n\nplaceholder=\"Type your message...\"\n\nstyle={defaultStyle} // styles for the input field\n\n\\>\n\n<Mention\n\ntrigger=\"@\"\n\ndata={agentNames}\n\ndisplayTransform={(id, display) => `@${display}`}\n\nclassName={styles.mentionStyle} // use your CSS module to style the mentions\n\n/>\n\n</MentionsInput>\n\n</Form.Group>\n\n</Form>\n\n<Button onClick={sendMessage} disabled={isSending}>\n\n{isSending ? <Spinner animation=\"border\" size=\"sm\" /> : 'Send Message'}\n\n</Button>\n\n</div>\n\n<div style={{ width: '20%', marginTop: '25px' }}> {/\\* You can adjust the marginTop as needed \\*/}\n\n<h2 style={{ fontSize: '1em' }}>Agents</h2>\n\n<ListGroup>\n\n{agents\n\n.sort((a, b) => {\n\nconst aQueueNumber = speakingQueue.indexOf(a.name);\n\nconst bQueueNumber = speakingQueue.indexOf(b.name);\n\n// If both agents are in the queue, compare their positions\n\nif (aQueueNumber !== -1 && bQueueNumber !== -1) {\n\nreturn aQueueNumber - bQueueNumber;\n\n}\n\n// If only one of the agents is in the queue, that agent comes first\n\nif (aQueueNumber !== -1) {\n\nreturn -1;\n\n}\n\nif (bQueueNumber !== -1) {\n\nreturn 1;\n\n}\n\n// If neither agent is in the queue, compare their names alphabetically\n\nreturn a.name.localeCompare(b.name);\n\n})\n\n.map((agent, index) => (\n\n<AgentItem\n\nkey={index}\n\nagent={agent}\n\nhandleClear={handleClear}\n\nspeakingQueue={speakingQueue}\n\nhandleReply={handleReply} // Add this line\n\n/>\n\n))}\n\n</ListGroup>\n\n</div>\n\n</div>\n\n);\n\n}\n\nexport default Chat;\n\nparliament.py:\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def try\\_to\\_speak(self):\n\nif self.should\\_speak() and not self.is\\_speaking:\n\nawait self.speak()\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nawait self.try\\_to\\_speak()\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.try\\_to\\_speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\": agent.calculate\\_relevancy(self.chat\\_history)\n\n})\n\nreturn agents\\_info\n\nServer.py:\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = self.parliament.get\\_agents\\_info() # Assuming that this method returns agents' information\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nawait self.update\\_queue(websocket)\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue.to\\_json()}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue.to\\_json()}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:31:01.721377Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "daa3a8ef-70f5-4c15-bd25-1039d886e331",
      "role": "assistant",
      "content": " This is happening because the \"typing\" status is not being cleared after the agent finishes speaking. When an agent starts speaking, it sends a \"typing\" message to indicate it is typing. However, after it finishes speaking, it does not send a \"stopped_typing\" message.\n\nTo fix this, you need to add logic to clear the typing status. I would suggest the following changes:\n\n1. Add a STOPPED_TYPING constant to indicate an agent has stopped typing:\n\n```py\nSTOPPED_TYPING = \"stopped_typing\"\n```\n\n2. When an agent finishes speaking, send a stopped_typing message:\n\n```py\nasync def speak(self):\n    ...\n    await self.websocket.send(json.dumps({\"action\": STOPPED_TYPING, \"sender\": self.name}))\n```\n\n3. Handle the STOPPED_TYPING message in Chat.js to clear the typing status:\n\n```js\nfunction handleReceiveMessage(event) {\n  ...\n  } else if (action === \"stopped_typing\" && sender !== parliament.username) {\n    setMessages((prevMessages) => [\n      ...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n    ]);\n  }\n  ...\n} \n```\n\n4. Clear the typingTimeoutId when STOPPED_TYPING is received, to avoid the \"typing...\" message being cleared automatically after 2 seconds:\n\n```js \nfunction handleReceiveMessage(event) {\n  ...\n  } else if (action === \"stopped_typing\" && sender !== parliament.username) {\n    clearTimeout(typingTimeoutId); // Clear the timeout\n    setMessages((prevMessages) => [\n      ...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n    ]);\n  }\n...\n}\n```\n\nImplementing these changes should resolve the issue and clear the typing status after an agent speaks. Let me know if you have any other questions!",
      "created_at": "2023-05-13T10:31:27.571257Z",
      "sequence_order": 9,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:31:27.571257Z",
            "stop_timestamp": "2023-05-13T10:31:27.571257Z",
            "flags": null,
            "type": "text",
            "text": " This is happening because the \"typing\" status is not being cleared after the agent finishes speaking. When an agent starts speaking, it sends a \"typing\" message to indicate it is typing. However, after it finishes speaking, it does not send a \"stopped_typing\" message.\n\nTo fix this, you need to add logic to clear the typing status. I would suggest the following changes:\n\n1. Add a STOPPED_TYPING constant to indicate an agent has stopped typing:\n\n```py\nSTOPPED_TYPING = \"stopped_typing\"\n```\n\n2. When an agent finishes speaking, send a stopped_typing message:\n\n```py\nasync def speak(self):\n    ...\n    await self.websocket.send(json.dumps({\"action\": STOPPED_TYPING, \"sender\": self.name}))\n```\n\n3. Handle the STOPPED_TYPING message in Chat.js to clear the typing status:\n\n```js\nfunction handleReceiveMessage(event) {\n  ...\n  } else if (action === \"stopped_typing\" && sender !== parliament.username) {\n    setMessages((prevMessages) => [\n      ...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n    ]);\n  }\n  ...\n} \n```\n\n4. Clear the typingTimeoutId when STOPPED_TYPING is received, to avoid the \"typing...\" message being cleared automatically after 2 seconds:\n\n```js \nfunction handleReceiveMessage(event) {\n  ...\n  } else if (action === \"stopped_typing\" && sender !== parliament.username) {\n    clearTimeout(typingTimeoutId); // Clear the timeout\n    setMessages((prevMessages) => [\n      ...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n    ]);\n  }\n...\n}\n```\n\nImplementing these changes should resolve the issue and clear the typing status after an agent speaks. Let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:31:27.571257Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "479ae0f0-b3a1-43e7-94f8-838dd3011e59",
      "role": "human",
      "content": "what files do I need to do that in?",
      "created_at": "2023-05-13T10:32:47.918810Z",
      "sequence_order": 10,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:32:47.918810Z",
            "stop_timestamp": "2023-05-13T10:32:47.918810Z",
            "flags": null,
            "type": "text",
            "text": "what files do I need to do that in?",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:32:47.918810Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "5314d882-7170-480d-8028-fb7cb40d047f",
      "role": "assistant",
      "content": " To implement the changes I suggested, you will need to modify:\n\n1. parliament.py - To send the STOPPED_TYPING message after an agent speaks\n\n2. Chat.js - To handle the STOPPED_TYPING message and clear the typing status\n\nSpecifically, you will need to make the following modifications:\n\nIn parliament.py:\n\n- Add a STOPPED_TYPING constant:\n\n```py\nSTOPPED_TYPING = \"stopped_typing\"\n```\n\n- Send STOPPED_TYPING after an agent speaks:\n\n```py\nasync def speak(self):\n    ...\n    await self.websocket.send(json.dumps({\"action\": STOPPED_TYPING, \"sender\": self.name}))\n```\n\nIn Chat.js:\n\n- Handle STOPPED_TYPING and clear the \"typing...\" message:\n\n```js\nfunction handleReceiveMessage(event) {\n  ...\n  } else if (action === \"stopped_typing\" && sender !== parliament.username) {\n    setMessages((prevMessages) => [\n      ...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n    ]);\n  }\n  ...\n}  \n```\n\n- Clear the typingTimeoutId to avoid the message being cleared after 2 seconds:\n\n```js\nfunction handleReceiveMessage(event) {\n  ...\n  } else if (action === \"stopped_typing\" && sender !== parliament.username) {\n    clearTimeout(typingTimeoutId); \n    setMessages((prevMessages) => [\n      ...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n    ]);\n  }\n...\n}\n```\n\nThose are the only two files you need to modify to implement clearing the typing status. Let me know if you have any other questions!",
      "created_at": "2023-05-13T10:33:10.046283Z",
      "sequence_order": 11,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:33:10.046283Z",
            "stop_timestamp": "2023-05-13T10:33:10.046283Z",
            "flags": null,
            "type": "text",
            "text": " To implement the changes I suggested, you will need to modify:\n\n1. parliament.py - To send the STOPPED_TYPING message after an agent speaks\n\n2. Chat.js - To handle the STOPPED_TYPING message and clear the typing status\n\nSpecifically, you will need to make the following modifications:\n\nIn parliament.py:\n\n- Add a STOPPED_TYPING constant:\n\n```py\nSTOPPED_TYPING = \"stopped_typing\"\n```\n\n- Send STOPPED_TYPING after an agent speaks:\n\n```py\nasync def speak(self):\n    ...\n    await self.websocket.send(json.dumps({\"action\": STOPPED_TYPING, \"sender\": self.name}))\n```\n\nIn Chat.js:\n\n- Handle STOPPED_TYPING and clear the \"typing...\" message:\n\n```js\nfunction handleReceiveMessage(event) {\n  ...\n  } else if (action === \"stopped_typing\" && sender !== parliament.username) {\n    setMessages((prevMessages) => [\n      ...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n    ]);\n  }\n  ...\n}  \n```\n\n- Clear the typingTimeoutId to avoid the message being cleared after 2 seconds:\n\n```js\nfunction handleReceiveMessage(event) {\n  ...\n  } else if (action === \"stopped_typing\" && sender !== parliament.username) {\n    clearTimeout(typingTimeoutId); \n    setMessages((prevMessages) => [\n      ...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n    ]);\n  }\n...\n}\n```\n\nThose are the only two files you need to modify to implement clearing the typing status. Let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:33:10.046283Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "c712c746-4f30-4ce1-8b2f-dcb3cf89f6ad",
      "role": "human",
      "content": "I want my chat.js file to show the relevancy score, right now it just says %\n\nimport styles from './Chat.module.css';\n\nimport { debounce } from 'lodash';\n\nimport { useState, useEffect, useRef, useCallback } from 'react';\n\nimport { ListGroup, Form, Button, Spinner, ListGroupItem } from 'react-bootstrap';\n\nimport { FaTimes, FaReply } from 'react-icons/fa'; // Importing cross and reply icons\n\nimport { MentionsInput, Mention } from 'react-mentions';\n\nfunction Message({ message, sender, handleReply }) {\n\nreturn (\n\n<ListGroup.Item className={styles.messageItem}>\n\n<div className={styles.flexCenter}>\n\n<strong>{sender}:</strong>\n\n<Button\n\nvariant=\"link\"\n\nstyle={{ padding: 0, color: '#007bff', textDecoration: 'none' }}\n\nonClick={() => handleReply(sender)}\n\n\\>\n\n<FaReply />\n\n</Button>\n\n</div>\n\n{message.text}\n\n</ListGroup.Item>\n\n);\n\n}\n\nfunction AgentItem({ agent, handleClear, speakingQueue, handleReply }) {\n\nconst queueNumber = speakingQueue.indexOf(agent.name);\n\nreturn (\n\n<ListGroup.Item className={styles.agentItem}>\n\n<div>\n\n{agent.name}\n\n<span style={{ marginLeft: '5px' }}>{agent.relevancy}%</span> {/\\* Add this line \\*/}\n\n</div>\n\n{queueNumber !== -1 && <span> Queue: {queueNumber + 1}</span>}\n\n<FaReply\n\nstyle={{ color: 'blue', cursor: 'pointer', marginRight: '5px' }}\n\nonClick={() => handleReply(agent.name)}\n\n/>\n\n<FaTimes style={{ color: 'red', cursor: 'pointer' }} onClick={() => handleClear(agent.name)} />\n\n</ListGroup.Item>\n\n);\n\n}\n\nfunction Chat({ parliament, websocket }) {\n\nconst \\[messages, setMessages\\] = useState(\\[\\]);\n\nconst \\[messageInput, setMessageInput\\] = useState('');\n\nconst \\[isSending, setIsSending\\] = useState(false);\n\nconst \\[agents, setAgents\\] = useState(parliament.agents);\n\nconst \\[speakingQueue, setSpeakingQueue\\] = useState(\\[\\]); // Add this line\n\nconst chatContainer = useRef(null);\n\nconst \\[isTyping, setIsTyping\\] = useState(false);\n\nconst \\[typingTimeoutId, setTypingTimeoutId\\] = useState(null);\n\nconst debouncedSendTypingStatus = useCallback(debounce(sendTypingStatus, 300), \\[sendTypingStatus\\]);\n\nconst defaultStyle = {\n\nfontWeight: 'bold',\n\ncolor: 'black',\n\nfontSize: '16px',\n\npadding: '10px',\n\nborderRadius: '4px',\n\nborder: '1px solid #ccc',\n\nwidth: '100%',\n\nboxSizing: 'border-box',\n\noutline: 'none',\n\nresize: 'none',\n\n};\n\nfunction handleError(error) {\n\nconsole.error(\"Websocket error: \", error);\n\n}\n\nuseEffect(() => {\n\nif (websocket) {\n\nwebsocket.onmessage = handleReceiveMessage;\n\nwebsocket.onerror = handleError;\n\n}\n\nsetInitialMessages();\n\n}, \\[parliament, websocket\\]);\n\nuseEffect(() => {\n\nsendTypingStatus();\n\n}, \\[isTyping\\]);\n\nuseEffect(() => {\n\nchatContainer.current.scrollTop = chatContainer.current.scrollHeight;\n\n}, \\[messages\\]);\n\nfunction setInitialMessages() {\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages,\n\n{\n\nsender: 'System',\n\ntext: `Parliament is in session, the topic of discussion is: ${parliament.topic}`,\n\n},\n\n{\n\nsender: 'System',\n\ntext: \\`The parliament members are ${parliament.agents\n\n.map((agent) => agent.name)\n\n.join(', ')}, and ${parliament.username}\\`,\n\n},\n\n\\]);\n\n}\n\nfunction handleReceiveMessage(event) {\n\nconst data = JSON.parse(event.data);\n\nconst { action, message, sender, updated\\_agents, queue } = data;\n\nif (action === \"receive\\_message\") {\n\nconsole.log(\"data received\")\n\nconsole.log(data)\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== parliament.username || message.text !== \"typing...\"),\n\n{ sender, text: message },\n\n\\]);\n\n} else if (action === \"typing\" && sender !== parliament.username) {\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n\n{ sender, text: \"typing...\" },\n\n\\]);\n\n} else if (action === \"stopped\\_typing\" && sender !== parliament.username) {\n\nclearTimeout(typingTimeoutId);\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n\n\\]);\n\n} else if (action === \"update\\_queue\") {\n\nsetSpeakingQueue(queue);\n\n}\n\nif (updated\\_agents) {\n\nsetAgents(updated\\_agents);\n\n}\n\n}\n\nfunction handleClear(agentName) {\n\n// Send a clear event to the server\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'clear\\_agent',\n\nagent\\_name: agentName,\n\n}),\n\n);\n\n}\n\nfunction sendTypingStatus() {\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'typing',\n\nsender: parliament.username,\n\nis\\_typing: isTyping,\n\n}),\n\n);\n\n}\n\nasync function sendMessage() {\n\nif (!messageInput.trim() || isSending) return;\n\nsetIsSending(true);\n\ntry {\n\nconsole.log(\\`\\[${parliament.username}\\] Sending message to server...\\`);\n\nconsole.log(messageInput.slice(0, 50) + '...');\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'send\\_message',\n\nsender: parliament.username,\n\nmessage: messageInput,\n\n}),\n\n);\n\nconsole.log(\\`\\[${parliament.username}\\] Message sent!\\`);\n\nsetMessageInput('');\n\nsetIsTyping(false);\n\n// Call broadcast\\_message here\n\nconsole.log(\\`\\[${parliament.username}\\] Sending broadcast message to server...\\`);\n\nconsole.log(messageInput.slice(0, 50) + '...');\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'broadcast\\_message',\n\nmessage: messageInput,\n\nsender: parliament.username,\n\n}),\n\n);\n\nconsole.log(\\`\\[${parliament.username}\\] Broadcast message sent!\\`);\n\n} catch (error) {\n\nconsole.log(\\`\\[${parliament.username}\\] Failed to send message:\\`, error);\n\n} finally {\n\nsetIsSending(false);\n\n}\n\n}\n\nfunction handleReply(sender) {\n\nsetMessageInput(\\`@${sender} \\`);\n\n}\n\nconst agentNames = agents.map(agent => ({ id: agent.name, display: agent.name }));\n\nreturn (\n\n<div style={{ display: 'flex', gap: '1em' }}>\n\n<div style={{ width: '70%' }}>\n\n<h1>Parliament Chat</h1>\n\n<ListGroup\n\nref={chatContainer}\n\nstyle={{ height: '400px', overflowY: 'scroll' }}>\n\n{messages.map((message, index) => (\n\n<Message\n\nkey={index}\n\nmessage={message}\n\nsender={message.sender}\n\nhandleReply={handleReply}\n\nuser={parliament.username}\n\n/>\n\n))}\n\n</ListGroup>\n\n<Form>\n\n<Form.Group controlId=\"messageInput\">\n\n<MentionsInput\n\nautoFocus\n\nvalue={messageInput}\n\nonChange={(event, value, plainTextValue) => {\n\nsetMessageInput(plainTextValue);\n\nsetIsTyping(true);\n\ndebouncedSendTypingStatus();\n\nclearTimeout(typingTimeoutId);\n\nsetTypingTimeoutId(setTimeout(() => setIsTyping(false), 2000));\n\n}}\n\nonKeyPress={(e) => {\n\nif (e.key === 'Enter') {\n\nsendMessage();\n\ne.preventDefault();\n\n}\n\n}}\n\nplaceholder=\"Type your message...\"\n\nstyle={defaultStyle} // styles for the input field\n\n\\>\n\n<Mention\n\ntrigger=\"@\"\n\ndata={agentNames}\n\ndisplayTransform={(id, display) => `@${display}`}\n\nclassName={styles.mentionStyle} // use your CSS module to style the mentions\n\n/>\n\n</MentionsInput>\n\n</Form.Group>\n\n</Form>\n\n<Button onClick={sendMessage} disabled={isSending}>\n\n{isSending ? <Spinner animation=\"border\" size=\"sm\" /> : 'Send Message'}\n\n</Button>\n\n</div>\n\n<div style={{ width: '20%', marginTop: '25px' }}> {/\\* You can adjust the marginTop as needed \\*/}\n\n<h2 style={{ fontSize: '1em' }}>Agents</h2>\n\n<ListGroup>\n\n{agents\n\n.sort((a, b) => {\n\nconst aQueueNumber = speakingQueue.indexOf(a.name);\n\nconst bQueueNumber = speakingQueue.indexOf(b.name);\n\n// If both agents are in the queue, compare their positions\n\nif (aQueueNumber !== -1 && bQueueNumber !== -1) {\n\nreturn aQueueNumber - bQueueNumber;\n\n}\n\n// If only one of the agents is in the queue, that agent comes first\n\nif (aQueueNumber !== -1) {\n\nreturn -1;\n\n}\n\nif (bQueueNumber !== -1) {\n\nreturn 1;\n\n}\n\n// If neither agent is in the queue, compare their names alphabetically\n\nreturn a.name.localeCompare(b.name);\n\n})\n\n.map((agent, index) => (\n\n<AgentItem\n\nkey={index}\n\nagent={agent}\n\nhandleClear={handleClear}\n\nspeakingQueue={speakingQueue}\n\nhandleReply={handleReply} // Add this line\n\n/>\n\n))}\n\n</ListGroup>\n\n</div>\n\n</div>\n\n);\n\n}\n\nexport default Chat;\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def try\\_to\\_speak(self):\n\nif self.should\\_speak() and not self.is\\_speaking:\n\nawait self.speak()\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nawait self.try\\_to\\_speak()\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.try\\_to\\_speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\": agent.calculate\\_relevancy(self.chat\\_history)\n\n})\n\nreturn agents\\_info\n\nHere's server.py:\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = self.parliament.get\\_agents\\_info() # Assuming that this method returns agents' information\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nawait self.update\\_queue(websocket)\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue.to\\_json()}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue.to\\_json()}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()",
      "created_at": "2023-05-13T10:43:40.663313Z",
      "sequence_order": 12,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:43:40.663313Z",
            "stop_timestamp": "2023-05-13T10:43:40.663313Z",
            "flags": null,
            "type": "text",
            "text": "I want my chat.js file to show the relevancy score, right now it just says %\n\nimport styles from './Chat.module.css';\n\nimport { debounce } from 'lodash';\n\nimport { useState, useEffect, useRef, useCallback } from 'react';\n\nimport { ListGroup, Form, Button, Spinner, ListGroupItem } from 'react-bootstrap';\n\nimport { FaTimes, FaReply } from 'react-icons/fa'; // Importing cross and reply icons\n\nimport { MentionsInput, Mention } from 'react-mentions';\n\nfunction Message({ message, sender, handleReply }) {\n\nreturn (\n\n<ListGroup.Item className={styles.messageItem}>\n\n<div className={styles.flexCenter}>\n\n<strong>{sender}:</strong>\n\n<Button\n\nvariant=\"link\"\n\nstyle={{ padding: 0, color: '#007bff', textDecoration: 'none' }}\n\nonClick={() => handleReply(sender)}\n\n\\>\n\n<FaReply />\n\n</Button>\n\n</div>\n\n{message.text}\n\n</ListGroup.Item>\n\n);\n\n}\n\nfunction AgentItem({ agent, handleClear, speakingQueue, handleReply }) {\n\nconst queueNumber = speakingQueue.indexOf(agent.name);\n\nreturn (\n\n<ListGroup.Item className={styles.agentItem}>\n\n<div>\n\n{agent.name}\n\n<span style={{ marginLeft: '5px' }}>{agent.relevancy}%</span> {/\\* Add this line \\*/}\n\n</div>\n\n{queueNumber !== -1 && <span> Queue: {queueNumber + 1}</span>}\n\n<FaReply\n\nstyle={{ color: 'blue', cursor: 'pointer', marginRight: '5px' }}\n\nonClick={() => handleReply(agent.name)}\n\n/>\n\n<FaTimes style={{ color: 'red', cursor: 'pointer' }} onClick={() => handleClear(agent.name)} />\n\n</ListGroup.Item>\n\n);\n\n}\n\nfunction Chat({ parliament, websocket }) {\n\nconst \\[messages, setMessages\\] = useState(\\[\\]);\n\nconst \\[messageInput, setMessageInput\\] = useState('');\n\nconst \\[isSending, setIsSending\\] = useState(false);\n\nconst \\[agents, setAgents\\] = useState(parliament.agents);\n\nconst \\[speakingQueue, setSpeakingQueue\\] = useState(\\[\\]); // Add this line\n\nconst chatContainer = useRef(null);\n\nconst \\[isTyping, setIsTyping\\] = useState(false);\n\nconst \\[typingTimeoutId, setTypingTimeoutId\\] = useState(null);\n\nconst debouncedSendTypingStatus = useCallback(debounce(sendTypingStatus, 300), \\[sendTypingStatus\\]);\n\nconst defaultStyle = {\n\nfontWeight: 'bold',\n\ncolor: 'black',\n\nfontSize: '16px',\n\npadding: '10px',\n\nborderRadius: '4px',\n\nborder: '1px solid #ccc',\n\nwidth: '100%',\n\nboxSizing: 'border-box',\n\noutline: 'none',\n\nresize: 'none',\n\n};\n\nfunction handleError(error) {\n\nconsole.error(\"Websocket error: \", error);\n\n}\n\nuseEffect(() => {\n\nif (websocket) {\n\nwebsocket.onmessage = handleReceiveMessage;\n\nwebsocket.onerror = handleError;\n\n}\n\nsetInitialMessages();\n\n}, \\[parliament, websocket\\]);\n\nuseEffect(() => {\n\nsendTypingStatus();\n\n}, \\[isTyping\\]);\n\nuseEffect(() => {\n\nchatContainer.current.scrollTop = chatContainer.current.scrollHeight;\n\n}, \\[messages\\]);\n\nfunction setInitialMessages() {\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages,\n\n{\n\nsender: 'System',\n\ntext: `Parliament is in session, the topic of discussion is: ${parliament.topic}`,\n\n},\n\n{\n\nsender: 'System',\n\ntext: \\`The parliament members are ${parliament.agents\n\n.map((agent) => agent.name)\n\n.join(', ')}, and ${parliament.username}\\`,\n\n},\n\n\\]);\n\n}\n\nfunction handleReceiveMessage(event) {\n\nconst data = JSON.parse(event.data);\n\nconst { action, message, sender, updated\\_agents, queue } = data;\n\nif (action === \"receive\\_message\") {\n\nconsole.log(\"data received\")\n\nconsole.log(data)\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== parliament.username || message.text !== \"typing...\"),\n\n{ sender, text: message },\n\n\\]);\n\n} else if (action === \"typing\" && sender !== parliament.username) {\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n\n{ sender, text: \"typing...\" },\n\n\\]);\n\n} else if (action === \"stopped\\_typing\" && sender !== parliament.username) {\n\nclearTimeout(typingTimeoutId);\n\nsetMessages((prevMessages) => \\[\n\n...prevMessages.filter((message) => message.sender !== sender || message.text !== \"typing...\"),\n\n\\]);\n\n} else if (action === \"update\\_queue\") {\n\nsetSpeakingQueue(queue);\n\n}\n\nif (updated\\_agents) {\n\nsetAgents(updated\\_agents);\n\n}\n\n}\n\nfunction handleClear(agentName) {\n\n// Send a clear event to the server\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'clear\\_agent',\n\nagent\\_name: agentName,\n\n}),\n\n);\n\n}\n\nfunction sendTypingStatus() {\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'typing',\n\nsender: parliament.username,\n\nis\\_typing: isTyping,\n\n}),\n\n);\n\n}\n\nasync function sendMessage() {\n\nif (!messageInput.trim() || isSending) return;\n\nsetIsSending(true);\n\ntry {\n\nconsole.log(\\`\\[${parliament.username}\\] Sending message to server...\\`);\n\nconsole.log(messageInput.slice(0, 50) + '...');\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'send\\_message',\n\nsender: parliament.username,\n\nmessage: messageInput,\n\n}),\n\n);\n\nconsole.log(\\`\\[${parliament.username}\\] Message sent!\\`);\n\nsetMessageInput('');\n\nsetIsTyping(false);\n\n// Call broadcast\\_message here\n\nconsole.log(\\`\\[${parliament.username}\\] Sending broadcast message to server...\\`);\n\nconsole.log(messageInput.slice(0, 50) + '...');\n\nwebsocket.send(\n\nJSON.stringify({\n\naction: 'broadcast\\_message',\n\nmessage: messageInput,\n\nsender: parliament.username,\n\n}),\n\n);\n\nconsole.log(\\`\\[${parliament.username}\\] Broadcast message sent!\\`);\n\n} catch (error) {\n\nconsole.log(\\`\\[${parliament.username}\\] Failed to send message:\\`, error);\n\n} finally {\n\nsetIsSending(false);\n\n}\n\n}\n\nfunction handleReply(sender) {\n\nsetMessageInput(\\`@${sender} \\`);\n\n}\n\nconst agentNames = agents.map(agent => ({ id: agent.name, display: agent.name }));\n\nreturn (\n\n<div style={{ display: 'flex', gap: '1em' }}>\n\n<div style={{ width: '70%' }}>\n\n<h1>Parliament Chat</h1>\n\n<ListGroup\n\nref={chatContainer}\n\nstyle={{ height: '400px', overflowY: 'scroll' }}>\n\n{messages.map((message, index) => (\n\n<Message\n\nkey={index}\n\nmessage={message}\n\nsender={message.sender}\n\nhandleReply={handleReply}\n\nuser={parliament.username}\n\n/>\n\n))}\n\n</ListGroup>\n\n<Form>\n\n<Form.Group controlId=\"messageInput\">\n\n<MentionsInput\n\nautoFocus\n\nvalue={messageInput}\n\nonChange={(event, value, plainTextValue) => {\n\nsetMessageInput(plainTextValue);\n\nsetIsTyping(true);\n\ndebouncedSendTypingStatus();\n\nclearTimeout(typingTimeoutId);\n\nsetTypingTimeoutId(setTimeout(() => setIsTyping(false), 2000));\n\n}}\n\nonKeyPress={(e) => {\n\nif (e.key === 'Enter') {\n\nsendMessage();\n\ne.preventDefault();\n\n}\n\n}}\n\nplaceholder=\"Type your message...\"\n\nstyle={defaultStyle} // styles for the input field\n\n\\>\n\n<Mention\n\ntrigger=\"@\"\n\ndata={agentNames}\n\ndisplayTransform={(id, display) => `@${display}`}\n\nclassName={styles.mentionStyle} // use your CSS module to style the mentions\n\n/>\n\n</MentionsInput>\n\n</Form.Group>\n\n</Form>\n\n<Button onClick={sendMessage} disabled={isSending}>\n\n{isSending ? <Spinner animation=\"border\" size=\"sm\" /> : 'Send Message'}\n\n</Button>\n\n</div>\n\n<div style={{ width: '20%', marginTop: '25px' }}> {/\\* You can adjust the marginTop as needed \\*/}\n\n<h2 style={{ fontSize: '1em' }}>Agents</h2>\n\n<ListGroup>\n\n{agents\n\n.sort((a, b) => {\n\nconst aQueueNumber = speakingQueue.indexOf(a.name);\n\nconst bQueueNumber = speakingQueue.indexOf(b.name);\n\n// If both agents are in the queue, compare their positions\n\nif (aQueueNumber !== -1 && bQueueNumber !== -1) {\n\nreturn aQueueNumber - bQueueNumber;\n\n}\n\n// If only one of the agents is in the queue, that agent comes first\n\nif (aQueueNumber !== -1) {\n\nreturn -1;\n\n}\n\nif (bQueueNumber !== -1) {\n\nreturn 1;\n\n}\n\n// If neither agent is in the queue, compare their names alphabetically\n\nreturn a.name.localeCompare(b.name);\n\n})\n\n.map((agent, index) => (\n\n<AgentItem\n\nkey={index}\n\nagent={agent}\n\nhandleClear={handleClear}\n\nspeakingQueue={speakingQueue}\n\nhandleReply={handleReply} // Add this line\n\n/>\n\n))}\n\n</ListGroup>\n\n</div>\n\n</div>\n\n);\n\n}\n\nexport default Chat;\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def try\\_to\\_speak(self):\n\nif self.should\\_speak() and not self.is\\_speaking:\n\nawait self.speak()\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nawait self.try\\_to\\_speak()\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.try\\_to\\_speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\": agent.calculate\\_relevancy(self.chat\\_history)\n\n})\n\nreturn agents\\_info\n\nHere's server.py:\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = self.parliament.get\\_agents\\_info() # Assuming that this method returns agents' information\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nawait self.update\\_queue(websocket)\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue.to\\_json()}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue.to\\_json()}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:43:40.663313Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "da4b8481-2dba-4dc6-add8-5fccae871d67",
      "role": "assistant",
      "content": " To show the actual relevancy score in the Chat.js file, you need to make a few changes:\n\n1. In parliament.py, return the relevancy_score in the agents_info object. The get_agents_info() method should return:\n\n```py\ndef get_agents_info(self) -> List[dict]:\n    agents_info = []\n    for agent_name, agent in self.agents.items():\n        agents_info.append({\n            \"name\": agent_name,\n            \"speaking_pressure\": agent.speaking_pressure,\n            \"speaking_pressure_increment\": agent.speaking_pressure_increment, \n            \"context\": agent.context,\n            \"agent_type\": agent.agent_type,\n            \"relevancy_score\": agent.relevancy_score  # Return the actual score here\n        })\n    return agents_info\n```\n\n2. In server.py, send the agents_info object containing the relevancy_scores to the client:\n\n```py \nagent_info = self.parliament.get_agents_info() \nawait websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info}))\n```\n\n3. In Chat.js, extract the relevancy_score from the agents object and display it:\n\n```js\nfunction AgentItem({ agent, handleClear, speakingQueue, handleReply }) {\n  const queueNumber = speakingQueue.indexOf(agent.name);\n  \n  return (\n    <ListGroup.Item className={styles.agentItem}>\n      <div>\n        {agent.name}\n        <span style={{ marginLeft: '5px' }}>{agent.relevancy_score}%</span>  \n      </div>\n      {/* Rest of the code... */}\n    </ListGroup.Item>\n  );\n}\n```\n\n4. When the agents are updated, the relevancy_scores will be displayed next to each agent.\n\nThis involves passing the relevancy score from the Agent object, to the Parliament object, to the server, and then to the frontend to be displayed. Let me know if you have any other questions!",
      "created_at": "2023-05-13T10:44:12.979310Z",
      "sequence_order": 13,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:44:12.979310Z",
            "stop_timestamp": "2023-05-13T10:44:12.979310Z",
            "flags": null,
            "type": "text",
            "text": " To show the actual relevancy score in the Chat.js file, you need to make a few changes:\n\n1. In parliament.py, return the relevancy_score in the agents_info object. The get_agents_info() method should return:\n\n```py\ndef get_agents_info(self) -> List[dict]:\n    agents_info = []\n    for agent_name, agent in self.agents.items():\n        agents_info.append({\n            \"name\": agent_name,\n            \"speaking_pressure\": agent.speaking_pressure,\n            \"speaking_pressure_increment\": agent.speaking_pressure_increment, \n            \"context\": agent.context,\n            \"agent_type\": agent.agent_type,\n            \"relevancy_score\": agent.relevancy_score  # Return the actual score here\n        })\n    return agents_info\n```\n\n2. In server.py, send the agents_info object containing the relevancy_scores to the client:\n\n```py \nagent_info = self.parliament.get_agents_info() \nawait websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info}))\n```\n\n3. In Chat.js, extract the relevancy_score from the agents object and display it:\n\n```js\nfunction AgentItem({ agent, handleClear, speakingQueue, handleReply }) {\n  const queueNumber = speakingQueue.indexOf(agent.name);\n  \n  return (\n    <ListGroup.Item className={styles.agentItem}>\n      <div>\n        {agent.name}\n        <span style={{ marginLeft: '5px' }}>{agent.relevancy_score}%</span>  \n      </div>\n      {/* Rest of the code... */}\n    </ListGroup.Item>\n  );\n}\n```\n\n4. When the agents are updated, the relevancy_scores will be displayed next to each agent.\n\nThis involves passing the relevancy score from the Agent object, to the Parliament object, to the server, and then to the frontend to be displayed. Let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:44:12.979310Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "98c5528f-330a-4113-a14a-d580e0e9acb3",
      "role": "human",
      "content": "the relevancy is not updated, it should update when the number is generated",
      "created_at": "2023-05-13T10:53:47.966202Z",
      "sequence_order": 14,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:53:47.966202Z",
            "stop_timestamp": "2023-05-13T10:53:47.966202Z",
            "flags": null,
            "type": "text",
            "text": "the relevancy is not updated, it should update when the number is generated",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:53:47.966202Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "ec8d257e-c94b-4b41-a5a5-6e674abae31f",
      "role": "assistant",
      "content": " You're right, the relevancy scores need to be updated dynamically whenever a new score is calculated. To implement this, you need to:\n\n1. In parliament.py, return the updated relevancy_score in the agent_info whenever it is calculated:\n\n```py\ndef calculate_relevancy(self, chat_context: str) -> int:\n    ...\n    self.relevancy_score = relevancy_score\n    print(f\"Relevancy score for {self.name}: {self.relevancy_score}%\")\n\n    agent_info = [\n        {\n            \"name\": self.name,\n            \"relevancy_score\": self.relevancy_score  # Return updated score here\n        }\n    ]\n    await self.websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info}))\n```\n\n2. In Chat.js, when the UPDATE_AGENTS message is received, update just the relevancy_score for that specific agent:\n\n```js\nfunction handleReceiveMessage(event) {\n  ...\n  } else if (action === \"update_agents\") {\n    const updatedAgents = data.agents;\n    setAgents(prevAgents => {\n      const newAgents = [...prevAgents];\n      for (const agent of updatedAgents) {\n        const index = prevAgents.findIndex(a => a.name === agent.name);\n        newAgents[index].relevancy_score = agent.relevancy_score;  // Update just the relevancy_score\n      }\n      return newAgents;\n    });\n  }\n ...  \n}\n```\n\n3. This will dynamically update the relevancy_score displayed next to each agent whenever a new score is calculated.\n\nThe flow would be:\n\n- Relevancy calculated in parliament.py\n- Updated relevancy_score sent to client \n- Client (Chat.js) receives UPDATE_AGENTS message\n- Client updates just the relevancy_score for that agent \n- Display is updated with new relevancy_score\n\nLet me know if you have any other questions!",
      "created_at": "2023-05-13T10:54:19.470225Z",
      "sequence_order": 15,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:54:19.470225Z",
            "stop_timestamp": "2023-05-13T10:54:19.470225Z",
            "flags": null,
            "type": "text",
            "text": " You're right, the relevancy scores need to be updated dynamically whenever a new score is calculated. To implement this, you need to:\n\n1. In parliament.py, return the updated relevancy_score in the agent_info whenever it is calculated:\n\n```py\ndef calculate_relevancy(self, chat_context: str) -> int:\n    ...\n    self.relevancy_score = relevancy_score\n    print(f\"Relevancy score for {self.name}: {self.relevancy_score}%\")\n\n    agent_info = [\n        {\n            \"name\": self.name,\n            \"relevancy_score\": self.relevancy_score  # Return updated score here\n        }\n    ]\n    await self.websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info}))\n```\n\n2. In Chat.js, when the UPDATE_AGENTS message is received, update just the relevancy_score for that specific agent:\n\n```js\nfunction handleReceiveMessage(event) {\n  ...\n  } else if (action === \"update_agents\") {\n    const updatedAgents = data.agents;\n    setAgents(prevAgents => {\n      const newAgents = [...prevAgents];\n      for (const agent of updatedAgents) {\n        const index = prevAgents.findIndex(a => a.name === agent.name);\n        newAgents[index].relevancy_score = agent.relevancy_score;  // Update just the relevancy_score\n      }\n      return newAgents;\n    });\n  }\n ...  \n}\n```\n\n3. This will dynamically update the relevancy_score displayed next to each agent whenever a new score is calculated.\n\nThe flow would be:\n\n- Relevancy calculated in parliament.py\n- Updated relevancy_score sent to client \n- Client (Chat.js) receives UPDATE_AGENTS message\n- Client updates just the relevancy_score for that agent \n- Display is updated with new relevancy_score\n\nLet me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:54:19.470225Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "63292247-b629-4bf0-9050-a0429b345620",
      "role": "human",
      "content": "it says \"await\" allowed only within async function inside: def calculate\\_relevancy(self, chat\\_context: str) -> int:",
      "created_at": "2023-05-13T10:55:49.338240Z",
      "sequence_order": 16,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:55:49.338240Z",
            "stop_timestamp": "2023-05-13T10:55:49.338240Z",
            "flags": null,
            "type": "text",
            "text": "it says \"await\" allowed only within async function inside: def calculate\\_relevancy(self, chat\\_context: str) -> int:",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:55:49.338240Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "91f41365-1e04-42f3-abe5-e98b04a3ce23",
      "role": "assistant",
      "content": " Good catch! That code will not work because calculate_relevancy() is not an async function. To fix this, you have two options:\n\n1. Make calculate_relevancy() an async function:\n\n```py\nasync def calculate_relevancy(self, chat_context: str) -> int:\n    ...\n    agent_info = [\n        {\n            \"name\": self.name,\n            \"relevancy_score\": self.relevancy_score  \n        }\n    ]\n    await self.websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info}))\n```\n\nNow the await keyword can be used inside the function.\n\n2. Call calculate_relevancy() from an async function, and await the call:\n\n```py\nasync def update_relevancy(self):\n    self.relevancy_score = calculate_relevancy(chat_context)\n    agent_info = [\n        {\n            \"name\": self.name,\n            \"relevancy_score\": self.relevancy_score  \n        }\n    ]\n    await self.websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info}))\n```\n\nThen you can call:\n```py\nasyncio.create_task(self.update_relevancy())\n```\n\nto run update_relevancy() asynchronously.\n\nEither of these options will allow you to use await to send the updated agent info to the client from within the relevancy calculation logic. Let me know if you have any other questions!",
      "created_at": "2023-05-13T10:56:16.862155Z",
      "sequence_order": 17,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:56:16.862155Z",
            "stop_timestamp": "2023-05-13T10:56:16.862155Z",
            "flags": null,
            "type": "text",
            "text": " Good catch! That code will not work because calculate_relevancy() is not an async function. To fix this, you have two options:\n\n1. Make calculate_relevancy() an async function:\n\n```py\nasync def calculate_relevancy(self, chat_context: str) -> int:\n    ...\n    agent_info = [\n        {\n            \"name\": self.name,\n            \"relevancy_score\": self.relevancy_score  \n        }\n    ]\n    await self.websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info}))\n```\n\nNow the await keyword can be used inside the function.\n\n2. Call calculate_relevancy() from an async function, and await the call:\n\n```py\nasync def update_relevancy(self):\n    self.relevancy_score = calculate_relevancy(chat_context)\n    agent_info = [\n        {\n            \"name\": self.name,\n            \"relevancy_score\": self.relevancy_score  \n        }\n    ]\n    await self.websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info}))\n```\n\nThen you can call:\n```py\nasyncio.create_task(self.update_relevancy())\n```\n\nto run update_relevancy() asynchronously.\n\nEither of these options will allow you to use await to send the updated agent info to the client from within the relevancy calculation logic. Let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:56:16.862155Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "5ca68025-c604-445f-ac14-fa7761633593",
      "role": "human",
      "content": "Can you be sure this is right, where do I put \"asyncio.create\\_task(self.update\\_relevancy())\"\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100",
      "created_at": "2023-05-13T10:57:50.733264Z",
      "sequence_order": 18,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:57:50.733264Z",
            "stop_timestamp": "2023-05-13T10:57:50.733264Z",
            "flags": null,
            "type": "text",
            "text": "Can you be sure this is right, where do I put \"asyncio.create\\_task(self.update\\_relevancy())\"\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:57:50.733264Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "87b70b53-4099-40f5-a70b-8d7357116ecf",
      "role": "assistant",
      "content": " You're right to double check - placing that code inside calculate_relevancy() would be incorrect.\n\nThe proper place to call asyncio.create_task(self.update_relevancy()) is:\n\n1. Inside Parliament.add_agent(), after adding the agent:\n\n```py\ndef add_agent(self, agent: Agent):\n    ...\n    asyncio.create_task(agent.update_relevancy())  # Call here\n```\n\nThis will run the update_relevancy() method for that agent in the background.\n\nThe full code would be:\n\n```py\nclass Agent:\n    ...\n    async def update_relevancy(self):\n        self.relevancy_score = calculate_relevancy(chat_context)\n        agent_info = [\n            {\n                \"name\": self.name,\n                \"relevancy_score\": self.relevancy_score  \n            }\n        ]\n        await self.websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info}))\n\nclass Parliament:\n    ...\n    def add_agent(self, agent: Agent):\n        ...\n        asyncio.create_task(agent.update_relevancy())  \n```\n\nThen, whenever calculate_relevancy() is called for an agent, it will send the updated relevancy_score to the client in the background.\n\nThe full flow would be:\n\n- Agent added to Parliament \n- Parliament calls agent.update_relevancy() to run in background \n- At some point, calculate_relevancy() is called for the agent\n- Inside calculate_relevancy(), the agent sends its updated relevancy_score to the client\n- The client receives the UPDATE_AGENTS message and updates the display\n\nLet me know if this makes sense or if you have any other questions!",
      "created_at": "2023-05-13T10:58:21.223863Z",
      "sequence_order": 19,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T10:58:21.223863Z",
            "stop_timestamp": "2023-05-13T10:58:21.223863Z",
            "flags": null,
            "type": "text",
            "text": " You're right to double check - placing that code inside calculate_relevancy() would be incorrect.\n\nThe proper place to call asyncio.create_task(self.update_relevancy()) is:\n\n1. Inside Parliament.add_agent(), after adding the agent:\n\n```py\ndef add_agent(self, agent: Agent):\n    ...\n    asyncio.create_task(agent.update_relevancy())  # Call here\n```\n\nThis will run the update_relevancy() method for that agent in the background.\n\nThe full code would be:\n\n```py\nclass Agent:\n    ...\n    async def update_relevancy(self):\n        self.relevancy_score = calculate_relevancy(chat_context)\n        agent_info = [\n            {\n                \"name\": self.name,\n                \"relevancy_score\": self.relevancy_score  \n            }\n        ]\n        await self.websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info}))\n\nclass Parliament:\n    ...\n    def add_agent(self, agent: Agent):\n        ...\n        asyncio.create_task(agent.update_relevancy())  \n```\n\nThen, whenever calculate_relevancy() is called for an agent, it will send the updated relevancy_score to the client in the background.\n\nThe full flow would be:\n\n- Agent added to Parliament \n- Parliament calls agent.update_relevancy() to run in background \n- At some point, calculate_relevancy() is called for the agent\n- Inside calculate_relevancy(), the agent sends its updated relevancy_score to the client\n- The client receives the UPDATE_AGENTS message and updates the display\n\nLet me know if this makes sense or if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T10:58:21.223863Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "9031b1de-f60f-445c-98db-0d99812b1c46",
      "role": "human",
      "content": "I don't need to do it after adding the agent, only after adding new messages",
      "created_at": "2023-05-13T11:00:28.398293Z",
      "sequence_order": 20,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T11:00:28.398293Z",
            "stop_timestamp": "2023-05-13T11:00:28.398293Z",
            "flags": null,
            "type": "text",
            "text": "I don't need to do it after adding the agent, only after adding new messages",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T11:00:28.398293Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "7c0ed4ba-9fe5-4d45-9458-2a8152b286c3",
      "role": "assistant",
      "content": " You're correct, updating the relevancy score after each new message is a better approach. To implement this, you can:\n\n1. Remove the call to asyncio.create_task(agent.update_relevancy()) from Parliament.add_agent().\n\n2. Add a call to agent.update_relevancy() from inside Parliament.broadcast_message(), after adding the new message to the chat history:\n\n```py\nasync def broadcast_message(self, message, sender, timestamp):\n    ...\n    self.chat_history.add(message, sender, timestamp)\n    ...\n    \n    for agent in self.agents.values():\n        await agent.update_relevancy()  # Call here after adding message\n```\n\n3. The agent.update_relevancy() method would remain the same:\n\n```py\nasync def update_relevancy(self): \n    self.relevancy_score = self.calculate_relevancy(self.parliament.chat_history.truncated_chat_history)\n    agent_info = [\n        {\n            \"name\": self.name,\n            \"relevancy_score\": self.relevancy_score  \n        }\n    ]\n    await self.websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info})) \n```\n\nThe full flow would now be:\n\n- New message received and added to chat history\n- Parliament calls agent.update_relevancy() for each agent\n- Inside update_relevancy(), the agent calculates its new relevancy score based on the updated chat history\n- The agent sends its updated relevancy_score to the client\n- The client receives the UPDATE_AGENTS message and updates the display with the new score\n\nThis approach will ensure the relevancy score for each agent is calculated and updated after each new user message is added. Let me know if you have any other questions!",
      "created_at": "2023-05-13T11:00:59.497848Z",
      "sequence_order": 21,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T11:00:59.497848Z",
            "stop_timestamp": "2023-05-13T11:00:59.497848Z",
            "flags": null,
            "type": "text",
            "text": " You're correct, updating the relevancy score after each new message is a better approach. To implement this, you can:\n\n1. Remove the call to asyncio.create_task(agent.update_relevancy()) from Parliament.add_agent().\n\n2. Add a call to agent.update_relevancy() from inside Parliament.broadcast_message(), after adding the new message to the chat history:\n\n```py\nasync def broadcast_message(self, message, sender, timestamp):\n    ...\n    self.chat_history.add(message, sender, timestamp)\n    ...\n    \n    for agent in self.agents.values():\n        await agent.update_relevancy()  # Call here after adding message\n```\n\n3. The agent.update_relevancy() method would remain the same:\n\n```py\nasync def update_relevancy(self): \n    self.relevancy_score = self.calculate_relevancy(self.parliament.chat_history.truncated_chat_history)\n    agent_info = [\n        {\n            \"name\": self.name,\n            \"relevancy_score\": self.relevancy_score  \n        }\n    ]\n    await self.websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info})) \n```\n\nThe full flow would now be:\n\n- New message received and added to chat history\n- Parliament calls agent.update_relevancy() for each agent\n- Inside update_relevancy(), the agent calculates its new relevancy score based on the updated chat history\n- The agent sends its updated relevancy_score to the client\n- The client receives the UPDATE_AGENTS message and updates the display with the new score\n\nThis approach will ensure the relevancy score for each agent is calculated and updated after each new user message is added. Let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T11:00:59.497848Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "5deabb29-97cb-4ba1-9dc3-2ffa13fff7e1",
      "role": "human",
      "content": "do we need the try\n\nto speak function still?\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def try\\_to\\_speak(self):\n\nif self.should\\_speak() and not self.is\\_speaking:\n\nawait self.speak()\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nawait self.try\\_to\\_speak()\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nawait agent.update\\_relevancy()\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.try\\_to\\_speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-13T16:24:15.928107Z",
      "sequence_order": 22,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T16:24:15.928107Z",
            "stop_timestamp": "2023-05-13T16:24:15.928107Z",
            "flags": null,
            "type": "text",
            "text": "do we need the try\n\nto speak function still?\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def try\\_to\\_speak(self):\n\nif self.should\\_speak() and not self.is\\_speaking:\n\nawait self.speak()\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nawait self.try\\_to\\_speak()\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nawait agent.update\\_relevancy()\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.try\\_to\\_speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T16:24:15.928107Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "c0ab69f7-ad49-4688-b612-ce9e79f1ff22",
      "role": "assistant",
      "content": " You're right, with the new logic to update relevancy scores after each message, the try_to_speak() method is no longer needed. \n\ntry_to_speak() was previously used to check if an agent should speak based on their relevancy, and call speak() if so. However, now that relevancy is being updated after each message, the speaking queue will always contain the agents most relevant for the current discussion.\n\nSo, the try_to_speak() method can be removed. The speak() method would be called directly whenever an agent reaches the front of the speaking queue.\n\nTo implement this, you can:\n\n1. Remove the try_to_speak() method from Agent:\n\n```py\nasync def try_to_speak(self):\n    pass  # Method removed\n```\n\n2. Call speak() directly from Parliament whenever the speaking queue is popped, instead of calling try_to_speak():\n\n```py\nasync def broadcast_message(self, message, sender, timestamp):\n    ...\n    first_agent_in_queue = self.speaking_queue.dequeue()\n    if first_agent_in_queue and not self.get_typing_status(first_agent_in_queue):\n        await self.agents[first_agent_in_queue].speak()  # Call speak() directly\n```\n\n3. Remove any calls to try_to_speak() from other methods.\n\nWith these changes, the new flow would be:\n\n- New message received and added to chat history\n- Relevancy scores updated for all agents\n- Most relevant agents added to speaking queue\n- When an agent reaches the front of the queue, speak() is called directly\n- Agent sends message and is removed from the queue\n\nUpdating relevancy scores after each message helps ensure the agents most relevant to the current discussion are in the speaking queue, so the try_to_speak() check is no longer needed.\n\nLet me know if this makes sense or if you have any other questions!",
      "created_at": "2023-05-13T16:25:23.599662Z",
      "sequence_order": 23,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T16:25:23.599662Z",
            "stop_timestamp": "2023-05-13T16:25:23.599662Z",
            "flags": null,
            "type": "text",
            "text": " You're right, with the new logic to update relevancy scores after each message, the try_to_speak() method is no longer needed. \n\ntry_to_speak() was previously used to check if an agent should speak based on their relevancy, and call speak() if so. However, now that relevancy is being updated after each message, the speaking queue will always contain the agents most relevant for the current discussion.\n\nSo, the try_to_speak() method can be removed. The speak() method would be called directly whenever an agent reaches the front of the speaking queue.\n\nTo implement this, you can:\n\n1. Remove the try_to_speak() method from Agent:\n\n```py\nasync def try_to_speak(self):\n    pass  # Method removed\n```\n\n2. Call speak() directly from Parliament whenever the speaking queue is popped, instead of calling try_to_speak():\n\n```py\nasync def broadcast_message(self, message, sender, timestamp):\n    ...\n    first_agent_in_queue = self.speaking_queue.dequeue()\n    if first_agent_in_queue and not self.get_typing_status(first_agent_in_queue):\n        await self.agents[first_agent_in_queue].speak()  # Call speak() directly\n```\n\n3. Remove any calls to try_to_speak() from other methods.\n\nWith these changes, the new flow would be:\n\n- New message received and added to chat history\n- Relevancy scores updated for all agents\n- Most relevant agents added to speaking queue\n- When an agent reaches the front of the queue, speak() is called directly\n- Agent sends message and is removed from the queue\n\nUpdating relevancy scores after each message helps ensure the agents most relevant to the current discussion are in the speaking queue, so the try_to_speak() check is no longer needed.\n\nLet me know if this makes sense or if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T16:25:23.599662Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "d6b287a2-d23c-4e2d-9517-1fb1f8c01dac",
      "role": "human",
      "content": "do we need the try\n\nto speak function still?\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def try\\_to\\_speak(self):\n\nif self.should\\_speak() and not self.is\\_speaking:\n\nawait self.speak()\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nawait self.try\\_to\\_speak()\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = '**http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin**'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nawait agent.update\\_relevancy()\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.try\\_to\\_speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-13T16:25:44.819023Z",
      "sequence_order": 24,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T16:25:44.819023Z",
            "stop_timestamp": "2023-05-13T16:25:44.819023Z",
            "flags": null,
            "type": "text",
            "text": "do we need the try\n\nto speak function still?\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def try\\_to\\_speak(self):\n\nif self.should\\_speak() and not self.is\\_speaking:\n\nawait self.speak()\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nawait self.try\\_to\\_speak()\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = '**http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin**'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nawait agent.update\\_relevancy()\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.try\\_to\\_speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\ndef update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T16:25:44.819023Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "94362fbd-a019-495e-94d4-8c79296d17ff",
      "role": "assistant",
      "content": " No, the try_to_speak() function is no longer needed now that agents are selected to speak based solely on their position in the speaking queue.\n\nSince agents are added to the speaking queue based on their calculated relevancy scores after each new message, the try_to_speak() logic for checking if an agent should speak is redundant.\n\nI would suggest removing the try_to_speak() method, and simply using should_speak() to check if an agent is at the front of the speaking queue before having them speak.\n\nThe updated Agent class would look something like this:\n\n```py\nclass Agent:\n    ...\n    def should_speak(self):\n        return self.parliament.speaker_manager.should_speak(self)  \n    \n    async def speak(self):\n        if self.should_speak():\n            ...  # Rest of speak logic\n```\n\nThen in Parliament, simply call agent.speak() directly rather than agent.try_to_speak():\n\n```py\nasync def broadcast_message(self, message, sender, timestamp):\n    ...\n    for agent in self.agents.values():\n        if not self.get_typing_status(agent.name):  \n            await agent.speak()  # Call speak directly\n```\n\nSince the agents that will speak are determined by their position in the queue, the try_to_speak() logic is redundant and can be removed. Let me know if you have any other questions!",
      "created_at": "2023-05-13T16:26:39.459235Z",
      "sequence_order": 25,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T16:26:39.459235Z",
            "stop_timestamp": "2023-05-13T16:26:39.459235Z",
            "flags": null,
            "type": "text",
            "text": " No, the try_to_speak() function is no longer needed now that agents are selected to speak based solely on their position in the speaking queue.\n\nSince agents are added to the speaking queue based on their calculated relevancy scores after each new message, the try_to_speak() logic for checking if an agent should speak is redundant.\n\nI would suggest removing the try_to_speak() method, and simply using should_speak() to check if an agent is at the front of the speaking queue before having them speak.\n\nThe updated Agent class would look something like this:\n\n```py\nclass Agent:\n    ...\n    def should_speak(self):\n        return self.parliament.speaker_manager.should_speak(self)  \n    \n    async def speak(self):\n        if self.should_speak():\n            ...  # Rest of speak logic\n```\n\nThen in Parliament, simply call agent.speak() directly rather than agent.try_to_speak():\n\n```py\nasync def broadcast_message(self, message, sender, timestamp):\n    ...\n    for agent in self.agents.values():\n        if not self.get_typing_status(agent.name):  \n            await agent.speak()  # Call speak directly\n```\n\nSince the agents that will speak are determined by their position in the queue, the try_to_speak() logic is redundant and can be removed. Let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T16:26:39.459235Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "ea49dd2e-4dbc-4beb-baa4-1359661c2b37",
      "role": "human",
      "content": "in your `update_relevancy` function, you're trying to send data over the websocket. This seems odd considering the function's purpose is just to update relevancy. You might want to separate concerns here and handle sending updates over the websocket elsewhere in your code.\n\n    pythonCopy code\n\n`async def update_relevancy(self): self.relevancy_score = self.calculate_relevancy(self.parliament.chat_history.truncated_chat_history)`\n\nYou could then handle sending updates over the websocket in a different function or part of your code where it makes more sense to do so.\n\nCan you help me update server.py:\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = await self.parliament.get\\_agents\\_info() # Assuming that this method returns agents' information\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nawait self.update\\_queue(websocket)\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue.to\\_json()}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue.to\\_json()}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()\n\nAnd parliament.py:\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nawait agent.update\\_relevancy()\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-13T16:41:45.637074Z",
      "sequence_order": 26,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T16:41:45.637074Z",
            "stop_timestamp": "2023-05-13T16:41:45.637074Z",
            "flags": null,
            "type": "text",
            "text": "in your `update_relevancy` function, you're trying to send data over the websocket. This seems odd considering the function's purpose is just to update relevancy. You might want to separate concerns here and handle sending updates over the websocket elsewhere in your code.\n\n    pythonCopy code\n\n`async def update_relevancy(self): self.relevancy_score = self.calculate_relevancy(self.parliament.chat_history.truncated_chat_history)`\n\nYou could then handle sending updates over the websocket in a different function or part of your code where it makes more sense to do so.\n\nCan you help me update server.py:\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = await self.parliament.get\\_agents\\_info() # Assuming that this method returns agents' information\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nawait self.update\\_queue(websocket)\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue.to\\_json()}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue.to\\_json()}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()\n\nAnd parliament.py:\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nawait agent.update\\_relevancy()\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\ndef get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T16:41:45.637074Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "e5aca0e9-1c52-41c1-9741-161ef1c12d96",
      "role": "assistant",
      "content": " Here are the main changes I would suggest:\n\nIn parliament.py:\n\n1. Remove the call to agent.update_relevancy() after a new message. Since relevancy is now being calculated asynchronously whenever a new message is received, this additional call is redundant.\n\n2. Make calculate_relevancy() an async method, since it makes an API call:\n\n```py\nasync def calculate_relevancy(self, chat_context: str) -> int:\n    ...\n```\n\n3. Update get_agents_info() to await the call to calculate_relevancy(), since it is now async:\n\n```py\nasync def get_agents_info(self) -> List[dict]:\n    ...\n    relevancy_score = await agent.calculate_relevancy(self.chat_history) \n    ...\n```\n\n4. Remove the call to send the agent info over websocket from inside calculate_relevancy(). As suggested, this muddies the separation of concerns. calculate_relevancy() should simply calculate and update the relevancy score.\n\nIn server.py:\n\n1. Await the call to get_agents_info(), since it is now async:\n\n```py\nagent_info = await self.parliament.get_agents_info()  \n```\n\n2. Move the logic to send the agent info to the client from inside calculate_relevancy() to server.py. A good place would be after calling broadcast_message():\n\n```py\nawait self.parliament.broadcast_message(user_message, sender, timestamp)\nagent_info = await self.parliament.get_agents_info()  \nawait websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info}))\n```\n\nThis helps properly separate the concerns of calculating relevancy scores and sending updates to connected clients.\n\nThe updated code should have:\n\n- An async get_agents_info() method in Parliament \n- Removed redundant calls to agent.update_relevancy()\n- Moved the logic to send agent info updates to the client from calculate_relevancy() to server.py\n\nLet me know if this helps explain the suggested changes or if you have any other questions!",
      "created_at": "2023-05-13T16:42:32.758824Z",
      "sequence_order": 27,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T16:42:32.758824Z",
            "stop_timestamp": "2023-05-13T16:42:32.758824Z",
            "flags": null,
            "type": "text",
            "text": " Here are the main changes I would suggest:\n\nIn parliament.py:\n\n1. Remove the call to agent.update_relevancy() after a new message. Since relevancy is now being calculated asynchronously whenever a new message is received, this additional call is redundant.\n\n2. Make calculate_relevancy() an async method, since it makes an API call:\n\n```py\nasync def calculate_relevancy(self, chat_context: str) -> int:\n    ...\n```\n\n3. Update get_agents_info() to await the call to calculate_relevancy(), since it is now async:\n\n```py\nasync def get_agents_info(self) -> List[dict]:\n    ...\n    relevancy_score = await agent.calculate_relevancy(self.chat_history) \n    ...\n```\n\n4. Remove the call to send the agent info over websocket from inside calculate_relevancy(). As suggested, this muddies the separation of concerns. calculate_relevancy() should simply calculate and update the relevancy score.\n\nIn server.py:\n\n1. Await the call to get_agents_info(), since it is now async:\n\n```py\nagent_info = await self.parliament.get_agents_info()  \n```\n\n2. Move the logic to send the agent info to the client from inside calculate_relevancy() to server.py. A good place would be after calling broadcast_message():\n\n```py\nawait self.parliament.broadcast_message(user_message, sender, timestamp)\nagent_info = await self.parliament.get_agents_info()  \nawait websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info}))\n```\n\nThis helps properly separate the concerns of calculating relevancy scores and sending updates to connected clients.\n\nThe updated code should have:\n\n- An async get_agents_info() method in Parliament \n- Removed redundant calls to agent.update_relevancy()\n- Moved the logic to send agent info updates to the client from calculate_relevancy() to server.py\n\nLet me know if this helps explain the suggested changes or if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T16:42:32.758824Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "05628bab-f546-4c08-a09d-2533d19908bb",
      "role": "human",
      "content": "can you give me the whole async def get\\_agents\\_info(self) -> List\\[dict\\]:?\n\nHere's what I got so far:\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-13T16:45:42.395764Z",
      "sequence_order": 28,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T16:45:42.395764Z",
            "stop_timestamp": "2023-05-13T16:45:42.395764Z",
            "flags": null,
            "type": "text",
            "text": "can you give me the whole async def get\\_agents\\_info(self) -> List\\[dict\\]:?\n\nHere's what I got so far:\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T16:45:42.395764Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "6de894f7-5d7e-4627-8a97-1b716e592eaf",
      "role": "assistant",
      "content": " Here is the updated async get_agents_info() method:\n\n```py\nasync def get_agents_info(self) -> List[dict]:\n    agents_info = []\n    for agent_name, agent in self.agents.items():\n        relevancy_score = await agent.calculate_relevancy(self.chat_history) \n        agents_info.append({\n            \"name\": agent_name,\n            \"speaking_pressure\": agent.speaking_pressure,\n            \"speaking_pressure_increment\": agent.speaking_pressure_increment, \n            \"context\": agent.context,\n            \"agent_type\": agent.agent_type,\n            \"relevancy_score\": relevancy_score  # Return the actual score here\n        })\n    return agents_info \n```\n\nThe main changes are:\n\n1. The method is now defined as async def since it awaits a call\n\n2. The call to agent.calculate_relevancy(self.chat_history) is awaited, since calculate_relevancy() is now an async method\n\n3. The relevancy_score returned is the value from the awaited call to calculate_relevancy()\n\n4. The logic to send the agent info over the websocket has been removed from this method.\n\nThis updated method will gather all the info for each agent, awaiting the asynchronous relevancy calculation for each agent, and return the full list of agent info. The server (or any calling code) is responsible for then sending this info to connected clients if needed.\n\nLet me know if you have any other questions!",
      "created_at": "2023-05-13T16:46:22.368468Z",
      "sequence_order": 29,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T16:46:22.368468Z",
            "stop_timestamp": "2023-05-13T16:46:22.368468Z",
            "flags": null,
            "type": "text",
            "text": " Here is the updated async get_agents_info() method:\n\n```py\nasync def get_agents_info(self) -> List[dict]:\n    agents_info = []\n    for agent_name, agent in self.agents.items():\n        relevancy_score = await agent.calculate_relevancy(self.chat_history) \n        agents_info.append({\n            \"name\": agent_name,\n            \"speaking_pressure\": agent.speaking_pressure,\n            \"speaking_pressure_increment\": agent.speaking_pressure_increment, \n            \"context\": agent.context,\n            \"agent_type\": agent.agent_type,\n            \"relevancy_score\": relevancy_score  # Return the actual score here\n        })\n    return agents_info \n```\n\nThe main changes are:\n\n1. The method is now defined as async def since it awaits a call\n\n2. The call to agent.calculate_relevancy(self.chat_history) is awaited, since calculate_relevancy() is now an async method\n\n3. The relevancy_score returned is the value from the awaited call to calculate_relevancy()\n\n4. The logic to send the agent info over the websocket has been removed from this method.\n\nThis updated method will gather all the info for each agent, awaiting the asynchronous relevancy calculation for each agent, and return the full list of agent info. The server (or any calling code) is responsible for then sending this info to connected clients if needed.\n\nLet me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T16:46:22.368468Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "ebd5e20d-2ae4-4266-8e1f-7b757fac96e1",
      "role": "human",
      "content": "got this error:\n\nQueue updated: \\[\\]\n\n/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py:290: RuntimeWarning: coroutine 'Parliament.update\\_speaking\\_queue' was never awaited\n\n self.update\\_speaking\\_queue()\n\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n\nBroadcast message: hi, hey red hat, say hiiiiii!\n\nCalculating relevancy...\n\nRelevancy score for White Hat: 80%\n\nCalculating relevancy...\n\nRelevancy score for Red Hat: 80%\n\nCalculating relevancy...\n\nRelevancy score for Black Hat: 75%\n\nCalculating relevancy...\n\nRelevancy score for Yellow Hat: 75%\n\nCalculating relevancy...\n\nRelevancy score for Green Hat: 70%\n\nCalculating relevancy...\n\nRelevancy score for Blue Hat: 66%\n\nQueue updated: \\[\\]\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nrelevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-13T16:48:05.994469Z",
      "sequence_order": 30,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T16:48:05.994469Z",
            "stop_timestamp": "2023-05-13T16:48:05.994469Z",
            "flags": null,
            "type": "text",
            "text": "got this error:\n\nQueue updated: \\[\\]\n\n/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py:290: RuntimeWarning: coroutine 'Parliament.update\\_speaking\\_queue' was never awaited\n\n self.update\\_speaking\\_queue()\n\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n\nBroadcast message: hi, hey red hat, say hiiiiii!\n\nCalculating relevancy...\n\nRelevancy score for White Hat: 80%\n\nCalculating relevancy...\n\nRelevancy score for Red Hat: 80%\n\nCalculating relevancy...\n\nRelevancy score for Black Hat: 75%\n\nCalculating relevancy...\n\nRelevancy score for Yellow Hat: 75%\n\nCalculating relevancy...\n\nRelevancy score for Green Hat: 70%\n\nCalculating relevancy...\n\nRelevancy score for Blue Hat: 66%\n\nQueue updated: \\[\\]\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakingQueue:\n\ndef **init**(self):\n\nself.queue = deque()\n\ndef add(self, agent\\_name):\n\nself.queue.append(agent\\_name)\n\ndef pop(self):\n\nif len(self.queue) > 0:\n\nreturn self.queue.pop()\n\nelse:\n\nreturn None\n\ndef enqueue(self, name):\n\nself.queue.append(name)\n\ndef dequeue(self):\n\nreturn self.queue.popleft() if self.queue else None\n\ndef front(self):\n\nreturn self.queue\\[0\\] if self.queue else None\n\ndef remove(self, name):\n\nself.queue.remove(name)\n\ndef contains(self, name):\n\nreturn name in self.queue\n\ndef to\\_json(self):\n\nreturn list(self.queue)\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue.front() == agent.name\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = SpeakingQueue()\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nself.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.dequeue()\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif not self.speaking\\_queue.contains(agent.name):\n\nself.speaking\\_queue.enqueue(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent:\n\nself.speaking\\_queue.enqueue(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nrelevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T16:48:05.994469Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "edf9f196-f994-4d4d-bf4a-d6b318a9d62b",
      "role": "assistant",
      "content": " The RuntimeWarning is occurring because you're calling self.update_speaking_queue() without awaiting it, since it is an async method. To fix this, you'll need to await the call:\n\n```py\nawait self.update_speaking_queue()\n```\n\nThe full updated code would be:\n\n```py\nasync def broadcast_message(self, message, sender, timestamp):\n    ...\n    await self.update_speaking_queue()\n    ...\n```\n\nAwaiting an async method \"pauses\" the execution of the calling method until the awaited method completes. So by awaiting update_speaking_queue(), you ensure it fully completes before broadcast_message() continues executing.\n\nTo fix the empty queue issue, it looks like you may not be properly calling trigger_next_speaker() after updating the speaking queue. I would suggest adding it back at the end of update_speaking_queue():\n\n```py\nasync def update_speaking_queue(self):\n    ...\n    self.add_tagged_agents_to_queue()\n    self.add_high_pressure_agents_to_queue()\n    self.trigger_next_speaker()  # Call this again \n```\n\nThe trigger_next_speaker() call will properly select the next agent to speak from the queue after it has been updated.\n\nSo in summary, the fixes would be:\n\n1. Await the call to self.update_speaking_queue() in broadcast_message()\n2. Add a call to self.trigger_next_speaker() at the end of update_speaking_queue()\n\nPlease let me know if applying these fixes resolves the issues or if you have any other questions!",
      "created_at": "2023-05-13T16:48:51.532764Z",
      "sequence_order": 31,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-13T16:48:51.532764Z",
            "stop_timestamp": "2023-05-13T16:48:51.532764Z",
            "flags": null,
            "type": "text",
            "text": " The RuntimeWarning is occurring because you're calling self.update_speaking_queue() without awaiting it, since it is an async method. To fix this, you'll need to await the call:\n\n```py\nawait self.update_speaking_queue()\n```\n\nThe full updated code would be:\n\n```py\nasync def broadcast_message(self, message, sender, timestamp):\n    ...\n    await self.update_speaking_queue()\n    ...\n```\n\nAwaiting an async method \"pauses\" the execution of the calling method until the awaited method completes. So by awaiting update_speaking_queue(), you ensure it fully completes before broadcast_message() continues executing.\n\nTo fix the empty queue issue, it looks like you may not be properly calling trigger_next_speaker() after updating the speaking queue. I would suggest adding it back at the end of update_speaking_queue():\n\n```py\nasync def update_speaking_queue(self):\n    ...\n    self.add_tagged_agents_to_queue()\n    self.add_high_pressure_agents_to_queue()\n    self.trigger_next_speaker()  # Call this again \n```\n\nThe trigger_next_speaker() call will properly select the next agent to speak from the queue after it has been updated.\n\nSo in summary, the fixes would be:\n\n1. Await the call to self.update_speaking_queue() in broadcast_message()\n2. Add a call to self.trigger_next_speaker() at the end of update_speaking_queue()\n\nPlease let me know if applying these fixes resolves the issues or if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-13T16:48:51.532764Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "a90e301a-a076-4142-95e0-55d7fd06533d",
      "role": "human",
      "content": "now getting this error:\n\nTraceback (most recent call last):\n\nFile \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 240, in handler\n\nawait self.ws\\_handler(self)\n\nFile \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n\nreturn await cast(\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 37, in chat\n\nawait self.handle\\_send\\_message(websocket, data)\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 75, in handle\\_send\\_message\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 259, in broadcast\\_message\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nUnboundLocalError: local variable 'first\\_agent\\_in\\_queue' referenced before assignment\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and most\\_relevant\\_agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nself.trigger\\_next\\_speaker()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nrelevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-14T00:06:02.870742Z",
      "sequence_order": 32,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T00:06:02.870742Z",
            "stop_timestamp": "2023-05-14T00:06:02.870742Z",
            "flags": null,
            "type": "text",
            "text": "now getting this error:\n\nTraceback (most recent call last):\n\nFile \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 240, in handler\n\nawait self.ws\\_handler(self)\n\nFile \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n\nreturn await cast(\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 37, in chat\n\nawait self.handle\\_send\\_message(websocket, data)\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 75, in handle\\_send\\_message\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 259, in broadcast\\_message\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nUnboundLocalError: local variable 'first\\_agent\\_in\\_queue' referenced before assignment\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and most\\_relevant\\_agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nself.trigger\\_next\\_speaker()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nrelevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T00:06:02.870742Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "5638edb7-1a6f-45b9-8fe4-0cfc4ccfb5fa",
      "role": "assistant",
      "content": " The UnboundLocalError is occurring because first_agent_in_queue is being referenced before it is assigned. This is likely happening because:\n\n1. The call to self.speaking_queue.pop(0) is failing, since the queue is empty.\n\n2. first_agent_in_queue is then being referenced, causing the error.\n\nTo fix this, I would suggest adding a check to ensure the speaking queue is not empty before calling .pop(0):\n\n```py\nasync def broadcast_message(self, message, sender, timestamp):\n    ...\n    first_agent_in_queue = None\n    if self.speaking_queue: \n        first_agent_in_queue = self.speaking_queue.pop(0)  # Add check \n    ...\n    if first_agent_in_queue and not self.get_typing_status(first_agent_in_queue):\n        await self.agents[first_agent_in_queue].speak()\n```\n\nThis will assign first_agent_in_queue to None if the speaking queue is empty, avoiding the reference before assignment error.\n\nAnother option would be to catch the IndexError from calling .pop(0) on an empty list:\n\n```py\nasync def broadcast_message(self, message, sender, timestamp):\n    ...\n    try: \n        first_agent_in_queue = self.speaking_queue.pop(0)  \n    except IndexError:\n        first_agent_in_queue = None\n    ...\n    if first_agent_in_queue and not self.get_typing_status(first_agent_in_queue):\n        await self.agents[first_agent_in_queue].speak()\n```\n\nEither of these options should resolve the UnboundLocalError by handling the case where the speaking queue is empty. Let me know if this helps or if you have any other questions!",
      "created_at": "2023-05-14T00:07:10.301801Z",
      "sequence_order": 33,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T00:07:10.301801Z",
            "stop_timestamp": "2023-05-14T00:07:10.301801Z",
            "flags": null,
            "type": "text",
            "text": " The UnboundLocalError is occurring because first_agent_in_queue is being referenced before it is assigned. This is likely happening because:\n\n1. The call to self.speaking_queue.pop(0) is failing, since the queue is empty.\n\n2. first_agent_in_queue is then being referenced, causing the error.\n\nTo fix this, I would suggest adding a check to ensure the speaking queue is not empty before calling .pop(0):\n\n```py\nasync def broadcast_message(self, message, sender, timestamp):\n    ...\n    first_agent_in_queue = None\n    if self.speaking_queue: \n        first_agent_in_queue = self.speaking_queue.pop(0)  # Add check \n    ...\n    if first_agent_in_queue and not self.get_typing_status(first_agent_in_queue):\n        await self.agents[first_agent_in_queue].speak()\n```\n\nThis will assign first_agent_in_queue to None if the speaking queue is empty, avoiding the reference before assignment error.\n\nAnother option would be to catch the IndexError from calling .pop(0) on an empty list:\n\n```py\nasync def broadcast_message(self, message, sender, timestamp):\n    ...\n    try: \n        first_agent_in_queue = self.speaking_queue.pop(0)  \n    except IndexError:\n        first_agent_in_queue = None\n    ...\n    if first_agent_in_queue and not self.get_typing_status(first_agent_in_queue):\n        await self.agents[first_agent_in_queue].speak()\n```\n\nEither of these options should resolve the UnboundLocalError by handling the case where the speaking queue is empty. Let me know if this helps or if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T00:07:10.301801Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "f7681fe8-542d-4fed-b319-7cefc7bd6d34",
      "role": "human",
      "content": "why does everyone start calculating their relevancy before a user says anything?\n\nAlso, why is the queue not updated with the most relevant agent?\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and most\\_relevant\\_agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nself.trigger\\_next\\_speaker()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nrelevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = await self.parliament.get\\_agents\\_info()\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nawait self.update\\_queue(websocket)\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()",
      "created_at": "2023-05-14T00:09:28.299490Z",
      "sequence_order": 34,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T00:09:28.299490Z",
            "stop_timestamp": "2023-05-14T00:09:28.299490Z",
            "flags": null,
            "type": "text",
            "text": "why does everyone start calculating their relevancy before a user says anything?\n\nAlso, why is the queue not updated with the most relevant agent?\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number. How relevant, from 0-100 is this context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-curie-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nawait asyncio.sleep(PRESSURE\\_INCREMENT\\_INTERVAL)\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, chat\\_history):\n\nsystem\\_msg = self.initial\\_context\n\nmessages = \\[{\"role\": \"system\", \"content\": system\\_msg}\\]\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False # Initialize typing status when adding an agent\n\nasyncio.create\\_task(agent.increment\\_pressure())\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and most\\_relevant\\_agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nself.trigger\\_next\\_speaker()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nrelevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = await self.parliament.get\\_agents\\_info()\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nawait self.update\\_queue(websocket)\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T00:09:28.299490Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "29a0e7bb-e8e6-4287-aff0-9e263074d3e3",
      "role": "assistant",
      "content": " There are a few issues here:\n\n1. Relevancy scores are being calculated immediately when agents are added, before any user messages. This is because agent.update_relevancy() is being called in Parliament.add_agent(). To fix this, remove the call to asyncio.create_task(agent.update_relevancy()) from add_agent(). Relevancy should only be calculated after new user messages are received.\n\n2. The most relevant agent is not being added to the front of the speaking queue. This is because the logic in update_speaking_queue() to add the most relevant agent is incorrectly checking if the agent's name is in the queue at all, rather than at the front. To fix this, change:\n\n```py\nif most_relevant_agent and most_relevant_agent.name not in self.speaking_queue:\n    self.speaking_queue.append(most_relevant_agent.name) \n```\n\nTo:\n\n```py\nif most_relevant_agent and self.speaking_queue and self.speaking_queue[0] != most_relevant_agent.name:\n    self.speaking_queue.insert(0, most_relevant_agent.name) \n```\n\nThis will insert the most relevant agent at the front of the queue.\n\nThe updated code would be:\n\n```py\ndef add_agent(self, agent: Agent):\n    self.agents[agent.name] = agent\n    self.typing_status[agent.name] = False  \n\nasync def update_speaking_queue(self):\n    ...\n    if most_relevant_agent and self.speaking_queue and self.speaking_queue[0] != most_relevant_agent.name:\n        self.speaking_queue.insert(0, most_relevant_agent.name)  \n```\n\nWith these fixes, relevancy scores should only be calculated after new user messages are received, and the most relevant agent should properly be added to the front of the speaking queue.\n\nLet me know if this helps explain and resolve the issues, or if you have any other questions!",
      "created_at": "2023-05-14T00:11:17.357940Z",
      "sequence_order": 35,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T00:11:17.357940Z",
            "stop_timestamp": "2023-05-14T00:11:17.357940Z",
            "flags": null,
            "type": "text",
            "text": " There are a few issues here:\n\n1. Relevancy scores are being calculated immediately when agents are added, before any user messages. This is because agent.update_relevancy() is being called in Parliament.add_agent(). To fix this, remove the call to asyncio.create_task(agent.update_relevancy()) from add_agent(). Relevancy should only be calculated after new user messages are received.\n\n2. The most relevant agent is not being added to the front of the speaking queue. This is because the logic in update_speaking_queue() to add the most relevant agent is incorrectly checking if the agent's name is in the queue at all, rather than at the front. To fix this, change:\n\n```py\nif most_relevant_agent and most_relevant_agent.name not in self.speaking_queue:\n    self.speaking_queue.append(most_relevant_agent.name) \n```\n\nTo:\n\n```py\nif most_relevant_agent and self.speaking_queue and self.speaking_queue[0] != most_relevant_agent.name:\n    self.speaking_queue.insert(0, most_relevant_agent.name) \n```\n\nThis will insert the most relevant agent at the front of the queue.\n\nThe updated code would be:\n\n```py\ndef add_agent(self, agent: Agent):\n    self.agents[agent.name] = agent\n    self.typing_status[agent.name] = False  \n\nasync def update_speaking_queue(self):\n    ...\n    if most_relevant_agent and self.speaking_queue and self.speaking_queue[0] != most_relevant_agent.name:\n        self.speaking_queue.insert(0, most_relevant_agent.name)  \n```\n\nWith these fixes, relevancy scores should only be calculated after new user messages are received, and the most relevant agent should properly be added to the front of the speaking queue.\n\nLet me know if this helps explain and resolve the issues, or if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T00:11:17.357940Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "77f4fa0e-3ff9-4262-9bef-61cbd36cacd2",
      "role": "human",
      "content": "can you assess the calculate relevancy function here:\n\nDo we need to do this part?\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nAlso here:\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nShouldn't we only have the agent speak with the most recent context history of the chat? the truncated chat, this seem like it's sending lots of messages?",
      "created_at": "2023-05-14T00:24:26.905489Z",
      "sequence_order": 36,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T00:24:26.905489Z",
            "stop_timestamp": "2023-05-14T00:24:26.905489Z",
            "flags": null,
            "type": "text",
            "text": "can you assess the calculate relevancy function here:\n\nDo we need to do this part?\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score # Return updated score here\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nAlso here:\n\nfor message in chat\\_history.chat\\_history:\n\nrole = message\\[\"sender\"\\]\n\nmsg = message\\[\"message\"\\]\n\nmessages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n\nShouldn't we only have the agent speak with the most recent context history of the chat? the truncated chat, this seem like it's sending lots of messages?",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T00:24:26.905489Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "ccadfc5f-af29-4ccc-9a82-aed040be5540",
      "role": "assistant",
      "content": " You're right, there are a few issues with the calculate_relevancy() method and how messages are being collected:\n\n1. The logic to send the agent's updated relevancy_score to the client is redundant inside calculate_relevancy(). Since relevancy scores are now only being updated after new user messages are received, and the full list of agent info is being sent to the client after those messages, this additional update is not needed. I would remove:\n\n```py\nagent_info = [\n    {\n        \"name\": self.name,\n        \"relevancy_score\": self.relevancy_score  \n    }\n]\nawait self.websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info})) \n```\n\n2. All messages are being collected to send to the OpenAI API, rather than just the most recent context. This can result in too much context being sent, confusing the model. I would modify this to only use the truncated_chat_history:\n\n```py\nmessages = [{\"role\": \"system\", \"content\": system_msg}]\nfor message in self.parliament.chat_history.truncated_chat_history:\n    role = message[\"sender\"]\n    msg = message[\"message\"]\n    messages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n```\n\nThis will collect only the most recent truncated_chat_history to send to the OpenAI API.\n\n3. Consider caching the OpenAI response to avoid hitting the rate limit. You can store the truncated_chat_history as a key in a cache, with the model response as the value. Then, before calling the OpenAI API, check if that truncated_chat_history exists in the cache. If so, return the cached response instead of calling the API again.\n\nWith these changes, the updated calculate_relevancy() method would be:\n```py\nasync def calculate_relevancy(self, chat_context: str) -> int:\n    messages = [{\"role\": \"system\", \"content\": system_msg}]\n    for message in self.parliament.chat_history.truncated_chat_history:\n        role = message[\"sender\"]\n        msg = message[\"message\"]\n        messages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})  \n\n    # Try retrieving cached response first \n    if truncated_chat_history in cache:\n        response = cache[truncated_chat_history]\n    else:\n        # Call OpenAI API\n        response = openai.ChatCompletion.create(messages=messages, ...) \n        # Cache the response \n        cache[truncated_chat_history] = response\n    \n    # Remaining logic...\n```\n\nThese changes should help optimize the relevancy calculation method and avoid issues from too much context or hitting rate limits. Let me know if you have any other questions!",
      "created_at": "2023-05-14T00:25:29.409102Z",
      "sequence_order": 37,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T00:25:29.409102Z",
            "stop_timestamp": "2023-05-14T00:25:29.409102Z",
            "flags": null,
            "type": "text",
            "text": " You're right, there are a few issues with the calculate_relevancy() method and how messages are being collected:\n\n1. The logic to send the agent's updated relevancy_score to the client is redundant inside calculate_relevancy(). Since relevancy scores are now only being updated after new user messages are received, and the full list of agent info is being sent to the client after those messages, this additional update is not needed. I would remove:\n\n```py\nagent_info = [\n    {\n        \"name\": self.name,\n        \"relevancy_score\": self.relevancy_score  \n    }\n]\nawait self.websocket.send(json.dumps({\"action\": UPDATE_AGENTS, \"agents\": agent_info})) \n```\n\n2. All messages are being collected to send to the OpenAI API, rather than just the most recent context. This can result in too much context being sent, confusing the model. I would modify this to only use the truncated_chat_history:\n\n```py\nmessages = [{\"role\": \"system\", \"content\": system_msg}]\nfor message in self.parliament.chat_history.truncated_chat_history:\n    role = message[\"sender\"]\n    msg = message[\"message\"]\n    messages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})\n```\n\nThis will collect only the most recent truncated_chat_history to send to the OpenAI API.\n\n3. Consider caching the OpenAI response to avoid hitting the rate limit. You can store the truncated_chat_history as a key in a cache, with the model response as the value. Then, before calling the OpenAI API, check if that truncated_chat_history exists in the cache. If so, return the cached response instead of calling the API again.\n\nWith these changes, the updated calculate_relevancy() method would be:\n```py\nasync def calculate_relevancy(self, chat_context: str) -> int:\n    messages = [{\"role\": \"system\", \"content\": system_msg}]\n    for message in self.parliament.chat_history.truncated_chat_history:\n        role = message[\"sender\"]\n        msg = message[\"message\"]\n        messages.append({\"role\": \"assistant\" if role != \"assistant\" else role, \"content\": msg})  \n\n    # Try retrieving cached response first \n    if truncated_chat_history in cache:\n        response = cache[truncated_chat_history]\n    else:\n        # Call OpenAI API\n        response = openai.ChatCompletion.create(messages=messages, ...) \n        # Cache the response \n        cache[truncated_chat_history] = response\n    \n    # Remaining logic...\n```\n\nThese changes should help optimize the relevancy calculation method and avoid issues from too much context or hitting rate limits. Let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T00:25:29.409102Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "13b08737-836d-47e9-8ba8-fc3fa45eeaf1",
      "role": "human",
      "content": "does this clean the issue with the response?\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue and self.speaking\\_queue\\[0\\] != most\\_relevant\\_agent.name:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nself.trigger\\_next\\_speaker()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nrelevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-14T00:35:06.076588Z",
      "sequence_order": 38,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T00:35:06.076588Z",
            "stop_timestamp": "2023-05-14T00:35:06.076588Z",
            "flags": null,
            "type": "text",
            "text": "does this clean the issue with the response?\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue and self.speaking\\_queue\\[0\\] != most\\_relevant\\_agent.name:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nself.trigger\\_next\\_speaker()\n\ndef get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\nreturn sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nrelevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T00:35:06.076588Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "625f0e7e-5f12-47a8-92b0-837b4cfe82a8",
      "role": "assistant",
      "content": "  Passing just the truncated_chat to the OpenAI agent's get_response() method should help resolve some of the issues, by only providing the most recent context. However, there are a few other optimizations needed:\n\n1. The OpenAIAgent class should cache responses to avoid repeated calls with the same input. This can be done by storing the truncated_chat as a key in a cache, and the response as the value. Then, the get_response() method should first check if that truncated_chat exists in the cache. If so, return the cached response. If not, call the OpenAI API and cache the result. This will prevent hitting rate limits from sending duplicate inputs.\n\n2. The message passed to OpenAI should contain the full context, in addition to just the most recent truncated_chat. The model needs sufficient context to generate a coherent response, so passing the entire conversation history is important. However, to avoid very long inputs, the chat history can be truncated to the last X number of messages.\n\n3. The OpenAI prompt should request responses that continue or extend the conversation, rather than just responding to the question \"How would you continue this conversation?\". A prompt like:\n\nContinue this conversation by responding to the previous message and advancing the discussion.\n\n[context]",
      "created_at": "2023-05-14T00:36:43.278874Z",
      "sequence_order": 39,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T00:36:43.278874Z",
            "stop_timestamp": "2023-05-14T00:36:43.278874Z",
            "flags": null,
            "type": "text",
            "text": "  Passing just the truncated_chat to the OpenAI agent's get_response() method should help resolve some of the issues, by only providing the most recent context. However, there are a few other optimizations needed:\n\n1. The OpenAIAgent class should cache responses to avoid repeated calls with the same input. This can be done by storing the truncated_chat as a key in a cache, and the response as the value. Then, the get_response() method should first check if that truncated_chat exists in the cache. If so, return the cached response. If not, call the OpenAI API and cache the result. This will prevent hitting rate limits from sending duplicate inputs.\n\n2. The message passed to OpenAI should contain the full context, in addition to just the most recent truncated_chat. The model needs sufficient context to generate a coherent response, so passing the entire conversation history is important. However, to avoid very long inputs, the chat history can be truncated to the last X number of messages.\n\n3. The OpenAI prompt should request responses that continue or extend the conversation, rather than just responding to the question \"How would you continue this conversation?\". A prompt like:\n\nContinue this conversation by responding to the previous message and advancing the discussion.\n\n[context]",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T00:36:43.278874Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "f504a832-83bb-4f42-bc52-fc014d3ba109",
      "role": "human",
      "content": "do we need relevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nin async def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nrelevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info\n\n?\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number by answering how relevant, from 0-100 is this agent and their context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nprint(prompt)\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nprint(completions)\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\ntruncated\\_chat = self.chat\\_history.truncated\\_chat\\_history\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(truncated\\_chat)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue and self.speaking\\_queue\\[0\\] != most\\_relevant\\_agent.name:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nself.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nrelevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-14T00:41:07.391036Z",
      "sequence_order": 40,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T00:41:07.391036Z",
            "stop_timestamp": "2023-05-14T00:41:07.391036Z",
            "flags": null,
            "type": "text",
            "text": "do we need relevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nin async def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nrelevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info\n\n?\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\nasync def update\\_relevancy(self):\n\nself.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nagent\\_info = \\[\n\n{\n\n\"name\": self.name,\n\n\"relevancy\\_score\": self.relevancy\\_score\n\n}\n\n\\]\n\nawait self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number by answering how relevant, from 0-100 is this agent and their context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nprint(prompt)\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nprint(completions)\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\ntruncated\\_chat = self.chat\\_history.truncated\\_chat\\_history\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(truncated\\_chat)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \\[\\]\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\nchat\\_context = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(chat\\_context)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue and self.speaking\\_queue\\[0\\] != most\\_relevant\\_agent.name:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nself.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nrelevancy\\_score = await agent.calculate\\_relevancy(self.chat\\_history)\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": relevancy\\_score # Return the actual score here\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T00:41:07.391036Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "622a4e19-5931-445a-a6d7-46f5c3d477c1",
      "role": "assistant",
      "content": " You're correct, calculating each agent's relevancy score based on the full chat history in get_agents_info() is redundant. Since relevancy scores are now being updated for each agent after every new user message, the scores returned from get_agents_info() will be outdated. \n\nI would suggest removing:\n\n```py\nrelevancy_score = await agent.calculate_relevancy(self.chat_history) \n``` \n\nAnd instead returning the agent's cached relevancy_score:\n\n```py\nrelevancy_score = agent.relevancy_score  \n```\n\nThe updated get_agents_info() method would be:\n\n```py\nasync def get_agents_info(self) -> List[dict]:\n    agents_info = []\n    for agent_name, agent in self.agents.items():\n        agents_info.append({\n            \"name\": agent_name,\n            \"speaking_pressure\": agent.speaking_pressure,\n            \"speaking_pressure_increment\": agent.speaking_pressure_increment,\n            \"context\": agent.context,\n            \"agent_type\": agent.agent_type,\n            \"relevancy_score\": agent.relevancy_score  # Return the cached score\n        })\n    return agents_info\n```\n\nWith this change, get_agents_info() will simply return each agent's latest cached relevancy score, rather than calculating a new score based on the full chat history.\n\nLet me know if this makes sense or if you have any other questions!",
      "created_at": "2023-05-14T00:42:06.098886Z",
      "sequence_order": 41,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T00:42:06.098886Z",
            "stop_timestamp": "2023-05-14T00:42:06.098886Z",
            "flags": null,
            "type": "text",
            "text": " You're correct, calculating each agent's relevancy score based on the full chat history in get_agents_info() is redundant. Since relevancy scores are now being updated for each agent after every new user message, the scores returned from get_agents_info() will be outdated. \n\nI would suggest removing:\n\n```py\nrelevancy_score = await agent.calculate_relevancy(self.chat_history) \n``` \n\nAnd instead returning the agent's cached relevancy_score:\n\n```py\nrelevancy_score = agent.relevancy_score  \n```\n\nThe updated get_agents_info() method would be:\n\n```py\nasync def get_agents_info(self) -> List[dict]:\n    agents_info = []\n    for agent_name, agent in self.agents.items():\n        agents_info.append({\n            \"name\": agent_name,\n            \"speaking_pressure\": agent.speaking_pressure,\n            \"speaking_pressure_increment\": agent.speaking_pressure_increment,\n            \"context\": agent.context,\n            \"agent_type\": agent.agent_type,\n            \"relevancy_score\": agent.relevancy_score  # Return the cached score\n        })\n    return agents_info\n```\n\nWith this change, get_agents_info() will simply return each agent's latest cached relevancy score, rather than calculating a new score based on the full chat history.\n\nLet me know if this makes sense or if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T00:42:06.098886Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "0cf959c6-8399-4e12-b5b0-4ffe2ac3cacb",
      "role": "human",
      "content": "calculate relevancy is not working:\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\n\\# async def update\\_relevancy(self):\n\n\\# self.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\n\\# agent\\_info = \\[\n\n\\# {\n\n\\# \"name\": self.name,\n\n\\# \"relevancy\\_score\": self.relevancy\\_score\n\n\\# }\n\n\\# \\]\n\n\\# await self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number by answering how relevant, from 0-100 is this agent and their context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nprint(prompt)\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nprint(completions)\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, truncated\\_chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\ntruncated\\_chat\\_history = self.parliament.chat\\_history.truncated\\_chat\\_history\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \"\"\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue and self.speaking\\_queue\\[0\\] != most\\_relevant\\_agent.name:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nself.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-14T00:53:25.504358Z",
      "sequence_order": 42,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T00:53:25.504358Z",
            "stop_timestamp": "2023-05-14T00:53:25.504358Z",
            "flags": null,
            "type": "text",
            "text": "calculate relevancy is not working:\n\nimport asyncio\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\n\\# async def update\\_relevancy(self):\n\n\\# self.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\n\\# agent\\_info = \\[\n\n\\# {\n\n\\# \"name\": self.name,\n\n\\# \"relevancy\\_score\": self.relevancy\\_score\n\n\\# }\n\n\\# \\]\n\n\\# await self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context: str) -> int:\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"Return the result as a number by answering how relevant, from 0-100 is this agent and their context: '{self.initial\\_context}' for this conversation: '{chat\\_context}', remember only respond with a number from 0 to 100? \"\n\nprint(prompt)\n\nmax\\_attempts = 3\n\nattempt = 1\n\nrelevancy\\_score = 0\n\nwhile attempt <= max\\_attempts:\n\ntry:\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\",\n\nprompt=prompt,\n\nmax\\_tokens=150,\n\ntemperature=0.0,\n\nn=1,\n\nstop=None\n\n)\n\ncompletions = response.choices\\[0\\].text.strip().split(\"\\\\n\")\n\nprint(completions)\n\nscore = int(completions\\[-1\\].strip())\n\nif 0 <= score <= 100: # Check if score is within valid range\n\nrelevancy\\_score = score\n\nbreak # Break out of the loop if a valid relevancy score is obtained\n\nelse:\n\nprint(\"Invalid response received. Please enter a number from 0 to 100.\")\n\nexcept (ValueError, KeyError):\n\nprint(\"Invalid response received. Please enter a valid number.\")\n\nattempt += 1\n\nself.initial\\_context = chat\\_context\n\nself.relevancy\\_score = relevancy\\_score\n\nprint(f\"Relevancy score for {self.name}: {self.relevancy\\_score}%\")\n\nelse:\n\nprint(\"Using cached relevancy score...\")\n\nreturn min(self.relevancy\\_score, 100) # Cap maximum score at 100\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, truncated\\_chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\ntruncated\\_chat\\_history = self.parliament.chat\\_history.truncated\\_chat\\_history\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \"\"\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue and self.speaking\\_queue\\[0\\] != most\\_relevant\\_agent.name:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nself.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T00:53:25.504358Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "fba71357-10fa-4bd5-83e0-4c62eb902351",
      "role": "assistant",
      "content": " There are a few issues I see in the updated code:\n\n1. calculate_relevancy() is now taking the truncated_chat_history as input, rather than the full chat_history. While using the most recent context is good, the model may need more context to appropriately determine relevancy. I would suggest passing the full chat_history again, but truncating it to a max length to avoid overly long inputs.\n\n2. The prompt provided to OpenAI asks how relevant the agent and their initial_context are to the current conversation. However, relevancy should be determined based on the agent's potential response or contribution, not just their initial context. The prompt should be reworked to consider the agent's hypothetical response.\n\n3. The logic to get the speaking queue front agent's name and check if the most relevant agent's name matches it seems incorrect. This logic assumes there can only be one agent at the front of the queue at a time, which may not be the case. I would rework this logic.\n\nTo address these issues, I would suggest the following changes:\n\n1. In calculate_relevancy(), pass the full chat_history again but truncate it to a max length:\n\n```py\nprompt = f\"Return the result as a number by answering how relevant, from 0-100 is this agent and their potential response for this conversation: '{self.chat_history[-2000:]}', remember only respond with a number from 0 to 100? \" \n```\n\n2. Rework the prompt to consider the agent's hypothetical response:\n\n```py\nprompt = f\"Return the result as a number by answering how relevant, from 0-100 would this agent's response be for continuing and advancing this conversation: '{self.chat_history[-2000:]}', remember only respond with a number from 0 to 100? \" \n```\n\n3. In update_speaking_queue(), rework the logic to check if the most relevant agent is at the front of the queue. A possible fix would be:\n\n```py \n# Check if the most relevant agent is already at the front of the queue \nfront_agents = [agent.name for agent in self.speaking_queue[:3]]\nif most_relevant_agent.name not in front_agents: \n    self.speaking_queue.insert(0, most_relevant_agent.name)\n```\n\nThis will insert the most relevant agent at the front of the queue only if they are not already in the top 3 agents.\n\nWith these changes, the relevancy calculation and queue logic should work as intended. Let me know if you have any other questions!",
      "created_at": "2023-05-14T00:56:03.481178Z",
      "sequence_order": 43,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T00:56:03.481178Z",
            "stop_timestamp": "2023-05-14T00:56:03.481178Z",
            "flags": null,
            "type": "text",
            "text": " There are a few issues I see in the updated code:\n\n1. calculate_relevancy() is now taking the truncated_chat_history as input, rather than the full chat_history. While using the most recent context is good, the model may need more context to appropriately determine relevancy. I would suggest passing the full chat_history again, but truncating it to a max length to avoid overly long inputs.\n\n2. The prompt provided to OpenAI asks how relevant the agent and their initial_context are to the current conversation. However, relevancy should be determined based on the agent's potential response or contribution, not just their initial context. The prompt should be reworked to consider the agent's hypothetical response.\n\n3. The logic to get the speaking queue front agent's name and check if the most relevant agent's name matches it seems incorrect. This logic assumes there can only be one agent at the front of the queue at a time, which may not be the case. I would rework this logic.\n\nTo address these issues, I would suggest the following changes:\n\n1. In calculate_relevancy(), pass the full chat_history again but truncate it to a max length:\n\n```py\nprompt = f\"Return the result as a number by answering how relevant, from 0-100 is this agent and their potential response for this conversation: '{self.chat_history[-2000:]}', remember only respond with a number from 0 to 100? \" \n```\n\n2. Rework the prompt to consider the agent's hypothetical response:\n\n```py\nprompt = f\"Return the result as a number by answering how relevant, from 0-100 would this agent's response be for continuing and advancing this conversation: '{self.chat_history[-2000:]}', remember only respond with a number from 0 to 100? \" \n```\n\n3. In update_speaking_queue(), rework the logic to check if the most relevant agent is at the front of the queue. A possible fix would be:\n\n```py \n# Check if the most relevant agent is already at the front of the queue \nfront_agents = [agent.name for agent in self.speaking_queue[:3]]\nif most_relevant_agent.name not in front_agents: \n    self.speaking_queue.insert(0, most_relevant_agent.name)\n```\n\nThis will insert the most relevant agent at the front of the queue only if they are not already in the top 3 agents.\n\nWith these changes, the relevancy calculation and queue logic should work as intended. Let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T00:56:03.481178Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "e86b285a-12f7-4d78-95d1-bd09ed4cd877",
      "role": "human",
      "content": "in the following code, even after the relevancy is calculated no one speaks, why?\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = await self.parliament.get\\_agents\\_info()\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()\n\nimport asyncio\n\nimport re\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\n\\# async def update\\_relevancy(self):\n\n\\# self.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\n\\# agent\\_info = \\[\n\n\\# {\n\n\\# \"name\": self.name,\n\n\\# \"relevancy\\_score\": self.relevancy\\_score\n\n\\# }\n\n\\# \\]\n\n\\# await self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context):\n\nrelevancy\\_score = 0\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"you are '{self.initial\\_context}' and you are being asked to respond to '{chat\\_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n\nprint(f\"Prompt: {prompt}\")\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\", # or \"text-curie-002\" for Curie\n\nprompt=prompt,\n\ntemperature=0.0,\n\nmax\\_tokens=300\n\n)\n\nprint(f\"Response: {response}\")\n\n\\# Here we assume the model's response will be a single number.\n\n\\# The strip is used to remove any whitespace around the number\n\nstripped\\_response = re.sub(\"\\[^0-9\\]\", \"\", response.choices\\[0\\].text)\n\ntry:\n\nrelevancy\\_score = int(stripped\\_response)\n\nexcept ValueError:\n\nrelevancy\\_score = 0\n\nprint(f\"Relevancy score: {relevancy\\_score}\")\n\nelse:\n\nprint(\"Relevancy score already calculated.\")\n\ntime.sleep(.5)\n\nreturn relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, truncated\\_chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\ntruncated\\_chat\\_history = self.parliament.chat\\_history.truncated\\_chat\\_history\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \"\"\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue and self.speaking\\_queue\\[0\\] != most\\_relevant\\_agent.name:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nself.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-14T01:17:31.265167Z",
      "sequence_order": 44,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:17:31.265167Z",
            "stop_timestamp": "2023-05-14T01:17:31.265167Z",
            "flags": null,
            "type": "text",
            "text": "in the following code, even after the relevancy is calculated no one speaks, why?\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = await self.parliament.get\\_agents\\_info()\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()\n\nimport asyncio\n\nimport re\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\n\\# async def update\\_relevancy(self):\n\n\\# self.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\n\\# agent\\_info = \\[\n\n\\# {\n\n\\# \"name\": self.name,\n\n\\# \"relevancy\\_score\": self.relevancy\\_score\n\n\\# }\n\n\\# \\]\n\n\\# await self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context):\n\nrelevancy\\_score = 0\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"you are '{self.initial\\_context}' and you are being asked to respond to '{chat\\_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n\nprint(f\"Prompt: {prompt}\")\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\", # or \"text-curie-002\" for Curie\n\nprompt=prompt,\n\ntemperature=0.0,\n\nmax\\_tokens=300\n\n)\n\nprint(f\"Response: {response}\")\n\n\\# Here we assume the model's response will be a single number.\n\n\\# The strip is used to remove any whitespace around the number\n\nstripped\\_response = re.sub(\"\\[^0-9\\]\", \"\", response.choices\\[0\\].text)\n\ntry:\n\nrelevancy\\_score = int(stripped\\_response)\n\nexcept ValueError:\n\nrelevancy\\_score = 0\n\nprint(f\"Relevancy score: {relevancy\\_score}\")\n\nelse:\n\nprint(\"Relevancy score already calculated.\")\n\ntime.sleep(.5)\n\nreturn relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, truncated\\_chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\ntruncated\\_chat\\_history = self.parliament.chat\\_history.truncated\\_chat\\_history\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \"\"\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker = self.get\\_agent(self.speaking\\_queue.pop()) # Remove the agent from the queue\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue and self.speaking\\_queue\\[0\\] != most\\_relevant\\_agent.name:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nself.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:17:31.265167Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "1dc8a6e0-c07f-422d-b40e-fd8e0bbf9ffd",
      "role": "assistant",
      "content": "  There are a few possible issues here preventing agents from speaking:\n\n1. The call to self.trigger_next_speaker() may be missing from update_speaking_queue(). Without calling this method, no agent will be selected to speak from the queue. I would suggest adding it back:\n\n```py\nasync def update_speaking_queue(self):\n    ...\n    self.trigger_next_speaker()\n```\n\n2. The logic in should_speak() to check if an agent is at the front of the queue may be incorrect. It is currently checking if the queue's first element matches the agent's name, but the queue could contain multiple agents with the same relevancy score. I would suggest reworking this logic to check if the agent's name is in the first N positions of the queue, e.g:\n\n```py\nfront_queue_agents = self.speaking_queue[:3]  # Top 3 agents \nreturn agent.name in front_queue_agents\n```\n\n3. The await call to self.agents[first_agent_in_queue].speak() may be trying to call a non-async function speak(). To fix this, speak() would need to be made async.\n\n4. There may be an issue with how agents are added to the speaking queue in update_speaking_queue(). The logic should be re-checked to ensure agents are added properly.\n\nTo summarize, here are some suggestions to try:\n\n1. Add a call to self.trigger_next_speaker() in update_speaking_queue()\n2. Rework the logic in should_speak() to check if the agent's name is in the first N positions of the queue \n3. Ensure speak() is defined as an async method \n4. Double check the logic in update_speaking_queue() to add agents to the speaking queue\n\nMaking these fixes and changes may resolve the issue preventing agents from speaking after their relevancy is calculated. Please let me know if these help fix the issue or if you have any other questions!",
      "created_at": "2023-05-14T01:18:44.407815Z",
      "sequence_order": 45,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:18:44.407815Z",
            "stop_timestamp": "2023-05-14T01:18:44.407815Z",
            "flags": null,
            "type": "text",
            "text": "  There are a few possible issues here preventing agents from speaking:\n\n1. The call to self.trigger_next_speaker() may be missing from update_speaking_queue(). Without calling this method, no agent will be selected to speak from the queue. I would suggest adding it back:\n\n```py\nasync def update_speaking_queue(self):\n    ...\n    self.trigger_next_speaker()\n```\n\n2. The logic in should_speak() to check if an agent is at the front of the queue may be incorrect. It is currently checking if the queue's first element matches the agent's name, but the queue could contain multiple agents with the same relevancy score. I would suggest reworking this logic to check if the agent's name is in the first N positions of the queue, e.g:\n\n```py\nfront_queue_agents = self.speaking_queue[:3]  # Top 3 agents \nreturn agent.name in front_queue_agents\n```\n\n3. The await call to self.agents[first_agent_in_queue].speak() may be trying to call a non-async function speak(). To fix this, speak() would need to be made async.\n\n4. There may be an issue with how agents are added to the speaking queue in update_speaking_queue(). The logic should be re-checked to ensure agents are added properly.\n\nTo summarize, here are some suggestions to try:\n\n1. Add a call to self.trigger_next_speaker() in update_speaking_queue()\n2. Rework the logic in should_speak() to check if the agent's name is in the first N positions of the queue \n3. Ensure speak() is defined as an async method \n4. Double check the logic in update_speaking_queue() to add agents to the speaking queue\n\nMaking these fixes and changes may resolve the issue preventing agents from speaking after their relevancy is calculated. Please let me know if these help fix the issue or if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:18:44.407815Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "35f8cb34-57f7-4e40-aa2e-feab2878b837",
      "role": "human",
      "content": "the queue is updated and always empty even after getting updated relevancy scores:\n\nimport asyncio\n\nimport re\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\n\\# async def update\\_relevancy(self):\n\n\\# self.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\n\\# agent\\_info = \\[\n\n\\# {\n\n\\# \"name\": self.name,\n\n\\# \"relevancy\\_score\": self.relevancy\\_score\n\n\\# }\n\n\\# \\]\n\n\\# await self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context):\n\nrelevancy\\_score = 0\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"you are '{self.initial\\_context}' and you are being asked to respond to '{chat\\_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n\nprint(f\"Prompt: {prompt}\")\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\", # or \"text-curie-002\" for Curie\n\nprompt=prompt,\n\ntemperature=0.0,\n\nmax\\_tokens=300\n\n)\n\nprint(f\"Response: {response}\")\n\n\\# Here we assume the model's response will be a single number.\n\n\\# The strip is used to remove any whitespace around the number\n\nstripped\\_response = re.sub(\"\\[^0-9\\]\", \"\", response.choices\\[0\\].text)\n\ntry:\n\nrelevancy\\_score = int(stripped\\_response)\n\nexcept ValueError:\n\nrelevancy\\_score = 0\n\nprint(f\"Relevancy score: {relevancy\\_score}\")\n\nelse:\n\nprint(\"Relevancy score already calculated.\")\n\ntime.sleep(.5)\n\nreturn relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, truncated\\_chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\ntruncated\\_chat\\_history = self.parliament.chat\\_history.truncated\\_chat\\_history\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \"\"\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nasync def trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker\\_name = self.speaking\\_queue.pop(0) # Remove the agent from the queue\n\nnext\\_speaker = self.get\\_agent(next\\_speaker\\_name)\n\nresponse = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue and self.speaking\\_queue\\[0\\] != most\\_relevant\\_agent.name:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nawait self.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = await self.parliament.get\\_agents\\_info()\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()\n\nwhy?",
      "created_at": "2023-05-14T01:27:04.380697Z",
      "sequence_order": 46,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:27:04.380697Z",
            "stop_timestamp": "2023-05-14T01:27:04.380697Z",
            "flags": null,
            "type": "text",
            "text": "the queue is updated and always empty even after getting updated relevancy scores:\n\nimport asyncio\n\nimport re\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\n\\# async def update\\_relevancy(self):\n\n\\# self.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\n\\# agent\\_info = \\[\n\n\\# {\n\n\\# \"name\": self.name,\n\n\\# \"relevancy\\_score\": self.relevancy\\_score\n\n\\# }\n\n\\# \\]\n\n\\# await self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context):\n\nrelevancy\\_score = 0\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"you are '{self.initial\\_context}' and you are being asked to respond to '{chat\\_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n\nprint(f\"Prompt: {prompt}\")\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\", # or \"text-curie-002\" for Curie\n\nprompt=prompt,\n\ntemperature=0.0,\n\nmax\\_tokens=300\n\n)\n\nprint(f\"Response: {response}\")\n\n\\# Here we assume the model's response will be a single number.\n\n\\# The strip is used to remove any whitespace around the number\n\nstripped\\_response = re.sub(\"\\[^0-9\\]\", \"\", response.choices\\[0\\].text)\n\ntry:\n\nrelevancy\\_score = int(stripped\\_response)\n\nexcept ValueError:\n\nrelevancy\\_score = 0\n\nprint(f\"Relevancy score: {relevancy\\_score}\")\n\nelse:\n\nprint(\"Relevancy score already calculated.\")\n\ntime.sleep(.5)\n\nreturn relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, truncated\\_chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\ntruncated\\_chat\\_history = self.parliament.chat\\_history.truncated\\_chat\\_history\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \"\"\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nasync def trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker\\_name = self.speaking\\_queue.pop(0) # Remove the agent from the queue\n\nnext\\_speaker = self.get\\_agent(next\\_speaker\\_name)\n\nresponse = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue and self.speaking\\_queue\\[0\\] != most\\_relevant\\_agent.name:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nawait self.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = await self.parliament.get\\_agents\\_info()\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type)\n\nif not agent\\_class:\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()\n\nwhy?",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:27:04.380697Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "dc47a13b-b20a-4f93-a393-2efa3453a4a5",
      "role": "assistant",
      "content": " There are a few possible issues that could be causing the speaking queue to remain empty:\n\n1. Agents may not actually be added to the speaking queue in update_speaking_queue(). I would double check the logic that adds the most relevant agents to ensure it is correct. Some things to check are:\n\n- Is most_relevant_agent properly assigned? \n- Is the check to see if the most relevant agent's name matches the front of the queue working properly? This logic may need to be reworked.\n- Are calls to add_tagged_agents_to_queue() and add_high_pressure_agents_to_queue() working properly?\n\n2. trigger_next_speaker() may not be properly called after the speaking queue is updated. Adding a call to await self.trigger_next_speaker() at the end of update_speaking_queue() may fix this.\n\n3. The logic in trigger_next_speaker() to get the next agent to speak may not be working properly. I would recheck this logic to ensure the correct agent is removed from the front of the queue.\n\n4. There could be an issue with how agents are added to or removed from the speaking queue in other methods like identify_and_tag_mentioned_agents(). The logic updating the queue in these methods should also be double checked.\n\nSome suggestions to try:\n\n1. Add logging or prints in update_speaking_queue() and trigger_next_speaker() to check if agents are properly added/removed from the queue.\n\n2. Add a call to await self.trigger_next_speaker() at the end of update_speaking_queue() to ensure an agent is selected to speak. \n\n3. Rework the logic in update_speaking_queue() and trigger_next_speaker() related to adding/removing from the speaking queue to handle edge cases.\n\n4. Check any other methods that update the speaking queue to ensure the logic is sound.\n\n5. Add temporary prints or logs in identify_and_tag_mentioned_agents() or anywhere else the queue is updated to check if it is working properly.\n\nMaking these checks and modifications to the speaking queue logic should hopefully resolve the issue of the queue remaining empty. Please let me know if you have any other questions!",
      "created_at": "2023-05-14T01:28:24.349059Z",
      "sequence_order": 47,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:28:24.349059Z",
            "stop_timestamp": "2023-05-14T01:28:24.349059Z",
            "flags": null,
            "type": "text",
            "text": " There are a few possible issues that could be causing the speaking queue to remain empty:\n\n1. Agents may not actually be added to the speaking queue in update_speaking_queue(). I would double check the logic that adds the most relevant agents to ensure it is correct. Some things to check are:\n\n- Is most_relevant_agent properly assigned? \n- Is the check to see if the most relevant agent's name matches the front of the queue working properly? This logic may need to be reworked.\n- Are calls to add_tagged_agents_to_queue() and add_high_pressure_agents_to_queue() working properly?\n\n2. trigger_next_speaker() may not be properly called after the speaking queue is updated. Adding a call to await self.trigger_next_speaker() at the end of update_speaking_queue() may fix this.\n\n3. The logic in trigger_next_speaker() to get the next agent to speak may not be working properly. I would recheck this logic to ensure the correct agent is removed from the front of the queue.\n\n4. There could be an issue with how agents are added to or removed from the speaking queue in other methods like identify_and_tag_mentioned_agents(). The logic updating the queue in these methods should also be double checked.\n\nSome suggestions to try:\n\n1. Add logging or prints in update_speaking_queue() and trigger_next_speaker() to check if agents are properly added/removed from the queue.\n\n2. Add a call to await self.trigger_next_speaker() at the end of update_speaking_queue() to ensure an agent is selected to speak. \n\n3. Rework the logic in update_speaking_queue() and trigger_next_speaker() related to adding/removing from the speaking queue to handle edge cases.\n\n4. Check any other methods that update the speaking queue to ensure the logic is sound.\n\n5. Add temporary prints or logs in identify_and_tag_mentioned_agents() or anywhere else the queue is updated to check if it is working properly.\n\nMaking these checks and modifications to the speaking queue logic should hopefully resolve the issue of the queue remaining empty. Please let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:28:24.349059Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "aea7c11a-81d8-4525-bee7-948198ea5e57",
      "role": "human",
      "content": "the update speaking queue expects the agent.calculate\\_relevancy to do one agent at a time and it does them all, how do we merge thees two functions to work together?\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue and self.speaking\\_queue\\[0\\] != most\\_relevant\\_agent.name:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nawait self.trigger\\_next\\_speaker()\n\nagent class:\n\nasync def calculate\\_relevancy(self, chat\\_context):\n\nrelevancy\\_score = 0\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"you are '{self.initial\\_context}' and you are being asked to respond to '{chat\\_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n\nprint(f\"Prompt: {prompt}\")\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\", # or \"text-curie-002\" for Curie\n\nprompt=prompt,\n\ntemperature=0.0,\n\nmax\\_tokens=300\n\n)\n\nprint(f\"Response: {response}\")\n\n\\# Here we assume the model's response will be a single number.\n\n\\# The strip is used to remove any whitespace around the number\n\nstripped\\_response = re.sub(\"\\[^0-9\\]\", \"\", response.choices\\[0\\].text)\n\ntry:\n\nrelevancy\\_score = int(stripped\\_response)\n\nexcept ValueError:\n\nrelevancy\\_score = 0\n\nprint(f\"Relevancy score: {relevancy\\_score}\")\n\nelse:\n\nprint(\"Relevancy score already calculated.\")\n\ntime.sleep(.5)\n\nreturn relevancy\\_score",
      "created_at": "2023-05-14T01:30:54.112052Z",
      "sequence_order": 48,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:30:54.112052Z",
            "stop_timestamp": "2023-05-14T01:30:54.112052Z",
            "flags": null,
            "type": "text",
            "text": "the update speaking queue expects the agent.calculate\\_relevancy to do one agent at a time and it does them all, how do we merge thees two functions to work together?\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue and self.speaking\\_queue\\[0\\] != most\\_relevant\\_agent.name:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nawait self.trigger\\_next\\_speaker()\n\nagent class:\n\nasync def calculate\\_relevancy(self, chat\\_context):\n\nrelevancy\\_score = 0\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"you are '{self.initial\\_context}' and you are being asked to respond to '{chat\\_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n\nprint(f\"Prompt: {prompt}\")\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\", # or \"text-curie-002\" for Curie\n\nprompt=prompt,\n\ntemperature=0.0,\n\nmax\\_tokens=300\n\n)\n\nprint(f\"Response: {response}\")\n\n\\# Here we assume the model's response will be a single number.\n\n\\# The strip is used to remove any whitespace around the number\n\nstripped\\_response = re.sub(\"\\[^0-9\\]\", \"\", response.choices\\[0\\].text)\n\ntry:\n\nrelevancy\\_score = int(stripped\\_response)\n\nexcept ValueError:\n\nrelevancy\\_score = 0\n\nprint(f\"Relevancy score: {relevancy\\_score}\")\n\nelse:\n\nprint(\"Relevancy score already calculated.\")\n\ntime.sleep(.5)\n\nreturn relevancy\\_score",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:30:54.112052Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "300ae814-27d0-45de-8ea6-e095e0c30ce9",
      "role": "assistant",
      "content": " To resolve this, you have a few options:\n\n1. Call calculate_relevancy() for each agent sequentially instead of in parallel. This would be:\n\n```py\nmost_relevant_agent = None\nhighest_relevancy_score = -1\nfor agent in self.agents.values():\n    relevancy_score = await agent.calculate_relevancy(truncated_chat_history)\n    if relevancy_score > highest_relevancy_score:\n        ...\n```\n\nInstead of:\n\n```py\nfor agent in self.agents.values():\n    relevancy_score = await agent.calculate_relevancy(truncated_chat_history)\n    if relevancy_score > highest_relevancy_score:\n        ... \n```\n\nThis will calculate the relevancy for one agent at a time, and track the highest score so far.\n\n2. Parallelize the calls to calculate_relevancy() using asyncio.gather():\n\n```py\nrelevancy_scores = await asyncio.gather(\n    *[(agent.calculate_relevancy(truncated_chat_history)) for agent in self.agents.values()]\n)\nmost_relevant_agent = max(zip(self.agents.values(), relevancy_scores), key=lambda x: x[1])[0] \n```\n\nThis will call calculate_relevancy() for all agents concurrently, aggregate the results, and get the agent with the max relevancy score.\n\n3. Split calculate_relevancy() into two methods - one to generate the prompt and call the API, and one to actually calculate the relevancy score from the model's response. Then, you can parallelize the API calls, and sequentially calculate the scores from the responses. This would look something like:\n\n```py\nasync def get_relevancy_prompt_response(self, chat_context):\n    prompt = f\"you are '{self.initial_context}' and you are being asked to respond to '{chat_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n    response = await openai.Completion.create(engine=\"...\")\n    return prompt, response\n\nasync def calculate_relevancy(self, prompt_response): \n    prompt, response = prompt_response \n    ...\n    relevancy_score = ...  # Calculate score from `response`\n    return relevancy_score  \n\n# In update_speaking_queue():\nprompt_responses = await asyncio.gather(\n    *[(agent.get_relevancy_prompt_response(truncated_chat_history)) for agent in self.agents.values()]\n)\nrelevancy_scores = []\nfor prompt_response in prompt_responses: \n    relevancy_score = await agent.calculate_relevancy(prompt_response)\n    relevancy_scores.append(relevancy_score)\nmost_relevant_agent = max(zip(self.agents.values(), relevancy_scores), key=lambda x: x[1])[0]  \n```\n\nThis approach parallelizes the API calls, but calculates the relevancy scores sequentially from the responses, resolving the issue.\n\nLet me know if any of these options help or if you have any other questions!",
      "created_at": "2023-05-14T01:33:00.583661Z",
      "sequence_order": 49,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:33:00.583661Z",
            "stop_timestamp": "2023-05-14T01:33:00.583661Z",
            "flags": null,
            "type": "text",
            "text": " To resolve this, you have a few options:\n\n1. Call calculate_relevancy() for each agent sequentially instead of in parallel. This would be:\n\n```py\nmost_relevant_agent = None\nhighest_relevancy_score = -1\nfor agent in self.agents.values():\n    relevancy_score = await agent.calculate_relevancy(truncated_chat_history)\n    if relevancy_score > highest_relevancy_score:\n        ...\n```\n\nInstead of:\n\n```py\nfor agent in self.agents.values():\n    relevancy_score = await agent.calculate_relevancy(truncated_chat_history)\n    if relevancy_score > highest_relevancy_score:\n        ... \n```\n\nThis will calculate the relevancy for one agent at a time, and track the highest score so far.\n\n2. Parallelize the calls to calculate_relevancy() using asyncio.gather():\n\n```py\nrelevancy_scores = await asyncio.gather(\n    *[(agent.calculate_relevancy(truncated_chat_history)) for agent in self.agents.values()]\n)\nmost_relevant_agent = max(zip(self.agents.values(), relevancy_scores), key=lambda x: x[1])[0] \n```\n\nThis will call calculate_relevancy() for all agents concurrently, aggregate the results, and get the agent with the max relevancy score.\n\n3. Split calculate_relevancy() into two methods - one to generate the prompt and call the API, and one to actually calculate the relevancy score from the model's response. Then, you can parallelize the API calls, and sequentially calculate the scores from the responses. This would look something like:\n\n```py\nasync def get_relevancy_prompt_response(self, chat_context):\n    prompt = f\"you are '{self.initial_context}' and you are being asked to respond to '{chat_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n    response = await openai.Completion.create(engine=\"...\")\n    return prompt, response\n\nasync def calculate_relevancy(self, prompt_response): \n    prompt, response = prompt_response \n    ...\n    relevancy_score = ...  # Calculate score from `response`\n    return relevancy_score  \n\n# In update_speaking_queue():\nprompt_responses = await asyncio.gather(\n    *[(agent.get_relevancy_prompt_response(truncated_chat_history)) for agent in self.agents.values()]\n)\nrelevancy_scores = []\nfor prompt_response in prompt_responses: \n    relevancy_score = await agent.calculate_relevancy(prompt_response)\n    relevancy_scores.append(relevancy_score)\nmost_relevant_agent = max(zip(self.agents.values(), relevancy_scores), key=lambda x: x[1])[0]  \n```\n\nThis approach parallelizes the API calls, but calculates the relevancy scores sequentially from the responses, resolving the issue.\n\nLet me know if any of these options help or if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:33:00.583661Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "3cc69fca-496a-4e6e-8148-7be35bf50cb7",
      "role": "human",
      "content": "what's this error about?\n\nAdded agent Red Hat to the front of the speaking queue. Current queue: \\['Red Hat'\\]\n\nTriggering next speaker...\n\nconnection handler failed\n\nTraceback (most recent call last):\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 240, in handler\n\n  await self.ws\\_handler(self)\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n\n  return await cast(\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 37, in chat\n\n  await self.handle\\_send\\_message(websocket, data)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 74, in handle\\_send\\_message\n\n  await self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 243, in broadcast\\_message\n\n  await self.update\\_speaking\\_queue()\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 344, in update\\_speaking\\_queue\n\n  await self.trigger\\_next\\_speaker()\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 320, in trigger\\_next\\_speaker\n\n  response = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nAttributeError: 'Agent' object has no attribute 'get\\_response'\n\nimport asyncio\n\nimport re\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\n\\# async def update\\_relevancy(self):\n\n\\# self.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\n\\# agent\\_info = \\[\n\n\\# {\n\n\\# \"name\": self.name,\n\n\\# \"relevancy\\_score\": self.relevancy\\_score\n\n\\# }\n\n\\# \\]\n\n\\# await self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context):\n\nrelevancy\\_score = 0\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"you are '{self.initial\\_context}' and you are being asked to respond to '{chat\\_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n\nprint(f\"Prompt: {prompt}\")\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\", # or \"text-curie-002\" for Curie\n\nprompt=prompt,\n\ntemperature=0.0,\n\nmax\\_tokens=300\n\n)\n\nprint(f\"Response: {response}\")\n\n\\# Here we assume the model's response will be a single number.\n\n\\# The strip is used to remove any whitespace around the number\n\nstripped\\_response = re.sub(\"\\[^0-9\\]\", \"\", response.choices\\[0\\].text)\n\ntry:\n\nrelevancy\\_score = int(stripped\\_response)\n\nexcept ValueError:\n\nrelevancy\\_score = 0\n\nprint(f\"Relevancy score: {relevancy\\_score}\")\n\nelse:\n\nprint(\"Relevancy score already calculated.\")\n\ntime.sleep(.5)\n\nreturn relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, truncated\\_chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\ntruncated\\_chat\\_history = self.parliament.chat\\_history.truncated\\_chat\\_history\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \"\"\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nasync def trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker\\_name = self.speaking\\_queue.pop(0) # Remove the agent from the queue\n\nnext\\_speaker = self.get\\_agent(next\\_speaker\\_name)\n\nresponse = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\nprint(f\"Calculated relevancy for agent {agent.name}. Score: {relevancy\\_score}\")\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue.count(most\\_relevant\\_agent.name) == 0:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nprint(f\"Added agent {most\\_relevant\\_agent.name} to the front of the speaking queue. Current queue: {self.speaking\\_queue}\")\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nprint(f\"Triggering next speaker...\")\n\nawait self.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-14T01:40:58.387389Z",
      "sequence_order": 50,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:40:58.387389Z",
            "stop_timestamp": "2023-05-14T01:40:58.387389Z",
            "flags": null,
            "type": "text",
            "text": "what's this error about?\n\nAdded agent Red Hat to the front of the speaking queue. Current queue: \\['Red Hat'\\]\n\nTriggering next speaker...\n\nconnection handler failed\n\nTraceback (most recent call last):\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 240, in handler\n\n  await self.ws\\_handler(self)\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n\n  return await cast(\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 37, in chat\n\n  await self.handle\\_send\\_message(websocket, data)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 74, in handle\\_send\\_message\n\n  await self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 243, in broadcast\\_message\n\n  await self.update\\_speaking\\_queue()\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 344, in update\\_speaking\\_queue\n\n  await self.trigger\\_next\\_speaker()\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 320, in trigger\\_next\\_speaker\n\n  response = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nAttributeError: 'Agent' object has no attribute 'get\\_response'\n\nimport asyncio\n\nimport re\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\n\\# async def update\\_relevancy(self):\n\n\\# self.relevancy\\_score = self.calculate\\_relevancy(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\n\\# agent\\_info = \\[\n\n\\# {\n\n\\# \"name\": self.name,\n\n\\# \"relevancy\\_score\": self.relevancy\\_score\n\n\\# }\n\n\\# \\]\n\n\\# await self.websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nasync def calculate\\_relevancy(self, chat\\_context):\n\nrelevancy\\_score = 0\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"you are '{self.initial\\_context}' and you are being asked to respond to '{chat\\_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n\nprint(f\"Prompt: {prompt}\")\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\", # or \"text-curie-002\" for Curie\n\nprompt=prompt,\n\ntemperature=0.0,\n\nmax\\_tokens=300\n\n)\n\nprint(f\"Response: {response}\")\n\n\\# Here we assume the model's response will be a single number.\n\n\\# The strip is used to remove any whitespace around the number\n\nstripped\\_response = re.sub(\"\\[^0-9\\]\", \"\", response.choices\\[0\\].text)\n\ntry:\n\nrelevancy\\_score = int(stripped\\_response)\n\nexcept ValueError:\n\nrelevancy\\_score = 0\n\nprint(f\"Relevancy score: {relevancy\\_score}\")\n\nelse:\n\nprint(\"Relevancy score already calculated.\")\n\ntime.sleep(.5)\n\nreturn relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\ndef get\\_agent\\_message(self, truncated\\_chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\ntruncated\\_chat\\_history = self.parliament.chat\\_history.truncated\\_chat\\_history\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = self.get\\_agent\\_message(self.parliament.chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \"\"\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nasync def trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker\\_name = self.speaking\\_queue.pop(0) # Remove the agent from the queue\n\nnext\\_speaker = self.get\\_agent(next\\_speaker\\_name)\n\nresponse = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\nprint(f\"Calculated relevancy for agent {agent.name}. Score: {relevancy\\_score}\")\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue.count(most\\_relevant\\_agent.name) == 0:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nprint(f\"Added agent {most\\_relevant\\_agent.name} to the front of the speaking queue. Current queue: {self.speaking\\_queue}\")\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nprint(f\"Triggering next speaker...\")\n\nawait self.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:40:58.387389Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "fd8f356c-757b-4bb9-9de4-cb8e4662c124",
      "role": "assistant",
      "content": " This error is occurring because trigger_next_speaker() is attempting to call next_speaker.get_response(), but Agent objects do not have a get_response() method.\n\nTo fix this, you'll need to rework trigger_next_speaker() to properly handle selecting the next agent to speak. Some options could be:\n\n1. Remove the call to next_speaker.get_response() since agents do not have this method. The trigger_next_speaker() logic could be reworked to simply announce the next speaker, e.g.:\n\n```py\nasync def trigger_next_speaker(self):\n    if self.speaking_queue:\n        next_speaker_name = self.speaking_queue.pop(0) \n        next_speaker = self.get_agent(next_speaker_name)\n        print(f\"The next speaker is: {next_speaker.name}\")\n```\n\n2. Move the call to get_agent_message() from speak() into trigger_next_speaker(). This would look something like:\n\n```py\nasync def trigger_next_speaker(self):\n    if self.speaking_queue:\n        next_speaker_name = self.speaking_queue.pop(0)\n        next_speaker = self.get_agent(next_speaker_name)\n        agent_message = next_speaker.get_agent_message(self.chat_history.truncated_chat_history)\n        print(f\"The next speaker is: {next_speaker.name}\")\n        print(f\"Message: {agent_message}\")\n```  \n\nThen in speak(), simply call:\n\n```py \nagent_message = await self.get_agent_message()  # Call trigger_next_speaker()\nawait self.websocket.send(json.dumps({\"action\": \"receive_message\", \"message\": agent_message, \"sender\": self.name}))\n```\n\n3. Define a get_response() method in the Agent class that handles calling get_agent_message(). This may look something like: \n\n```py\ndef get_response(self, truncated_chat_history):\n    agent_message = self.get_agent_message(truncated_chat_history)\n    return agent_message\n```\n\nThen the updated trigger_next_speaker() could call:\n\n```py\nresponse = await next_speaker.get_response(self.chat_history.truncated_chat_history)\n```\n\nThese are a few options to try resolving the error by reworking how the next agent to speak is selected and their message obtained. Please let me know if any of these help fix the issue or if you have any other questions!",
      "created_at": "2023-05-14T01:43:18.918268Z",
      "sequence_order": 51,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:43:18.918268Z",
            "stop_timestamp": "2023-05-14T01:43:18.918268Z",
            "flags": null,
            "type": "text",
            "text": " This error is occurring because trigger_next_speaker() is attempting to call next_speaker.get_response(), but Agent objects do not have a get_response() method.\n\nTo fix this, you'll need to rework trigger_next_speaker() to properly handle selecting the next agent to speak. Some options could be:\n\n1. Remove the call to next_speaker.get_response() since agents do not have this method. The trigger_next_speaker() logic could be reworked to simply announce the next speaker, e.g.:\n\n```py\nasync def trigger_next_speaker(self):\n    if self.speaking_queue:\n        next_speaker_name = self.speaking_queue.pop(0) \n        next_speaker = self.get_agent(next_speaker_name)\n        print(f\"The next speaker is: {next_speaker.name}\")\n```\n\n2. Move the call to get_agent_message() from speak() into trigger_next_speaker(). This would look something like:\n\n```py\nasync def trigger_next_speaker(self):\n    if self.speaking_queue:\n        next_speaker_name = self.speaking_queue.pop(0)\n        next_speaker = self.get_agent(next_speaker_name)\n        agent_message = next_speaker.get_agent_message(self.chat_history.truncated_chat_history)\n        print(f\"The next speaker is: {next_speaker.name}\")\n        print(f\"Message: {agent_message}\")\n```  \n\nThen in speak(), simply call:\n\n```py \nagent_message = await self.get_agent_message()  # Call trigger_next_speaker()\nawait self.websocket.send(json.dumps({\"action\": \"receive_message\", \"message\": agent_message, \"sender\": self.name}))\n```\n\n3. Define a get_response() method in the Agent class that handles calling get_agent_message(). This may look something like: \n\n```py\ndef get_response(self, truncated_chat_history):\n    agent_message = self.get_agent_message(truncated_chat_history)\n    return agent_message\n```\n\nThen the updated trigger_next_speaker() could call:\n\n```py\nresponse = await next_speaker.get_response(self.chat_history.truncated_chat_history)\n```\n\nThese are a few options to try resolving the error by reworking how the next agent to speak is selected and their message obtained. Please let me know if any of these help fix the issue or if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:43:18.918268Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "bbb841ea-88df-4835-916c-57983db0ed8b",
      "role": "human",
      "content": "what are a few options and their pros and cons and pick the best tone",
      "created_at": "2023-05-14T01:45:07.381322Z",
      "sequence_order": 52,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:45:07.381322Z",
            "stop_timestamp": "2023-05-14T01:45:07.381322Z",
            "flags": null,
            "type": "text",
            "text": "what are a few options and their pros and cons and pick the best tone",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:45:07.381322Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "319aa2eb-e2cd-4654-a115-614687bc3257",
      "role": "assistant",
      "content": " Here are some options for handling agent responses in trigger_next_speaker() and their pros/cons:\n\n1. Remove the call to next_speaker.get_response() and simply announce the next speaker.\n\nPros: Simple, avoids the error.\nCons: Does not actually obtain or send an agent response.\n\n2. Move the call to get_agent_message() into trigger_next_speaker() and handle constructing the full agent response there.\n\nPros: Obtains the agent response and could send it directly to clients.\nCons: Duplicates logic from speak() in trigger_next_speaker(). \n\n3. Define a get_response() method in Agent to call get_agent_message(). Have trigger_next_speaker() call next_speaker.get_response(). \n\nPros: Keeps the responsibility of obtaining agent responses within the Agent class. trigger_next_speaker() can simply call this method.\nCons: Additional complexity of defining another method.\n\nIn my view, Option 3 is the best approach for a few reasons:\n\n1. It keeps the logic for obtaining agent responses encapsulated within the Agent class. trigger_next_speaker() simply needs to call the get_response() method, keeping responsibilities separated.\n\n2. It avoids duplicating logic from speak() in trigger_next_speaker(), which Option 2 would require.\n\n3. It actually obtains an agent response, unlike Option 1 which only announces the next speaker.\n\nThe updated code could look something like this:\n\nIn Agent:\n```py\ndef get_response(self, truncated_chat_history):\n    agent_message = self.get_agent_message(truncated_chat_history)\n    return agent_message\n```\n\nIn Parliament:\n```py \nasync def trigger_next_speaker(self):\n    if self.speaking_queue:\n        next_speaker_name = self.speaking_queue.pop(0)  \n        next_speaker = self.get_agent(next_speaker_name)\n        response = await next_speaker.get_response(self.chat_history.truncated_chat_history)\n        print(f\"The next speaker is: {next_speaker.name}\") \n        print(f\"Message: {response}\")\n```\n\nIn speak():\n```py \nagent_message = await self.get_response(self.parliament.chat_history.truncated_chat_history)\nawait self.websocket.send(json.dumps({\"action\": \"receive_message\", \"message\": agent_message, \"sender\": self.name})) \n```\n\nOverall this approach keeps related logic encapsulated, avoids duplication, and properly obtains agent responses, so I would recommend Option 3 as the best approach. Let me know if you have any other questions!",
      "created_at": "2023-05-14T01:46:34.315371Z",
      "sequence_order": 53,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:46:34.315371Z",
            "stop_timestamp": "2023-05-14T01:46:34.315371Z",
            "flags": null,
            "type": "text",
            "text": " Here are some options for handling agent responses in trigger_next_speaker() and their pros/cons:\n\n1. Remove the call to next_speaker.get_response() and simply announce the next speaker.\n\nPros: Simple, avoids the error.\nCons: Does not actually obtain or send an agent response.\n\n2. Move the call to get_agent_message() into trigger_next_speaker() and handle constructing the full agent response there.\n\nPros: Obtains the agent response and could send it directly to clients.\nCons: Duplicates logic from speak() in trigger_next_speaker(). \n\n3. Define a get_response() method in Agent to call get_agent_message(). Have trigger_next_speaker() call next_speaker.get_response(). \n\nPros: Keeps the responsibility of obtaining agent responses within the Agent class. trigger_next_speaker() can simply call this method.\nCons: Additional complexity of defining another method.\n\nIn my view, Option 3 is the best approach for a few reasons:\n\n1. It keeps the logic for obtaining agent responses encapsulated within the Agent class. trigger_next_speaker() simply needs to call the get_response() method, keeping responsibilities separated.\n\n2. It avoids duplicating logic from speak() in trigger_next_speaker(), which Option 2 would require.\n\n3. It actually obtains an agent response, unlike Option 1 which only announces the next speaker.\n\nThe updated code could look something like this:\n\nIn Agent:\n```py\ndef get_response(self, truncated_chat_history):\n    agent_message = self.get_agent_message(truncated_chat_history)\n    return agent_message\n```\n\nIn Parliament:\n```py \nasync def trigger_next_speaker(self):\n    if self.speaking_queue:\n        next_speaker_name = self.speaking_queue.pop(0)  \n        next_speaker = self.get_agent(next_speaker_name)\n        response = await next_speaker.get_response(self.chat_history.truncated_chat_history)\n        print(f\"The next speaker is: {next_speaker.name}\") \n        print(f\"Message: {response}\")\n```\n\nIn speak():\n```py \nagent_message = await self.get_response(self.parliament.chat_history.truncated_chat_history)\nawait self.websocket.send(json.dumps({\"action\": \"receive_message\", \"message\": agent_message, \"sender\": self.name})) \n```\n\nOverall this approach keeps related logic encapsulated, avoids duplication, and properly obtains agent responses, so I would recommend Option 3 as the best approach. Let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:46:34.315371Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "73e2aea1-6f99-46ad-a9f4-b305605bd6f0",
      "role": "human",
      "content": "got this error: Relevancy score: 10011\n\nCalculated relevancy for agent Blue Hat. Score: 10011\n\nAdded agent White Hat to the front of the speaking queue. Current queue: \\['White Hat'\\]\n\nTriggering next speaker...\n\nconnection handler failed\n\nTraceback (most recent call last):\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 240, in handler\n\n  await self.ws\\_handler(self)\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n\n  return await cast(\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 37, in chat\n\n  await self.handle\\_send\\_message(websocket, data)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 74, in handle\\_send\\_message\n\n  await self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 238, in broadcast\\_message\n\n  await self.update\\_speaking\\_queue()\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 342, in update\\_speaking\\_queue\n\n  await self.trigger\\_next\\_speaker()\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 315, in trigger\\_next\\_speaker\n\n  response = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 34, in get\\_response\n\n  agent\\_message = self.get\\_agent\\_message(truncated\\_chat\\_history)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 82, in get\\_agent\\_message\n\n  response = self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 142, in get\\_response\n\n  messages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nNameError: name 'messages' is not defined",
      "created_at": "2023-05-14T01:49:57.878220Z",
      "sequence_order": 54,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:49:57.878220Z",
            "stop_timestamp": "2023-05-14T01:49:57.878220Z",
            "flags": null,
            "type": "text",
            "text": "got this error: Relevancy score: 10011\n\nCalculated relevancy for agent Blue Hat. Score: 10011\n\nAdded agent White Hat to the front of the speaking queue. Current queue: \\['White Hat'\\]\n\nTriggering next speaker...\n\nconnection handler failed\n\nTraceback (most recent call last):\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 240, in handler\n\n  await self.ws\\_handler(self)\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n\n  return await cast(\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 37, in chat\n\n  await self.handle\\_send\\_message(websocket, data)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 74, in handle\\_send\\_message\n\n  await self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 238, in broadcast\\_message\n\n  await self.update\\_speaking\\_queue()\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 342, in update\\_speaking\\_queue\n\n  await self.trigger\\_next\\_speaker()\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 315, in trigger\\_next\\_speaker\n\n  response = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 34, in get\\_response\n\n  agent\\_message = self.get\\_agent\\_message(truncated\\_chat\\_history)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 82, in get\\_agent\\_message\n\n  response = self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 142, in get\\_response\n\n  messages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nNameError: name 'messages' is not defined",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:49:57.878220Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "42112fbf-f9d1-4586-9c88-643f09b8b0b4",
      "role": "assistant",
      "content": "  This error is occurring because the name messages is undefined in OpenAIAgent.get_response(). It looks like this method is attempting to append to a messages list to pass to the OpenAI API, but that list was never defined.\n\nTo fix this, you'll need to define the messages list in OpenAIAgent.get_response() before appending to it, e.g.:\n\n```py\ndef get_response(self, truncated_chat):\n    messages = []  # Define the list \n    messages.append({\"role\": \"assistant\", \"content\": truncated_chat})\n    ...\n```\n\nWith messages defined, the full OpenAIAgent.get_response() method would be:\n\n```py\ndef get_response(self, truncated_chat):\n    messages = []  \n    messages.append({\"role\": \"assistant\", \"content\": truncated_chat})\n    \n    system_msg = self.initial_context\n    messages.append({\"role\": \"system\", \"content\": system_msg})\n    \n    max_retries = 3\n    retry_delay = 5\n    for i in range(max_retries):\n        try:  \n            # Call the OpenAI API with the messages array\n            response = openai.ChatCompletion.create(\n                model=self.model,\n                messages=messages,\n                ...\n            )\n            # Extract and return the assistant's response from the API response\n            agent_response = response.choices[0].message[\"content\"].strip()\n            print(f\" {agent_response}\")  # Add logging\n            return agent_response\n        except RateLimitError as e:\n            ...\n```\n\nDefining messages before using it should resolve the NameError and allow OpenAIAgent.get_response() to function properly.\n\nHowever, there are a few other issues I noticed here:\n\n1. The extremely high relevancy score of 10011 for an agent is clearly invalid. This logic should be reworked to validate scores are between 0-100. \n\n2. The error is mentioning a White Hat agent, but only Red Hat and Blue Hat agents seem to be defined. The logic adding agents to the parliament may need to be double checked.\n\n3. There may be an unhandled exception if the OpenAI API call fails and max_retries is exceeded. A catch-all except block should be added to handle any uncaught exceptions.\n\nSome suggestions to address these would be:\n\n1. Validate relevancy scores are between 0-100, e.g:\n\n```py\nif 0 <= relevancy_score <= 100: \n    ...\nelse: \n    print(\"Invalid relevancy score. Using 0.\")\n    relevancy_score = 0  # Set to 0 if invalid \n```\n\n2. Double check the logic in handle_add_agents() to ensure all defined agents are properly added to the parliament. \n\n3. Add a catch-all except block to OpenAIAgent.get_response():\n\n```py\nexcept: \n    print(\"Error occurred attempting to call OpenAI API!\")\n    raise  # Re-raise the exception \n```\n\nI hope this helps explain and resolve the issues you're seeing. Let me know if you have any other questions!",
      "created_at": "2023-05-14T01:51:32.843350Z",
      "sequence_order": 55,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:51:32.843350Z",
            "stop_timestamp": "2023-05-14T01:51:32.843350Z",
            "flags": null,
            "type": "text",
            "text": "  This error is occurring because the name messages is undefined in OpenAIAgent.get_response(). It looks like this method is attempting to append to a messages list to pass to the OpenAI API, but that list was never defined.\n\nTo fix this, you'll need to define the messages list in OpenAIAgent.get_response() before appending to it, e.g.:\n\n```py\ndef get_response(self, truncated_chat):\n    messages = []  # Define the list \n    messages.append({\"role\": \"assistant\", \"content\": truncated_chat})\n    ...\n```\n\nWith messages defined, the full OpenAIAgent.get_response() method would be:\n\n```py\ndef get_response(self, truncated_chat):\n    messages = []  \n    messages.append({\"role\": \"assistant\", \"content\": truncated_chat})\n    \n    system_msg = self.initial_context\n    messages.append({\"role\": \"system\", \"content\": system_msg})\n    \n    max_retries = 3\n    retry_delay = 5\n    for i in range(max_retries):\n        try:  \n            # Call the OpenAI API with the messages array\n            response = openai.ChatCompletion.create(\n                model=self.model,\n                messages=messages,\n                ...\n            )\n            # Extract and return the assistant's response from the API response\n            agent_response = response.choices[0].message[\"content\"].strip()\n            print(f\" {agent_response}\")  # Add logging\n            return agent_response\n        except RateLimitError as e:\n            ...\n```\n\nDefining messages before using it should resolve the NameError and allow OpenAIAgent.get_response() to function properly.\n\nHowever, there are a few other issues I noticed here:\n\n1. The extremely high relevancy score of 10011 for an agent is clearly invalid. This logic should be reworked to validate scores are between 0-100. \n\n2. The error is mentioning a White Hat agent, but only Red Hat and Blue Hat agents seem to be defined. The logic adding agents to the parliament may need to be double checked.\n\n3. There may be an unhandled exception if the OpenAI API call fails and max_retries is exceeded. A catch-all except block should be added to handle any uncaught exceptions.\n\nSome suggestions to address these would be:\n\n1. Validate relevancy scores are between 0-100, e.g:\n\n```py\nif 0 <= relevancy_score <= 100: \n    ...\nelse: \n    print(\"Invalid relevancy score. Using 0.\")\n    relevancy_score = 0  # Set to 0 if invalid \n```\n\n2. Double check the logic in handle_add_agents() to ensure all defined agents are properly added to the parliament. \n\n3. Add a catch-all except block to OpenAIAgent.get_response():\n\n```py\nexcept: \n    print(\"Error occurred attempting to call OpenAI API!\")\n    raise  # Re-raise the exception \n```\n\nI hope this helps explain and resolve the issues you're seeing. Let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:51:32.843350Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "f19940fa-fb54-43cd-9ad5-546510d9e240",
      "role": "human",
      "content": "Calculated relevancy for agent Cost Structure. Score: 1\n\nAdded agent Customer to the front of the speaking queue. Current queue: \\['Customer'\\]\n\nTriggering next speaker...\n\nA customer is a person, organization, or company that purchases goods or services from another person, organization, or company.\n\nconnection handler failed\n\nTraceback (most recent call last):\n\nFile \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 240, in handler\n\nawait self.ws\\_handler(self)\n\nFile \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n\nreturn await cast(\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 37, in chat\n\nawait self.handle\\_send\\_message(websocket, data)\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 74, in handle\\_send\\_message\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 239, in broadcast\\_message\n\nawait self.update\\_speaking\\_queue()\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 343, in update\\_speaking\\_queue\n\nawait self.trigger\\_next\\_speaker()\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 316, in trigger\\_next\\_speaker\n\nresponse = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nTypeError: object str can't be used in 'await' expression\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nasync def trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker\\_name = self.speaking\\_queue.pop(0)\n\nnext\\_speaker = self.get\\_agent(next\\_speaker\\_name)\n\nresponse = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nprint(f\"The next speaker is: {next\\_speaker.name}\")\n\nprint(f\"Message: {response}\")\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\nprint(f\"Calculated relevancy for agent {agent.name}. Score: {relevancy\\_score}\")\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue.count(most\\_relevant\\_agent.name) == 0:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nprint(f\"Added agent {most\\_relevant\\_agent.name} to the front of the speaking queue. Current queue: {self.speaking\\_queue}\")\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nprint(f\"Triggering next speaker...\")\n\nawait self.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-14T01:53:14.969081Z",
      "sequence_order": 56,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:53:14.969081Z",
            "stop_timestamp": "2023-05-14T01:53:14.969081Z",
            "flags": null,
            "type": "text",
            "text": "Calculated relevancy for agent Cost Structure. Score: 1\n\nAdded agent Customer to the front of the speaking queue. Current queue: \\['Customer'\\]\n\nTriggering next speaker...\n\nA customer is a person, organization, or company that purchases goods or services from another person, organization, or company.\n\nconnection handler failed\n\nTraceback (most recent call last):\n\nFile \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 240, in handler\n\nawait self.ws\\_handler(self)\n\nFile \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n\nreturn await cast(\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 37, in chat\n\nawait self.handle\\_send\\_message(websocket, data)\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 74, in handle\\_send\\_message\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 239, in broadcast\\_message\n\nawait self.update\\_speaking\\_queue()\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 343, in update\\_speaking\\_queue\n\nawait self.trigger\\_next\\_speaker()\n\nFile \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 316, in trigger\\_next\\_speaker\n\nresponse = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nTypeError: object str can't be used in 'await' expression\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent.name)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nasync def trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker\\_name = self.speaking\\_queue.pop(0)\n\nnext\\_speaker = self.get\\_agent(next\\_speaker\\_name)\n\nresponse = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nprint(f\"The next speaker is: {next\\_speaker.name}\")\n\nprint(f\"Message: {response}\")\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\nprint(f\"Calculated relevancy for agent {agent.name}. Score: {relevancy\\_score}\")\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue.count(most\\_relevant\\_agent.name) == 0:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nprint(f\"Added agent {most\\_relevant\\_agent.name} to the front of the speaking queue. Current queue: {self.speaking\\_queue}\")\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nprint(f\"Triggering next speaker...\")\n\nawait self.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:53:14.969081Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "390824a5-0f8f-4756-9d8b-7add3ed58e23",
      "role": "assistant",
      "content": " This error is occurring because next_speaker is a string, and you cannot await a string. It looks like trigger_next_speaker() is attempting to call next_speaker.get_response(), but next_speaker was never actually assigned to an agent object.\n\nTo fix this, you'll need to properly get a reference to the next agent object to speak. Some options could be:\n\n1. Remove the await call from next_speaker.get_response() and simply have trigger_next_speaker() announce the next speaker's name:\n\n```py\nasync def trigger_next_speaker(self):\n    if self.speaking_queue:\n        next_speaker_name = self.speaking_queue.pop(0) \n        print(f\"The next speaker is: {next_speaker_name}\")\n```\n\nThen in speak(), call self.get_agent(next_speaker_name) to actually get the agent object and call its get_response() method.\n\n2. Get a reference to the agent object directly in trigger_next_speaker() instead of just its name:\n\n```py\nasync def trigger_next_speaker(self):\n    if self.speaking_queue:\n        next_speaker_name = self.speaking_queue.pop(0) \n        next_speaker = self.get_agent(next_speaker_name)\n        await next_speaker.get_response(self.chat_history.truncated_chat_history) \n```\n\n3. Define next_speaker as a default parameter in trigger_next_speaker() and pass the actual agent object to that method:\n\n```py\nasync def trigger_next_speaker(self, next_speaker=None):\n    if not next_speaker:  # Assign default agent if none passed\n        if self.speaking_queue:\n            next_speaker_name = self.speaking_queue.pop(0) \n            next_speaker = self.get_agent(next_speaker_name)\n    await next_speaker.get_response(self.chat_history.truncated_chat_history) \n```\n\nThen call the method by passing the actual agent object:\n```py\nawait self.trigger_next_speaker(next_speaker=self.get_agent(next_speaker_name)) \n```\n\nAny of these options should resolve the error by properly obtaining a reference to the next agent object instead of just its name. Let me know if this helps explain and fix the issue or if you have any other questions!",
      "created_at": "2023-05-14T01:55:14.610426Z",
      "sequence_order": 57,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:55:14.610426Z",
            "stop_timestamp": "2023-05-14T01:55:14.610426Z",
            "flags": null,
            "type": "text",
            "text": " This error is occurring because next_speaker is a string, and you cannot await a string. It looks like trigger_next_speaker() is attempting to call next_speaker.get_response(), but next_speaker was never actually assigned to an agent object.\n\nTo fix this, you'll need to properly get a reference to the next agent object to speak. Some options could be:\n\n1. Remove the await call from next_speaker.get_response() and simply have trigger_next_speaker() announce the next speaker's name:\n\n```py\nasync def trigger_next_speaker(self):\n    if self.speaking_queue:\n        next_speaker_name = self.speaking_queue.pop(0) \n        print(f\"The next speaker is: {next_speaker_name}\")\n```\n\nThen in speak(), call self.get_agent(next_speaker_name) to actually get the agent object and call its get_response() method.\n\n2. Get a reference to the agent object directly in trigger_next_speaker() instead of just its name:\n\n```py\nasync def trigger_next_speaker(self):\n    if self.speaking_queue:\n        next_speaker_name = self.speaking_queue.pop(0) \n        next_speaker = self.get_agent(next_speaker_name)\n        await next_speaker.get_response(self.chat_history.truncated_chat_history) \n```\n\n3. Define next_speaker as a default parameter in trigger_next_speaker() and pass the actual agent object to that method:\n\n```py\nasync def trigger_next_speaker(self, next_speaker=None):\n    if not next_speaker:  # Assign default agent if none passed\n        if self.speaking_queue:\n            next_speaker_name = self.speaking_queue.pop(0) \n            next_speaker = self.get_agent(next_speaker_name)\n    await next_speaker.get_response(self.chat_history.truncated_chat_history) \n```\n\nThen call the method by passing the actual agent object:\n```py\nawait self.trigger_next_speaker(next_speaker=self.get_agent(next_speaker_name)) \n```\n\nAny of these options should resolve the error by properly obtaining a reference to the next agent object instead of just its name. Let me know if this helps explain and fix the issue or if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:55:14.610426Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "63508386-9891-4546-a648-81e0d62238f0",
      "role": "human",
      "content": "Relevancy score: 50\n\nCalculated relevancy for agent Cost Structure. Score: 50\n\nAdded agent Value Proposition to the front of the speaking queue. Current queue: \\['Value Proposition'\\]\n\nTriggering next speaker...\n\nI'm sorry, I don't understand what you mean. Could you please provide more context or clarify your request?\n\nconnection handler failed\n\nTraceback (most recent call last):\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 240, in handler\n\n  await self.ws\\_handler(self)\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n\n  return await cast(\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 37, in chat\n\n  await self.handle\\_send\\_message(websocket, data)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 74, in handle\\_send\\_message\n\n  await self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 239, in broadcast\\_message\n\n  await self.update\\_speaking\\_queue()\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 343, in update\\_speaking\\_queue\n\n  await self.trigger\\_next\\_speaker()\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 316, in trigger\\_next\\_speaker\n\n  response = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 82, in get\\_agent\\_message\n\n  response = await self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nTypeError: object str can't be used in 'await' expression\n\nimport asyncio\n\nimport re\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef get\\_response(self, truncated\\_chat\\_history):\n\nagent\\_message = self.get\\_agent\\_message(truncated\\_chat\\_history)\n\nreturn agent\\_message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\nasync def calculate\\_relevancy(self, chat\\_context):\n\nrelevancy\\_score = 0\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"you are '{self.initial\\_context}' and you are being asked to respond to '{chat\\_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n\nprint(f\"Prompt: {prompt}\")\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\", # or \"text-curie-002\" for Curie\n\nprompt=prompt,\n\ntemperature=0.0,\n\nmax\\_tokens=300\n\n)\n\nprint(f\"Response: {response}\")\n\n\\# Here we assume the model's response will be a single number.\n\n\\# The strip is used to remove any whitespace around the number\n\nstripped\\_response = re.sub(\"\\[^0-9\\]\", \"\", response.choices\\[0\\].text)\n\ntry:\n\nrelevancy\\_score = int(stripped\\_response)\n\nexcept ValueError:\n\nrelevancy\\_score = 0\n\nprint(f\"Relevancy score: {relevancy\\_score}\")\n\nelse:\n\nprint(\"Relevancy score already calculated.\")\n\ntime.sleep(.5)\n\nreturn relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nasync def get\\_agent\\_message(self, truncated\\_chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = await self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\ntruncated\\_chat\\_history = self.parliament.chat\\_history.truncated\\_chat\\_history\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = await self.get\\_response(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \"\"\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages=\\[\\]\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nasync def trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker\\_name = self.speaking\\_queue.pop(0)\n\nnext\\_speaker = self.get\\_agent(next\\_speaker\\_name)\n\nresponse = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nprint(f\"The next speaker is: {next\\_speaker.name}\")\n\nprint(f\"Message: {response}\")\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\nprint(f\"Calculated relevancy for agent {agent.name}. Score: {relevancy\\_score}\")\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue.count(most\\_relevant\\_agent.name) == 0:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nprint(f\"Added agent {most\\_relevant\\_agent.name} to the front of the speaking queue. Current queue: {self.speaking\\_queue}\")\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nprint(f\"Triggering next speaker...\")\n\nawait self.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-14T01:58:30.746047Z",
      "sequence_order": 58,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T01:58:30.746047Z",
            "stop_timestamp": "2023-05-14T01:58:30.746047Z",
            "flags": null,
            "type": "text",
            "text": "Relevancy score: 50\n\nCalculated relevancy for agent Cost Structure. Score: 50\n\nAdded agent Value Proposition to the front of the speaking queue. Current queue: \\['Value Proposition'\\]\n\nTriggering next speaker...\n\nI'm sorry, I don't understand what you mean. Could you please provide more context or clarify your request?\n\nconnection handler failed\n\nTraceback (most recent call last):\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 240, in handler\n\n  await self.ws\\_handler(self)\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n\n  return await cast(\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 37, in chat\n\n  await self.handle\\_send\\_message(websocket, data)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 74, in handle\\_send\\_message\n\n  await self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 239, in broadcast\\_message\n\n  await self.update\\_speaking\\_queue()\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 343, in update\\_speaking\\_queue\n\n  await self.trigger\\_next\\_speaker()\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 316, in trigger\\_next\\_speaker\n\n  response = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/parliament.py\", line 82, in get\\_agent\\_message\n\n  response = await self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nTypeError: object str can't be used in 'await' expression\n\nimport asyncio\n\nimport re\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef get\\_response(self, truncated\\_chat\\_history):\n\nagent\\_message = self.get\\_agent\\_message(truncated\\_chat\\_history)\n\nreturn agent\\_message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\nasync def calculate\\_relevancy(self, chat\\_context):\n\nrelevancy\\_score = 0\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"you are '{self.initial\\_context}' and you are being asked to respond to '{chat\\_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n\nprint(f\"Prompt: {prompt}\")\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\", # or \"text-curie-002\" for Curie\n\nprompt=prompt,\n\ntemperature=0.0,\n\nmax\\_tokens=300\n\n)\n\nprint(f\"Response: {response}\")\n\n\\# Here we assume the model's response will be a single number.\n\n\\# The strip is used to remove any whitespace around the number\n\nstripped\\_response = re.sub(\"\\[^0-9\\]\", \"\", response.choices\\[0\\].text)\n\ntry:\n\nrelevancy\\_score = int(stripped\\_response)\n\nexcept ValueError:\n\nrelevancy\\_score = 0\n\nprint(f\"Relevancy score: {relevancy\\_score}\")\n\nelse:\n\nprint(\"Relevancy score already calculated.\")\n\ntime.sleep(.5)\n\nreturn relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nasync def get\\_agent\\_message(self, truncated\\_chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = await self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\ntruncated\\_chat\\_history = self.parliament.chat\\_history.truncated\\_chat\\_history\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = await self.get\\_response(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \"\"\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages=\\[\\]\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = 100\n\nself.agents\\[agent\\_name\\].tagged = True\n\nprint(f\"Tagged agent: {agent\\_name}\") # Debug print\n\nreturn mentioned\\_agents\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nasync def trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker\\_name = self.speaking\\_queue.pop(0)\n\nnext\\_speaker = self.get\\_agent(next\\_speaker\\_name)\n\nresponse = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nprint(f\"The next speaker is: {next\\_speaker.name}\")\n\nprint(f\"Message: {response}\")\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\nprint(f\"Calculated relevancy for agent {agent.name}. Score: {relevancy\\_score}\")\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue.count(most\\_relevant\\_agent.name) == 0:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nprint(f\"Added agent {most\\_relevant\\_agent.name} to the front of the speaking queue. Current queue: {self.speaking\\_queue}\")\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nprint(f\"Triggering next speaker...\")\n\nawait self.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T01:58:30.746047Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "729da738-bc0a-42ee-a5b5-bf099ddad273",
      "role": "assistant",
      "content": "  This error is occurring for a similar reason as the previous one - get_agent_message() cannot be awaited because agent_instance is a string, not an object with an async get_response() method.\n\nTo fix this, you'll need to properly initialize agent_instance to the correct class instance, either OpenAIAgent or GPT4AllAgent. Some things to check:\n\n1. Double check that agent_type is being properly extracted from the agent data, and the correct class is being retrieved from self.agent_types.\n\n2. Ensure the if check to verify agent_instance is an instance of the expected class is working properly. For example, it should be:\n\n```py\nif isinstance(self.agent_instance, self.parliament.agent_types[self.agent_type]):\n    ... \n```\n\n3. Check that the class instance (e.g. OpenAIAgent(initial_context)) is properly instantiated and passed to the Agent constructor.\n\n4. To handle the \"Sorry, I don't understand...\" response, add a catch-all except block to get_agent_message() to return a default response if an exception occurs:\n\n```py\nasync def get_agent_message(self, truncated_chat_history):\n    ...\n    except:\n        print(\"Error occurred attempting to get agent message!\")\n        return \"Sorry, I'm unable to generate a relevant response at this time.\"\n```  \n\nSome sample fixes could be:\n\n\n```py \nagent_type = agent_data.get(\"type\")  # Get \"openai\" or \"gpt4all\"\nagent_class = self.parliament.agent_types.get(agent_type)  # Get OpenAIAgent or GPT4AllAgent\nagent_instance = agent_class(initial_context)  # Properly instantiate the class\n\n...\n\nif isinstance(self.agent_instance, self.parliament.agent_types[self.agent_type]):\n    ...\nexcept: \n    print(\"Error occurred attempting to get agent message!\")\n    return \"Sorry, I'm unable to generate a relevant response at this time.\" \n```\n\nI hope this helps identify the source of the error and appropriate fixes. Let me know if you have any other questions!",
      "created_at": "2023-05-14T02:00:02.392425Z",
      "sequence_order": 59,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T02:00:02.392425Z",
            "stop_timestamp": "2023-05-14T02:00:02.392425Z",
            "flags": null,
            "type": "text",
            "text": "  This error is occurring for a similar reason as the previous one - get_agent_message() cannot be awaited because agent_instance is a string, not an object with an async get_response() method.\n\nTo fix this, you'll need to properly initialize agent_instance to the correct class instance, either OpenAIAgent or GPT4AllAgent. Some things to check:\n\n1. Double check that agent_type is being properly extracted from the agent data, and the correct class is being retrieved from self.agent_types.\n\n2. Ensure the if check to verify agent_instance is an instance of the expected class is working properly. For example, it should be:\n\n```py\nif isinstance(self.agent_instance, self.parliament.agent_types[self.agent_type]):\n    ... \n```\n\n3. Check that the class instance (e.g. OpenAIAgent(initial_context)) is properly instantiated and passed to the Agent constructor.\n\n4. To handle the \"Sorry, I don't understand...\" response, add a catch-all except block to get_agent_message() to return a default response if an exception occurs:\n\n```py\nasync def get_agent_message(self, truncated_chat_history):\n    ...\n    except:\n        print(\"Error occurred attempting to get agent message!\")\n        return \"Sorry, I'm unable to generate a relevant response at this time.\"\n```  \n\nSome sample fixes could be:\n\n\n```py \nagent_type = agent_data.get(\"type\")  # Get \"openai\" or \"gpt4all\"\nagent_class = self.parliament.agent_types.get(agent_type)  # Get OpenAIAgent or GPT4AllAgent\nagent_instance = agent_class(initial_context)  # Properly instantiate the class\n\n...\n\nif isinstance(self.agent_instance, self.parliament.agent_types[self.agent_type]):\n    ...\nexcept: \n    print(\"Error occurred attempting to get agent message!\")\n    return \"Sorry, I'm unable to generate a relevant response at this time.\" \n```\n\nI hope this helps identify the source of the error and appropriate fixes. Let me know if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T02:00:02.392425Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "10208b4c-a65a-4582-a359-10012c99f232",
      "role": "human",
      "content": "getting this error now\n\nTraceback (most recent call last):\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 240, in handler\n\n  await self.ws\\_handler(self)\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n\n  return await cast(\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 39, in chat\n\n  await self.handle\\_add\\_agents(websocket, data)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 108, in handle\\_add\\_agents\n\n  if isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nAttributeError: 'ParliamentServer' object has no attribute 'agent\\_instance'\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = await self.parliament.get\\_agents\\_info()\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type) # Get OpenAIAgent or GPT4AllAgent\n\nagent\\_instance = agent\\_class(initial\\_context) # Properly instantiate the class\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nelse:\n\nprint(\"Error occurred attempting to get agent message!\")\n\nreturn \"Sorry, I'm unable to generate a relevant response at this time.\"\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()\n\nimport asyncio\n\nimport re\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef get\\_response(self, truncated\\_chat\\_history):\n\nagent\\_message = self.get\\_agent\\_message(truncated\\_chat\\_history)\n\nreturn agent\\_message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\nasync def calculate\\_relevancy(self, chat\\_context):\n\nrelevancy\\_score = 0\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"you are '{self.initial\\_context}' and you are being asked to respond to '{chat\\_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n\nprint(f\"Prompt: {prompt}\")\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\", # or \"text-curie-002\" for Curie\n\nprompt=prompt,\n\ntemperature=0.0,\n\nmax\\_tokens=300\n\n)\n\nprint(f\"Response: {response}\")\n\n\\# Here we assume the model's response will be a single number.\n\n\\# The strip is used to remove any whitespace around the number\n\nstripped\\_response = re.sub(\"\\[^0-9\\]\", \"\", response.choices\\[0\\].text)\n\ntry:\n\nrelevancy\\_score = int(stripped\\_response)\n\nexcept ValueError:\n\nrelevancy\\_score = 0\n\nprint(f\"Relevancy score: {relevancy\\_score}\")\n\nelse:\n\nprint(\"Relevancy score already calculated.\")\n\ntime.sleep(.5)\n\nreturn relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nasync def get\\_agent\\_message(self, truncated\\_chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = await self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\ntruncated\\_chat\\_history = self.parliament.chat\\_history.truncated\\_chat\\_history\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = await self.get\\_response(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \"\"\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages=\\[\\]\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nasync def trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker\\_name = self.speaking\\_queue.pop(0)\n\nnext\\_speaker = self.get\\_agent(next\\_speaker\\_name)\n\nresponse = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nprint(f\"The next speaker is: {next\\_speaker.name}\")\n\nprint(f\"Message: {response}\")\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\nprint(f\"Calculated relevancy for agent {agent.name}. Score: {relevancy\\_score}\")\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue.count(most\\_relevant\\_agent.name) == 0:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nprint(f\"Added agent {most\\_relevant\\_agent.name} to the front of the speaking queue. Current queue: {self.speaking\\_queue}\")\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nprint(f\"Triggering next speaker...\")\n\nawait self.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info",
      "created_at": "2023-05-14T09:26:18.063383Z",
      "sequence_order": 60,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T09:26:18.063383Z",
            "stop_timestamp": "2023-05-14T09:26:18.063383Z",
            "flags": null,
            "type": "text",
            "text": "getting this error now\n\nTraceback (most recent call last):\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 240, in handler\n\n  await self.ws\\_handler(self)\n\n File \"/Users/bilalghalib/mambaforge/lib/python3.10/site-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n\n  return await cast(\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 39, in chat\n\n  await self.handle\\_add\\_agents(websocket, data)\n\n File \"/Users/bilalghalib/Projects/scripts/Parliment/parliament-chat-nextjs/pages/parliament\\_backend/server.py\", line 108, in handle\\_add\\_agents\n\n  if isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nAttributeError: 'ParliamentServer' object has no attribute 'agent\\_instance'\n\nfrom datetime import datetime\n\nimport asyncio\n\nimport json\n\nfrom websockets import serve\n\nfrom parliament import Parliament, GPT4AllAgent, OpenAIAgent, Agent\n\nimport openai\n\n\\# Constants\n\nSET\\_PARLIAMENT = \"set\\_parliament\"\n\nSEND\\_MESSAGE = \"send\\_message\"\n\nADD\\_AGENTS = \"add\\_agents\"\n\nUPDATE\\_QUEUE = \"update\\_queue\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nRECEIVE\\_MESSAGE = \"receive\\_message\"\n\nERROR = \"error\"\n\nTYPING = \"typing\"\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nclass ParliamentServer:\n\ndef **init**(self, host=\"localhost\", port=8080):\n\nself.host = host\n\nself.port = port\n\nself.parliament = None\n\nself.start\\_server = serve(self.chat, self.host, self.port)\n\ndef configure\\_openai(self):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nasync def chat(self, websocket, path):\n\nasync for message in websocket:\n\ndata = json.loads(message)\n\naction = data.get(\"action\")\n\nif action == SET\\_PARLIAMENT:\n\nawait self.handle\\_set\\_parliament(websocket, data)\n\nelif action == SEND\\_MESSAGE:\n\nawait self.handle\\_send\\_message(websocket, data)\n\nelif action == ADD\\_AGENTS:\n\nawait self.handle\\_add\\_agents(websocket, data)\n\nelif action in \\[TYPING, STOPPED\\_TYPING\\]:\n\nawait self.handle\\_typing(websocket, data)\n\nasync def handle\\_typing(self, websocket, data):\n\nsender = data.get('sender')\n\naction = data.get('action')\n\nis\\_typing = action == TYPING\n\nself.parliament.set\\_typing\\_status(sender, is\\_typing)\n\n\\# Send the typing status update to other clients:\n\nawait websocket.send(json.dumps({\n\n\"action\": action,\n\n\"sender\": sender,\n\n\"is\\_typing\": is\\_typing\n\n}))\n\nasync def handle\\_set\\_parliament(self, websocket, data):\n\nparliament\\_name = data.get(\"name\")\n\nif parliament\\_name:\n\nself.parliament = Parliament(parliament\\_name)\n\nasync def handle\\_send\\_message(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nsender = data.get(\"sender\")\n\nuser\\_message = data.get(\"message\")\n\ntimestamp = datetime.now()\n\nawait websocket.send(json.dumps({\n\n\"action\": RECEIVE\\_MESSAGE,\n\n\"message\": user\\_message,\n\n\"sender\": sender,\n\n\"timestamp\": timestamp.isoformat()}))\n\nawait self.parliament.broadcast\\_message(user\\_message, sender, timestamp)\n\nprint(f\"Broadcast message: {user\\_message}\") # Add logging\n\n\\# Sending agent info after broadcasting message\n\nagent\\_info = await self.parliament.get\\_agents\\_info()\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_AGENTS, \"agents\": agent\\_info}))\n\nawait self.update\\_queue(websocket)\n\nasync def handle\\_add\\_agents(self, websocket, data):\n\nif not self.parliament:\n\nawait self.send\\_error(websocket, \"Parliament not set.\")\n\nreturn\n\nagents\\_data = data.get(\"agents\")\n\nif not agents\\_data:\n\nawait self.send\\_error(websocket, \"No agent data provided.\")\n\nreturn\n\nfor agent\\_data in agents\\_data:\n\nagent\\_name = agent\\_data.get(\"name\") # Extract the name\n\nspeaking\\_pressure\\_increment = int(agent\\_data.get(\"speaking\\_pressure\\_increment\"))\n\nother\\_agents = \", \".join(\\[agent\\['name'\\] for agent in agents\\_data if agent\\['name'\\] != agent\\_name\\])\n\ninitial\\_context = (\n\nf\"Hello assistant, your name is: @{agent\\_data.get('name')} and your context is \"\n\nf\"{agent\\_data.get('context')}. Your job is to help {data.get('username')} by discussing {data.get('topic')} \"\n\nf\"alongside your fellow agents in a parliament, which is a sort \"\n\nf\"of town hall or large meeting, called {self.parliament.name}. The other agents discussing \"\n\nf\"with you are {other\\_agents}, please prefix their name with an @ when mentioning them, and don't @mention yourself. Your talkativity is \"\n\nf\"{agent\\_data.get('speaking\\_pressure\\_increment')} out of 100\"\n\n)\n\nagent\\_type = agent\\_data.get(\"type\")\n\nagent\\_class = self.parliament.agent\\_types.get(agent\\_type) # Get OpenAIAgent or GPT4AllAgent\n\nagent\\_instance = agent\\_class(initial\\_context) # Properly instantiate the class\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nagent\\_instance = OpenAIAgent(initial\\_context) # Create the OpenAIAgent instance\n\nagent = Agent(self.parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket)\n\nself.parliament.add\\_agent(agent) # Pass the agent instance to add\\_agent\n\nelse:\n\nprint(\"Error occurred attempting to get agent message!\")\n\nreturn \"Sorry, I'm unable to generate a relevant response at this time.\"\n\nawait self.send\\_error(websocket, f\"Unknown agent type: {agent\\_type}\")\n\ncontinue\n\nasync def update\\_queue(self, websocket):\n\nif self.parliament:\n\nspeaking\\_queue = self.parliament.get\\_speaking\\_queue()\n\nprint(f\"Queue updated: {speaking\\_queue}\")\n\nawait websocket.send(json.dumps({\"action\": UPDATE\\_QUEUE, \"queue\": speaking\\_queue}))\n\nasync def send\\_error(self, websocket, message):\n\nawait websocket.send(json.dumps({\"action\": ERROR, \"message\": message}))\n\ndef run(self):\n\nself.configure\\_openai()\n\nasyncio.get\\_event\\_loop().run\\_until\\_complete(self.start\\_server)\n\nprint(\"Starting WebSocket server...\")\n\nprint(f\"WebSocket server is running on ws://{self.host}:{self.port}\")\n\nasyncio.get\\_event\\_loop().run\\_forever()\n\nif **name** == \"\\_\\_main\\_\\_\":\n\nserver = ParliamentServer()\n\nserver.run()\n\nimport asyncio\n\nimport re\n\nimport random\n\nimport json\n\nimport openai\n\nfrom openai.error import RateLimitError\n\nimport time\n\nfrom typing import List\n\nfrom collections import deque\n\nPRESSURE\\_INCREMENT\\_INTERVAL = 15\n\nSPEAKING\\_PRESSURE\\_LIMIT = 100\n\nBROADCAST\\_DELAY = 10\n\nRELEVANCY\\_THRESHOLD = 100\n\nSTOPPED\\_TYPING = \"stopped\\_typing\"\n\nUPDATE\\_AGENTS = \"update\\_agents\"\n\nclass Agent:\n\ndef **init**(self, parliament, agent\\_name, speaking\\_pressure\\_increment, initial\\_context, agent\\_type, agent\\_instance, websocket=None):\n\nself.name = agent\\_name\n\nself.speaking\\_pressure\\_increment = speaking\\_pressure\\_increment\n\nself.context = initial\\_context\n\nself.initial\\_context = initial\\_context\n\nself.speaking\\_pressure = speaking\\_pressure\\_increment\n\nself.tagged = False\n\nself.agent\\_type = agent\\_type\n\nself.agent\\_instance = agent\\_instance\n\nself.websocket = websocket\n\nself.parliament = parliament\n\nself.is\\_speaking = False\n\nself.relevancy\\_score = None\n\ndef get\\_response(self, truncated\\_chat\\_history):\n\nagent\\_message = self.get\\_agent\\_message(truncated\\_chat\\_history)\n\nreturn agent\\_message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue\n\nasync def calculate\\_relevancy(self, chat\\_context):\n\nrelevancy\\_score = 0\n\nif self.relevancy\\_score is None or self.initial\\_context != chat\\_context:\n\nprint(\"Calculating relevancy...\")\n\nprompt = f\"you are '{self.initial\\_context}' and you are being asked to respond to '{chat\\_context}'. How relevant are you? Only respond with a number from 0 to 100, if you don't know say 1.\"\n\nprint(f\"Prompt: {prompt}\")\n\nresponse = openai.Completion.create(\n\nengine=\"text-davinci-001\", # or \"text-curie-002\" for Curie\n\nprompt=prompt,\n\ntemperature=0.0,\n\nmax\\_tokens=300\n\n)\n\nprint(f\"Response: {response}\")\n\n\\# Here we assume the model's response will be a single number.\n\n\\# The strip is used to remove any whitespace around the number\n\nstripped\\_response = re.sub(\"\\[^0-9\\]\", \"\", response.choices\\[0\\].text)\n\ntry:\n\nrelevancy\\_score = int(stripped\\_response)\n\nexcept ValueError:\n\nrelevancy\\_score = 0\n\nprint(f\"Relevancy score: {relevancy\\_score}\")\n\nelse:\n\nprint(\"Relevancy score already calculated.\")\n\ntime.sleep(.5)\n\nreturn relevancy\\_score\n\ndef should\\_speak(self):\n\nreturn self.parliament.speaker\\_manager.should\\_speak(self) # Call parliament's speaker\\_manager\n\nasync def increment\\_pressure(self):\n\nwhile True:\n\nself.speaking\\_pressure += self.speaking\\_pressure\\_increment\n\nasync def get\\_agent\\_message(self, truncated\\_chat\\_history):\n\n\"\"\"\n\nGenerate a message from this agent based on the current chat history.\n\n\"\"\"\n\nmessage = \"\"\n\nif isinstance(self.agent\\_instance, self.parliament.agent\\_types\\[self.agent\\_type\\]):\n\nresponse = await self.agent\\_instance.get\\_response(truncated\\_chat\\_history)\n\nmessage = f\"{response}\"\n\nreturn message\n\ndef in\\_queue(self):\n\nreturn self.name in self.parliament.speaking\\_queue.queue\n\nasync def speak(self):\n\ntruncated\\_chat\\_history = self.parliament.chat\\_history.truncated\\_chat\\_history\n\nif self.should\\_speak():\n\nself.is\\_speaking = True\n\nawait self.websocket.send(json.dumps({\"action\": \"typing\", \"sender\": self.name}))\n\nawait self.websocket.send(json.dumps({\"action\": STOPPED\\_TYPING, \"sender\": self.name}))\n\nagent\\_message = await self.get\\_response(self.parliament.chat\\_history.truncated\\_chat\\_history)\n\nawait self.websocket.send(json.dumps({\"action\": \"receive\\_message\", \"message\": agent\\_message, \"sender\": self.name}))\n\nself.speaking\\_pressure = 0\n\nself.is\\_speaking = False\n\nif self.in\\_queue():\n\nself.parliament.speaking\\_queue.remove(self.name)\n\ndef reset\\_pressure(self):\n\nself.speaking\\_pressure = 0\n\nclass ChatHistory:\n\ndef **init**(self):\n\nself.chat\\_history = \\[\\]\n\nself.truncated\\_chat\\_history = \"\"\n\ndef add(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\nself.truncate\\_chat\\_history()\n\ndef truncate\\_chat\\_history(self, max\\_length=2000):\n\n\"\"\"\n\nTruncate the chat history to the last max\\_length characters.\n\n\"\"\"\n\nchat\\_history\\_str = '\\\\n'.join(\\[message\\[\"message\"\\] for message in self.chat\\_history\\])\n\nself.truncated\\_chat\\_history = chat\\_history\\_str\\[-max\\_length:\\]\n\ndef last\\_message(self):\n\nreturn self.chat\\_history\\[-1\\] if self.chat\\_history else None\n\nclass SpeakerManager:\n\ndef **init**(self, parliament):\n\nself.parliament = parliament\n\ndef should\\_speak(self, agent):\n\nreturn self.parliament.speaking\\_queue\\[0\\] == agent.name if self.parliament.speaking\\_queue else False\n\nclass OpenAIAgent:\n\ndef **init**(self, initial\\_context: str):\n\nopenai.organization = \"org-ADENd47ZFNjAt0kqYo7foBYK\"\n\nopenai.api\\_key = \"sk-XXXXXXXXXXXXXXXXXXXX_REDACTED_REDACTED_REDACTED\"\n\nself.model = \"gpt-3.5-turbo\"\n\nself.initial\\_context = initial\\_context\n\ndef get\\_response(self, truncated\\_chat):\n\nsystem\\_msg = self.initial\\_context\n\nmessages=\\[\\]\n\nmessages.append({\"role\": \"assistant\", \"content\": truncated\\_chat})\n\nmax\\_retries = 3\n\nretry\\_delay = 5\n\nfor i in range(max\\_retries):\n\ntry:\n\n\\# Call the OpenAI API with the messages array\n\nresponse = openai.ChatCompletion.create(\n\nmodel=self.model,\n\nmessages=messages,\n\nmax\\_tokens=150,\n\ntemperature=0.5,\n\ntop\\_p=1,\n\nfrequency\\_penalty=0,\n\npresence\\_penalty=0,\n\n)\n\n\\# Extract and return the assistant's response from the API response\n\nagent\\_response = response.choices\\[0\\].message\\[\"content\"\\].strip()\n\nprint(f\" {agent\\_response}\") # Add logging\n\nreturn agent\\_response\n\nexcept RateLimitError as e:\n\nprint(f\"Rate limit error: {e}. Retrying in {retry\\_delay} seconds...\")\n\nif i < max\\_retries - 1: # Don't sleep on the last iteration\n\ntime.sleep(retry\\_delay)\n\nelse:\n\nraise\n\nreturn \"Sorry, I am unable to respond at the moment.\"\n\nclass GPT4AllAgent:\n\ndef **init**(self):\n\n\\# Download and set up the GPT4All model\n\nself.\\_initialize\\_gpt4all\\_model()\n\nself.\\_initialize\\_llm\\_chain()\n\ndef _initialize_gpt4all\\_model(self):\n\nlocal\\_path = \"/Users/bilalghalib/Projects/scripts/models/ggml-gpt4all-j.bin\"\n\nPath(local\\_path).parent.mkdir(parents=True, exist\\_ok=True)\n\nurl = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n\nresponse = requests.get(url, stream=True)\n\nwith open(local\\_path, 'wb') as f:\n\nfor chunk in tqdm(response.iter\\_content(chunk\\_size=8192)):\n\nif chunk:\n\nf.write(chunk)\n\nself.model\\_path = local\\_path\n\ndef _initialize_llm\\_chain(self):\n\ntemplate = \"\"\"{chat\\_history}\n\nQuestion: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input\\_variables=\\[\"chat\\_history\", \"question\"\\])\n\ncallbacks = \\[StreamingStdOutCallbackHandler()\\]\n\nllm = GPT4All(model=self.model\\_path, callbacks=callbacks, verbose=True)\n\nself.llm\\_chain = LLMChain(prompt=prompt, llm=llm)\n\ndef get\\_response(self, chat\\_history, question):\n\nreturn self.llm\\_chain.run(chat\\_history=chat\\_history, question=question)\n\nclass Parliament:\n\ndef **init**(self, name: str):\n\nself.name = name\n\nself.speaker\\_manager = SpeakerManager(self)\n\nself.agents = {}\n\nself.chat\\_history = ChatHistory()\n\nself.speaking\\_queue = \\[\\]\n\nself.agent\\_types = {\"openai\": OpenAIAgent, \"gpt4all\": GPT4AllAgent}\n\nself.typing\\_status = {}\n\ndef set\\_typing\\_status(self, agent\\_name: str, status: bool):\n\nself.typing\\_status\\[agent\\_name\\] = status\n\ndef get\\_typing\\_status(self, agent\\_name: str):\n\nreturn self.typing\\_status.get(agent\\_name, False)\n\nasync def broadcast\\_message(self, message, sender, timestamp):\n\n\\# Assign a value to first\\_agent\\_in\\_queue before using it\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue\\[0\\] if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\nself.chat\\_history.add(message, sender, timestamp)\n\nmentioned\\_agents = self.identify\\_and\\_tag\\_mentioned\\_agents(message)\n\nself.increment\\_speaking\\_pressure(mentioned\\_agents)\n\nawait self.update\\_speaking\\_queue()\n\nfor agent in self.agents.values():\n\nif not self.get\\_typing\\_status(agent.name):\n\nawait agent.speak()\n\nawait asyncio.sleep(BROADCAST\\_DELAY)\n\nfirst\\_agent\\_in\\_queue = self.speaking\\_queue.pop(0) if self.speaking\\_queue else None\n\nif first\\_agent\\_in\\_queue and not self.get\\_typing\\_status(first\\_agent\\_in\\_queue):\n\nawait self.agents\\[first\\_agent\\_in\\_queue\\].speak()\n\ndef add\\_to\\_speaking\\_queue(self, agent):\n\nif agent.name not in self.speaking\\_queue:\n\nself.speaking\\_queue.append(agent)\n\ndef increment\\_speaking\\_pressure(self, mentioned\\_agents):\n\nfor agent in self.agents.values():\n\nif agent.name not in mentioned\\_agents:\n\nagent.speaking\\_pressure += agent.speaking\\_pressure\\_increment\n\ndef identify\\_and\\_tag\\_mentioned\\_agents(self, message):\n\nmentioned\\_agents = \\[\\]\n\nfor agent\\_name in self.agents:\n\nif '@'+agent\\_name in message:\n\nmentioned\\_agents.append(agent\\_name)\n\nself.agents\\[agent\\_name\\].speaking\\_pressure = SPEAKING\\_PRESSURE\\_LIMIT\n\nself.agents\\[agent\\_name\\].tagged = True\n\nreturn mentioned\\_agents\n\ndef get\\_speaking\\_queue(self):\n\nreturn self.speaking\\_queue\n\ndef add\\_agent(self, agent: Agent):\n\nself.agents\\[agent.name\\] = agent\n\nself.typing\\_status\\[agent.name\\] = False\n\ndef remove\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\ndel self.agents\\[agent\\_name\\]\n\ndef get\\_agent(self, agent\\_name: str):\n\nif agent\\_name not in self.agents:\n\nraise ValueError(f\"Agent '{agent\\_name}' not found.\")\n\nreturn self.agents\\[agent\\_name\\]\n\ndef add\\_to\\_chat\\_history(self, message, sender, timestamp):\n\nself.chat\\_history.append({\"sender\": sender, \"message\": message, \"timestamp\": timestamp})\n\ndef add\\_tagged\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.tagged and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.tagged = False\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\ndef add\\_high\\_pressure\\_agents\\_to\\_queue(self):\n\nfor agent in self.agents.values():\n\nif agent.speaking\\_pressure > 100 and agent.name not in self.speaking\\_queue.queue:\n\nself.speaking\\_queue.add(agent.name)\n\nagent.speaking\\_pressure = 0 # Reset speaking pressure\n\nasync def trigger\\_next\\_speaker(self):\n\nif self.speaking\\_queue:\n\nnext\\_speaker\\_name = self.speaking\\_queue.pop(0)\n\nnext\\_speaker = self.get\\_agent(next\\_speaker\\_name)\n\nresponse = await next\\_speaker.get\\_response(self.chat\\_history.truncated\\_chat\\_history)\n\nprint(f\"The next speaker is: {next\\_speaker.name}\")\n\nprint(f\"Message: {response}\")\n\nasync def update\\_speaking\\_queue(self):\n\n\"\"\"Update the speaking queue by calculating relevancy for each agent and adding the most relevant agents to the queue.\"\"\"\n\nmost\\_relevant\\_agent = None\n\nhighest\\_relevancy\\_score = -1\n\nfor agent in self.agents.values():\n\ntruncated\\_chat\\_history = self.chat\\_history.truncated\\_chat\\_history\n\nrelevancy\\_score = await agent.calculate\\_relevancy(truncated\\_chat\\_history)\n\nif relevancy\\_score > highest\\_relevancy\\_score:\n\nhighest\\_relevancy\\_score = relevancy\\_score\n\nmost\\_relevant\\_agent = agent\n\nprint(f\"Calculated relevancy for agent {agent.name}. Score: {relevancy\\_score}\")\n\n\\# Add the most relevant agent to the speaking queue\n\nif most\\_relevant\\_agent and self.speaking\\_queue.count(most\\_relevant\\_agent.name) == 0:\n\nself.speaking\\_queue.insert(0, most\\_relevant\\_agent.name)\n\nprint(f\"Added agent {most\\_relevant\\_agent.name} to the front of the speaking queue. Current queue: {self.speaking\\_queue}\")\n\nself.add\\_tagged\\_agents\\_to\\_queue()\n\nself.add\\_high\\_pressure\\_agents\\_to\\_queue()\n\nprint(f\"Triggering next speaker...\")\n\nawait self.trigger\\_next\\_speaker()\n\n#def get\\_sorted\\_agents\\_based\\_on\\_relevancy(self):\n\n\\# return sorted(self.agents.values(), key=lambda agent: agent.calculate\\_relevancy(self.chat\\_history) if agent.calculate\\_relevancy(self.chat\\_history) is not None else 0, reverse=True)\n\nasync def get\\_agents\\_info(self) -> List\\[dict\\]:\n\nagents\\_info = \\[\\]\n\nfor agent\\_name, agent in self.agents.items():\n\nagents\\_info.append({\n\n\"name\": agent\\_name,\n\n\"speaking\\_pressure\": agent.speaking\\_pressure,\n\n\"speaking\\_pressure\\_increment\": agent.speaking\\_pressure\\_increment,\n\n\"context\": agent.context,\n\n\"agent\\_type\": agent.agent\\_type,\n\n\"relevancy\\_score\": agent.relevancy\\_score # Return the cached score\n\n})\n\nreturn agents\\_info",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T09:26:18.063383Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "fa3c2a57-9190-4454-b3ca-36fc8144699b",
      "role": "assistant",
      "content": " This error is occurring because ParliamentServer does not have an agent_instance attribute. That check is incorrectly placed within the server code, rather than in the Agent or Parliament classes where self.agent_instance would exist.\n\nTo fix this, you'll need to move that check into the Agent class where self.agent_instance is defined. A suggested fix would be:\n\nIn Agent:\n```py\nasync def get_agent_message(self, truncated_chat_history):\n    \"\"\"\n    Generate a message from this agent based on the current chat history.\n    \"\"\"\n    message = \"\"\n    if isinstance(self.agent_instance, self.parliament.agent_types[self.agent_type]):\n        response = await self.agent_instance.get_response(truncated_chat_history)\n        message = f\"{response}\"\n    else:\n        print(\"Error occurred attempting to get agent message!\")\n        message = \"Sorry, I'm unable to generate a relevant response at this time.\"\n    return message \n```\n\nIn ParliamentServer's handle_add_agents() method:\n```py\nagent = Agent(self.parliament, agent_name, speaking_pressure_increment, initial_context, agent_type, agent_instance, websocket)\n```\n\nWith these changes, the check for the agent_instance type is properly handled within the Agent class. The ParliamentServer code simply instantiates the Agent, passing the expected agent_instance, without needing to check its type.\n\nOther notes:\n\n- The else block to return a default message if an invalid agent_instance is passed should be retained. This handles any unforeseen edge cases.\n\n- Double check that agent_class is properly set based on agent_type before instantiating the class. If agent_class ends up None, a TypeError will be raised when trying to instantiate it.\n\nPlease let me know if this helps explain and resolve the issue, or if you have any other questions!",
      "created_at": "2023-05-14T09:28:25.328630Z",
      "sequence_order": 61,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2023-05-14T09:28:25.328630Z",
            "stop_timestamp": "2023-05-14T09:28:25.328630Z",
            "flags": null,
            "type": "text",
            "text": " This error is occurring because ParliamentServer does not have an agent_instance attribute. That check is incorrectly placed within the server code, rather than in the Agent or Parliament classes where self.agent_instance would exist.\n\nTo fix this, you'll need to move that check into the Agent class where self.agent_instance is defined. A suggested fix would be:\n\nIn Agent:\n```py\nasync def get_agent_message(self, truncated_chat_history):\n    \"\"\"\n    Generate a message from this agent based on the current chat history.\n    \"\"\"\n    message = \"\"\n    if isinstance(self.agent_instance, self.parliament.agent_types[self.agent_type]):\n        response = await self.agent_instance.get_response(truncated_chat_history)\n        message = f\"{response}\"\n    else:\n        print(\"Error occurred attempting to get agent message!\")\n        message = \"Sorry, I'm unable to generate a relevant response at this time.\"\n    return message \n```\n\nIn ParliamentServer's handle_add_agents() method:\n```py\nagent = Agent(self.parliament, agent_name, speaking_pressure_increment, initial_context, agent_type, agent_instance, websocket)\n```\n\nWith these changes, the check for the agent_instance type is properly handled within the Agent class. The ParliamentServer code simply instantiates the Agent, passing the expected agent_instance, without needing to check its type.\n\nOther notes:\n\n- The else block to return a default message if an invalid agent_instance is passed should be retained. This handles any unforeseen edge cases.\n\n- Double check that agent_class is properly set based on agent_type before instantiating the class. If agent_class ends up None, a TypeError will be raised when trying to instantiate it.\n\nPlease let me know if this helps explain and resolve the issue, or if you have any other questions!",
            "citations": []
          }
        ],
        "updated_at": "2023-05-14T09:28:25.328630Z",
        "attachments": [],
        "files": []
      }
    }
  ]
}