{
  "id": "5559aa2e-75fa-4dd1-955b-768dcc7006c1",
  "source": "claude",
  "title": "Merging or Creating Average Score Columns",
  "created_at": "2024-06-18T15:33:35.933245Z",
  "updated_at": "2024-06-19T05:45:49.641572Z",
  "summary": "",
  "message_count": 46,
  "messages": [
    {
      "id": "b41afdcc-42fd-4917-a761-ff2aca1ba386",
      "role": "human",
      "content": "help me update the code so that if the average columns exist it will merge the. new score recieved with it, otherwise it will make the columns",
      "created_at": "2024-06-18T15:43:05.847327Z",
      "sequence_order": 0,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T15:43:05.847327Z",
            "stop_timestamp": "2024-06-18T15:43:05.847327Z",
            "flags": null,
            "type": "text",
            "text": "help me update the code so that if the average columns exist it will merge the. new score recieved with it, otherwise it will make the columns",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T15:43:05.847327Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 9705,
            "file_type": "txt",
            "extracted_content": "when it loads each row in the dataframe it should send it the user prompt in the YAML only once for each of the 5 questions, then it sohuld only send the numbers 1 through 5\nalso we have an error again:\n\n\ncode:\nimport nltk\nimport pandas as pd\nimport json\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt = config['prompts']['system_prompt']\nuser_prompt = config['prompts']['user_prompt']\n\n# Initialize Ollama client\nclient = ollama.Client()\n\ndef reset_ollama_client():\n    global client\n    client._client.close()\n    client = ollama.Client()\n\n# %%\ndata = pd.read_csv('./updated_activities_with_clarity_instruct_20240523012406_1287.csv').head(2)\n\ndef extract_json_and_remaining_text(text):\n    json_pattern = re.compile(r'\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}', re.DOTALL)\n    matches = json_pattern.findall(text)\n    \n    extracted_json = None\n    for match in matches:\n        try:\n            json_object = json.loads(match)\n            extracted_json = json_object\n            break\n        except json.JSONDecodeError:\n            continue\n\n    remaining_text = re.sub(json_pattern, '', text)\n    return extracted_json, remaining_text.strip()\n\ndef get_clarity_instruct_score(description_without_html, question_number):\n    try:\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'system': system_prompt,\n                'content': user_prompt + f\"\\n{description_without_html}\"\n            },\n            {\n                'role': 'user',\n                'content': str(question_number)\n            }\n        ])\n        print(f\"Sent to Ollama for question {question_number}: {user_prompt} {description_without_html}\\nQuestion: {question_number}\")\n        score = response['message']['content']\n        if score.isdigit():\n            score = int(score)\n            if 20 <= score <= 100:\n                print(f\"Received from Ollama for question {question_number}: {score}\")\n                return score\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    return None\n\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\ndata['readability_score'] = data['description_without_html'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['fullDescription'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['fullDescription'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['fullDescription'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['fullDescription'].apply(check_worksheet)\n\ndef get_scores_for_row(description):\n    scores = {}\n    for q in range(1, 6):\n        question_score = get_clarity_instruct_score(description, q)\n        if question_score is not None:\n            scores[f'question_{q}_score'] = question_score\n    return scores\n\ndef update_or_add_scores(row_scores, index, df):\n    for key, new_score in row_scores.items():\n        if key not in df.columns:\n            df[key] = None\n        if pd.notna(df.at[index, key]):\n            df.at[index, key] = (df.at[index, key] + new_score) / 2\n        else:\n            df.at[index, key] = new_score\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores = get_scores_for_row(description)\n    update_or_add_scores(scores, index, data)\n    print(f\"Updated DataFrame Columns: {data.columns}\")\n\n# Calculate the average score for each row\nquestion_columns = ['question_1_score', 'question_2_score', 'question_3_score', 'question_4_score', 'question_5_score']\ndata['average_score'] = data[question_columns].mean(axis=1)\n\nprint(data.head())\n\n# Save the updated DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n\n# Reset the Ollama client after processing all rows\nreset_ollama_client()\n\n\nOutput:\n\nQuestion: 3\nSent to Ollama for question 4: Afif, your job is to assess the rubric element Clarity_of_Instructions for the following activity encased in brackets below \nActivity follows here between 3 brackets:\n[[[shortCodetitle shortDescription fullDescription categories\n23Ab34 ðŸŽ¥ Video: How to Talk to Users \"Welcome!  ðŸŒ± In this video, Eric Migicovsky shares a framework for asking questions and collecting feedback from your users. \nHe recommends asking these questions: \nðŸŒ» What's the hardest thing about [doing this thing]?\nðŸŒ» Tell me about the last time you encountered that problem...\nðŸŒ» Why was that hard?\nðŸŒ» What, if anything, have you done to try to solve the problem?\nðŸŒ» What don't you love about the solutions you've tried?\nLet's get started!  ðŸŽ¬ Watch the video hereApply your knowledge!  Have you talked to your users or customers? Reach out personally to three people and start a conversation with the questions introduced in the video. \n\" ðŸ“² Products and Services:::ðŸ—£ Speaking]]]\nI will ask you for a question by its number give me the score and only the score as an integer with no additional information or text.\nStart by giving the score for question\n Welcome!  Consider how this statement describes your usual mental state: \"A still mind sees what is good and present. An anxious mind sees what is negative and missing.\" Still your mind for five minutes in the morning and in the evening.\n\nQuestion: 4\nSent to Ollama for question 5: Afif, your job is to assess the rubric element Clarity_of_Instructions for the following activity encased in brackets below \nActivity follows here between 3 brackets:\n[[[shortCodetitle shortDescription fullDescription categories\n23Ab34 ðŸŽ¥ Video: How to Talk to Users \"Welcome!  ðŸŒ± In this video, Eric Migicovsky shares a framework for asking questions and collecting feedback from your users. \nHe recommends asking these questions: \nðŸŒ» What's the hardest thing about [doing this thing]?\nðŸŒ» Tell me about the last time you encountered that problem...\nðŸŒ» Why was that hard?\nðŸŒ» What, if anything, have you done to try to solve the problem?\nðŸŒ» What don't you love about the solutions you've tried?\nLet's get started!  ðŸŽ¬ Watch the video hereApply your knowledge!  Have you talked to your users or customers? Reach out personally to three people and start a conversation with the questions introduced in the video. \n\" ðŸ“² Products and Services:::ðŸ—£ Speaking]]]\nI will ask you for a question by its number give me the score and only the score as an integer with no additional information or text.\nStart by giving the score for question\n Welcome!  Consider how this statement describes your usual mental state: \"A still mind sees what is good and present. An anxious mind sees what is negative and missing.\" Still your mind for five minutes in the morning and in the evening.\n\nQuestion: 5\nUpdated DataFrame Columns: Index(['_id', 'shortCode', 'title', 'shortDescription', 'fullDescription',\n       'categories', 'labels', 'status', 'imageId', 'imageLink',\n       'newImageGdriveLink', 'createdBy_email', 'createdBy_username',\n       'reviewNotes', 'subActivitiesShortCodes', 'clarity_computed_Bool',\n       'clarity_instruct_score', 'clarity_instruction_Explanation',\n       'clarity_response', 'description_without_html',\n       'characters_count_total', 'readability_score',\n       'appropriate_readability_bool', '60_to_300_characters_bool',\n       'has_apply_knowledge_bool', 'has_worksheet_bool', 'clear_bool'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/Users/bilalghalib/Projects/localLLM/LibraryUpdates/createClarityScore.py\", line 135, in <module>\n    data['average_score'] = data[question_columns].mean(axis=1)\n  File \"/Users/bilalghalib/mambaforge/envs/py39_native/lib/python3.9/site-packages/pandas/core/frame.py\", line 4108, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n  File \"/Users/bilalghalib/mambaforge/envs/py39_native/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6200, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/Users/bilalghalib/mambaforge/envs/py39_native/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6249, in _raise_if_missing\n    raise KeyError(f\"None of [{key}] are in the [{axis_name}]\")\nKeyError: \"None of [Index(['question_1_score', 'question_2_score', 'question_3_score',\\n       'question_4_score', 'question_5_score'],\\n      dtype='object')] are in the [columns]\""
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          }
        ]
      }
    },
    {
      "id": "152bdd75-057a-4f83-bda1-cdccc08864c5",
      "role": "assistant",
      "content": " To update the code to handle merging or creating new columns for the average scores, you can modify the `update_or_add_scores` function and the subsequent code that calculates the average score. Here's the updated code:\n\n```python\ndef update_or_add_scores(row_scores, index, df):\n    for key, new_score in row_scores.items():\n        if key not in df.columns:\n            df[key] = None\n        if pd.notna(df.at[index, key]):\n            df.at[index, key] = (df.at[index, key] + new_score) / 2\n        else:\n            df.at[index, key] = new_score\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores = get_scores_for_row(description)\n    update_or_add_scores(scores, index, data)\n    print(f\"Updated DataFrame Columns: {data.columns}\")\n\n# Calculate the average score for each row\nquestion_columns = ['question_1_score', 'question_2_score', 'question_3_score', 'question_4_score', 'question_5_score']\nexisting_question_columns = [col for col in question_columns if col in data.columns]\n\nif existing_question_columns:\n    data['average_score'] = data[existing_question_columns].mean(axis=1)\nelse:\n    data['average_score'] = None\n\nprint(data.head())\n```\n\nIn the updated code:\n\n1. The `update_or_add_scores` function remains the same. It checks if a column exists in the DataFrame and updates the score by averaging it with the existing score if the column already exists, or creates a new column with the score if it doesn't exist.\n\n2. After updating the scores for each row, we define the `question_columns` list as before.\n\n3. We create a new list called `existing_question_columns` by filtering the `question_columns` list to include only the columns that actually exist in the DataFrame.\n\n4. We check if `existing_question_columns` is not empty. If it is not empty, we calculate the average score using only the existing question columns. If it is empty, we assign `None` to the `average_score` column.\n\nWith these changes, the code will handle merging the new scores with existing scores if the columns already exist, or create new columns for the scores if they don't exist. The average score will be calculated based on the available question columns.\n\nLet me know if you have any further questions!",
      "created_at": "2024-06-18T15:43:05.847327Z",
      "sequence_order": 1,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T15:43:05.847327Z",
            "stop_timestamp": "2024-06-18T15:43:05.847327Z",
            "flags": null,
            "type": "text",
            "text": " To update the code to handle merging or creating new columns for the average scores, you can modify the `update_or_add_scores` function and the subsequent code that calculates the average score. Here's the updated code:\n\n```python\ndef update_or_add_scores(row_scores, index, df):\n    for key, new_score in row_scores.items():\n        if key not in df.columns:\n            df[key] = None\n        if pd.notna(df.at[index, key]):\n            df.at[index, key] = (df.at[index, key] + new_score) / 2\n        else:\n            df.at[index, key] = new_score\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores = get_scores_for_row(description)\n    update_or_add_scores(scores, index, data)\n    print(f\"Updated DataFrame Columns: {data.columns}\")\n\n# Calculate the average score for each row\nquestion_columns = ['question_1_score', 'question_2_score', 'question_3_score', 'question_4_score', 'question_5_score']\nexisting_question_columns = [col for col in question_columns if col in data.columns]\n\nif existing_question_columns:\n    data['average_score'] = data[existing_question_columns].mean(axis=1)\nelse:\n    data['average_score'] = None\n\nprint(data.head())\n```\n\nIn the updated code:\n\n1. The `update_or_add_scores` function remains the same. It checks if a column exists in the DataFrame and updates the score by averaging it with the existing score if the column already exists, or creates a new column with the score if it doesn't exist.\n\n2. After updating the scores for each row, we define the `question_columns` list as before.\n\n3. We create a new list called `existing_question_columns` by filtering the `question_columns` list to include only the columns that actually exist in the DataFrame.\n\n4. We check if `existing_question_columns` is not empty. If it is not empty, we calculate the average score using only the existing question columns. If it is empty, we assign `None` to the `average_score` column.\n\nWith these changes, the code will handle merging the new scores with existing scores if the columns already exist, or create new columns for the scores if they don't exist. The average score will be calculated based on the available question columns.\n\nLet me know if you have any further questions!",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T15:43:05.847327Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "a29a8f7b-e6c7-4b96-8de2-d6366192302f",
      "role": "human",
      "content": "when it loads each row in the dataframe it should send it the user prompt in the YAML only once for each of the 5 questions, then it sohuld only send the numbers 1 through 5 this is our YAML file:\nsettings:\n  model: llama3\nprompts:\n  system_prompt: |\n    you are afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online training entrepreneurship training program. \n    You will see the following sub-questions for Clarity_of_Instructions from 0 being very poor to 100 for excellent and will return only an integer and no other information or text.: \n    1. Is there an outline?\n    2. Is the outline comprehensive? \n    3. Is the activity written clearly with no jargon \n    4. Does the activity have any grammatical problems? \n    5. Does the activity have actionable directions?\n  user_prompt: |\n    Afif, your job is to assess the rubric element Clarity_of_Instructions for the following activity encased in 3 brackets below:\n    [[[shortCodetitle shortDescription fullDescription categories\n    23Ab34 ðŸŽ¥ Video: How to Talk to Users \"Welcome!  ðŸŒ± In this video, Eric Migicovsky shares a framework for asking questions and collecting feedback from your users. \n    He recommends asking these questions: \n    ðŸŒ» What's the hardest thing about [doing this thing]?\n    ðŸŒ» Tell me about the last time you encountered that problem...\n    ðŸŒ» Why was that hard?\n    ðŸŒ» What, if anything, have you done to try to solve the problem?\n    ðŸŒ» What don't you love about the solutions you've tried?\n    Let's get started!  ðŸŽ¬ Watch the video hereApply your knowledge!  Have you talked to your users or customers? Reach out personally to three people and start a conversation with the questions introduced in the video. \n    \" ðŸ“² Products and Services:::ðŸ—£ Speaking]]]\n    I will ask you a question by its number. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\n  end_prompt: |\n    Please return the average of all questions as an integer without any additional text or information",
      "created_at": "2024-06-18T15:44:44.451828Z",
      "sequence_order": 2,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T15:44:44.451828Z",
            "stop_timestamp": "2024-06-18T15:44:44.451828Z",
            "flags": null,
            "type": "text",
            "text": "when it loads each row in the dataframe it should send it the user prompt in the YAML only once for each of the 5 questions, then it sohuld only send the numbers 1 through 5 this is our YAML file:\nsettings:\n  model: llama3\nprompts:\n  system_prompt: |\n    you are afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online training entrepreneurship training program. \n    You will see the following sub-questions for Clarity_of_Instructions from 0 being very poor to 100 for excellent and will return only an integer and no other information or text.: \n    1. Is there an outline?\n    2. Is the outline comprehensive? \n    3. Is the activity written clearly with no jargon \n    4. Does the activity have any grammatical problems? \n    5. Does the activity have actionable directions?\n  user_prompt: |\n    Afif, your job is to assess the rubric element Clarity_of_Instructions for the following activity encased in 3 brackets below:\n    [[[shortCodetitle shortDescription fullDescription categories\n    23Ab34 ðŸŽ¥ Video: How to Talk to Users \"Welcome!  ðŸŒ± In this video, Eric Migicovsky shares a framework for asking questions and collecting feedback from your users. \n    He recommends asking these questions: \n    ðŸŒ» What's the hardest thing about [doing this thing]?\n    ðŸŒ» Tell me about the last time you encountered that problem...\n    ðŸŒ» Why was that hard?\n    ðŸŒ» What, if anything, have you done to try to solve the problem?\n    ðŸŒ» What don't you love about the solutions you've tried?\n    Let's get started!  ðŸŽ¬ Watch the video hereApply your knowledge!  Have you talked to your users or customers? Reach out personally to three people and start a conversation with the questions introduced in the video. \n    \" ðŸ“² Products and Services:::ðŸ—£ Speaking]]]\n    I will ask you a question by its number. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\n  end_prompt: |\n    Please return the average of all questions as an integer without any additional text or information",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T15:44:44.451828Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "1021b5f0-a6ce-4261-80da-9488c32ccc0b",
      "role": "assistant",
      "content": " To update the code to send the user prompt only once for each row and then send only the question numbers, you can modify the `get_scores_for_row` function. Here's the updated code:\n\n```python\ndef get_scores_for_row(description):\n    scores = {}\n    \n    # Send the user prompt only once for each row\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt,\n            'content': user_prompt.replace('[[[shortCodetitle shortDescription fullDescription categories', f'[[[{description}')\n        },\n        {\n            'role': 'user',\n            'content': 'yes'\n        }\n    ])\n    \n    for q in range(1, 6):\n        # Send only the question number\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': str(q)\n            }\n        ])\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 20 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n    \n    return scores\n```\n\nIn the updated code:\n\n1. Inside the `get_scores_for_row` function, we first send the user prompt only once for each row. We replace the placeholder `[[[shortCodetitle shortDescription fullDescription categories` with the actual `description` for the current row.\n\n2. After sending the user prompt, we send a response of 'yes' to indicate that we are ready to proceed with the questions.\n\n3. We iterate over the question numbers from 1 to 5 using a `for` loop.\n\n4. For each question, we send only the question number as the user's content.\n\n5. We receive the response from the model and extract the question score.\n\n6. If the question score is a valid integer between 20 and 100, we add it to the `scores` dictionary with the corresponding key.\n\n7. Finally, we return the `scores` dictionary containing the scores for each question.\n\nWith these modifications, the code will send the user prompt only once for each row and then send only the question numbers for each question. The rest of the code remains the same.\n\nLet me know if you have any further questions!",
      "created_at": "2024-06-18T15:44:44.451828Z",
      "sequence_order": 3,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T15:44:44.451828Z",
            "stop_timestamp": "2024-06-18T15:44:44.451828Z",
            "flags": null,
            "type": "text",
            "text": " To update the code to send the user prompt only once for each row and then send only the question numbers, you can modify the `get_scores_for_row` function. Here's the updated code:\n\n```python\ndef get_scores_for_row(description):\n    scores = {}\n    \n    # Send the user prompt only once for each row\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt,\n            'content': user_prompt.replace('[[[shortCodetitle shortDescription fullDescription categories', f'[[[{description}')\n        },\n        {\n            'role': 'user',\n            'content': 'yes'\n        }\n    ])\n    \n    for q in range(1, 6):\n        # Send only the question number\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': str(q)\n            }\n        ])\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 20 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n    \n    return scores\n```\n\nIn the updated code:\n\n1. Inside the `get_scores_for_row` function, we first send the user prompt only once for each row. We replace the placeholder `[[[shortCodetitle shortDescription fullDescription categories` with the actual `description` for the current row.\n\n2. After sending the user prompt, we send a response of 'yes' to indicate that we are ready to proceed with the questions.\n\n3. We iterate over the question numbers from 1 to 5 using a `for` loop.\n\n4. For each question, we send only the question number as the user's content.\n\n5. We receive the response from the model and extract the question score.\n\n6. If the question score is a valid integer between 20 and 100, we add it to the `scores` dictionary with the corresponding key.\n\n7. Finally, we return the `scores` dictionary containing the scores for each question.\n\nWith these modifications, the code will send the user prompt only once for each row and then send only the question numbers for each question. The rest of the code remains the same.\n\nLet me know if you have any further questions!",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T15:44:44.451828Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "6caf8633-238a-4325-9373-281840e18a1d",
      "role": "human",
      "content": "revised system prompt and approach to code, review it and critique it with 5 pros and 5 cons:\nsettings:\n  model: llama3\nprompts:\n  system_prompt: |\n    you are afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online training entrepreneurship training program. \n    You will see the following sub-questions for Clarity_of_Instructions from 0 being very poor to 100 for excellent and will return only an integer and no other information or text.: \n    1. Is there an outline?\n    2. Is the outline comprehensive? \n    3. Is the activity written clearly with no jargon \n    4. Does the activity have any grammatical problems? \n    5. Does the activity have actionable directions?\n  user_prompt: |\n    Afif, your job is to assess the rubric element Clarity_of_Instructions for the following activity.    \n    After I send the activity, I will ask you a question by its number. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\n  end_prompt: |\n    Please return the average of all questions as an integer without any additional text or information",
      "created_at": "2024-06-18T15:48:49.770116Z",
      "sequence_order": 4,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T15:48:49.770116Z",
            "stop_timestamp": "2024-06-18T15:48:49.770116Z",
            "flags": null,
            "type": "text",
            "text": "revised system prompt and approach to code, review it and critique it with 5 pros and 5 cons:\nsettings:\n  model: llama3\nprompts:\n  system_prompt: |\n    you are afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online training entrepreneurship training program. \n    You will see the following sub-questions for Clarity_of_Instructions from 0 being very poor to 100 for excellent and will return only an integer and no other information or text.: \n    1. Is there an outline?\n    2. Is the outline comprehensive? \n    3. Is the activity written clearly with no jargon \n    4. Does the activity have any grammatical problems? \n    5. Does the activity have actionable directions?\n  user_prompt: |\n    Afif, your job is to assess the rubric element Clarity_of_Instructions for the following activity.    \n    After I send the activity, I will ask you a question by its number. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\n  end_prompt: |\n    Please return the average of all questions as an integer without any additional text or information",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T15:48:49.770116Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 5832,
            "file_type": "txt",
            "extracted_content": "import nltk\nimport pandas as pd\nimport json\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt = config['prompts']['system_prompt']\nuser_prompt = config['prompts']['user_prompt']\n\n# Initialize Ollama client\nclient = ollama.Client()\n\ndef reset_ollama_client():\n    global client\n    client._client.close()\n    client = ollama.Client()\n\n# %%\ndata = pd.read_csv('./updated_activities_with_clarity_instruct_20240523012406_1287.csv').head(2)\n\ndef extract_json_and_remaining_text(text):\n    json_pattern = re.compile(r'\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}', re.DOTALL)\n    matches = json_pattern.findall(text)\n    \n    extracted_json = None\n    for match in matches:\n        try:\n            json_object = json.loads(match)\n            extracted_json = json_object\n            break\n        except json.JSONDecodeError:\n            continue\n\n    remaining_text = re.sub(json_pattern, '', text)\n    return extracted_json, remaining_text.strip()\n\ndef get_clarity_instruct_score(description_without_html, question_number):\n    try:\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'system': system_prompt,\n                'content': user_prompt + f\"\\n{description_without_html}\"\n            },\n            {\n                'role': 'user',\n                'content': str(question_number)\n            }\n        ])\n        print(f\"Sent to Ollama for question {question_number}: {user_prompt} {description_without_html}\\nQuestion: {question_number}\")\n        score = response['message']['content']\n        if score.isdigit():\n            score = int(score)\n            if 20 <= score <= 100:\n                print(f\"Received from Ollama for question {question_number}: {score}\")\n                return score\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    return None\n\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\ndata['readability_score'] = data['description_without_html'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['fullDescription'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['fullDescription'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['fullDescription'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['fullDescription'].apply(check_worksheet)\n\ndef get_scores_for_row(description):\n    scores = {}\n    \n    # Send the user prompt only once for each row\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt,\n            'content': user_prompt.replace('[[[shortCodetitle shortDescription fullDescription categories', f'[[[{description}')\n        },\n        {\n            'role': 'user',\n            'content': 'yes'\n        }\n    ])\n    \n    for q in range(1, 6):\n        # Send only the question number\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': str(q)\n            }\n        ])\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 20 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n    \n    return scores\n    \ndef update_or_add_scores(row_scores, index, df):\n    for key, new_score in row_scores.items():\n        if key not in df.columns:\n            df[key] = None\n        if pd.notna(df.at[index, key]):\n            df.at[index, key] = (df.at[index, key] + new_score) / 2\n        else:\n            df.at[index, key] = new_score\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores = get_scores_for_row(description)\n    update_or_add_scores(scores, index, data)\n    print(f\"Updated DataFrame Columns: {data.columns}\")\n\n# Calculate the average score for each row\nquestion_columns = ['question_1_score', 'question_2_score', 'question_3_score', 'question_4_score', 'question_5_score']\nexisting_question_columns = [col for col in question_columns if col in data.columns]\n\nif existing_question_columns:\n    data['average_score'] = data[existing_question_columns].mean(axis=1)\nelse:\n    data['average_score'] = None\n\nprint(data.head())\n\n# Save the updated DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n\n# Reset the Ollama client after processing all rows\nreset_ollama_client()\n"
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          }
        ]
      }
    },
    {
      "id": "acaa7ea2-cc3c-44fc-941c-4690319d1f37",
      "role": "assistant",
      "content": " Pros:\n1. The revised system prompt provides a clear description of your role as Afif, a monitoring and evaluation expert, and the purpose of assessing the clarity of activities for an online entrepreneurship training program.\n\n2. The sub-questions for Clarity_of_Instructions are well-defined and cover important aspects such as the presence of an outline, clarity of writing, absence of jargon, grammatical correctness, and actionable directions.\n\n3. The user prompt clearly instructs you to assess the rubric element Clarity_of_Instructions for the provided activity and sets expectations for the response format (integer score only).\n\n4. The user prompt also includes a confirmation step to ensure you are ready to proceed with the assessment, which helps maintain a structured flow.\n\n5. The end prompt provides a clear instruction to return the average of all question scores as an integer without any additional text or information, ensuring a concise and standardized response.\n\nCons:\n1. The system prompt doesn't specify the scale for the scores (e.g., 0-100), which could lead to inconsistencies if not clearly communicated elsewhere.\n\n2. The user prompt doesn't explicitly mention that the activity will be enclosed in triple brackets ([[[]]]), which may cause confusion if the format is not consistently followed.\n\n3. The system prompt and user prompt don't provide guidance on how to handle edge cases, such as when an activity doesn't have an outline or when the writing is unclear. Additional instructions on how to score in these situations could be beneficial.\n\n4. The prompts don't specify how to handle activities that may not fit the typical structure or content expected for the assessment. Guidance on how to approach and score such activities could be helpful.\n\n5. The prompts don't include any instructions or considerations for providing constructive feedback or suggestions for improvement alongside the scores. While the current focus is on providing numeric scores, the ability to offer qualitative feedback could enhance the usefulness of the assessment.\n\nRegarding the code, here are a few observations:\n\n1. The code effectively incorporates the revised system prompt and user prompt from the YAML file.\n\n2. The `get_scores_for_row` function now sends the user prompt only once for each row and sends only the question numbers for each question, which aligns with the revised approach.\n\n3. The code handles the case when the average score columns already exist by merging the new scores with the existing ones, or creates new columns if they don't exist.\n\n4. The code saves the updated DataFrame to a new CSV file and resets the Ollama client after processing all rows, which is a good practice.\n\nOverall, the revised prompts provide a clearer structure and guidance for the assessment process, and the code effectively incorporates these changes. However, there may be room for improvement in terms of handling edge cases, providing qualitative feedback, and ensuring consistency in the activity format and scoring scale.",
      "created_at": "2024-06-18T15:48:49.770116Z",
      "sequence_order": 5,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T15:48:49.770116Z",
            "stop_timestamp": "2024-06-18T15:48:49.770116Z",
            "flags": null,
            "type": "text",
            "text": " Pros:\n1. The revised system prompt provides a clear description of your role as Afif, a monitoring and evaluation expert, and the purpose of assessing the clarity of activities for an online entrepreneurship training program.\n\n2. The sub-questions for Clarity_of_Instructions are well-defined and cover important aspects such as the presence of an outline, clarity of writing, absence of jargon, grammatical correctness, and actionable directions.\n\n3. The user prompt clearly instructs you to assess the rubric element Clarity_of_Instructions for the provided activity and sets expectations for the response format (integer score only).\n\n4. The user prompt also includes a confirmation step to ensure you are ready to proceed with the assessment, which helps maintain a structured flow.\n\n5. The end prompt provides a clear instruction to return the average of all question scores as an integer without any additional text or information, ensuring a concise and standardized response.\n\nCons:\n1. The system prompt doesn't specify the scale for the scores (e.g., 0-100), which could lead to inconsistencies if not clearly communicated elsewhere.\n\n2. The user prompt doesn't explicitly mention that the activity will be enclosed in triple brackets ([[[]]]), which may cause confusion if the format is not consistently followed.\n\n3. The system prompt and user prompt don't provide guidance on how to handle edge cases, such as when an activity doesn't have an outline or when the writing is unclear. Additional instructions on how to score in these situations could be beneficial.\n\n4. The prompts don't specify how to handle activities that may not fit the typical structure or content expected for the assessment. Guidance on how to approach and score such activities could be helpful.\n\n5. The prompts don't include any instructions or considerations for providing constructive feedback or suggestions for improvement alongside the scores. While the current focus is on providing numeric scores, the ability to offer qualitative feedback could enhance the usefulness of the assessment.\n\nRegarding the code, here are a few observations:\n\n1. The code effectively incorporates the revised system prompt and user prompt from the YAML file.\n\n2. The `get_scores_for_row` function now sends the user prompt only once for each row and sends only the question numbers for each question, which aligns with the revised approach.\n\n3. The code handles the case when the average score columns already exist by merging the new scores with the existing ones, or creates new columns if they don't exist.\n\n4. The code saves the updated DataFrame to a new CSV file and resets the Ollama client after processing all rows, which is a good practice.\n\nOverall, the revised prompts provide a clearer structure and guidance for the assessment process, and the code effectively incorporates these changes. However, there may be room for improvement in terms of handling edge cases, providing qualitative feedback, and ensuring consistency in the activity format and scoring scale.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T15:48:49.770116Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "c95247f6-202c-4ce5-9c61-2b0957fff3cd",
      "role": "human",
      "content": "address the cons and improve the pros and return the code",
      "created_at": "2024-06-18T16:04:30.032197Z",
      "sequence_order": 6,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T16:04:30.032197Z",
            "stop_timestamp": "2024-06-18T16:04:30.032197Z",
            "flags": null,
            "type": "text",
            "text": "address the cons and improve the pros and return the code",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T16:04:30.032197Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "1ee63039-b1b7-4ed1-bbde-bbf3743c4add",
      "role": "assistant",
      "content": " Here's the updated system prompt and user prompt based on the addressed cons and improved pros:\n\n```yaml\nsettings:\n  model: llama3\nprompts:\n  system_prompt: |\n    You are Afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online entrepreneurship training program.\n    You will be provided with an activity enclosed in triple brackets ([[[]]]) and will assess its Clarity_of_Instructions based on the following sub-questions:\n    1. Is there a clear outline or structure to the activity? (0-100, where 0 is no outline and 100 is a comprehensive outline)\n    2. Is the activity written clearly with minimal jargon and easy-to-understand language? (0-100, where 0 is very unclear and 100 is exceptionally clear)\n    3. Does the activity have any significant grammatical or typographical errors that hinder understanding? (0-100, where 0 indicates many errors and 100 indicates no errors)\n    4. Does the activity provide actionable directions for the learner to follow? (0-100, where 0 is no actionable directions and 100 is clear and comprehensive directions)\n    5. Overall, how would you rate the clarity of instructions for this activity? (0-100, where 0 is very poor clarity and 100 is excellent clarity)\n    Please provide only the integer score for each sub-question, without any additional information or text.\n    If the activity does not fit the expected structure or content, please score based on the available information and provide a brief explanation for your scoring decision.\n  user_prompt: |\n    Afif, your task is to assess the rubric element Clarity_of_Instructions for the following activity, which will be enclosed in triple brackets ([[[]]]).\n    After I send the activity, I will ask you a question by its number. Please provide only the integer score as your response, without any additional information or text.\n    If you encounter any issues or have feedback regarding the activity's structure or content, please wait until the end of the scoring process to provide your comments.\n    Are you ready to begin? Please respond with \"yes\" or \"no\" only.\n  end_prompt: |\n    Please provide the average of all question scores as an integer, followed by any additional feedback or suggestions for improving the clarity of the activity, if applicable.\n```\n\nAnd here's the updated code incorporating the changes:\n\n```python\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    # Send the user prompt only once for each row\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt,\n            'content': user_prompt.replace('the following activity, which will be enclosed in triple brackets ([[[]]]).', f'the following activity:\\n\\n[[[{description}]]]\\n\\n')\n        },\n        {\n            'role': 'user',\n            'content': 'yes'\n        }\n    ])\n\n    for q in range(1, 6):\n        # Send only the question number\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': str(q)\n            }\n        ])\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n            else:\n                scores[f'question_{q}_score'] = 0\n                feedback += f\"Question {q} score is outside the valid range of 0-100. \"\n        else:\n            scores[f'question_{q}_score'] = 0\n            feedback += f\"Question {q} score is not a valid integer. \"\n\n    # Get feedback and suggestions\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'content': 'end'\n        }\n    ])\n    feedback += response['message']['content']\n\n    return scores, feedback.strip()\n\n# ...\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores, feedback = get_scores_for_row(description)\n    update_or_add_scores(scores, index, data)\n    data.at[index, 'feedback'] = feedback\n    print(f\"Updated DataFrame Columns: {data.columns}\")\n\n# ...\n```\n\nThe updated system prompt and user prompt address the cons by:\n1. Specifying the scale for each sub-question (0-100) and providing clear descriptions for the scoring criteria.\n2. Explicitly mentioning that the activity will be enclosed in triple brackets ([[[]]]).\n3. Providing instructions on how to score activities that don't fit the expected structure or content.\n4. Allowing for additional feedback and suggestions to be provided at the end of the scoring process.\n\nThe updated code incorporates these changes by:\n1. Replacing the placeholder in the user prompt with the actual activity enclosed in triple brackets.\n2. Validating the question scores to ensure they are within the valid range of 0-100 and providing feedback if they are not.\n3. Adding a step to retrieve feedback and suggestions from the model after scoring all questions.\n4. Storing the feedback in a new 'feedback' column in the DataFrame.\n\nThese improvements enhance the clarity and robustness of the assessment process while maintaining the effective parts of the previous version.",
      "created_at": "2024-06-18T16:04:30.032197Z",
      "sequence_order": 7,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T16:04:30.032197Z",
            "stop_timestamp": "2024-06-18T16:04:30.032197Z",
            "flags": null,
            "type": "text",
            "text": " Here's the updated system prompt and user prompt based on the addressed cons and improved pros:\n\n```yaml\nsettings:\n  model: llama3\nprompts:\n  system_prompt: |\n    You are Afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online entrepreneurship training program.\n    You will be provided with an activity enclosed in triple brackets ([[[]]]) and will assess its Clarity_of_Instructions based on the following sub-questions:\n    1. Is there a clear outline or structure to the activity? (0-100, where 0 is no outline and 100 is a comprehensive outline)\n    2. Is the activity written clearly with minimal jargon and easy-to-understand language? (0-100, where 0 is very unclear and 100 is exceptionally clear)\n    3. Does the activity have any significant grammatical or typographical errors that hinder understanding? (0-100, where 0 indicates many errors and 100 indicates no errors)\n    4. Does the activity provide actionable directions for the learner to follow? (0-100, where 0 is no actionable directions and 100 is clear and comprehensive directions)\n    5. Overall, how would you rate the clarity of instructions for this activity? (0-100, where 0 is very poor clarity and 100 is excellent clarity)\n    Please provide only the integer score for each sub-question, without any additional information or text.\n    If the activity does not fit the expected structure or content, please score based on the available information and provide a brief explanation for your scoring decision.\n  user_prompt: |\n    Afif, your task is to assess the rubric element Clarity_of_Instructions for the following activity, which will be enclosed in triple brackets ([[[]]]).\n    After I send the activity, I will ask you a question by its number. Please provide only the integer score as your response, without any additional information or text.\n    If you encounter any issues or have feedback regarding the activity's structure or content, please wait until the end of the scoring process to provide your comments.\n    Are you ready to begin? Please respond with \"yes\" or \"no\" only.\n  end_prompt: |\n    Please provide the average of all question scores as an integer, followed by any additional feedback or suggestions for improving the clarity of the activity, if applicable.\n```\n\nAnd here's the updated code incorporating the changes:\n\n```python\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    # Send the user prompt only once for each row\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt,\n            'content': user_prompt.replace('the following activity, which will be enclosed in triple brackets ([[[]]]).', f'the following activity:\\n\\n[[[{description}]]]\\n\\n')\n        },\n        {\n            'role': 'user',\n            'content': 'yes'\n        }\n    ])\n\n    for q in range(1, 6):\n        # Send only the question number\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': str(q)\n            }\n        ])\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n            else:\n                scores[f'question_{q}_score'] = 0\n                feedback += f\"Question {q} score is outside the valid range of 0-100. \"\n        else:\n            scores[f'question_{q}_score'] = 0\n            feedback += f\"Question {q} score is not a valid integer. \"\n\n    # Get feedback and suggestions\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'content': 'end'\n        }\n    ])\n    feedback += response['message']['content']\n\n    return scores, feedback.strip()\n\n# ...\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores, feedback = get_scores_for_row(description)\n    update_or_add_scores(scores, index, data)\n    data.at[index, 'feedback'] = feedback\n    print(f\"Updated DataFrame Columns: {data.columns}\")\n\n# ...\n```\n\nThe updated system prompt and user prompt address the cons by:\n1. Specifying the scale for each sub-question (0-100) and providing clear descriptions for the scoring criteria.\n2. Explicitly mentioning that the activity will be enclosed in triple brackets ([[[]]]).\n3. Providing instructions on how to score activities that don't fit the expected structure or content.\n4. Allowing for additional feedback and suggestions to be provided at the end of the scoring process.\n\nThe updated code incorporates these changes by:\n1. Replacing the placeholder in the user prompt with the actual activity enclosed in triple brackets.\n2. Validating the question scores to ensure they are within the valid range of 0-100 and providing feedback if they are not.\n3. Adding a step to retrieve feedback and suggestions from the model after scoring all questions.\n4. Storing the feedback in a new 'feedback' column in the DataFrame.\n\nThese improvements enhance the clarity and robustness of the assessment process while maintaining the effective parts of the previous version.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T16:04:30.032197Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "de13bbc7-55da-449d-b78f-ff456b7410de",
      "role": "human",
      "content": "return the whole file",
      "created_at": "2024-06-18T17:02:28.347426Z",
      "sequence_order": 8,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T17:02:28.347426Z",
            "stop_timestamp": "2024-06-18T17:02:28.347426Z",
            "flags": null,
            "type": "text",
            "text": "return the whole file",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T17:02:28.347426Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "f886715d-c4ef-4aa9-9224-f6ba4936e51a",
      "role": "assistant",
      "content": " Here's the complete updated code file:\n\n```python\nimport nltk\nimport pandas as pd\nimport json\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt = config['prompts']['system_prompt']\nuser_prompt = config['prompts']['user_prompt']\n\n# Initialize Ollama client\nclient = ollama.Client()\n\ndef reset_ollama_client():\n    global client\n    client._client.close()\n    client = ollama.Client()\n\n# %%\ndata = pd.read_csv('./updated_activities_with_clarity_instruct_20240523012406_1287.csv').head(2)\n\ndef extract_json_and_remaining_text(text):\n    json_pattern = re.compile(r'\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}', re.DOTALL)\n    matches = json_pattern.findall(text)\n    \n    extracted_json = None\n    for match in matches:\n        try:\n            json_object = json.loads(match)\n            extracted_json = json_object\n            break\n        except json.JSONDecodeError:\n            continue\n\n    remaining_text = re.sub(json_pattern, '', text)\n    return extracted_json, remaining_text.strip()\n\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\ndata['readability_score'] = data['description_without_html'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['fullDescription'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['fullDescription'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['fullDescription'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['fullDescription'].apply(check_worksheet)\n\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    # Send the user prompt only once for each row\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt,\n            'content': user_prompt.replace('the following activity, which will be enclosed in triple brackets ([[[]]]).', f'the following activity:\\n\\n[[[{description}]]]\\n\\n')\n        },\n        {\n            'role': 'user',\n            'content': 'yes'\n        }\n    ])\n\n    for q in range(1, 6):\n        # Send only the question number\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': str(q)\n            }\n        ])\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n            else:\n                scores[f'question_{q}_score'] = 0\n                feedback += f\"Question {q} score is outside the valid range of 0-100. \"\n        else:\n            scores[f'question_{q}_score'] = 0\n            feedback += f\"Question {q} score is not a valid integer. \"\n\n    # Get feedback and suggestions\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'content': 'end'\n        }\n    ])\n    feedback += response['message']['content']\n\n    return scores, feedback.strip()\n\ndef update_or_add_scores(row_scores, index, df):\n    for key, new_score in row_scores.items():\n        if key not in df.columns:\n            df[key] = None\n        if pd.notna(df.at[index, key]):\n            df.at[index, key] = (df.at[index, key] + new_score) / 2\n        else:\n            df.at[index, key] = new_score\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores, feedback = get_scores_for_row(description)\n    update_or_add_scores(scores, index, data)\n    data.at[index, 'feedback'] = feedback\n    print(f\"Updated DataFrame Columns: {data.columns}\")\n\n# Calculate the average score for each row\nquestion_columns = ['question_1_score', 'question_2_score', 'question_3_score', 'question_4_score', 'question_5_score']\nexisting_question_columns = [col for col in question_columns if col in data.columns]\n\nif existing_question_columns:\n    data['average_score'] = data[existing_question_columns].mean(axis=1)\nelse:\n    data['average_score'] = None\n\nprint(data.head())\n\n# Save the updated DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n\n# Reset the Ollama client after processing all rows\nreset_ollama_client()\n```\n\nThis code incorporates all the changes discussed earlier, including the updated system prompt and user prompt, as well as the modifications to the `get_scores_for_row` function to handle feedback and suggestions.\n\nThe code also includes the additional steps to store the feedback in a new 'feedback' column in the DataFrame and calculate the average score based on the existing question columns.\n\nPlease make sure to update the `settings.yaml` file with the revised system prompt and user prompt as discussed in the previous response.\n\nLet me know if you have any further questions or if there's anything else I can assist you with!",
      "created_at": "2024-06-18T17:02:28.347426Z",
      "sequence_order": 9,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T17:02:28.347426Z",
            "stop_timestamp": "2024-06-18T17:02:28.347426Z",
            "flags": null,
            "type": "text",
            "text": " Here's the complete updated code file:\n\n```python\nimport nltk\nimport pandas as pd\nimport json\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt = config['prompts']['system_prompt']\nuser_prompt = config['prompts']['user_prompt']\n\n# Initialize Ollama client\nclient = ollama.Client()\n\ndef reset_ollama_client():\n    global client\n    client._client.close()\n    client = ollama.Client()\n\n# %%\ndata = pd.read_csv('./updated_activities_with_clarity_instruct_20240523012406_1287.csv').head(2)\n\ndef extract_json_and_remaining_text(text):\n    json_pattern = re.compile(r'\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}', re.DOTALL)\n    matches = json_pattern.findall(text)\n    \n    extracted_json = None\n    for match in matches:\n        try:\n            json_object = json.loads(match)\n            extracted_json = json_object\n            break\n        except json.JSONDecodeError:\n            continue\n\n    remaining_text = re.sub(json_pattern, '', text)\n    return extracted_json, remaining_text.strip()\n\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\ndata['readability_score'] = data['description_without_html'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['fullDescription'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['fullDescription'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['fullDescription'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['fullDescription'].apply(check_worksheet)\n\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    # Send the user prompt only once for each row\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt,\n            'content': user_prompt.replace('the following activity, which will be enclosed in triple brackets ([[[]]]).', f'the following activity:\\n\\n[[[{description}]]]\\n\\n')\n        },\n        {\n            'role': 'user',\n            'content': 'yes'\n        }\n    ])\n\n    for q in range(1, 6):\n        # Send only the question number\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': str(q)\n            }\n        ])\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n            else:\n                scores[f'question_{q}_score'] = 0\n                feedback += f\"Question {q} score is outside the valid range of 0-100. \"\n        else:\n            scores[f'question_{q}_score'] = 0\n            feedback += f\"Question {q} score is not a valid integer. \"\n\n    # Get feedback and suggestions\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'content': 'end'\n        }\n    ])\n    feedback += response['message']['content']\n\n    return scores, feedback.strip()\n\ndef update_or_add_scores(row_scores, index, df):\n    for key, new_score in row_scores.items():\n        if key not in df.columns:\n            df[key] = None\n        if pd.notna(df.at[index, key]):\n            df.at[index, key] = (df.at[index, key] + new_score) / 2\n        else:\n            df.at[index, key] = new_score\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores, feedback = get_scores_for_row(description)\n    update_or_add_scores(scores, index, data)\n    data.at[index, 'feedback'] = feedback\n    print(f\"Updated DataFrame Columns: {data.columns}\")\n\n# Calculate the average score for each row\nquestion_columns = ['question_1_score', 'question_2_score', 'question_3_score', 'question_4_score', 'question_5_score']\nexisting_question_columns = [col for col in question_columns if col in data.columns]\n\nif existing_question_columns:\n    data['average_score'] = data[existing_question_columns].mean(axis=1)\nelse:\n    data['average_score'] = None\n\nprint(data.head())\n\n# Save the updated DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n\n# Reset the Ollama client after processing all rows\nreset_ollama_client()\n```\n\nThis code incorporates all the changes discussed earlier, including the updated system prompt and user prompt, as well as the modifications to the `get_scores_for_row` function to handle feedback and suggestions.\n\nThe code also includes the additional steps to store the feedback in a new 'feedback' column in the DataFrame and calculate the average score based on the existing question columns.\n\nPlease make sure to update the `settings.yaml` file with the revised system prompt and user prompt as discussed in the previous response.\n\nLet me know if you have any further questions or if there's anything else I can assist you with!",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T17:02:28.347426Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "28a7dd35-8d94-4887-acfa-14f929687350",
      "role": "human",
      "content": "how is the average score 0, let's log as we go - i want to see the input and output into and out of ollama - the conversation should be logged",
      "created_at": "2024-06-18T17:07:01.336345Z",
      "sequence_order": 10,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T17:07:01.336345Z",
            "stop_timestamp": "2024-06-18T17:07:01.336345Z",
            "flags": null,
            "type": "text",
            "text": "how is the average score 0, let's log as we go - i want to see the input and output into and out of ollama - the conversation should be logged",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T17:07:01.336345Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "5b4e7511-825e-40ab-aa0a-3a400549816a",
      "role": "assistant",
      "content": " To log the input and output of the conversation with Ollama and see how the average score is calculated, you can modify the code to include logging statements. Here's the updated code with logging:\n\n```python\nimport nltk\nimport pandas as pd\nimport json\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt = config['prompts']['system_prompt']\nuser_prompt = config['prompts']['user_prompt']\n\n# Initialize Ollama client\nclient = ollama.Client()\n\ndef reset_ollama_client():\n    global client\n    client._client.close()\n    client = ollama.Client()\n\n# %%\ndata = pd.read_csv('./updated_activities_with_clarity_instruct_20240523012406_1287.csv').head(2)\n\ndef extract_json_and_remaining_text(text):\n    json_pattern = re.compile(r'\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}', re.DOTALL)\n    matches = json_pattern.findall(text)\n    \n    extracted_json = None\n    for match in matches:\n        try:\n            json_object = json.loads(match)\n            extracted_json = json_object\n            break\n        except json.JSONDecodeError:\n            continue\n\n    remaining_text = re.sub(json_pattern, '', text)\n    return extracted_json, remaining_text.strip()\n\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\ndata['readability_score'] = data['description_without_html'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['fullDescription'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['fullDescription'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['fullDescription'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['fullDescription'].apply(check_worksheet)\n\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    # Send the user prompt only once for each row\n    user_prompt_content = user_prompt.replace('the following activity, which will be enclosed in triple brackets ([[[]]]).', f'the following activity:\\n\\n[[[{description}]]]\\n\\n')\n    logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt_content}\")\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt,\n            'content': user_prompt_content\n        },\n        {\n            'role': 'user',\n            'content': 'yes'\n        }\n    ])\n    logging.info(f\"Received response from Ollama:\\n{response}\")\n\n    for q in range(1, 6):\n        # Send only the question number\n        logging.info(f\"Sending question number to Ollama: {q}\")\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': str(q)\n            }\n        ])\n        logging.info(f\"Received response from Ollama for question {q}:\\n{response}\")\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n            else:\n                scores[f'question_{q}_score'] = 0\n                feedback += f\"Question {q} score is outside the valid range of 0-100. \"\n        else:\n            scores[f'question_{q}_score'] = 0\n            feedback += f\"Question {q} score is not a valid integer. \"\n\n    # Get feedback and suggestions\n    logging.info(\"Sending 'end' prompt to Ollama\")\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'content': 'end'\n        }\n    ])\n    logging.info(f\"Received feedback and suggestions from Ollama:\\n{response}\")\n    feedback += response['message']['content']\n\n    return scores, feedback.strip()\n\ndef update_or_add_scores(row_scores, index, df):\n    for key, new_score in row_scores.items():\n        if key not in df.columns:\n            df[key] = None\n        if pd.notna(df.at[index, key]):\n            df.at[index, key] = (df.at[index, key] + new_score) / 2\n        else:\n            df.at[index, key] = new_score\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores, feedback = get_scores_for_row(description)\n    logging.info(f\"Scores for row {index}: {scores}\")\n    update_or_add_scores(scores, index, data)\n    data.at[index, 'feedback'] = feedback\n    logging.info(f\"Updated DataFrame Columns: {data.columns}\")\n\n# Calculate the average score for each row\nquestion_columns = ['question_1_score', 'question_2_score', 'question_3_score', 'question_4_score', 'question_5_score']\nexisting_question_columns = [col for col in question_columns if col in data.columns]\n\nif existing_question_columns:\n    data['average_score'] = data[existing_question_columns].mean(axis=1)\nelse:\n    data['average_score'] = None\n\nlogging.info(f\"Final DataFrame:\\n{data}\")\n\n# Save the updated DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n\n# Reset the Ollama client after processing all rows\nreset_ollama_client()\n```\n\nIn this updated code, logging statements are added at various points to log the input and output of the conversation with Ollama. The logging statements include:\n\n- Sending the user prompt to Ollama\n- Receiving the response from Ollama for the user prompt\n- Sending each question number to Ollama\n- Receiving the response from Ollama for each question\n- Sending the 'end' prompt to Ollama\n- Receiving feedback and suggestions from Ollama\n- Logging the scores for each row\n- Logging the updated DataFrame columns\n- Logging the final DataFrame\n\nThe logging level is set to `logging.INFO`, and the log messages include a timestamp, log level, and the message itself.\n\nWhen you run this code, you should see the logged messages in the console, which will provide insights into the input and output of the conversation with Ollama and how the average score is calculated.\n\nPlease make sure to update the `settings.yaml` file with the revised system prompt and user prompt as discussed earlier.\n\nLet me know if you have any further questions or if there's anything else I can assist you with!",
      "created_at": "2024-06-18T17:07:01.336345Z",
      "sequence_order": 11,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T17:07:01.336345Z",
            "stop_timestamp": "2024-06-18T17:07:01.336345Z",
            "flags": null,
            "type": "text",
            "text": " To log the input and output of the conversation with Ollama and see how the average score is calculated, you can modify the code to include logging statements. Here's the updated code with logging:\n\n```python\nimport nltk\nimport pandas as pd\nimport json\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt = config['prompts']['system_prompt']\nuser_prompt = config['prompts']['user_prompt']\n\n# Initialize Ollama client\nclient = ollama.Client()\n\ndef reset_ollama_client():\n    global client\n    client._client.close()\n    client = ollama.Client()\n\n# %%\ndata = pd.read_csv('./updated_activities_with_clarity_instruct_20240523012406_1287.csv').head(2)\n\ndef extract_json_and_remaining_text(text):\n    json_pattern = re.compile(r'\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}', re.DOTALL)\n    matches = json_pattern.findall(text)\n    \n    extracted_json = None\n    for match in matches:\n        try:\n            json_object = json.loads(match)\n            extracted_json = json_object\n            break\n        except json.JSONDecodeError:\n            continue\n\n    remaining_text = re.sub(json_pattern, '', text)\n    return extracted_json, remaining_text.strip()\n\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\ndata['readability_score'] = data['description_without_html'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['fullDescription'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['fullDescription'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['fullDescription'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['fullDescription'].apply(check_worksheet)\n\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    # Send the user prompt only once for each row\n    user_prompt_content = user_prompt.replace('the following activity, which will be enclosed in triple brackets ([[[]]]).', f'the following activity:\\n\\n[[[{description}]]]\\n\\n')\n    logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt_content}\")\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt,\n            'content': user_prompt_content\n        },\n        {\n            'role': 'user',\n            'content': 'yes'\n        }\n    ])\n    logging.info(f\"Received response from Ollama:\\n{response}\")\n\n    for q in range(1, 6):\n        # Send only the question number\n        logging.info(f\"Sending question number to Ollama: {q}\")\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': str(q)\n            }\n        ])\n        logging.info(f\"Received response from Ollama for question {q}:\\n{response}\")\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n            else:\n                scores[f'question_{q}_score'] = 0\n                feedback += f\"Question {q} score is outside the valid range of 0-100. \"\n        else:\n            scores[f'question_{q}_score'] = 0\n            feedback += f\"Question {q} score is not a valid integer. \"\n\n    # Get feedback and suggestions\n    logging.info(\"Sending 'end' prompt to Ollama\")\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'content': 'end'\n        }\n    ])\n    logging.info(f\"Received feedback and suggestions from Ollama:\\n{response}\")\n    feedback += response['message']['content']\n\n    return scores, feedback.strip()\n\ndef update_or_add_scores(row_scores, index, df):\n    for key, new_score in row_scores.items():\n        if key not in df.columns:\n            df[key] = None\n        if pd.notna(df.at[index, key]):\n            df.at[index, key] = (df.at[index, key] + new_score) / 2\n        else:\n            df.at[index, key] = new_score\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores, feedback = get_scores_for_row(description)\n    logging.info(f\"Scores for row {index}: {scores}\")\n    update_or_add_scores(scores, index, data)\n    data.at[index, 'feedback'] = feedback\n    logging.info(f\"Updated DataFrame Columns: {data.columns}\")\n\n# Calculate the average score for each row\nquestion_columns = ['question_1_score', 'question_2_score', 'question_3_score', 'question_4_score', 'question_5_score']\nexisting_question_columns = [col for col in question_columns if col in data.columns]\n\nif existing_question_columns:\n    data['average_score'] = data[existing_question_columns].mean(axis=1)\nelse:\n    data['average_score'] = None\n\nlogging.info(f\"Final DataFrame:\\n{data}\")\n\n# Save the updated DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n\n# Reset the Ollama client after processing all rows\nreset_ollama_client()\n```\n\nIn this updated code, logging statements are added at various points to log the input and output of the conversation with Ollama. The logging statements include:\n\n- Sending the user prompt to Ollama\n- Receiving the response from Ollama for the user prompt\n- Sending each question number to Ollama\n- Receiving the response from Ollama for each question\n- Sending the 'end' prompt to Ollama\n- Receiving feedback and suggestions from Ollama\n- Logging the scores for each row\n- Logging the updated DataFrame columns\n- Logging the final DataFrame\n\nThe logging level is set to `logging.INFO`, and the log messages include a timestamp, log level, and the message itself.\n\nWhen you run this code, you should see the logged messages in the console, which will provide insights into the input and output of the conversation with Ollama and how the average score is calculated.\n\nPlease make sure to update the `settings.yaml` file with the revised system prompt and user prompt as discussed earlier.\n\nLet me know if you have any further questions or if there's anything else I can assist you with!",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T17:07:01.336345Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "2d451adc-0d4f-4d11-9692-f3a929982ea2",
      "role": "human",
      "content": "update the code to use this new yaml file:\nsettings:\n  model: llama3\nprompts:\n  system_prompt: |\n    you are afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online training entrepreneurship training program. \n    You will see the following sub-questions for {{assessment_rubric}}\n    and will be asked to score them from \n    1. {{question_1}}\n    2. {{question_2}}\n    3. {{question_3}}\n    4. {{question_4}}\n    5. {{question_5}}\n    Your answer will be a score from 0 being very poor to 100 for excellent. \n    When I ask you for the question's score, you will return only the score for that question as an integer and no other information or text.\n  user_prompt: |\n    Afif, your job is to assess the following activity against the {{assessment_rubric}}'s sub-questions.\n    After I send the activity, I will request one of the 5 sub-questions. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\nassessment:\n  assessment_rubric: Clarity of Instructions\n  question_1: Is there an outline?\n  question_2: Is the outline comprehensive? \n  question_3: Is the activity written clearly with no jargon \n  question_4: Does the activity have any grammatical problems? \n  question_5: Does the activity have actionable directions?\n  \ninstead of giving only the number give it the YAML variable for each question\nIn the YAML file use the variables to fill in the {{}} variable demarcations \nThink step by step about our goals and how to acheive them and write the code\nWe don't want to average with the ollama - nor do we need the json parser verifier",
      "created_at": "2024-06-18T17:18:51.201484Z",
      "sequence_order": 12,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T17:18:51.201484Z",
            "stop_timestamp": "2024-06-18T17:18:51.201484Z",
            "flags": null,
            "type": "text",
            "text": "update the code to use this new yaml file:\nsettings:\n  model: llama3\nprompts:\n  system_prompt: |\n    you are afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online training entrepreneurship training program. \n    You will see the following sub-questions for {{assessment_rubric}}\n    and will be asked to score them from \n    1. {{question_1}}\n    2. {{question_2}}\n    3. {{question_3}}\n    4. {{question_4}}\n    5. {{question_5}}\n    Your answer will be a score from 0 being very poor to 100 for excellent. \n    When I ask you for the question's score, you will return only the score for that question as an integer and no other information or text.\n  user_prompt: |\n    Afif, your job is to assess the following activity against the {{assessment_rubric}}'s sub-questions.\n    After I send the activity, I will request one of the 5 sub-questions. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\nassessment:\n  assessment_rubric: Clarity of Instructions\n  question_1: Is there an outline?\n  question_2: Is the outline comprehensive? \n  question_3: Is the activity written clearly with no jargon \n  question_4: Does the activity have any grammatical problems? \n  question_5: Does the activity have actionable directions?\n  \ninstead of giving only the number give it the YAML variable for each question\nIn the YAML file use the variables to fill in the {{}} variable demarcations \nThink step by step about our goals and how to acheive them and write the code\nWe don't want to average with the ollama - nor do we need the json parser verifier",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T17:18:51.201484Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "07222b78-045d-4f13-aab2-76fcc4989e2c",
      "role": "assistant",
      "content": " To update the code to use the new YAML file and achieve the desired goals, let's break it down step by step:\n\n1. Modify the YAML file to use variables for the assessment rubric and questions.\n2. Update the code to load the new YAML file and extract the necessary variables.\n3. Modify the `get_scores_for_row` function to send the assessment rubric and question variables to Ollama.\n4. Remove the averaging and JSON parser verifier code.\n5. Update the logging statements to reflect the changes.\n\nHere's the updated code:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt = config['prompts']['system_prompt']\nuser_prompt = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\n# Initialize Ollama client\nclient = ollama.Client()\n\ndef reset_ollama_client():\n    global client\n    client._client.close()\n    client = ollama.Client()\n\n# %%\ndata = pd.read_csv('./updated_activities_with_clarity_instruct_20240523012406_1287.csv').head(2)\n\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\ndata['readability_score'] = data['description_without_html'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['fullDescription'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['fullDescription'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['fullDescription'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['fullDescription'].apply(check_worksheet)\n\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    # Send the user prompt only once for each row\n    user_prompt_content = user_prompt.replace('{{assessment_rubric}}', assessment_rubric)\n    logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt_content}\")\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt.replace('{{assessment_rubric}}', assessment_rubric)\n                                    .replace('{{question_1}}', question_1)\n                                    .replace('{{question_2}}', question_2)\n                                    .replace('{{question_3}}', question_3)\n                                    .replace('{{question_4}}', question_4)\n                                    .replace('{{question_5}}', question_5),\n            'content': user_prompt_content + f\"\\n\\n{description}\"\n        },\n        {\n            'role': 'user',\n            'content': 'yes'\n        }\n    ])\n    logging.info(f\"Received response from Ollama:\\n{response}\")\n\n    for q in range(1, 6):\n        # Send the question variable to Ollama\n        question_var = config['assessment'][f'question_{q}']\n        logging.info(f\"Sending question variable to Ollama: {question_var}\")\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': question_var\n            }\n        ])\n        logging.info(f\"Received response from Ollama for question {q}:\\n{response}\")\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n            else:\n                scores[f'question_{q}_score'] = 0\n                feedback += f\"Question {q} score is outside the valid range of 0-100. \"\n        else:\n            scores[f'question_{q}_score'] = 0\n            feedback += f\"Question {q} score is not a valid integer. \"\n\n    return scores, feedback.strip()\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores, feedback = get_scores_for_row(description)\n    logging.info(f\"Scores for row {index}: {scores}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        data.at[index, f'question_{q}_score'] = scores[f'question_{q}_score']\n\nlogging.info(f\"Final DataFrame:\\n{data}\")\n\n# Save the updated DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n\n# Reset the Ollama client after processing all rows\nreset_ollama_client()\n```\n\nIn this updated code:\n\n1. The YAML file is updated to use variables for the assessment rubric and questions.\n2. The code loads the new YAML file and extracts the assessment rubric and question variables.\n3. The `get_scores_for_row` function is modified to send the assessment rubric and question variables to Ollama. The system prompt is updated to replace the variables with their corresponding values.\n4. The averaging and JSON parser verifier code is removed.\n5. The logging statements are updated to reflect the changes.\n\nThe code now sends the assessment rubric and question variables to Ollama for each row in the DataFrame. The scores for each question are stored in the DataFrame in separate columns (`question_1_score`, `question_2_score`, etc.), and the feedback is stored in the 'feedback' column.\n\nPlease make sure to update the `settings.yaml` file with the new structure and variables as provided in your message.\n\nLet me know if you have any further questions or if there's anything else I can assist you with!",
      "created_at": "2024-06-18T17:18:51.201484Z",
      "sequence_order": 13,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T17:18:51.201484Z",
            "stop_timestamp": "2024-06-18T17:18:51.201484Z",
            "flags": null,
            "type": "text",
            "text": " To update the code to use the new YAML file and achieve the desired goals, let's break it down step by step:\n\n1. Modify the YAML file to use variables for the assessment rubric and questions.\n2. Update the code to load the new YAML file and extract the necessary variables.\n3. Modify the `get_scores_for_row` function to send the assessment rubric and question variables to Ollama.\n4. Remove the averaging and JSON parser verifier code.\n5. Update the logging statements to reflect the changes.\n\nHere's the updated code:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt = config['prompts']['system_prompt']\nuser_prompt = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\n# Initialize Ollama client\nclient = ollama.Client()\n\ndef reset_ollama_client():\n    global client\n    client._client.close()\n    client = ollama.Client()\n\n# %%\ndata = pd.read_csv('./updated_activities_with_clarity_instruct_20240523012406_1287.csv').head(2)\n\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\ndata['readability_score'] = data['description_without_html'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['fullDescription'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['fullDescription'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['fullDescription'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['fullDescription'].apply(check_worksheet)\n\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    # Send the user prompt only once for each row\n    user_prompt_content = user_prompt.replace('{{assessment_rubric}}', assessment_rubric)\n    logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt_content}\")\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt.replace('{{assessment_rubric}}', assessment_rubric)\n                                    .replace('{{question_1}}', question_1)\n                                    .replace('{{question_2}}', question_2)\n                                    .replace('{{question_3}}', question_3)\n                                    .replace('{{question_4}}', question_4)\n                                    .replace('{{question_5}}', question_5),\n            'content': user_prompt_content + f\"\\n\\n{description}\"\n        },\n        {\n            'role': 'user',\n            'content': 'yes'\n        }\n    ])\n    logging.info(f\"Received response from Ollama:\\n{response}\")\n\n    for q in range(1, 6):\n        # Send the question variable to Ollama\n        question_var = config['assessment'][f'question_{q}']\n        logging.info(f\"Sending question variable to Ollama: {question_var}\")\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': question_var\n            }\n        ])\n        logging.info(f\"Received response from Ollama for question {q}:\\n{response}\")\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n            else:\n                scores[f'question_{q}_score'] = 0\n                feedback += f\"Question {q} score is outside the valid range of 0-100. \"\n        else:\n            scores[f'question_{q}_score'] = 0\n            feedback += f\"Question {q} score is not a valid integer. \"\n\n    return scores, feedback.strip()\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores, feedback = get_scores_for_row(description)\n    logging.info(f\"Scores for row {index}: {scores}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        data.at[index, f'question_{q}_score'] = scores[f'question_{q}_score']\n\nlogging.info(f\"Final DataFrame:\\n{data}\")\n\n# Save the updated DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n\n# Reset the Ollama client after processing all rows\nreset_ollama_client()\n```\n\nIn this updated code:\n\n1. The YAML file is updated to use variables for the assessment rubric and questions.\n2. The code loads the new YAML file and extracts the assessment rubric and question variables.\n3. The `get_scores_for_row` function is modified to send the assessment rubric and question variables to Ollama. The system prompt is updated to replace the variables with their corresponding values.\n4. The averaging and JSON parser verifier code is removed.\n5. The logging statements are updated to reflect the changes.\n\nThe code now sends the assessment rubric and question variables to Ollama for each row in the DataFrame. The scores for each question are stored in the DataFrame in separate columns (`question_1_score`, `question_2_score`, etc.), and the feedback is stored in the 'feedback' column.\n\nPlease make sure to update the `settings.yaml` file with the new structure and variables as provided in your message.\n\nLet me know if you have any further questions or if there's anything else I can assist you with!",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T17:18:51.201484Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "1b36c691-d7fe-476f-951e-0202823a4e68",
      "role": "human",
      "content": "what's this for? { 'role': 'user', 'content': 'yes' }\n\nThis is obviously wrong, it needs to send the activity after the rubric and sub questions \nand only needs to send the system prompt once:\n\n(py39_native) bilalghalib@mac LibraryUpdates % python createClarityScore.py\n2024-06-18 19:19:33,979 - INFO - Sending user prompt to Ollama:\nAfif, your job is to assess the following activity against the Clarity of Instructions's sub-questions.\nAfter I send the activity, I will request one of the 5 sub-questions. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\n\n2024-06-18 19:19:40,502 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 19:19:40,503 - INFO - Received response from Ollama:\n{'model': 'llama3', 'created_at': '2024-06-18T17:19:40.500731Z', 'message': {'role': 'assistant', 'content': '5'}, 'done_reason': 'stop', 'done': True, 'total_duration': 6517804750, 'load_duration': 5057600792, 'prompt_eval_count': 119, 'prompt_eval_duration': 1369950000, 'eval_count': 2, 'eval_duration': 81090000}\n2024-06-18 19:19:40,503 - INFO - Sending question variable to Ollama: Is there an outline?\n\n2024-06-18 19:20:04,776 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 19:20:04,779 - INFO - Received response from Ollama for question 1:\n{'model': 'llama3', 'created_at': '2024-06-18T17:20:04.769608Z', 'message': {'role': 'assistant', 'content': \"I'm happy to provide an outline for you!\\n\\nSince our conversation just started, I didn't have a specific topic or subject in mind. However, if you'd like to create an outline together, we can do that!\\n\\nPlease let me know what topic or subject you'd like to explore, and I'll help us create an outline from scratch.\\n\\nIf you need some inspiration, feel free to pick one of the following topics:\\n\\n1. A specific project or task\\n2. A personal goal or objective\\n3. A favorite hobby or interest\\n4. A current event or news article\\n\\nLet me know which direction you'd like to take, and we'll get started on creating an outline together!\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 24264619667, 'load_duration': 1362000, 'prompt_eval_count': 10, 'prompt_eval_duration': 743381000, 'eval_count': 142, 'eval_duration': 23518748000}\n2024-06-18 19:20:04,779 - INFO - Sending question variable to Ollama: Is the outline comprehensive?\n2024-06-18 19:20:19,029 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 19:20:19,030 - INFO - Received response from Ollama for question 2:\n{'model': 'llama3', 'created_at': '2024-06-18T17:20:19.02741Z', 'message': {'role': 'assistant', 'content': 'I\\'m happy to help you with that!\\n\\nSince I don\\'t have any specific information or topic to work with, it\\'s difficult for me to determine if an outline is comprehensive. An outline can be considered comprehensive depending on the context and the level of detail required.\\n\\nCould you please provide more details about what kind of outline you\\'re referring to? What topic or subject does it cover? And what do you mean by \"comprehensive\"?'}, 'done_reason': 'stop', 'done': True, 'total_duration': 14245020917, 'load_duration': 3106167, 'prompt_eval_count': 9, 'prompt_eval_duration': 694504000, 'eval_count': 88, 'eval_duration': 13545770000}\n2024-06-18 19:20:19,030 - INFO - Sending question variable to Ollama: Is the activity written clearly with no jargon",
      "created_at": "2024-06-18T17:26:02.297375Z",
      "sequence_order": 14,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T17:26:02.297375Z",
            "stop_timestamp": "2024-06-18T17:26:02.297375Z",
            "flags": null,
            "type": "text",
            "text": "what's this for? { 'role': 'user', 'content': 'yes' }\n\nThis is obviously wrong, it needs to send the activity after the rubric and sub questions \nand only needs to send the system prompt once:\n\n(py39_native) bilalghalib@mac LibraryUpdates % python createClarityScore.py\n2024-06-18 19:19:33,979 - INFO - Sending user prompt to Ollama:\nAfif, your job is to assess the following activity against the Clarity of Instructions's sub-questions.\nAfter I send the activity, I will request one of the 5 sub-questions. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\n\n2024-06-18 19:19:40,502 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 19:19:40,503 - INFO - Received response from Ollama:\n{'model': 'llama3', 'created_at': '2024-06-18T17:19:40.500731Z', 'message': {'role': 'assistant', 'content': '5'}, 'done_reason': 'stop', 'done': True, 'total_duration': 6517804750, 'load_duration': 5057600792, 'prompt_eval_count': 119, 'prompt_eval_duration': 1369950000, 'eval_count': 2, 'eval_duration': 81090000}\n2024-06-18 19:19:40,503 - INFO - Sending question variable to Ollama: Is there an outline?\n\n2024-06-18 19:20:04,776 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 19:20:04,779 - INFO - Received response from Ollama for question 1:\n{'model': 'llama3', 'created_at': '2024-06-18T17:20:04.769608Z', 'message': {'role': 'assistant', 'content': \"I'm happy to provide an outline for you!\\n\\nSince our conversation just started, I didn't have a specific topic or subject in mind. However, if you'd like to create an outline together, we can do that!\\n\\nPlease let me know what topic or subject you'd like to explore, and I'll help us create an outline from scratch.\\n\\nIf you need some inspiration, feel free to pick one of the following topics:\\n\\n1. A specific project or task\\n2. A personal goal or objective\\n3. A favorite hobby or interest\\n4. A current event or news article\\n\\nLet me know which direction you'd like to take, and we'll get started on creating an outline together!\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 24264619667, 'load_duration': 1362000, 'prompt_eval_count': 10, 'prompt_eval_duration': 743381000, 'eval_count': 142, 'eval_duration': 23518748000}\n2024-06-18 19:20:04,779 - INFO - Sending question variable to Ollama: Is the outline comprehensive?\n2024-06-18 19:20:19,029 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 19:20:19,030 - INFO - Received response from Ollama for question 2:\n{'model': 'llama3', 'created_at': '2024-06-18T17:20:19.02741Z', 'message': {'role': 'assistant', 'content': 'I\\'m happy to help you with that!\\n\\nSince I don\\'t have any specific information or topic to work with, it\\'s difficult for me to determine if an outline is comprehensive. An outline can be considered comprehensive depending on the context and the level of detail required.\\n\\nCould you please provide more details about what kind of outline you\\'re referring to? What topic or subject does it cover? And what do you mean by \"comprehensive\"?'}, 'done_reason': 'stop', 'done': True, 'total_duration': 14245020917, 'load_duration': 3106167, 'prompt_eval_count': 9, 'prompt_eval_duration': 694504000, 'eval_count': 88, 'eval_duration': 13545770000}\n2024-06-18 19:20:19,030 - INFO - Sending question variable to Ollama: Is the activity written clearly with no jargon",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T17:26:02.297375Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "651f4241-faba-42fc-a983-26c35e9cc547",
      "role": "assistant",
      "content": " Apologies for the confusion. Let me clarify and provide an updated code snippet to address the issues you mentioned:\n\n1. The `{ 'role': 'user', 'content': 'yes' }` message is not needed and can be removed.\n2. The system prompt should be sent only once at the beginning of the conversation.\n3. The activity should be sent after the rubric and sub-questions.\n\nHere's the updated code:\n\n```python\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    # Send the system prompt only once at the beginning\n    system_prompt_content = system_prompt.replace('{{assessment_rubric}}', assessment_rubric)\n                                          .replace('{{question_1}}', question_1)\n                                          .replace('{{question_2}}', question_2)\n                                          .replace('{{question_3}}', question_3)\n                                          .replace('{{question_4}}', question_4)\n                                          .replace('{{question_5}}', question_5)\n    logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt_content}\")\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt_content,\n            'content': user_prompt.replace('{{assessment_rubric}}', assessment_rubric)\n        }\n    ])\n    logging.info(f\"Received response from Ollama:\\n{response}\")\n\n    # Send the activity\n    logging.info(f\"Sending activity to Ollama:\\n{description}\")\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'content': description\n        }\n    ])\n    logging.info(f\"Received response from Ollama:\\n{response}\")\n\n    for q in range(1, 6):\n        # Send the question variable to Ollama\n        question_var = config['assessment'][f'question_{q}']\n        logging.info(f\"Sending question variable to Ollama: {question_var}\")\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': question_var\n            }\n        ])\n        logging.info(f\"Received response from Ollama for question {q}:\\n{response}\")\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n            else:\n                scores[f'question_{q}_score'] = 0\n                feedback += f\"Question {q} score is outside the valid range of 0-100. \"\n        else:\n            scores[f'question_{q}_score'] = 0\n            feedback += f\"Question {q} score is not a valid integer. \"\n\n    return scores, feedback.strip()\n```\n\nIn this updated code:\n\n1. The `{ 'role': 'user', 'content': 'yes' }` message is removed.\n2. The system prompt is sent only once at the beginning of the conversation. The `system_prompt_content` variable is created by replacing the placeholders with their corresponding values from the YAML file.\n3. The activity is sent after the rubric and sub-questions. A new message is added to send the `description` as the activity.\n4. The rest of the code remains the same, where the question variables are sent to Ollama, and the scores and feedback are collected.\n\nWith these changes, the conversation flow should be as follows:\n\n1. Send the system prompt with the assessment rubric and sub-questions.\n2. Send the activity.\n3. Send each question variable and collect the scores and feedback.\n\nPlease update the `get_scores_for_row` function in your code with the provided changes.\n\nLet me know if you have any further questions or if there's anything else I can assist you with!",
      "created_at": "2024-06-18T17:26:02.297375Z",
      "sequence_order": 15,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T17:26:02.297375Z",
            "stop_timestamp": "2024-06-18T17:26:02.297375Z",
            "flags": null,
            "type": "text",
            "text": " Apologies for the confusion. Let me clarify and provide an updated code snippet to address the issues you mentioned:\n\n1. The `{ 'role': 'user', 'content': 'yes' }` message is not needed and can be removed.\n2. The system prompt should be sent only once at the beginning of the conversation.\n3. The activity should be sent after the rubric and sub-questions.\n\nHere's the updated code:\n\n```python\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    # Send the system prompt only once at the beginning\n    system_prompt_content = system_prompt.replace('{{assessment_rubric}}', assessment_rubric)\n                                          .replace('{{question_1}}', question_1)\n                                          .replace('{{question_2}}', question_2)\n                                          .replace('{{question_3}}', question_3)\n                                          .replace('{{question_4}}', question_4)\n                                          .replace('{{question_5}}', question_5)\n    logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt_content}\")\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'system': system_prompt_content,\n            'content': user_prompt.replace('{{assessment_rubric}}', assessment_rubric)\n        }\n    ])\n    logging.info(f\"Received response from Ollama:\\n{response}\")\n\n    # Send the activity\n    logging.info(f\"Sending activity to Ollama:\\n{description}\")\n    response = client.chat(model=model, messages=[\n        {\n            'role': 'user',\n            'content': description\n        }\n    ])\n    logging.info(f\"Received response from Ollama:\\n{response}\")\n\n    for q in range(1, 6):\n        # Send the question variable to Ollama\n        question_var = config['assessment'][f'question_{q}']\n        logging.info(f\"Sending question variable to Ollama: {question_var}\")\n        response = client.chat(model=model, messages=[\n            {\n                'role': 'user',\n                'content': question_var\n            }\n        ])\n        logging.info(f\"Received response from Ollama for question {q}:\\n{response}\")\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores[f'question_{q}_score'] = question_score\n            else:\n                scores[f'question_{q}_score'] = 0\n                feedback += f\"Question {q} score is outside the valid range of 0-100. \"\n        else:\n            scores[f'question_{q}_score'] = 0\n            feedback += f\"Question {q} score is not a valid integer. \"\n\n    return scores, feedback.strip()\n```\n\nIn this updated code:\n\n1. The `{ 'role': 'user', 'content': 'yes' }` message is removed.\n2. The system prompt is sent only once at the beginning of the conversation. The `system_prompt_content` variable is created by replacing the placeholders with their corresponding values from the YAML file.\n3. The activity is sent after the rubric and sub-questions. A new message is added to send the `description` as the activity.\n4. The rest of the code remains the same, where the question variables are sent to Ollama, and the scores and feedback are collected.\n\nWith these changes, the conversation flow should be as follows:\n\n1. Send the system prompt with the assessment rubric and sub-questions.\n2. Send the activity.\n3. Send each question variable and collect the scores and feedback.\n\nPlease update the `get_scores_for_row` function in your code with the provided changes.\n\nLet me know if you have any further questions or if there's anything else I can assist you with!",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T17:26:02.297375Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "a75b30dd-cb33-4f9b-94b3-fb99a62d5b25",
      "role": "human",
      "content": "review the updates to the code and give me suggestions for improvements:",
      "created_at": "2024-06-18T18:16:16.329882Z",
      "sequence_order": 16,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T18:16:16.329882Z",
            "stop_timestamp": "2024-06-18T18:16:16.329882Z",
            "flags": null,
            "type": "text",
            "text": "review the updates to the code and give me suggestions for improvements:",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T18:16:16.329882Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 4934,
            "file_type": "txt",
            "extracted_content": "import nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    question_1=question_1,\n    question_2=question_2,\n    question_3=question_3,\n    question_4=question_4,\n    question_5=question_5\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data\ndata = pd.read_csv('./updated_activities_with_clarity_instruct_20240523012406_1287.csv').head(2)\n\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\ndata['readability_score'] = data['description_without_html'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['fullDescription'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['fullDescription'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['fullDescription'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['fullDescription'].apply(check_worksheet)\n\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                      .replace('{{activity_content}}', description)\n    \n    logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n    logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n    \n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for i, question in enumerate(questions, start=1):\n        messages = [\n            {'role': 'system', 'content': system_prompt},\n            {'role': 'user', 'content': user_prompt},\n            {'role': 'user', 'content': question}\n        ]\n        \n        logging.info(f\"Sending question variable to Ollama: {question}\")\n        response = client.chat(model=model, messages=messages)\n        logging.info(f\"Received response from Ollama for question {i}:\\n{response}\")\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores[f'question_{i}_score'] = question_score\n            else:\n                scores[f'question_{i}_score'] = 0\n                feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n        else:\n            scores[f'question_{i}_score'] = 0\n            feedback += f\"Question {i} score is not a valid integer. \"\n\n    return scores, feedback.strip()\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores, feedback = get_scores_for_row(description)\n    logging.info(f\"Scores for row {index}: {scores}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        data.at[index, f'question_{q}_score'] = scores.get(f'question_{q}_score', 0)\n\nlogging.info(f\"Final DataFrame:\\n{data}\")\n\n# Save the updated DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n"
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          }
        ]
      }
    },
    {
      "id": "115b5439-cd82-4ce4-83cc-d2cc35b245b2",
      "role": "assistant",
      "content": " The updated code looks much better and addresses the previous issues. Here are a few suggestions for further improvements:\n\n1. Use string formatting instead of string concatenation:\n   Instead of using string concatenation to create the `user_prompt`, you can use string formatting with the `format()` method. This makes the code more readable and consistent with how you created the `system_prompt`. For example:\n\n   ```python\n   user_prompt = user_prompt_template.format(assessment_rubric=assessment_rubric, activity_content=description)\n   ```\n\n2. Use a loop to send questions:\n   Instead of manually sending each question, you can use a loop to iterate over the questions. This makes the code more concise and easier to maintain. For example:\n\n   ```python\n   questions = [\n       question_1,\n       question_2,\n       question_3,\n       question_4,\n       question_5\n   ]\n\n   for i, question in enumerate(questions, start=1):\n       messages = [\n           {'role': 'system', 'content': system_prompt},\n           {'role': 'user', 'content': user_prompt},\n           {'role': 'user', 'content': question}\n       ]\n       \n       logging.info(f\"Sending question variable to Ollama: {question}\")\n       response = client.chat(model=model, messages=messages)\n       logging.info(f\"Received response from Ollama for question {i}:\\n{response}\")\n       # ... process the response and store scores ...\n   ```\n\n3. Handle missing scores:\n   When assigning scores to the DataFrame, you can use the `get()` method with a default value to handle cases where a score might be missing. For example:\n\n   ```python\n   for q in range(1, 6):\n       data.at[index, f'question_{q}_score'] = scores.get(f'question_{q}_score', 0)\n   ```\n\n   This ensures that if a score is missing for a particular question, it will be assigned a default value of 0.\n\n4. Close the Ollama client:\n   After processing all the rows and saving the updated DataFrame, it's a good practice to close the Ollama client to release resources. You can add the following line at the end of your code:\n\n   ```python\n   client.close()\n   ```\n\n   This ensures that the client is properly closed after usage.\n\n5. Consider adding error handling:\n   While the code handles cases where the score is not a valid integer or outside the valid range, you can consider adding more robust error handling. For example, you can wrap the main processing logic inside a `try-except` block to catch and handle any exceptions that may occur during the API calls or data processing.\n\nOverall, the code looks good and incorporates the necessary changes. The suggestions above are minor improvements to enhance readability, maintainability, and robustness.",
      "created_at": "2024-06-18T18:16:16.329882Z",
      "sequence_order": 17,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T18:16:16.329882Z",
            "stop_timestamp": "2024-06-18T18:16:16.329882Z",
            "flags": null,
            "type": "text",
            "text": " The updated code looks much better and addresses the previous issues. Here are a few suggestions for further improvements:\n\n1. Use string formatting instead of string concatenation:\n   Instead of using string concatenation to create the `user_prompt`, you can use string formatting with the `format()` method. This makes the code more readable and consistent with how you created the `system_prompt`. For example:\n\n   ```python\n   user_prompt = user_prompt_template.format(assessment_rubric=assessment_rubric, activity_content=description)\n   ```\n\n2. Use a loop to send questions:\n   Instead of manually sending each question, you can use a loop to iterate over the questions. This makes the code more concise and easier to maintain. For example:\n\n   ```python\n   questions = [\n       question_1,\n       question_2,\n       question_3,\n       question_4,\n       question_5\n   ]\n\n   for i, question in enumerate(questions, start=1):\n       messages = [\n           {'role': 'system', 'content': system_prompt},\n           {'role': 'user', 'content': user_prompt},\n           {'role': 'user', 'content': question}\n       ]\n       \n       logging.info(f\"Sending question variable to Ollama: {question}\")\n       response = client.chat(model=model, messages=messages)\n       logging.info(f\"Received response from Ollama for question {i}:\\n{response}\")\n       # ... process the response and store scores ...\n   ```\n\n3. Handle missing scores:\n   When assigning scores to the DataFrame, you can use the `get()` method with a default value to handle cases where a score might be missing. For example:\n\n   ```python\n   for q in range(1, 6):\n       data.at[index, f'question_{q}_score'] = scores.get(f'question_{q}_score', 0)\n   ```\n\n   This ensures that if a score is missing for a particular question, it will be assigned a default value of 0.\n\n4. Close the Ollama client:\n   After processing all the rows and saving the updated DataFrame, it's a good practice to close the Ollama client to release resources. You can add the following line at the end of your code:\n\n   ```python\n   client.close()\n   ```\n\n   This ensures that the client is properly closed after usage.\n\n5. Consider adding error handling:\n   While the code handles cases where the score is not a valid integer or outside the valid range, you can consider adding more robust error handling. For example, you can wrap the main processing logic inside a `try-except` block to catch and handle any exceptions that may occur during the API calls or data processing.\n\nOverall, the code looks good and incorporates the necessary changes. The suggestions above are minor improvements to enhance readability, maintainability, and robustness.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T18:16:16.329882Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "49675c76-1641-487a-b631-272960a8e125",
      "role": "human",
      "content": "do we have a message history that starts just after the system prompt and has the user prompt the activity and then each question and answer. Then we clear the message history and start again 2 more times, before egoing to the next question, clearing again between each question, If that makes sense explain the goal to me and rewrite the code",
      "created_at": "2024-06-18T18:27:43.943148Z",
      "sequence_order": 18,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T18:27:43.943148Z",
            "stop_timestamp": "2024-06-18T18:27:43.943148Z",
            "flags": null,
            "type": "text",
            "text": "do we have a message history that starts just after the system prompt and has the user prompt the activity and then each question and answer. Then we clear the message history and start again 2 more times, before egoing to the next question, clearing again between each question, If that makes sense explain the goal to me and rewrite the code",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T18:27:43.943148Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "182f835d-d628-46a4-8abe-cfe102dc3654",
      "role": "assistant",
      "content": " Yes, I understand what you're suggesting. The goal is to maintain a message history that starts with the system prompt, followed by the user prompt, the activity, and then each question and answer. After each question-answer pair, the message history is cleared before moving on to the next question. This process is repeated for a total of three times for each question before moving on to the next question. The message history is also cleared between each question.\n\nThe purpose of this approach is to provide a consistent context for each question-answer interaction while allowing the model to focus on one question at a time. By repeating the process three times for each question, you can obtain multiple responses and potentially capture different perspectives or variations in the model's output.\n\nHere's the updated code incorporating this approach:\n\n```python\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    user_prompt = user_prompt_template.format(assessment_rubric=assessment_rubric, activity_content=description)\n    \n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for i, question in enumerate(questions, start=1):\n        question_scores = []\n        \n        for _ in range(3):\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt},\n                {'role': 'user', 'content': question}\n            ]\n            \n            logging.info(f\"Sending question variable to Ollama: {question}\")\n            response = client.chat(model=model, messages=messages)\n            logging.info(f\"Received response from Ollama for question {i}:\\n{response}\")\n            question_score = response['message']['content']\n            if question_score.isdigit():\n                question_score = int(question_score)\n                if 0 <= question_score <= 100:\n                    question_scores.append(question_score)\n                else:\n                    feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n            else:\n                feedback += f\"Question {i} score is not a valid integer. \"\n            \n            # Clear the message history\n            messages.clear()\n        \n        if question_scores:\n            scores[f'question_{i}_score'] = sum(question_scores) // len(question_scores)\n        else:\n            scores[f'question_{i}_score'] = 0\n        \n        # Clear the message history between questions\n        messages.clear()\n\n    return scores, feedback.strip()\n```\n\nIn this updated code:\n\n1. The `get_scores_for_row` function now iterates over each question using a loop.\n\n2. For each question, it repeats the process of sending the system prompt, user prompt, and question to the Ollama API three times using an inner loop.\n\n3. The responses for each question are collected in the `question_scores` list.\n\n4. After each question-answer interaction, the message history is cleared using `messages.clear()`.\n\n5. Once all three responses for a question are obtained, the average score is calculated and stored in the `scores` dictionary.\n\n6. The message history is also cleared between questions using `messages.clear()`.\n\n7. The rest of the code remains the same, where the scores and feedback are returned.\n\nBy following this approach, you maintain a consistent context for each question-answer interaction, repeat the process three times for each question, and clear the message history between questions. This allows the model to focus on one question at a time and potentially provide different responses for each repetition.\n\nPlease note that clearing the message history assumes that the Ollama API supports this behavior. If the API maintains its own internal conversation history, you may need to find an alternative way to reset the context between questions.",
      "created_at": "2024-06-18T18:27:43.943148Z",
      "sequence_order": 19,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T18:27:43.943148Z",
            "stop_timestamp": "2024-06-18T18:27:43.943148Z",
            "flags": null,
            "type": "text",
            "text": " Yes, I understand what you're suggesting. The goal is to maintain a message history that starts with the system prompt, followed by the user prompt, the activity, and then each question and answer. After each question-answer pair, the message history is cleared before moving on to the next question. This process is repeated for a total of three times for each question before moving on to the next question. The message history is also cleared between each question.\n\nThe purpose of this approach is to provide a consistent context for each question-answer interaction while allowing the model to focus on one question at a time. By repeating the process three times for each question, you can obtain multiple responses and potentially capture different perspectives or variations in the model's output.\n\nHere's the updated code incorporating this approach:\n\n```python\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    user_prompt = user_prompt_template.format(assessment_rubric=assessment_rubric, activity_content=description)\n    \n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for i, question in enumerate(questions, start=1):\n        question_scores = []\n        \n        for _ in range(3):\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt},\n                {'role': 'user', 'content': question}\n            ]\n            \n            logging.info(f\"Sending question variable to Ollama: {question}\")\n            response = client.chat(model=model, messages=messages)\n            logging.info(f\"Received response from Ollama for question {i}:\\n{response}\")\n            question_score = response['message']['content']\n            if question_score.isdigit():\n                question_score = int(question_score)\n                if 0 <= question_score <= 100:\n                    question_scores.append(question_score)\n                else:\n                    feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n            else:\n                feedback += f\"Question {i} score is not a valid integer. \"\n            \n            # Clear the message history\n            messages.clear()\n        \n        if question_scores:\n            scores[f'question_{i}_score'] = sum(question_scores) // len(question_scores)\n        else:\n            scores[f'question_{i}_score'] = 0\n        \n        # Clear the message history between questions\n        messages.clear()\n\n    return scores, feedback.strip()\n```\n\nIn this updated code:\n\n1. The `get_scores_for_row` function now iterates over each question using a loop.\n\n2. For each question, it repeats the process of sending the system prompt, user prompt, and question to the Ollama API three times using an inner loop.\n\n3. The responses for each question are collected in the `question_scores` list.\n\n4. After each question-answer interaction, the message history is cleared using `messages.clear()`.\n\n5. Once all three responses for a question are obtained, the average score is calculated and stored in the `scores` dictionary.\n\n6. The message history is also cleared between questions using `messages.clear()`.\n\n7. The rest of the code remains the same, where the scores and feedback are returned.\n\nBy following this approach, you maintain a consistent context for each question-answer interaction, repeat the process three times for each question, and clear the message history between questions. This allows the model to focus on one question at a time and potentially provide different responses for each repetition.\n\nPlease note that clearing the message history assumes that the Ollama API supports this behavior. If the API maintains its own internal conversation history, you may need to find an alternative way to reset the context between questions.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T18:27:43.943148Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "f74e4ca6-d5b1-488f-9a6d-121921e4e276",
      "role": "human",
      "content": "that's great, except incorrect, for each activity i want to keep the context for all 5 questions and then clear the history and present the user prompt and activity and then do all 5 again two more times and then repeat the whole process again for the next activity",
      "created_at": "2024-06-18T18:29:07.989589Z",
      "sequence_order": 20,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T18:29:07.989589Z",
            "stop_timestamp": "2024-06-18T18:29:07.989589Z",
            "flags": null,
            "type": "text",
            "text": "that's great, except incorrect, for each activity i want to keep the context for all 5 questions and then clear the history and present the user prompt and activity and then do all 5 again two more times and then repeat the whole process again for the next activity",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T18:29:07.989589Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "24480703-66cb-4811-a232-9dfda4564ec3",
      "role": "assistant",
      "content": " Apologies for the misunderstanding. I now understand that you want to maintain the context for all 5 questions within an activity, repeat the process three times for each activity, and then move on to the next activity. Here's the updated code incorporating this approach:\n\n```python\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    user_prompt = user_prompt_template.format(assessment_rubric=assessment_rubric, activity_content=description)\n    \n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(3):\n        messages = [\n            {'role': 'system', 'content': system_prompt},\n            {'role': 'user', 'content': user_prompt}\n        ]\n        \n        for i, question in enumerate(questions, start=1):\n            messages.append({'role': 'user', 'content': question})\n            \n            logging.info(f\"Sending question variable to Ollama: {question}\")\n            response = client.chat(model=model, messages=messages)\n            logging.info(f\"Received response from Ollama for question {i}:\\n{response}\")\n            question_score = response['message']['content']\n            if question_score.isdigit():\n                question_score = int(question_score)\n                if 0 <= question_score <= 100:\n                    scores[f'question_{i}_score_{_+1}'] = question_score\n                else:\n                    feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n            else:\n                feedback += f\"Question {i} score is not a valid integer. \"\n        \n        # Clear the message history after each iteration\n        messages.clear()\n\n    return scores, feedback.strip()\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores, feedback = get_scores_for_row(description)\n    logging.info(f\"Scores for row {index}: {scores}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        for r in range(1, 4):\n            score_column = f'question_{q}_score_{r}'\n            data.at[index, score_column] = scores.get(score_column, 0)\n\nlogging.info(f\"Final DataFrame:\\n{data}\")\n\n# Save the updated DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nIn this updated code:\n\n1. The `get_scores_for_row` function now has an outer loop that repeats the process three times for each activity.\n\n2. Within each iteration, the system prompt and user prompt are added to the `messages` list.\n\n3. The function then iterates over each question and appends it to the `messages` list.\n\n4. The response for each question is obtained from the Ollama API, and the score is extracted and stored in the `scores` dictionary with a key that includes the question number and iteration number (e.g., `question_1_score_1`, `question_1_score_2`, etc.).\n\n5. After all questions have been processed for an iteration, the message history is cleared using `messages.clear()`.\n\n6. The scores and feedback are returned as before.\n\n7. When assigning the scores to the DataFrame, the code now includes an additional loop to assign scores for each iteration (1 to 3) for each question.\n\n8. The DataFrame is saved to a new CSV file.\n\nWith this approach, the code maintains the context for all 5 questions within an activity, repeats the process three times for each activity, and then moves on to the next activity. The message history is cleared after each iteration to ensure a fresh context for the next iteration.\n\nPlease note that the DataFrame will now include additional columns for each question and iteration combination (e.g., `question_1_score_1`, `question_1_score_2`, `question_1_score_3`, etc.).",
      "created_at": "2024-06-18T18:29:07.989589Z",
      "sequence_order": 21,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T18:29:07.989589Z",
            "stop_timestamp": "2024-06-18T18:29:07.989589Z",
            "flags": null,
            "type": "text",
            "text": " Apologies for the misunderstanding. I now understand that you want to maintain the context for all 5 questions within an activity, repeat the process three times for each activity, and then move on to the next activity. Here's the updated code incorporating this approach:\n\n```python\ndef get_scores_for_row(description):\n    scores = {}\n    feedback = \"\"\n\n    user_prompt = user_prompt_template.format(assessment_rubric=assessment_rubric, activity_content=description)\n    \n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(3):\n        messages = [\n            {'role': 'system', 'content': system_prompt},\n            {'role': 'user', 'content': user_prompt}\n        ]\n        \n        for i, question in enumerate(questions, start=1):\n            messages.append({'role': 'user', 'content': question})\n            \n            logging.info(f\"Sending question variable to Ollama: {question}\")\n            response = client.chat(model=model, messages=messages)\n            logging.info(f\"Received response from Ollama for question {i}:\\n{response}\")\n            question_score = response['message']['content']\n            if question_score.isdigit():\n                question_score = int(question_score)\n                if 0 <= question_score <= 100:\n                    scores[f'question_{i}_score_{_+1}'] = question_score\n                else:\n                    feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n            else:\n                feedback += f\"Question {i} score is not a valid integer. \"\n        \n        # Clear the message history after each iteration\n        messages.clear()\n\n    return scores, feedback.strip()\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    scores, feedback = get_scores_for_row(description)\n    logging.info(f\"Scores for row {index}: {scores}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        for r in range(1, 4):\n            score_column = f'question_{q}_score_{r}'\n            data.at[index, score_column] = scores.get(score_column, 0)\n\nlogging.info(f\"Final DataFrame:\\n{data}\")\n\n# Save the updated DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nIn this updated code:\n\n1. The `get_scores_for_row` function now has an outer loop that repeats the process three times for each activity.\n\n2. Within each iteration, the system prompt and user prompt are added to the `messages` list.\n\n3. The function then iterates over each question and appends it to the `messages` list.\n\n4. The response for each question is obtained from the Ollama API, and the score is extracted and stored in the `scores` dictionary with a key that includes the question number and iteration number (e.g., `question_1_score_1`, `question_1_score_2`, etc.).\n\n5. After all questions have been processed for an iteration, the message history is cleared using `messages.clear()`.\n\n6. The scores and feedback are returned as before.\n\n7. When assigning the scores to the DataFrame, the code now includes an additional loop to assign scores for each iteration (1 to 3) for each question.\n\n8. The DataFrame is saved to a new CSV file.\n\nWith this approach, the code maintains the context for all 5 questions within an activity, repeats the process three times for each activity, and then moves on to the next activity. The message history is cleared after each iteration to ensure a fresh context for the next iteration.\n\nPlease note that the DataFrame will now include additional columns for each question and iteration combination (e.g., `question_1_score_1`, `question_1_score_2`, `question_1_score_3`, etc.).",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T18:29:07.989589Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "312cacc8-ae1d-4441-8006-b31bd65dfc21",
      "role": "human",
      "content": "why do i have zeros?",
      "created_at": "2024-06-18T18:31:39.677756Z",
      "sequence_order": 22,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T18:31:39.677756Z",
            "stop_timestamp": "2024-06-18T18:31:39.677756Z",
            "flags": null,
            "type": "text",
            "text": "why do i have zeros?",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T18:31:39.677756Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 19142,
            "file_type": "txt",
            "extracted_content": "(py39_native) bilalghalib@mac LibraryUpdates % python createClarityScore.py\n2024-06-18 20:29:38,207 - INFO - Sending question variable to Ollama: Is there an outline?\n2024-06-18 20:29:45,393 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:45,395 - INFO - Received response from Ollama for question 1:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:45.388882Z', 'message': {'role': 'assistant', 'content': '62'}, 'done_reason': 'stop', 'done': True, 'total_duration': 7178945667, 'load_duration': 5074009500, 'prompt_eval_count': 251, 'prompt_eval_duration': 1941711000, 'eval_count': 2, 'eval_duration': 155170000}\n2024-06-18 20:29:45,395 - INFO - Sending question variable to Ollama: Is the outline comprehensive?\n2024-06-18 20:29:46,570 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:46,571 - INFO - Received response from Ollama for question 2:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:46.569359Z', 'message': {'role': 'assistant', 'content': '85'}, 'done_reason': 'stop', 'done': True, 'total_duration': 1172191209, 'load_duration': 2501375, 'prompt_eval_count': 15, 'prompt_eval_duration': 1079726000, 'eval_count': 2, 'eval_duration': 85284000}\n2024-06-18 20:29:46,571 - INFO - Sending question variable to Ollama: Is the activity written clearly with no jargon\n2024-06-18 20:29:46,985 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:46,986 - INFO - Received response from Ollama for question 3:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:46.9842Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 411526166, 'load_duration': 1055791, 'prompt_eval_count': 19, 'prompt_eval_duration': 407067000, 'eval_count': 1, 'eval_duration': 10000}\n2024-06-18 20:29:46,987 - INFO - Sending question variable to Ollama: Does the activity have any grammatical problems?\n2024-06-18 20:29:47,524 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:47,525 - INFO - Received response from Ollama for question 4:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:47.522464Z', 'message': {'role': 'assistant', 'content': '62'}, 'done_reason': 'stop', 'done': True, 'total_duration': 533541250, 'load_duration': 808875, 'prompt_eval_count': 19, 'prompt_eval_duration': 426419000, 'eval_count': 2, 'eval_duration': 101625000}\n2024-06-18 20:29:47,525 - INFO - Sending question variable to Ollama: Does the activity have actionable directions?\n2024-06-18 20:29:48,005 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:48,006 - INFO - Received response from Ollama for question 5:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:48.003771Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 476850375, 'load_duration': 1304500, 'prompt_eval_count': 17, 'prompt_eval_duration': 470568000, 'eval_count': 1, 'eval_duration': 9000}\n2024-06-18 20:29:48,006 - INFO - Sending question variable to Ollama: Is there an outline?\n2024-06-18 20:29:48,225 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:48,227 - INFO - Received response from Ollama for question 1:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:48.224393Z', 'message': {'role': 'assistant', 'content': '60'}, 'done_reason': 'stop', 'done': True, 'total_duration': 216578375, 'load_duration': 938208, 'prompt_eval_duration': 91745000, 'eval_count': 2, 'eval_duration': 121669000}\n2024-06-18 20:29:48,227 - INFO - Sending question variable to Ollama: Is the outline comprehensive?\n2024-06-18 20:29:49,361 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:49,362 - INFO - Received response from Ollama for question 2:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:49.360774Z', 'message': {'role': 'assistant', 'content': '88'}, 'done_reason': 'stop', 'done': True, 'total_duration': 1132132791, 'load_duration': 675500, 'prompt_eval_count': 15, 'prompt_eval_duration': 1041299000, 'eval_count': 2, 'eval_duration': 87322000}\n2024-06-18 20:29:49,362 - INFO - Sending question variable to Ollama: Is the activity written clearly with no jargon\n2024-06-18 20:29:49,905 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:49,905 - INFO - Received response from Ollama for question 3:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:49.904199Z', 'message': {'role': 'assistant', 'content': '76'}, 'done_reason': 'stop', 'done': True, 'total_duration': 540186666, 'load_duration': 674958, 'prompt_eval_count': 19, 'prompt_eval_duration': 446633000, 'eval_count': 2, 'eval_duration': 89533000}\n2024-06-18 20:29:49,906 - INFO - Sending question variable to Ollama: Does the activity have any grammatical problems?\n2024-06-18 20:29:50,398 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:50,399 - INFO - Received response from Ollama for question 4:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:50.396581Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 489293500, 'load_duration': 978750, 'prompt_eval_count': 19, 'prompt_eval_duration': 484784000, 'eval_count': 1, 'eval_duration': 11000}\n2024-06-18 20:29:50,399 - INFO - Sending question variable to Ollama: Does the activity have actionable directions?\n2024-06-18 20:29:50,815 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:50,816 - INFO - Received response from Ollama for question 5:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:50.814391Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 413099041, 'load_duration': 836041, 'prompt_eval_count': 17, 'prompt_eval_duration': 407240000, 'eval_count': 1, 'eval_duration': 7000}\n2024-06-18 20:29:50,817 - INFO - Sending question variable to Ollama: Is there an outline?\n2024-06-18 20:29:51,025 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:51,026 - INFO - Received response from Ollama for question 1:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:51.024561Z', 'message': {'role': 'assistant', 'content': '98'}, 'done_reason': 'stop', 'done': True, 'total_duration': 206052084, 'load_duration': 730500, 'prompt_eval_duration': 85091000, 'eval_count': 2, 'eval_duration': 118035000}\n2024-06-18 20:29:51,027 - INFO - Sending question variable to Ollama: Is the outline comprehensive?\n2024-06-18 20:29:52,228 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:52,229 - INFO - Received response from Ollama for question 2:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:52.227104Z', 'message': {'role': 'assistant', 'content': '83'}, 'done_reason': 'stop', 'done': True, 'total_duration': 1198496125, 'load_duration': 948042, 'prompt_eval_count': 15, 'prompt_eval_duration': 1110588000, 'eval_count': 2, 'eval_duration': 84313000}\n2024-06-18 20:29:52,229 - INFO - Sending question variable to Ollama: Is the activity written clearly with no jargon\n2024-06-18 20:29:52,725 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:52,726 - INFO - Received response from Ollama for question 3:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:52.723948Z', 'message': {'role': 'assistant', 'content': '62'}, 'done_reason': 'stop', 'done': True, 'total_duration': 492563417, 'load_duration': 1316209, 'prompt_eval_count': 19, 'prompt_eval_duration': 400910000, 'eval_count': 2, 'eval_duration': 84606000}\n2024-06-18 20:29:52,726 - INFO - Sending question variable to Ollama: Does the activity have any grammatical problems?\n2024-06-18 20:29:53,263 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:53,264 - INFO - Received response from Ollama for question 4:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:53.262362Z', 'message': {'role': 'assistant', 'content': '82'}, 'done_reason': 'stop', 'done': True, 'total_duration': 533711083, 'load_duration': 1990166, 'prompt_eval_count': 19, 'prompt_eval_duration': 441046000, 'eval_count': 2, 'eval_duration': 86825000}\n2024-06-18 20:29:53,265 - INFO - Sending question variable to Ollama: Does the activity have actionable directions?\n2024-06-18 20:29:53,872 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:53,873 - INFO - Received response from Ollama for question 5:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:53.871192Z', 'message': {'role': 'assistant', 'content': '84'}, 'done_reason': 'stop', 'done': True, 'total_duration': 604634375, 'load_duration': 696000, 'prompt_eval_count': 17, 'prompt_eval_duration': 493308000, 'eval_count': 2, 'eval_duration': 106239000}\n2024-06-18 20:29:53,874 - INFO - Scores for row 0: {'question_1_score_1': 62, 'question_2_score_1': 85, 'question_4_score_1': 62, 'question_1_score_2': 60, 'question_2_score_2': 88, 'question_3_score_2': 76, 'question_1_score_3': 98, 'question_2_score_3': 83, 'question_3_score_3': 62, 'question_4_score_3': 82, 'question_5_score_3': 84}\n2024-06-18 20:29:53,893 - INFO - Sending question variable to Ollama: Is there an outline?\n2024-06-18 20:29:54,146 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:54,147 - INFO - Received response from Ollama for question 1:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:54.144941Z', 'message': {'role': 'assistant', 'content': '85'}, 'done_reason': 'stop', 'done': True, 'total_duration': 250021375, 'load_duration': 1220834, 'prompt_eval_duration': 111773000, 'eval_count': 2, 'eval_duration': 134468000}\n2024-06-18 20:29:54,147 - INFO - Sending question variable to Ollama: Is the outline comprehensive?\n2024-06-18 20:29:55,378 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:55,378 - INFO - Received response from Ollama for question 2:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:55.377845Z', 'message': {'role': 'assistant', 'content': '64'}, 'done_reason': 'stop', 'done': True, 'total_duration': 1228621209, 'load_duration': 733209, 'prompt_eval_count': 15, 'prompt_eval_duration': 1104053000, 'eval_count': 2, 'eval_duration': 120877000}\n2024-06-18 20:29:55,379 - INFO - Sending question variable to Ollama: Is the activity written clearly with no jargon\n2024-06-18 20:29:56,195 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:56,196 - INFO - Received response from Ollama for question 3:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:56.194351Z', 'message': {'role': 'assistant', 'content': '90'}, 'done_reason': 'stop', 'done': True, 'total_duration': 814276416, 'load_duration': 998333, 'prompt_eval_count': 19, 'prompt_eval_duration': 659620000, 'eval_count': 2, 'eval_duration': 148639000}\n2024-06-18 20:29:56,196 - INFO - Sending question variable to Ollama: Does the activity have any grammatical problems?\n2024-06-18 20:29:56,859 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:56,860 - INFO - Received response from Ollama for question 4:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:56.857643Z', 'message': {'role': 'assistant', 'content': '92'}, 'done_reason': 'stop', 'done': True, 'total_duration': 659740458, 'load_duration': 1080458, 'prompt_eval_count': 19, 'prompt_eval_duration': 558677000, 'eval_count': 2, 'eval_duration': 95027000}\n2024-06-18 20:29:56,860 - INFO - Sending question variable to Ollama: Does the activity have actionable directions?\n2024-06-18 20:29:57,502 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:57,503 - INFO - Received response from Ollama for question 5:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:57.500677Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 638739375, 'load_duration': 2094125, 'prompt_eval_count': 17, 'prompt_eval_duration': 631429000, 'eval_count': 1, 'eval_duration': 20000}\n2024-06-18 20:29:57,504 - INFO - Sending question variable to Ollama: Is there an outline?\n2024-06-18 20:29:57,912 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:57,913 - INFO - Received response from Ollama for question 1:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:57.911225Z', 'message': {'role': 'assistant', 'content': '64'}, 'done_reason': 'stop', 'done': True, 'total_duration': 405569833, 'load_duration': 993208, 'prompt_eval_duration': 119674000, 'eval_count': 2, 'eval_duration': 282007000}\n2024-06-18 20:29:57,913 - INFO - Sending question variable to Ollama: Is the outline comprehensive?\n2024-06-18 20:29:59,108 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:59,109 - INFO - Received response from Ollama for question 2:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:59.107306Z', 'message': {'role': 'assistant', 'content': '62'}, 'done_reason': 'stop', 'done': True, 'total_duration': 1191960083, 'load_duration': 927042, 'prompt_eval_count': 15, 'prompt_eval_duration': 1102265000, 'eval_count': 2, 'eval_duration': 85565000}\n2024-06-18 20:29:59,109 - INFO - Sending question variable to Ollama: Is the activity written clearly with no jargon\n2024-06-18 20:29:59,619 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:29:59,621 - INFO - Received response from Ollama for question 3:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:59.618334Z', 'message': {'role': 'assistant', 'content': '82'}, 'done_reason': 'stop', 'done': True, 'total_duration': 507666875, 'load_duration': 1090459, 'prompt_eval_count': 19, 'prompt_eval_duration': 416875000, 'eval_count': 2, 'eval_duration': 86250000}\n2024-06-18 20:29:59,621 - INFO - Sending question variable to Ollama: Does the activity have any grammatical problems?\n2024-06-18 20:30:00,135 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:30:00,142 - INFO - Received response from Ollama for question 4:\n{'model': 'llama3', 'created_at': '2024-06-18T18:30:00.134918Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 512334541, 'load_duration': 1649375, 'prompt_eval_count': 19, 'prompt_eval_duration': 506698000, 'eval_count': 1, 'eval_duration': 14000}\n2024-06-18 20:30:00,142 - INFO - Sending question variable to Ollama: Does the activity have actionable directions?\n2024-06-18 20:30:00,723 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:30:00,725 - INFO - Received response from Ollama for question 5:\n{'model': 'llama3', 'created_at': '2024-06-18T18:30:00.722449Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 578297417, 'load_duration': 2839667, 'prompt_eval_count': 17, 'prompt_eval_duration': 569984000, 'eval_count': 1, 'eval_duration': 15000}\n2024-06-18 20:30:00,725 - INFO - Sending question variable to Ollama: Is there an outline?\n2024-06-18 20:30:00,948 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:30:00,950 - INFO - Received response from Ollama for question 1:\n{'model': 'llama3', 'created_at': '2024-06-18T18:30:00.944689Z', 'message': {'role': 'assistant', 'content': '87'}, 'done_reason': 'stop', 'done': True, 'total_duration': 217837500, 'load_duration': 954750, 'prompt_eval_duration': 92665000, 'eval_count': 2, 'eval_duration': 121388000}\n2024-06-18 20:30:00,950 - INFO - Sending question variable to Ollama: Is the outline comprehensive?\n2024-06-18 20:30:02,368 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:30:02,369 - INFO - Received response from Ollama for question 2:\n{'model': 'llama3', 'created_at': '2024-06-18T18:30:02.365935Z', 'message': {'role': 'assistant', 'content': '50'}, 'done_reason': 'stop', 'done': True, 'total_duration': 1413913500, 'load_duration': 1186500, 'prompt_eval_count': 15, 'prompt_eval_duration': 1305028000, 'eval_count': 2, 'eval_duration': 105118000}\n2024-06-18 20:30:02,370 - INFO - Sending question variable to Ollama: Is the activity written clearly with no jargon\n2024-06-18 20:30:03,020 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:30:03,021 - INFO - Received response from Ollama for question 3:\n{'model': 'llama3', 'created_at': '2024-06-18T18:30:03.018613Z', 'message': {'role': 'assistant', 'content': '72'}, 'done_reason': 'stop', 'done': True, 'total_duration': 647015333, 'load_duration': 2017250, 'prompt_eval_count': 19, 'prompt_eval_duration': 534334000, 'eval_count': 2, 'eval_duration': 106864000}\n2024-06-18 20:30:03,021 - INFO - Sending question variable to Ollama: Does the activity have any grammatical problems?\n2024-06-18 20:30:03,597 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:30:03,598 - INFO - Received response from Ollama for question 4:\n{'model': 'llama3', 'created_at': '2024-06-18T18:30:03.596095Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 573545875, 'load_duration': 1183083, 'prompt_eval_count': 19, 'prompt_eval_duration': 568455000, 'eval_count': 1, 'eval_duration': 16000}\n2024-06-18 20:30:03,598 - INFO - Sending question variable to Ollama: Does the activity have actionable directions?\n2024-06-18 20:30:04,091 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n2024-06-18 20:30:04,091 - INFO - Received response from Ollama for question 5:\n{'model': 'llama3', 'created_at': '2024-06-18T18:30:04.09029Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 490633250, 'load_duration': 1578417, 'prompt_eval_count': 17, 'prompt_eval_duration': 483749000, 'eval_count': 1, 'eval_duration': 14000}\n2024-06-18 20:30:04,091 - INFO - Scores for row 1: {'question_1_score_1': 85, 'question_2_score_1': 64, 'question_3_score_1': 90, 'question_4_score_1': 92, 'question_1_score_2': 64, 'question_2_score_2': 62, 'question_3_score_2': 82, 'question_1_score_3': 87, 'question_2_score_3': 50, 'question_3_score_3': 72}\n2024-06-18 20:30:04,148 - INFO - Final DataFrame:\n                        _id shortCode                                              title  ... question_5_score_1 question_5_score_2 question_5_score_3\n0  65154d47c78659b4c0809f8f    #A0001  âœ… Activity: Listing what has happened and find...  ...                0.0                0.0               84.0\n1  65154d48c78659b4c0809f96    #A0002          âœ… Activity: Describing usual mental state  ...                0.0                0.0                0.0\n\n[2 rows x 43 columns]\n"
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          }
        ]
      }
    },
    {
      "id": "6ce29293-c831-4c77-8821-09f5802e893d",
      "role": "assistant",
      "content": " Based on the provided output, there are zeros in the `question_5_score_1`, `question_5_score_2`, and `question_5_score_3` columns for both rows. The reason for these zeros is that the Ollama API is returning an empty response for the question \"Does the activity have actionable directions?\" in all three iterations for both activities.\n\nIf you look at the logs, you'll see the following lines:\n\n```\n2024-06-18 20:29:48,006 - INFO - Received response from Ollama for question 5:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:48.003771Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, ...}\n\n2024-06-18 20:29:50,816 - INFO - Received response from Ollama for question 5:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:50.814391Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, ...}\n\n2024-06-18 20:29:53,873 - INFO - Received response from Ollama for question 5:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:53.871192Z', 'message': {'role': 'assistant', 'content': '84'}, 'done_reason': 'stop', 'done': True, ...}\n```\n\nFor the first activity (row 0), the response for question 5 is empty in the first two iterations, resulting in zeros for `question_5_score_1` and `question_5_score_2`. In the third iteration, the response is '84', which is assigned to `question_5_score_3`.\n\nSimilarly, for the second activity (row 1), the response for question 5 is empty in all three iterations, resulting in zeros for `question_5_score_1`, `question_5_score_2`, and `question_5_score_3`.\n\nThis indicates that the Ollama API is not providing a valid score for the question \"Does the activity have actionable directions?\" in some cases. It could be due to the specific content of the activities or limitations of the model in understanding and assessing the presence of actionable directions.\n\nTo handle this situation, you can modify the code to assign a default value or handle empty responses differently based on your requirements. For example, you can assign a specific score (e.g., 0 or -1) to indicate that the model couldn't provide a valid score for that question.\n\nAdditionally, you may want to investigate further why the Ollama API is returning empty responses for certain questions and see if there are any improvements or adjustments you can make to the prompts or the way you present the activities to the model to get more consistent and valid responses.",
      "created_at": "2024-06-18T18:31:39.677756Z",
      "sequence_order": 23,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T18:31:39.677756Z",
            "stop_timestamp": "2024-06-18T18:31:39.677756Z",
            "flags": null,
            "type": "text",
            "text": " Based on the provided output, there are zeros in the `question_5_score_1`, `question_5_score_2`, and `question_5_score_3` columns for both rows. The reason for these zeros is that the Ollama API is returning an empty response for the question \"Does the activity have actionable directions?\" in all three iterations for both activities.\n\nIf you look at the logs, you'll see the following lines:\n\n```\n2024-06-18 20:29:48,006 - INFO - Received response from Ollama for question 5:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:48.003771Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, ...}\n\n2024-06-18 20:29:50,816 - INFO - Received response from Ollama for question 5:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:50.814391Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, ...}\n\n2024-06-18 20:29:53,873 - INFO - Received response from Ollama for question 5:\n{'model': 'llama3', 'created_at': '2024-06-18T18:29:53.871192Z', 'message': {'role': 'assistant', 'content': '84'}, 'done_reason': 'stop', 'done': True, ...}\n```\n\nFor the first activity (row 0), the response for question 5 is empty in the first two iterations, resulting in zeros for `question_5_score_1` and `question_5_score_2`. In the third iteration, the response is '84', which is assigned to `question_5_score_3`.\n\nSimilarly, for the second activity (row 1), the response for question 5 is empty in all three iterations, resulting in zeros for `question_5_score_1`, `question_5_score_2`, and `question_5_score_3`.\n\nThis indicates that the Ollama API is not providing a valid score for the question \"Does the activity have actionable directions?\" in some cases. It could be due to the specific content of the activities or limitations of the model in understanding and assessing the presence of actionable directions.\n\nTo handle this situation, you can modify the code to assign a default value or handle empty responses differently based on your requirements. For example, you can assign a specific score (e.g., 0 or -1) to indicate that the model couldn't provide a valid score for that question.\n\nAdditionally, you may want to investigate further why the Ollama API is returning empty responses for certain questions and see if there are any improvements or adjustments you can make to the prompts or the way you present the activities to the model to get more consistent and valid responses.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T18:31:39.677756Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "135d4cd3-6219-45db-8c67-10a67d52a870",
      "role": "human",
      "content": "review this code and assess it for its objectives at getting a more accurate score for each question by clearing the context and \"starting fresh\"",
      "created_at": "2024-06-18T18:52:07.269147Z",
      "sequence_order": 24,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T18:52:07.269147Z",
            "stop_timestamp": "2024-06-18T18:52:07.269147Z",
            "flags": null,
            "type": "text",
            "text": "review this code and assess it for its objectives at getting a more accurate score for each question by clearing the context and \"starting fresh\"",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T18:52:07.269147Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "8c4eca79-3775-480a-b02b-d549c4f4da5e",
      "role": "assistant",
      "content": " Certainly! Let's review the code and assess its objectives of getting a more accurate score for each question by clearing the context and \"starting fresh.\"\n\nObjectives:\n1. Maintain the context for all 5 questions within an activity.\n2. Repeat the process three times for each activity to get multiple scores for each question.\n3. Clear the message history after each iteration to start fresh for the next iteration.\n4. Move on to the next activity and repeat the process.\n\nAssessment:\n\n1. Maintaining context for all 5 questions within an activity:\n   - The code uses a loop to iterate over all 5 questions for each activity.\n   - The system prompt and user prompt are sent only once at the beginning of each iteration.\n   - The activity description is included in the user prompt, providing the necessary context for all questions.\n   - This approach ensures that the model has the complete context of the activity when answering each question.\n\n2. Repeating the process three times for each activity:\n   - The code uses an outer loop to repeat the process three times for each activity.\n   - Each iteration sends the system prompt, user prompt, and activity description to the model.\n   - The model's responses for each question are collected and stored in the `scores` dictionary with appropriate keys.\n   - Repeating the process three times allows for obtaining multiple scores for each question, which can help assess the consistency and reliability of the model's responses.\n\n3. Clearing the message history after each iteration:\n   - After each iteration (i.e., after processing all 5 questions for an activity), the code clears the message history using `messages.clear()`.\n   - Clearing the message history ensures that the model starts fresh for the next iteration without carrying over any context from the previous iteration.\n   - This helps in obtaining independent scores for each iteration and prevents any potential biases or influences from previous responses.\n\n4. Moving on to the next activity and repeating the process:\n   - The code iterates over each row in the DataFrame, representing different activities.\n   - For each activity, the process of maintaining context, repeating the iterations, and clearing the message history is applied.\n   - This ensures that each activity is assessed independently, and the scores are obtained for all questions across multiple iterations.\n\nOverall, the code effectively achieves its objectives of maintaining context for all questions within an activity, repeating the process multiple times, clearing the message history between iterations, and moving on to the next activity. This approach aims to obtain more accurate and consistent scores for each question by starting fresh and providing the model with the complete context for each activity.\n\nHowever, it's important to note that the effectiveness of this approach may depend on the specific characteristics of the Ollama API and the model being used. The model's ability to provide consistent and accurate scores across multiple iterations and its sensitivity to context changes may vary. Therefore, it's recommended to monitor the results and assess the quality and consistency of the scores obtained to determine if this approach leads to improved accuracy and reliability in the scoring process.\n\nAdditionally, you may want to consider further refinements or experiments, such as adjusting the number of iterations, fine-tuning the prompts, or exploring alternative approaches to optimize the scoring process based on the specific requirements and characteristics of your use case.",
      "created_at": "2024-06-18T18:52:07.269147Z",
      "sequence_order": 25,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T18:52:07.269147Z",
            "stop_timestamp": "2024-06-18T18:52:07.269147Z",
            "flags": null,
            "type": "text",
            "text": " Certainly! Let's review the code and assess its objectives of getting a more accurate score for each question by clearing the context and \"starting fresh.\"\n\nObjectives:\n1. Maintain the context for all 5 questions within an activity.\n2. Repeat the process three times for each activity to get multiple scores for each question.\n3. Clear the message history after each iteration to start fresh for the next iteration.\n4. Move on to the next activity and repeat the process.\n\nAssessment:\n\n1. Maintaining context for all 5 questions within an activity:\n   - The code uses a loop to iterate over all 5 questions for each activity.\n   - The system prompt and user prompt are sent only once at the beginning of each iteration.\n   - The activity description is included in the user prompt, providing the necessary context for all questions.\n   - This approach ensures that the model has the complete context of the activity when answering each question.\n\n2. Repeating the process three times for each activity:\n   - The code uses an outer loop to repeat the process three times for each activity.\n   - Each iteration sends the system prompt, user prompt, and activity description to the model.\n   - The model's responses for each question are collected and stored in the `scores` dictionary with appropriate keys.\n   - Repeating the process three times allows for obtaining multiple scores for each question, which can help assess the consistency and reliability of the model's responses.\n\n3. Clearing the message history after each iteration:\n   - After each iteration (i.e., after processing all 5 questions for an activity), the code clears the message history using `messages.clear()`.\n   - Clearing the message history ensures that the model starts fresh for the next iteration without carrying over any context from the previous iteration.\n   - This helps in obtaining independent scores for each iteration and prevents any potential biases or influences from previous responses.\n\n4. Moving on to the next activity and repeating the process:\n   - The code iterates over each row in the DataFrame, representing different activities.\n   - For each activity, the process of maintaining context, repeating the iterations, and clearing the message history is applied.\n   - This ensures that each activity is assessed independently, and the scores are obtained for all questions across multiple iterations.\n\nOverall, the code effectively achieves its objectives of maintaining context for all questions within an activity, repeating the process multiple times, clearing the message history between iterations, and moving on to the next activity. This approach aims to obtain more accurate and consistent scores for each question by starting fresh and providing the model with the complete context for each activity.\n\nHowever, it's important to note that the effectiveness of this approach may depend on the specific characteristics of the Ollama API and the model being used. The model's ability to provide consistent and accurate scores across multiple iterations and its sensitivity to context changes may vary. Therefore, it's recommended to monitor the results and assess the quality and consistency of the scores obtained to determine if this approach leads to improved accuracy and reliability in the scoring process.\n\nAdditionally, you may want to consider further refinements or experiments, such as adjusting the number of iterations, fine-tuning the prompts, or exploring alternative approaches to optimize the scoring process based on the specific requirements and characteristics of your use case.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T18:52:07.269147Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "f1261a7b-66b9-46fa-833e-f9a26218f6c1",
      "role": "human",
      "content": "review this code for those objectives",
      "created_at": "2024-06-18T18:52:54.188516Z",
      "sequence_order": 26,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T18:52:54.188516Z",
            "stop_timestamp": "2024-06-18T18:52:54.188516Z",
            "flags": null,
            "type": "text",
            "text": "review this code for those objectives",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T18:52:54.188516Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 5683,
            "file_type": "txt",
            "extracted_content": "import nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    question_1=question_1,\n    question_2=question_2,\n    question_3=question_3,\n    question_4=question_4,\n    question_5=question_5\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data\ndata = pd.read_csv('./updated_activities_with_clarity_instruct_20240523012406_1287.csv').head(2)\n\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\ndata['readability_score'] = data['description_without_html'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['fullDescription'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['fullDescription'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['fullDescription'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['fullDescription'].apply(check_worksheet)\n\ndef get_average_score(description, question):\n    scores = []\n    for _ in range(3):\n        client = ollama.Client()  # Reset the client to clear history\n\n        user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                          .replace('{{activity_content}}', description)\n\n        messages = [\n            {'role': 'system', 'content': system_prompt},\n            {'role': 'user', 'content': user_prompt},\n            {'role': 'user', 'content': question}\n        ]\n\n        logging.info(f\"Sending question variable to Ollama: {question}\")\n        response = client.chat(model=model, messages=messages)\n        logging.info(f\"Received response from Ollama for question:\\n{response}\")\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores.append(question_score)\n            else:\n                logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n        else:\n            logging.warning(f\"Question score {question_score} is not a valid integer.\")\n    \n    if scores:\n        return sum(scores) / len(scores)\n    else:\n        return 0\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, 6)}\n    feedback = \"\"\n\n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(3):\n        for i, question in enumerate(questions, start=1):\n            average_score = get_average_score(description, question)\n            scores[f'question_{i}_score'].append(average_score)\n            if average_score == 0:\n                feedback += f\"Question {i} did not return valid scores. \"\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        data.at[index, f'question_{q}_score'] = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\nlogging.info(f\"Final DataFrame:\\n{data}\")\n\n# Save the updated DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n"
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          }
        ]
      }
    },
    {
      "id": "4d1c714e-777f-47b6-954a-213e4f3f4783",
      "role": "assistant",
      "content": " The updated code addresses the objectives of getting a more accurate score for each question by clearing the context and \"starting fresh.\" Let's review the changes made and assess how they align with the objectives:\n\n1. Resetting the client to clear history:\n   - Inside the `get_average_score` function, a new `ollama.Client()` is created for each iteration.\n   - This ensures that the client's history is cleared before sending the question to the model.\n   - By resetting the client, the model starts fresh without any previous context, allowing for independent scoring of each question.\n\n2. Getting average scores for each question:\n   - The `get_average_score` function is introduced to calculate the average score for a specific question.\n   - It sends the question to the model three times and collects the scores.\n   - The scores are validated to ensure they are within the valid range of 0-100.\n   - If any invalid scores are encountered, a warning is logged.\n   - The function returns the average of the valid scores or 0 if no valid scores are obtained.\n\n3. Iterating over questions for each activity:\n   - The `get_scores_for_activity` function is updated to iterate over each question for a given activity.\n   - It calls the `get_average_score` function for each question and stores the average scores in the `scores` dictionary.\n   - The function also calculates the differences between the maximum and minimum scores for each question to assess score variability.\n   - If any question does not return valid scores, feedback is accumulated to indicate the issue.\n\n4. Updating the DataFrame with averaged scores and differences:\n   - The code iterates over each row in the DataFrame, representing different activities.\n   - For each activity, the `get_scores_for_activity` function is called to obtain the averaged scores, differences, and feedback.\n   - The averaged scores and differences are assigned to the corresponding columns in the DataFrame.\n   - The feedback is also stored in the 'feedback' column.\n\n5. Logging and saving the updated DataFrame:\n   - The code logs the averaged scores and differences for each row to provide visibility into the scoring process.\n   - The final DataFrame, including the new columns for averaged scores and differences, is logged.\n   - The updated DataFrame is saved to a new CSV file.\n\nAssessment:\n\nThe updated code effectively addresses the objectives of getting a more accurate score for each question by clearing the context and starting fresh. The key improvements include:\n\n- Resetting the client for each iteration to clear the history and ensure independent scoring.\n- Calculating the average score for each question by sending it to the model multiple times and validating the scores.\n- Iterating over each question for a given activity and storing the averaged scores and differences.\n- Providing feedback if any question does not return valid scores, indicating potential issues.\n- Updating the DataFrame with the averaged scores, differences, and feedback for each activity.\n\nThese changes aim to mitigate the impact of context carry-over and obtain more reliable and consistent scores for each question. By averaging the scores from multiple iterations and assessing score variability, the code provides a more robust evaluation of each question's clarity.\n\nHowever, it's important to note that the effectiveness of this approach may still depend on the specific characteristics of the Ollama API and the model being used. The model's ability to provide consistent and accurate scores across multiple iterations and its sensitivity to context changes may vary. Therefore, it's recommended to monitor the results and assess the quality and consistency of the scores obtained to determine if this approach leads to improved accuracy and reliability in the scoring process.\n\nOverall, the updated code aligns well with the objectives and makes meaningful improvements to the scoring process by clearing the context, starting fresh, and obtaining averaged scores for each question.",
      "created_at": "2024-06-18T18:52:54.188516Z",
      "sequence_order": 27,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T18:52:54.188516Z",
            "stop_timestamp": "2024-06-18T18:52:54.188516Z",
            "flags": null,
            "type": "text",
            "text": " The updated code addresses the objectives of getting a more accurate score for each question by clearing the context and \"starting fresh.\" Let's review the changes made and assess how they align with the objectives:\n\n1. Resetting the client to clear history:\n   - Inside the `get_average_score` function, a new `ollama.Client()` is created for each iteration.\n   - This ensures that the client's history is cleared before sending the question to the model.\n   - By resetting the client, the model starts fresh without any previous context, allowing for independent scoring of each question.\n\n2. Getting average scores for each question:\n   - The `get_average_score` function is introduced to calculate the average score for a specific question.\n   - It sends the question to the model three times and collects the scores.\n   - The scores are validated to ensure they are within the valid range of 0-100.\n   - If any invalid scores are encountered, a warning is logged.\n   - The function returns the average of the valid scores or 0 if no valid scores are obtained.\n\n3. Iterating over questions for each activity:\n   - The `get_scores_for_activity` function is updated to iterate over each question for a given activity.\n   - It calls the `get_average_score` function for each question and stores the average scores in the `scores` dictionary.\n   - The function also calculates the differences between the maximum and minimum scores for each question to assess score variability.\n   - If any question does not return valid scores, feedback is accumulated to indicate the issue.\n\n4. Updating the DataFrame with averaged scores and differences:\n   - The code iterates over each row in the DataFrame, representing different activities.\n   - For each activity, the `get_scores_for_activity` function is called to obtain the averaged scores, differences, and feedback.\n   - The averaged scores and differences are assigned to the corresponding columns in the DataFrame.\n   - The feedback is also stored in the 'feedback' column.\n\n5. Logging and saving the updated DataFrame:\n   - The code logs the averaged scores and differences for each row to provide visibility into the scoring process.\n   - The final DataFrame, including the new columns for averaged scores and differences, is logged.\n   - The updated DataFrame is saved to a new CSV file.\n\nAssessment:\n\nThe updated code effectively addresses the objectives of getting a more accurate score for each question by clearing the context and starting fresh. The key improvements include:\n\n- Resetting the client for each iteration to clear the history and ensure independent scoring.\n- Calculating the average score for each question by sending it to the model multiple times and validating the scores.\n- Iterating over each question for a given activity and storing the averaged scores and differences.\n- Providing feedback if any question does not return valid scores, indicating potential issues.\n- Updating the DataFrame with the averaged scores, differences, and feedback for each activity.\n\nThese changes aim to mitigate the impact of context carry-over and obtain more reliable and consistent scores for each question. By averaging the scores from multiple iterations and assessing score variability, the code provides a more robust evaluation of each question's clarity.\n\nHowever, it's important to note that the effectiveness of this approach may still depend on the specific characteristics of the Ollama API and the model being used. The model's ability to provide consistent and accurate scores across multiple iterations and its sensitivity to context changes may vary. Therefore, it's recommended to monitor the results and assess the quality and consistency of the scores obtained to determine if this approach leads to improved accuracy and reliability in the scoring process.\n\nOverall, the updated code aligns well with the objectives and makes meaningful improvements to the scoring process by clearing the context, starting fresh, and obtaining averaged scores for each question.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T18:52:54.188516Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "35039f47-a9a0-44d1-a0bf-1085fd89fe6c",
      "role": "human",
      "content": "",
      "created_at": "2024-06-18T19:27:25.052882Z",
      "sequence_order": 28,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T19:27:25.052882Z",
            "stop_timestamp": "2024-06-18T19:27:25.052882Z",
            "flags": null,
            "type": "text",
            "text": "",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T19:27:25.052882Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 6736,
            "file_type": "txt",
            "extracted_content": "print to the console log the ollama message we are sending with the variables replaced as well as the ollama response as well as the question and answers. log everything being said\nalso, we should send the title as well as the description without HTML and the category which are these columns:\ntitle categories\tlabels description_without_html \nSend that as the full descriotion of the \"activity\"\n\nThink step by step about our goal and how to accomplish it and make a plan then execute and return a new script full file\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    question_1=question_1,\n    question_2=question_2,\n    question_3=question_3,\n    question_4=question_4,\n    question_5=question_5\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# Helper functions\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndata['readability_score'] = data['description_without_html'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['fullDescription'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['fullDescription'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['fullDescription'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['fullDescription'].apply(check_worksheet)\n\ndef get_average_score(description, question):\n    scores = []\n    for _ in range(3):\n        client = ollama.Client()  # Reset the client to clear history\n\n        user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                          .replace('{{activity_content}}', description)\n\n        messages = [\n            {'role': 'system', 'content': system_prompt},\n            {'role': 'user', 'content': user_prompt},\n            {'role': 'user', 'content': question}\n        ]\n\n        logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n        logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n        logging.info(f\"Sending question to Ollama: {question}\")\n        \n        response = client.chat(model=model, messages=messages)\n        logging.info(f\"Received response from Ollama:\\n{response}\")\n\n        question_score = response['message']['content']\n        if question_score.isdigit():\n            question_score = int(question_score)\n            if 0 <= question_score <= 100:\n                scores.append(question_score)\n            else:\n                logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n        else:\n            logging.warning(f\"Question score {question_score} is not a valid integer.\")\n    \n    if scores:\n        return sum(scores) / len(scores)\n    else:\n        return 0\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, 6)}\n    feedback = \"\"\n\n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(3):\n        for i, question in enumerate(questions, start=1):\n            average_score = get_average_score(description, question)\n            scores[f'question_{i}_score'].append(average_score)\n            if average_score == 0:\n                feedback += f\"Question {i} did not return valid scores. \"\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['description_without_html']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        data.at[index, f'question_{q}_score'] = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n    \n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n"
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          }
        ]
      }
    },
    {
      "id": "0ca834d1-cc29-4113-bcc3-346d30a52618",
      "role": "assistant",
      "content": " Here are a few suggestions to further improve the code and address potential issues:\n\n1. Error handling for API calls:\n   - Consider adding error handling for the API calls to the Ollama client.\n   - Wrap the `client.chat()` call in a try-except block to catch any exceptions that may occur during the API request.\n   - Log the error details and handle the exception gracefully, possibly by skipping the current iteration or providing a default score.\n\n2. Handling missing or invalid scores:\n   - In the `get_average_score` function, if no valid scores are obtained after multiple iterations, the function returns 0.\n   - Consider using a special value (e.g., -1) to indicate missing or invalid scores, distinguishing them from a valid score of 0.\n   - Update the code that assigns scores to the DataFrame to handle the special value appropriately.\n\n3. Configurable number of iterations:\n   - The code currently performs three iterations for each question.\n   - Consider making the number of iterations configurable by adding a parameter to the `get_average_score` and `get_scores_for_activity` functions.\n   - This allows flexibility in adjusting the number of iterations based on specific requirements or experimentation.\n\n4. Logging level configuration:\n   - The code uses `logging.info()` to log various messages, including the scores and differences for each row.\n   - Consider adding a configuration option to control the logging level (e.g., INFO, DEBUG, WARNING) based on the desired verbosity.\n   - This allows users to adjust the logging output according to their needs and avoid excessive logging in production environments.\n\n5. Efficiency improvements:\n   - The code creates a new `ollama.Client()` for each iteration within the `get_average_score` function.\n   - Consider reusing the client instance across iterations to avoid the overhead of creating a new client for each request.\n   - You can create the client instance outside the loop and reset its history using an appropriate method or by sending a special message to clear the context.\n\n6. Debugging and troubleshooting:\n   - To assist with debugging and troubleshooting, consider adding more logging statements or print statements at critical points in the code.\n   - Log the request data sent to the Ollama API and the raw response received from the API.\n   - This helps in identifying any issues related to the input data or the API responses.\n\n7. Modularity and code organization:\n   - Consider separating the code into smaller, focused functions or modules for better organization and maintainability.\n   - Move the data loading, preprocessing, and utility functions into separate modules or files.\n   - Create a dedicated module for interacting with the Ollama API, encapsulating the client creation, request handling, and response processing.\n\n8. Testing and validation:\n   - Develop a comprehensive set of test cases to validate the functionality and accuracy of the code.\n   - Write unit tests for individual functions, such as `get_average_score`, to ensure they handle different scenarios correctly.\n   - Perform integration tests to verify the end-to-end flow of the code, including data loading, preprocessing, scoring, and DataFrame updates.\n   - Validate the output CSV file to ensure the scores and differences are correctly calculated and stored.\n\n9. Documentation and comments:\n   - Add clear and concise comments to the code, explaining the purpose and functionality of each function and important code blocks.\n   - Provide a README file or documentation that explains how to set up and run the code, including dependencies, configuration, and expected inputs/outputs.\n   - Document any assumptions, limitations, or known issues related to the code or the Ollama API.\n\nBy addressing these suggestions, you can enhance the robustness, reliability, and maintainability of the code. However, keep in mind that the specific improvements and modifications may depend on your project's requirements, constraints, and the characteristics of the Ollama API.",
      "created_at": "2024-06-18T19:27:25.052882Z",
      "sequence_order": 29,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T19:27:25.052882Z",
            "stop_timestamp": "2024-06-18T19:27:25.052882Z",
            "flags": null,
            "type": "text",
            "text": " Here are a few suggestions to further improve the code and address potential issues:\n\n1. Error handling for API calls:\n   - Consider adding error handling for the API calls to the Ollama client.\n   - Wrap the `client.chat()` call in a try-except block to catch any exceptions that may occur during the API request.\n   - Log the error details and handle the exception gracefully, possibly by skipping the current iteration or providing a default score.\n\n2. Handling missing or invalid scores:\n   - In the `get_average_score` function, if no valid scores are obtained after multiple iterations, the function returns 0.\n   - Consider using a special value (e.g., -1) to indicate missing or invalid scores, distinguishing them from a valid score of 0.\n   - Update the code that assigns scores to the DataFrame to handle the special value appropriately.\n\n3. Configurable number of iterations:\n   - The code currently performs three iterations for each question.\n   - Consider making the number of iterations configurable by adding a parameter to the `get_average_score` and `get_scores_for_activity` functions.\n   - This allows flexibility in adjusting the number of iterations based on specific requirements or experimentation.\n\n4. Logging level configuration:\n   - The code uses `logging.info()` to log various messages, including the scores and differences for each row.\n   - Consider adding a configuration option to control the logging level (e.g., INFO, DEBUG, WARNING) based on the desired verbosity.\n   - This allows users to adjust the logging output according to their needs and avoid excessive logging in production environments.\n\n5. Efficiency improvements:\n   - The code creates a new `ollama.Client()` for each iteration within the `get_average_score` function.\n   - Consider reusing the client instance across iterations to avoid the overhead of creating a new client for each request.\n   - You can create the client instance outside the loop and reset its history using an appropriate method or by sending a special message to clear the context.\n\n6. Debugging and troubleshooting:\n   - To assist with debugging and troubleshooting, consider adding more logging statements or print statements at critical points in the code.\n   - Log the request data sent to the Ollama API and the raw response received from the API.\n   - This helps in identifying any issues related to the input data or the API responses.\n\n7. Modularity and code organization:\n   - Consider separating the code into smaller, focused functions or modules for better organization and maintainability.\n   - Move the data loading, preprocessing, and utility functions into separate modules or files.\n   - Create a dedicated module for interacting with the Ollama API, encapsulating the client creation, request handling, and response processing.\n\n8. Testing and validation:\n   - Develop a comprehensive set of test cases to validate the functionality and accuracy of the code.\n   - Write unit tests for individual functions, such as `get_average_score`, to ensure they handle different scenarios correctly.\n   - Perform integration tests to verify the end-to-end flow of the code, including data loading, preprocessing, scoring, and DataFrame updates.\n   - Validate the output CSV file to ensure the scores and differences are correctly calculated and stored.\n\n9. Documentation and comments:\n   - Add clear and concise comments to the code, explaining the purpose and functionality of each function and important code blocks.\n   - Provide a README file or documentation that explains how to set up and run the code, including dependencies, configuration, and expected inputs/outputs.\n   - Document any assumptions, limitations, or known issues related to the code or the Ollama API.\n\nBy addressing these suggestions, you can enhance the robustness, reliability, and maintainability of the code. However, keep in mind that the specific improvements and modifications may depend on your project's requirements, constraints, and the characteristics of the Ollama API.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T19:27:25.052882Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "6002dad1-8116-4566-bcd2-5f5245052c1d",
      "role": "human",
      "content": "2024-06-18 21:40:12,835 - ERROR - Error during API call: local variable 'client' referenced before assignment\n2024-06-18 21:40:12,835 - INFO - Scores for row 2161: {'question_1_score': -1.0, 'question_2_score': -1.0, 'question_3_score': -1.0, 'question_4_score': -1.0, 'question_5_score': -1.0}\n2024-06-18 21:40:12,835 - INFO - Differences for row 2161: {'question_1_score': 0, 'question_2_score': 0, 'question_3_score': 0, 'question_4_score': 0, 'question_5_score': 0}",
      "created_at": "2024-06-18T19:41:23.676908Z",
      "sequence_order": 30,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T19:41:23.676908Z",
            "stop_timestamp": "2024-06-18T19:41:23.676908Z",
            "flags": null,
            "type": "text",
            "text": "2024-06-18 21:40:12,835 - ERROR - Error during API call: local variable 'client' referenced before assignment\n2024-06-18 21:40:12,835 - INFO - Scores for row 2161: {'question_1_score': -1.0, 'question_2_score': -1.0, 'question_3_score': -1.0, 'question_4_score': -1.0, 'question_5_score': -1.0}\n2024-06-18 21:40:12,835 - INFO - Differences for row 2161: {'question_1_score': 0, 'question_2_score': 0, 'question_3_score': 0, 'question_4_score': 0, 'question_5_score': 0}",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T19:41:23.676908Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 6598,
            "file_type": "txt",
            "extracted_content": "import nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    question_1=question_1,\n    question_2=question_2,\n    question_3=question_3,\n    question_4=question_4,\n    question_5=question_5\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# Helper functions\ndef remove_html(text):\n    soup = BeautifulSoup(str(text), 'html.parser')\n    return soup.get_text(separator='\\n')\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef get_average_score(description, question):\n    scores = []\n    for _ in range(3):\n        try:\n            # Reset the client to clear history\n            client._client.close()\n            client = ollama.Client()\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt},\n                {'role': 'user', 'content': question}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n            logging.info(f\"Sending question to Ollama: {question}\")\n\n            response = client.chat(model=model, messages=messages)\n            logging.info(f\"Received response from Ollama:\\n{response}\")\n\n            question_score = response['message']['content']\n            if question_score.isdigit():\n                question_score = int(question_score)\n                if 0 <= question_score <= 100:\n                    scores.append(question_score)\n                else:\n                    logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n            else:\n                logging.warning(f\"Question score {question_score} is not a valid integer.\")\n        except Exception as e:\n            logging.error(f\"Error during API call: {e}\")\n    \n    if scores:\n        return sum(scores) / len(scores)\n    else:\n        return -1  # Use -1 to indicate invalid scores\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, 6)}\n    feedback = \"\"\n\n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(3):\n        for i, question in enumerate(questions, start=1):\n            average_score = get_average_score(description, question)\n            scores[f'question_{i}_score'].append(average_score)\n            if average_score == -1:\n                feedback += f\"Question {i} did not return valid scores. \"\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        data.at[index, f'question_{q}_score'] = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n    \n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n"
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          }
        ]
      }
    },
    {
      "id": "03e1342e-2b0e-411b-b2aa-422e8528f55e",
      "role": "assistant",
      "content": " The error message \"local variable 'client' referenced before assignment\" suggests that there is an issue with the `client` variable within the `get_average_score` function. It seems that the `client` variable is being referenced before it is properly assigned.\n\nTo fix this issue, you can modify the `get_average_score` function to handle the case when an exception occurs during the API call. Here's an updated version of the function:\n\n```python\ndef get_average_score(description, question):\n    scores = []\n    for _ in range(3):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt},\n                {'role': 'user', 'content': question}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n            logging.info(f\"Sending question to Ollama: {question}\")\n            \n            response = client.chat(model=model, messages=messages)\n            logging.info(f\"Received response from Ollama:\\n{response}\")\n\n            question_score = response['message']['content']\n            if question_score.isdigit():\n                question_score = int(question_score)\n                if 0 <= question_score <= 100:\n                    scores.append(question_score)\n                else:\n                    logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n            else:\n                logging.warning(f\"Question score {question_score} is not a valid integer.\")\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n    \n    if scores:\n        return sum(scores) / len(scores)\n    else:\n        return -1  # Return -1 to indicate missing or invalid scores\n```\n\nIn this updated version, the API call to `client.chat()` is wrapped inside a try-except block. If an exception occurs during the API call, it is caught and logged using `logging.error()`. The function continues to the next iteration without appending any score to the `scores` list.\n\nAfter the iterations, if no valid scores are obtained (i.e., `scores` is empty), the function returns -1 to indicate missing or invalid scores. This distinguishes it from a valid score of 0.\n\nAdditionally, you'll need to update the code that assigns scores to the DataFrame to handle the case when the score is -1. For example:\n\n```python\nfor q in range(1, 6):\n    score = averaged_scores.get(f'question_{q}_score', -1)\n    data.at[index, f'question_{q}_score'] = score if score != -1 else 0\n    data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n```\n\nIn this code, if the score is -1 (indicating missing or invalid scores), it is assigned a value of 0 in the DataFrame. You can modify this behavior based on your specific requirements.\n\nBy making these changes, the code should handle the case when an exception occurs during the API call and continue processing the remaining activities without raising the \"local variable 'client' referenced before assignment\" error.",
      "created_at": "2024-06-18T19:41:23.676908Z",
      "sequence_order": 31,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T19:41:23.676908Z",
            "stop_timestamp": "2024-06-18T19:41:23.676908Z",
            "flags": null,
            "type": "text",
            "text": " The error message \"local variable 'client' referenced before assignment\" suggests that there is an issue with the `client` variable within the `get_average_score` function. It seems that the `client` variable is being referenced before it is properly assigned.\n\nTo fix this issue, you can modify the `get_average_score` function to handle the case when an exception occurs during the API call. Here's an updated version of the function:\n\n```python\ndef get_average_score(description, question):\n    scores = []\n    for _ in range(3):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt},\n                {'role': 'user', 'content': question}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n            logging.info(f\"Sending question to Ollama: {question}\")\n            \n            response = client.chat(model=model, messages=messages)\n            logging.info(f\"Received response from Ollama:\\n{response}\")\n\n            question_score = response['message']['content']\n            if question_score.isdigit():\n                question_score = int(question_score)\n                if 0 <= question_score <= 100:\n                    scores.append(question_score)\n                else:\n                    logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n            else:\n                logging.warning(f\"Question score {question_score} is not a valid integer.\")\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n    \n    if scores:\n        return sum(scores) / len(scores)\n    else:\n        return -1  # Return -1 to indicate missing or invalid scores\n```\n\nIn this updated version, the API call to `client.chat()` is wrapped inside a try-except block. If an exception occurs during the API call, it is caught and logged using `logging.error()`. The function continues to the next iteration without appending any score to the `scores` list.\n\nAfter the iterations, if no valid scores are obtained (i.e., `scores` is empty), the function returns -1 to indicate missing or invalid scores. This distinguishes it from a valid score of 0.\n\nAdditionally, you'll need to update the code that assigns scores to the DataFrame to handle the case when the score is -1. For example:\n\n```python\nfor q in range(1, 6):\n    score = averaged_scores.get(f'question_{q}_score', -1)\n    data.at[index, f'question_{q}_score'] = score if score != -1 else 0\n    data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n```\n\nIn this code, if the score is -1 (indicating missing or invalid scores), it is assigned a value of 0 in the DataFrame. You can modify this behavior based on your specific requirements.\n\nBy making these changes, the code should handle the case when an exception occurs during the API call and continue processing the remaining activities without raising the \"local variable 'client' referenced before assignment\" error.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T19:41:23.676908Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "a630da2e-cfde-4505-9292-550d6a5ec8af",
      "role": "human",
      "content": "make sure the system prompt only happens once and that the all five questions are asked first for each activity before the message queue is cleared and then ask all five again twice for the current activity and then move on to the next activity",
      "created_at": "2024-06-18T20:02:14.226135Z",
      "sequence_order": 32,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T20:02:14.226135Z",
            "stop_timestamp": "2024-06-18T20:02:14.226135Z",
            "flags": null,
            "type": "text",
            "text": "make sure the system prompt only happens once and that the all five questions are asked first for each activity before the message queue is cleared and then ask all five again twice for the current activity and then move on to the next activity",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T20:02:14.226135Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 6675,
            "file_type": "txt",
            "extracted_content": "import nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    question_1=question_1,\n    question_2=question_2,\n    question_3=question_3,\n    question_4=question_4,\n    question_5=question_5\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# Helper functions\ndef remove_html(text):\n    soup = BeautifulSoup(str(text), 'html.parser')\n    return soup.get_text(separator='\\n')\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef send_to_ollama(client, messages):\n    response = \"\"\n    try:\n        stream = client.chat(model=model, messages=messages, stream=True)\n        for chunk in stream:\n            part = chunk['message']['content']\n            print(part, end='', flush=True)\n            response += part\n    except Exception as e:\n        logging.error(f\"Error during API call: {e}\")\n    return response\n\ndef get_average_score(description, question):\n    scores = []\n    for _ in range(3):\n        messages = [\n            {'role': 'system', 'content': system_prompt},\n            {'role': 'user', 'content': user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric).replace('{{activity_content}}', description)},\n            {'role': 'user', 'content': question}\n        ]\n\n        logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n        logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric).replace('{{activity_content}}', description)}\")\n        logging.info(f\"Sending question to Ollama: {question}\")\n\n        response = send_to_ollama(client, messages)\n        logging.info(f\"Received response from Ollama:\\n{response}\")\n\n        if response.isdigit():\n            question_score = int(response)\n            if 0 <= question_score <= 100:\n                scores.append(question_score)\n            else:\n                logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n        else:\n            logging.warning(f\"Question score {response} is not a valid integer.\")\n    \n    if scores:\n        return sum(scores) / len(scores)\n    else:\n        return -1  # Use -1 to indicate invalid scores\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, 6)}\n    feedback = \"\"\n\n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(3):\n        for i, question in enumerate(questions, start=1):\n            average_score = get_average_score(description, question)\n            scores[f'question_{i}_score'].append(average_score)\n            if average_score == -1:\n                feedback += f\"Question {i} did not return valid scores. \"\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        data.at[index, f'question_{q}_score'] = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n    \n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n\n# Close the client when done\nclient._client.close()\n"
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          }
        ]
      }
    },
    {
      "id": "da3480c9-2832-4d61-951e-2bdbd79976c6",
      "role": "assistant",
      "content": " Certainly! Here's the updated code that ensures the system prompt is sent only once and all five questions are asked for each activity before clearing the message queue and repeating the process two more times for the current activity before moving on to the next activity:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    question_1=question_1,\n    question_2=question_2,\n    question_3=question_3,\n    question_4=question_4,\n    question_5=question_5\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# Helper functions\ndef remove_html(text):\n    soup = BeautifulSoup(str(text), 'html.parser')\n    return soup.get_text(separator='\\n')\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, 6)}\n    feedback = \"\"\n\n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(3):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n\n            for i, question in enumerate(questions, start=1):\n                messages.append({'role': 'user', 'content': question})\n                logging.info(f\"Sending question to Ollama: {question}\")\n                response = client.chat(model=model, messages=messages)\n                logging.info(f\"Received response from Ollama:\\n{response}\")\n\n                question_score = response['message']['content']\n                if question_score.isdigit():\n                    question_score = int(question_score)\n                    if 0 <= question_score <= 100:\n                        scores[f'question_{i}_score'].append(question_score)\n                    else:\n                        logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                else:\n                    logging.warning(f\"Question score {question_score} is not a valid integer.\")\n\n            messages.clear()  # Clear the message queue for the next iteration\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else -1 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        score = averaged_scores.get(f'question_{q}_score', -1)\n        data.at[index, f'question_{q}_score'] = score if score != -1 else 0\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nIn this updated code:\n\n- The system prompt is sent only once at the beginning of each iteration for an activity.\n- The user prompt is updated with the activity description and sent along with the system prompt in the `messages` list.\n- All five questions are appended to the `messages` list one by one.\n- The API call to `client.chat()` is made for each question, and the responses are logged.\n- The scores for each question are collected and appended to the respective lists in the `scores` dictionary.\n- After all five questions have been asked, the `messages` list is cleared using `messages.clear()` to prepare for the next iteration.\n- The process is repeated two more times (a total of three iterations) for the current activity before moving on to the next activity.\n\nThe rest of the code remains the same, including the error handling, score averaging, difference calculation, and saving snapshots.\n\nWith these updates, the code ensures that the system prompt is sent only once, all five questions are asked for each activity, the message queue is cleared after each iteration, and the process is repeated three times for each activity before moving on to the next one.",
      "created_at": "2024-06-18T20:02:14.226135Z",
      "sequence_order": 33,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T20:02:14.226135Z",
            "stop_timestamp": "2024-06-18T20:02:14.226135Z",
            "flags": null,
            "type": "text",
            "text": " Certainly! Here's the updated code that ensures the system prompt is sent only once and all five questions are asked for each activity before clearing the message queue and repeating the process two more times for the current activity before moving on to the next activity:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    question_1=question_1,\n    question_2=question_2,\n    question_3=question_3,\n    question_4=question_4,\n    question_5=question_5\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# Helper functions\ndef remove_html(text):\n    soup = BeautifulSoup(str(text), 'html.parser')\n    return soup.get_text(separator='\\n')\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, 6)}\n    feedback = \"\"\n\n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(3):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n\n            for i, question in enumerate(questions, start=1):\n                messages.append({'role': 'user', 'content': question})\n                logging.info(f\"Sending question to Ollama: {question}\")\n                response = client.chat(model=model, messages=messages)\n                logging.info(f\"Received response from Ollama:\\n{response}\")\n\n                question_score = response['message']['content']\n                if question_score.isdigit():\n                    question_score = int(question_score)\n                    if 0 <= question_score <= 100:\n                        scores[f'question_{i}_score'].append(question_score)\n                    else:\n                        logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                else:\n                    logging.warning(f\"Question score {question_score} is not a valid integer.\")\n\n            messages.clear()  # Clear the message queue for the next iteration\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else -1 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        score = averaged_scores.get(f'question_{q}_score', -1)\n        data.at[index, f'question_{q}_score'] = score if score != -1 else 0\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nIn this updated code:\n\n- The system prompt is sent only once at the beginning of each iteration for an activity.\n- The user prompt is updated with the activity description and sent along with the system prompt in the `messages` list.\n- All five questions are appended to the `messages` list one by one.\n- The API call to `client.chat()` is made for each question, and the responses are logged.\n- The scores for each question are collected and appended to the respective lists in the `scores` dictionary.\n- After all five questions have been asked, the `messages` list is cleared using `messages.clear()` to prepare for the next iteration.\n- The process is repeated two more times (a total of three iterations) for the current activity before moving on to the next activity.\n\nThe rest of the code remains the same, including the error handling, score averaging, difference calculation, and saving snapshots.\n\nWith these updates, the code ensures that the system prompt is sent only once, all five questions are asked for each activity, the message queue is cleared after each iteration, and the process is repeated three times for each activity before moving on to the next one.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T20:02:14.226135Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "c5d00684-a985-4f8a-b36f-363a5a7a4a90",
      "role": "human",
      "content": "can you explain why the script is not savingthe data right?\nQuestion 1 did not return valid scores. Question 2 did not return valid scores. Question 3 did not return valid scores. Question 4 did not return valid scores. Question 5 did not return valid scores. Question 1 did not return valid scores. Question 2 did not return valid scores. Question 3 did not return valid scores. Question 4 did not return valid scores. Question 5 did not return valid scores. Question 1 did not return valid scores. Question 2 did not return valid scores. Question 3 did not return valid scores. Question 4 did not return valid scores. Question 5 did not return valid scores.\t-1.0\n\nSays everything is-1  when it's not",
      "created_at": "2024-06-18T20:41:33.900191Z",
      "sequence_order": 34,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T20:41:33.900191Z",
            "stop_timestamp": "2024-06-18T20:41:33.900191Z",
            "flags": null,
            "type": "text",
            "text": "can you explain why the script is not savingthe data right?\nQuestion 1 did not return valid scores. Question 2 did not return valid scores. Question 3 did not return valid scores. Question 4 did not return valid scores. Question 5 did not return valid scores. Question 1 did not return valid scores. Question 2 did not return valid scores. Question 3 did not return valid scores. Question 4 did not return valid scores. Question 5 did not return valid scores. Question 1 did not return valid scores. Question 2 did not return valid scores. Question 3 did not return valid scores. Question 4 did not return valid scores. Question 5 did not return valid scores.\t-1.0\n\nSays everything is-1  when it's not",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T20:41:33.900191Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 6485,
            "file_type": "txt",
            "extracted_content": "import nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    question_1=question_1,\n    question_2=question_2,\n    question_3=question_3,\n    question_4=question_4,\n    question_5=question_5\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n#First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, 6)}\n    feedback = \"\"\n\n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(3):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n\n            for i, question in enumerate(questions, start=1):\n                messages.append({'role': 'user', 'content': question})\n                logging.info(f\"Sending question to Ollama: {question}\")\n                response = client.chat(model=model, messages=messages)\n                logging.info(f\"Received response from Ollama:\\n{response}\")\n\n                question_score = response['message']['content']\n                if question_score.isdigit():\n                    question_score = int(question_score)\n                    if 0 <= question_score <= 100:\n                        scores[f'question_{i}_score'].append(question_score)\n                    else:\n                        logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                else:\n                    logging.warning(f\"Question score {question_score} is not a valid integer.\")\n\n            messages.clear()  # Clear the message queue for the next iteration\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else -1 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        score = averaged_scores.get(f'question_{q}_score', -1)\n        data.at[index, f'question_{q}_score'] = score if score != -1 else 0\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)"
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          }
        ]
      }
    },
    {
      "id": "fe02353f-40e0-4f28-bbe9-123c006a8556",
      "role": "assistant",
      "content": " The issue with the script not saving the data correctly and returning -1 for all questions lies in the `get_scores_for_activity` function. The problem occurs when the API call fails or returns an invalid response, causing the scores for all questions to be -1.\n\nHere's the explanation:\n\n1. In the `get_scores_for_activity` function, the scores for each question are stored in the `scores` dictionary. The keys are in the format `question_{i}_score`, and the corresponding values are lists to store the scores for each iteration.\n\n2. The function iterates three times (as specified by `range(3)`), and for each iteration, it creates a new `ollama.Client()` to reset the client and clear the history.\n\n3. Inside each iteration, the function appends the questions to the `messages` list and sends them to the Ollama API using `client.chat()`.\n\n4. The response from the API is then processed, and if the question score is a valid integer between 0 and 100, it is appended to the corresponding list in the `scores` dictionary.\n\n5. However, if an exception occurs during the API call (e.g., due to network issues or API errors), the exception is caught and logged using `logging.error()`. In this case, no scores are appended to the `scores` dictionary for that iteration.\n\n6. After all iterations, the `averaged_scores` dictionary is created using a dictionary comprehension. It calculates the average score for each question by summing the scores and dividing by the length of the list. If the list is empty (i.e., no valid scores were obtained), it assigns a value of -1 to indicate missing or invalid scores.\n\n7. The `differences` dictionary is also created to store the difference between the maximum and minimum scores for each question.\n\n8. Finally, the `averaged_scores`, `differences`, and `feedback` are returned.\n\nThe issue arises when all iterations fail to retrieve valid scores due to API errors or exceptions. In such cases, the `scores` dictionary remains empty, and the `averaged_scores` dictionary assigns -1 to all questions.\n\nTo address this issue, you can modify the code to handle API errors more gracefully and provide appropriate feedback. Here are a few suggestions:\n\n1. Instead of catching the exception silently and logging the error, you can raise the exception or handle it in a way that allows the script to continue with the next activity while providing meaningful feedback.\n\n2. You can add a check after each API call to verify if the response is valid and contains the expected data. If the response is invalid or missing required fields, you can log an appropriate error message and skip the current iteration.\n\n3. In the `averaged_scores` dictionary comprehension, you can modify the condition to handle empty lists differently. Instead of assigning -1, you can assign a special value or provide a more informative message indicating that no valid scores were obtained for that question.\n\n4. You can also enhance the feedback mechanism to provide more detailed information about the API errors or exceptions encountered during the scoring process.\n\nBy implementing these modifications, you can improve the script's ability to handle API errors and provide more accurate and informative feedback when saving the data.",
      "created_at": "2024-06-18T20:41:33.900191Z",
      "sequence_order": 35,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T20:41:33.900191Z",
            "stop_timestamp": "2024-06-18T20:41:33.900191Z",
            "flags": null,
            "type": "text",
            "text": " The issue with the script not saving the data correctly and returning -1 for all questions lies in the `get_scores_for_activity` function. The problem occurs when the API call fails or returns an invalid response, causing the scores for all questions to be -1.\n\nHere's the explanation:\n\n1. In the `get_scores_for_activity` function, the scores for each question are stored in the `scores` dictionary. The keys are in the format `question_{i}_score`, and the corresponding values are lists to store the scores for each iteration.\n\n2. The function iterates three times (as specified by `range(3)`), and for each iteration, it creates a new `ollama.Client()` to reset the client and clear the history.\n\n3. Inside each iteration, the function appends the questions to the `messages` list and sends them to the Ollama API using `client.chat()`.\n\n4. The response from the API is then processed, and if the question score is a valid integer between 0 and 100, it is appended to the corresponding list in the `scores` dictionary.\n\n5. However, if an exception occurs during the API call (e.g., due to network issues or API errors), the exception is caught and logged using `logging.error()`. In this case, no scores are appended to the `scores` dictionary for that iteration.\n\n6. After all iterations, the `averaged_scores` dictionary is created using a dictionary comprehension. It calculates the average score for each question by summing the scores and dividing by the length of the list. If the list is empty (i.e., no valid scores were obtained), it assigns a value of -1 to indicate missing or invalid scores.\n\n7. The `differences` dictionary is also created to store the difference between the maximum and minimum scores for each question.\n\n8. Finally, the `averaged_scores`, `differences`, and `feedback` are returned.\n\nThe issue arises when all iterations fail to retrieve valid scores due to API errors or exceptions. In such cases, the `scores` dictionary remains empty, and the `averaged_scores` dictionary assigns -1 to all questions.\n\nTo address this issue, you can modify the code to handle API errors more gracefully and provide appropriate feedback. Here are a few suggestions:\n\n1. Instead of catching the exception silently and logging the error, you can raise the exception or handle it in a way that allows the script to continue with the next activity while providing meaningful feedback.\n\n2. You can add a check after each API call to verify if the response is valid and contains the expected data. If the response is invalid or missing required fields, you can log an appropriate error message and skip the current iteration.\n\n3. In the `averaged_scores` dictionary comprehension, you can modify the condition to handle empty lists differently. Instead of assigning -1, you can assign a special value or provide a more informative message indicating that no valid scores were obtained for that question.\n\n4. You can also enhance the feedback mechanism to provide more detailed information about the API errors or exceptions encountered during the scoring process.\n\nBy implementing these modifications, you can improve the script's ability to handle API errors and provide more accurate and informative feedback when saving the data.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T20:41:33.900191Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "0ede1115-d8c2-4924-bfe0-515560eddb7a",
      "role": "human",
      "content": "remove the -1 and add 0 only if there is an errorand return the full script with the improvements",
      "created_at": "2024-06-18T20:44:56.709999Z",
      "sequence_order": 36,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T20:44:56.709999Z",
            "stop_timestamp": "2024-06-18T20:44:56.709999Z",
            "flags": null,
            "type": "text",
            "text": "remove the -1 and add 0 only if there is an errorand return the full script with the improvements",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T20:44:56.709999Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "b4150da8-caad-43b4-9575-d9ff2a8df427",
      "role": "assistant",
      "content": " Here's the updated script with the suggested improvements:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    question_1=question_1,\n    question_2=question_2,\n    question_3=question_3,\n    question_4=question_4,\n    question_5=question_5\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n#First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, 6)}\n    feedback = \"\"\n\n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(3):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n\n            for i, question in enumerate(questions, start=1):\n                messages.append({'role': 'user', 'content': question})\n                logging.info(f\"Sending question to Ollama: {question}\")\n                response = client.chat(model=model, messages=messages)\n                logging.info(f\"Received response from Ollama:\\n{response}\")\n\n                if 'message' in response and 'content' in response['message']:\n                    question_score = response['message']['content']\n                    if question_score.isdigit():\n                        question_score = int(question_score)\n                        if 0 <= question_score <= 100:\n                            scores[f'question_{i}_score'].append(question_score)\n                        else:\n                            logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                            feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n                    else:\n                        logging.warning(f\"Question score {question_score} is not a valid integer.\")\n                        feedback += f\"Question {i} score is not a valid integer. \"\n                else:\n                    logging.error(f\"Invalid response from Ollama for question {i}: {response}\")\n                    feedback += f\"Invalid response from Ollama for question {i}. \"\n\n            messages.clear()  # Clear the message queue for the next iteration\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n            feedback += f\"Error during API call: {str(e)}. \"\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        score = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_score'] = score\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nIn this updated script:\n\n1. The condition for assigning scores in the `averaged_scores` dictionary comprehension has been modified. If the list of scores for a question is empty, it assigns 0 instead of -1.\n\n2. Additional checks have been added after receiving the response from the Ollama API:\n   - It verifies if the response contains the expected 'message' and 'content' fields.\n   - If the response is invalid or missing the required fields, it logs an error message and appends appropriate feedback.\n\n3. The feedback mechanism has been enhanced to provide more detailed information about the issues encountered during the scoring process:\n   - If a question score is outside the valid range of 0-100, it appends a feedback message indicating the issue.\n   - If a question score is not a valid integer, it appends a feedback message indicating the problem.\n   - If there is an error during the API call, it appends the error message to the feedback.\n\n4. When assigning scores to the DataFrame, the default value for missing scores has been changed from -1 to 0.\n\nThese modifications ensure that:\n- If there are no valid scores for a question, it assigns 0 instead of -1.\n- More informative feedback is provided when issues occur during the scoring process.\n- The script continues processing the next activity even if there are errors or invalid responses from the Ollama API.\n\nPlease note that you may need to adjust the code further based on your specific requirements and the behavior of the Ollama API.",
      "created_at": "2024-06-18T20:44:56.709999Z",
      "sequence_order": 37,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-18T20:44:56.709999Z",
            "stop_timestamp": "2024-06-18T20:44:56.709999Z",
            "flags": null,
            "type": "text",
            "text": " Here's the updated script with the suggested improvements:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    question_1=question_1,\n    question_2=question_2,\n    question_3=question_3,\n    question_4=question_4,\n    question_5=question_5\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n#First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, 6)}\n    feedback = \"\"\n\n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(3):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n\n            for i, question in enumerate(questions, start=1):\n                messages.append({'role': 'user', 'content': question})\n                logging.info(f\"Sending question to Ollama: {question}\")\n                response = client.chat(model=model, messages=messages)\n                logging.info(f\"Received response from Ollama:\\n{response}\")\n\n                if 'message' in response and 'content' in response['message']:\n                    question_score = response['message']['content']\n                    if question_score.isdigit():\n                        question_score = int(question_score)\n                        if 0 <= question_score <= 100:\n                            scores[f'question_{i}_score'].append(question_score)\n                        else:\n                            logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                            feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n                    else:\n                        logging.warning(f\"Question score {question_score} is not a valid integer.\")\n                        feedback += f\"Question {i} score is not a valid integer. \"\n                else:\n                    logging.error(f\"Invalid response from Ollama for question {i}: {response}\")\n                    feedback += f\"Invalid response from Ollama for question {i}. \"\n\n            messages.clear()  # Clear the message queue for the next iteration\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n            feedback += f\"Error during API call: {str(e)}. \"\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        score = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_score'] = score\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nIn this updated script:\n\n1. The condition for assigning scores in the `averaged_scores` dictionary comprehension has been modified. If the list of scores for a question is empty, it assigns 0 instead of -1.\n\n2. Additional checks have been added after receiving the response from the Ollama API:\n   - It verifies if the response contains the expected 'message' and 'content' fields.\n   - If the response is invalid or missing the required fields, it logs an error message and appends appropriate feedback.\n\n3. The feedback mechanism has been enhanced to provide more detailed information about the issues encountered during the scoring process:\n   - If a question score is outside the valid range of 0-100, it appends a feedback message indicating the issue.\n   - If a question score is not a valid integer, it appends a feedback message indicating the problem.\n   - If there is an error during the API call, it appends the error message to the feedback.\n\n4. When assigning scores to the DataFrame, the default value for missing scores has been changed from -1 to 0.\n\nThese modifications ensure that:\n- If there are no valid scores for a question, it assigns 0 instead of -1.\n- More informative feedback is provided when issues occur during the scoring process.\n- The script continues processing the next activity even if there are errors or invalid responses from the Ollama API.\n\nPlease note that you may need to adjust the code further based on your specific requirements and the behavior of the Ollama API.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-18T20:44:56.709999Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "52eb9d10-5866-4e68-bba7-3edac343fc5d",
      "role": "human",
      "content": "update the code so that it can take in questions arbitrarily from the YAML file, it. now has 7 questions: - also i added the average_iterations so that we can set how many times it should loop to create averages. Update the code to use these variables dynamically \nYAML file:\nsettings:\n  model: llama3\n  average_iterations: 10\nprompts:\n  system_prompt: |\n    You are Afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online training entrepreneurship training program. \n    You will see the following sub-questions for {{assessment_rubric}}\n    and will be asked to score them from \n    1: Is there an outline?\n    2: Is the outline comprehensive? \n    3: Is the activity written clearly with no jargon?\n    4: Does the activity have actionable directions?\n    5: Are there any discrepancies in the agreement between subjects and verbs? \n    6: Do pronouns have clear and unambiguous antecedents throughout the passage\n    7: Are sentences concise without run-on sentences within the activity?\n    Your answer will be a score from 0, being very poor, to 100, for excellent. \n    When I ask you for the question's score, you will return only the score for that question as an integer and no other information or text.\n  user_prompt: |\n    Afif, your job is to assess the following activity against the {{assessment_rubric}}'s sub-questions.\n    After I send the activity, I will request one of the 5 sub-questions. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\n    Activity follows in three brackets:[[[{{activity_content}}]]]\nassessment:\n  assessment_rubric: Clarity of Instructions\n  question_1: Is there an outline?\n  question_2: Is the outline comprehensive? \n  question_3: Is the activity written clearly with no jargon?\n  question_4: Does the activity have actionable directions?\n  question_5: Are there any discrepancies in the agreement between subjects and verbs? \n  question_6: Do pronouns have clear and unambiguous antecedents throughout the passage\n  question_7: Are sentences concise without run-on sentences within the activity?",
      "created_at": "2024-06-19T04:59:13.186520Z",
      "sequence_order": 38,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-19T04:59:13.186520Z",
            "stop_timestamp": "2024-06-19T04:59:13.186520Z",
            "flags": null,
            "type": "text",
            "text": "update the code so that it can take in questions arbitrarily from the YAML file, it. now has 7 questions: - also i added the average_iterations so that we can set how many times it should loop to create averages. Update the code to use these variables dynamically \nYAML file:\nsettings:\n  model: llama3\n  average_iterations: 10\nprompts:\n  system_prompt: |\n    You are Afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online training entrepreneurship training program. \n    You will see the following sub-questions for {{assessment_rubric}}\n    and will be asked to score them from \n    1: Is there an outline?\n    2: Is the outline comprehensive? \n    3: Is the activity written clearly with no jargon?\n    4: Does the activity have actionable directions?\n    5: Are there any discrepancies in the agreement between subjects and verbs? \n    6: Do pronouns have clear and unambiguous antecedents throughout the passage\n    7: Are sentences concise without run-on sentences within the activity?\n    Your answer will be a score from 0, being very poor, to 100, for excellent. \n    When I ask you for the question's score, you will return only the score for that question as an integer and no other information or text.\n  user_prompt: |\n    Afif, your job is to assess the following activity against the {{assessment_rubric}}'s sub-questions.\n    After I send the activity, I will request one of the 5 sub-questions. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\n    Activity follows in three brackets:[[[{{activity_content}}]]]\nassessment:\n  assessment_rubric: Clarity of Instructions\n  question_1: Is there an outline?\n  question_2: Is the outline comprehensive? \n  question_3: Is the activity written clearly with no jargon?\n  question_4: Does the activity have actionable directions?\n  question_5: Are there any discrepancies in the agreement between subjects and verbs? \n  question_6: Do pronouns have clear and unambiguous antecedents throughout the passage\n  question_7: Are sentences concise without run-on sentences within the activity?",
            "citations": []
          }
        ],
        "updated_at": "2024-06-19T04:59:13.186520Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 7022,
            "file_type": "txt",
            "extracted_content": "import nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    question_1=question_1,\n    question_2=question_2,\n    question_3=question_3,\n    question_4=question_4,\n    question_5=question_5\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n#First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, 6)}\n    feedback = \"\"\n\n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(10):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n\n            for i, question in enumerate(questions, start=1):\n                messages.append({'role': 'user', 'content': question})\n                logging.info(f\"Sending question to Ollama: {question}\")\n                response = client.chat(model=model, messages=messages)\n                logging.info(f\"Received response from Ollama:\\n{response}\")\n\n                if 'message' in response and 'content' in response['message']:\n                    question_score = response['message']['content']\n                    if question_score.isdigit():\n                        question_score = int(question_score)\n                        if 0 <= question_score <= 100:\n                            scores[f'question_{i}_score'].append(question_score)\n                        else:\n                            logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                            feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n                    else:\n                        logging.warning(f\"Question score {question_score} is not a valid integer.\")\n                        feedback += f\"Question {i} score is not a valid integer. \"\n                else:\n                    logging.error(f\"Invalid response from Ollama for question {i}: {response}\")\n                    feedback += f\"Invalid response from Ollama for question {i}. \"\n\n            messages.clear()  # Clear the message queue for the next iteration\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n            feedback += f\"Error during API call: {str(e)}. \"\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        score = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_score'] = score\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)"
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          }
        ]
      }
    },
    {
      "id": "cd314833-8183-4d28-8c6e-e0e6c7f1b977",
      "role": "assistant",
      "content": " Here's the updated code that dynamically reads the questions from the YAML file and uses the `average_iterations` variable to determine the number of times to loop for creating averages:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\naverage_iterations = config['settings']['average_iterations']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestions = [config['assessment'][f'question_{i}'] for i in range(1, 8)]\n\nsystem_prompt = system_prompt_template.format(assessment_rubric=assessment_rubric)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, len(questions) + 1)}\n    feedback = \"\"\n\n    for _ in range(average_iterations):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n\n            for i, question in enumerate(questions, start=1):\n                messages.append({'role': 'user', 'content': question})\n                logging.info(f\"Sending question to Ollama: {question}\")\n                response = client.chat(model=model, messages=messages)\n                logging.info(f\"Received response from Ollama:\\n{response}\")\n\n                if 'message' in response and 'content' in response['message']:\n                    question_score = response['message']['content']\n                    if question_score.isdigit():\n                        question_score = int(question_score)\n                        if 0 <= question_score <= 100:\n                            scores[f'question_{i}_score'].append(question_score)\n                        else:\n                            logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                            feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n                    else:\n                        logging.warning(f\"Question score {question_score} is not a valid integer.\")\n                        feedback += f\"Question {i} score is not a valid integer. \"\n                else:\n                    logging.error(f\"Invalid response from Ollama for question {i}: {response}\")\n                    feedback += f\"Invalid response from Ollama for question {i}. \"\n\n            messages.clear()  # Clear the message queue for the next iteration\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n            feedback += f\"Error during API call: {str(e)}. \"\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, len(questions) + 1):\n        score = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_score'] = score\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nThe key changes made to the code are as follows:\n\n1. The `questions` list is dynamically created based on the questions defined in the YAML file. It reads the questions from `question_1` to `question_7` using a list comprehension.\n\n2. The `average_iterations` variable is read from the YAML file under the `settings` section. It determines the number of times to loop for creating averages.\n\n3. In the `get_scores_for_activity` function, the `scores` dictionary is created dynamically based on the number of questions. It uses `len(questions)` to determine the number of questions.\n\n4. The loop for iterating over the questions in the `get_scores_for_activity` function now uses `range(1, len(questions) + 1)` to accommodate the dynamic number of questions.\n\n5. When processing each activity and assigning scores to the DataFrame, the loop now uses `range(1, len(questions) + 1)` to iterate over the dynamic number of questions.\n\nWith these updates, the code can handle an arbitrary number of questions defined in the YAML file and use the `average_iterations` variable to control the number of times to loop for creating averages.\n\nPlease make sure to update your YAML file with the new structure and additional questions as shown in your example.",
      "created_at": "2024-06-19T04:59:13.186520Z",
      "sequence_order": 39,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-19T04:59:13.186520Z",
            "stop_timestamp": "2024-06-19T04:59:13.186520Z",
            "flags": null,
            "type": "text",
            "text": " Here's the updated code that dynamically reads the questions from the YAML file and uses the `average_iterations` variable to determine the number of times to loop for creating averages:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\naverage_iterations = config['settings']['average_iterations']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestions = [config['assessment'][f'question_{i}'] for i in range(1, 8)]\n\nsystem_prompt = system_prompt_template.format(assessment_rubric=assessment_rubric)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, len(questions) + 1)}\n    feedback = \"\"\n\n    for _ in range(average_iterations):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n\n            for i, question in enumerate(questions, start=1):\n                messages.append({'role': 'user', 'content': question})\n                logging.info(f\"Sending question to Ollama: {question}\")\n                response = client.chat(model=model, messages=messages)\n                logging.info(f\"Received response from Ollama:\\n{response}\")\n\n                if 'message' in response and 'content' in response['message']:\n                    question_score = response['message']['content']\n                    if question_score.isdigit():\n                        question_score = int(question_score)\n                        if 0 <= question_score <= 100:\n                            scores[f'question_{i}_score'].append(question_score)\n                        else:\n                            logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                            feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n                    else:\n                        logging.warning(f\"Question score {question_score} is not a valid integer.\")\n                        feedback += f\"Question {i} score is not a valid integer. \"\n                else:\n                    logging.error(f\"Invalid response from Ollama for question {i}: {response}\")\n                    feedback += f\"Invalid response from Ollama for question {i}. \"\n\n            messages.clear()  # Clear the message queue for the next iteration\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n            feedback += f\"Error during API call: {str(e)}. \"\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, len(questions) + 1):\n        score = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_score'] = score\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nThe key changes made to the code are as follows:\n\n1. The `questions` list is dynamically created based on the questions defined in the YAML file. It reads the questions from `question_1` to `question_7` using a list comprehension.\n\n2. The `average_iterations` variable is read from the YAML file under the `settings` section. It determines the number of times to loop for creating averages.\n\n3. In the `get_scores_for_activity` function, the `scores` dictionary is created dynamically based on the number of questions. It uses `len(questions)` to determine the number of questions.\n\n4. The loop for iterating over the questions in the `get_scores_for_activity` function now uses `range(1, len(questions) + 1)` to accommodate the dynamic number of questions.\n\n5. When processing each activity and assigning scores to the DataFrame, the loop now uses `range(1, len(questions) + 1)` to iterate over the dynamic number of questions.\n\nWith these updates, the code can handle an arbitrary number of questions defined in the YAML file and use the `average_iterations` variable to control the number of times to loop for creating averages.\n\nPlease make sure to update your YAML file with the new structure and additional questions as shown in your example.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-19T04:59:13.186520Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "e3c0f9d9-d5cb-46cb-af21-6200250d8340",
      "role": "human",
      "content": "update the code so that it can take in questions arbitrarily from the YAML file, it. now has 7 questions: - also i added the average_iterations so that we can set how many times it should loop to create averages. Update the code to use these variables dynamically YAML file: \nupdated the YAML so the number of questions can be dynamic too, update the code accordingly: settings:\n  model: llama3\n  average_iterations: 10\n  average_iterations: 10\n  assessment_rubric: Clarity of Instructions\n  number_of_questions: 7\nprompts:\n  system_prompt: |\n    You are Afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online training entrepreneurship training program. \n    You will see the following sub-questions for {{assessment_rubric}}\n    and will be asked to score them from \n    1: Is there an outline?\n    2: Is the outline comprehensive? \n    3: Is the activity written clearly with no jargon?\n    4: Does the activity have actionable directions?\n    5: Are there any discrepancies in the agreement between subjects and verbs? \n    6: Do pronouns have clear and unambiguous antecedents throughout the passage\n    7: Are sentences concise without run-on sentences within the activity?\n    Your answer will be a score from 0, being very poor, to 100, for excellent. \n    When I ask you for the question's score, you will return only the score for that question as an integer and no other information or text.\n  user_prompt: |\n    Afif, your job is to assess the following activity against the {{assessment_rubric}}'s sub-questions.\n    After I send the activity, I will request one of the 5 sub-questions. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\n    Activity follows in three brackets:[[[{{activity_content}}]]]\nassessment_questions:\n  1: Is there an outline?\n  2: Is the outline comprehensive? \n  3: Is the activity written clearly with no jargon?\n  4: Does the activity have actionable directions?\n  5: Are there any discrepancies in the agreement between subjects and verbs? \n  6: Do pronouns have clear and unambiguous antecedents throughout the passage\n  7: Are sentences concise without run-on sentences within the activity?",
      "created_at": "2024-06-19T05:13:01.015249Z",
      "sequence_order": 40,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-19T05:13:01.015249Z",
            "stop_timestamp": "2024-06-19T05:13:01.015249Z",
            "flags": null,
            "type": "text",
            "text": "update the code so that it can take in questions arbitrarily from the YAML file, it. now has 7 questions: - also i added the average_iterations so that we can set how many times it should loop to create averages. Update the code to use these variables dynamically YAML file: \nupdated the YAML so the number of questions can be dynamic too, update the code accordingly: settings:\n  model: llama3\n  average_iterations: 10\n  average_iterations: 10\n  assessment_rubric: Clarity of Instructions\n  number_of_questions: 7\nprompts:\n  system_prompt: |\n    You are Afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online training entrepreneurship training program. \n    You will see the following sub-questions for {{assessment_rubric}}\n    and will be asked to score them from \n    1: Is there an outline?\n    2: Is the outline comprehensive? \n    3: Is the activity written clearly with no jargon?\n    4: Does the activity have actionable directions?\n    5: Are there any discrepancies in the agreement between subjects and verbs? \n    6: Do pronouns have clear and unambiguous antecedents throughout the passage\n    7: Are sentences concise without run-on sentences within the activity?\n    Your answer will be a score from 0, being very poor, to 100, for excellent. \n    When I ask you for the question's score, you will return only the score for that question as an integer and no other information or text.\n  user_prompt: |\n    Afif, your job is to assess the following activity against the {{assessment_rubric}}'s sub-questions.\n    After I send the activity, I will request one of the 5 sub-questions. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\n    Activity follows in three brackets:[[[{{activity_content}}]]]\nassessment_questions:\n  1: Is there an outline?\n  2: Is the outline comprehensive? \n  3: Is the activity written clearly with no jargon?\n  4: Does the activity have actionable directions?\n  5: Are there any discrepancies in the agreement between subjects and verbs? \n  6: Do pronouns have clear and unambiguous antecedents throughout the passage\n  7: Are sentences concise without run-on sentences within the activity?",
            "citations": []
          }
        ],
        "updated_at": "2024-06-19T05:13:01.015249Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 7022,
            "file_type": "txt",
            "extracted_content": "import nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_rubric = config['assessment']['assessment_rubric']\nquestion_1 = config['assessment']['question_1']\nquestion_2 = config['assessment']['question_2']\nquestion_3 = config['assessment']['question_3']\nquestion_4 = config['assessment']['question_4']\nquestion_5 = config['assessment']['question_5']\n\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    question_1=question_1,\n    question_2=question_2,\n    question_3=question_3,\n    question_4=question_4,\n    question_5=question_5\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n#First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, 6)}\n    feedback = \"\"\n\n    questions = [\n        question_1,\n        question_2,\n        question_3,\n        question_4,\n        question_5\n    ]\n\n    for _ in range(10):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n\n            for i, question in enumerate(questions, start=1):\n                messages.append({'role': 'user', 'content': question})\n                logging.info(f\"Sending question to Ollama: {question}\")\n                response = client.chat(model=model, messages=messages)\n                logging.info(f\"Received response from Ollama:\\n{response}\")\n\n                if 'message' in response and 'content' in response['message']:\n                    question_score = response['message']['content']\n                    if question_score.isdigit():\n                        question_score = int(question_score)\n                        if 0 <= question_score <= 100:\n                            scores[f'question_{i}_score'].append(question_score)\n                        else:\n                            logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                            feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n                    else:\n                        logging.warning(f\"Question score {question_score} is not a valid integer.\")\n                        feedback += f\"Question {i} score is not a valid integer. \"\n                else:\n                    logging.error(f\"Invalid response from Ollama for question {i}: {response}\")\n                    feedback += f\"Invalid response from Ollama for question {i}. \"\n\n            messages.clear()  # Clear the message queue for the next iteration\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n            feedback += f\"Error during API call: {str(e)}. \"\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, 6):\n        score = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_score'] = score\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)"
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          }
        ]
      }
    },
    {
      "id": "34e98d7e-205c-43ec-a957-9e94d6cdb70a",
      "role": "assistant",
      "content": " Here's the updated code that takes into account the dynamic number of questions and the `average_iterations` variable from the YAML file:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\naverage_iterations = config['settings']['average_iterations']\nassessment_rubric = config['settings']['assessment_rubric']\nnumber_of_questions = config['settings']['number_of_questions']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_questions = config['assessment_questions']\n\n# Generate the system prompt with dynamic questions\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    questions='\\n'.join(f\"{i}: {question}\" for i, question in assessment_questions.items())\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, number_of_questions + 1)}\n    feedback = \"\"\n\n    questions = list(assessment_questions.values())\n\n    for _ in range(average_iterations):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n\n            for i, question in enumerate(questions, start=1):\n                messages.append({'role': 'user', 'content': question})\n                logging.info(f\"Sending question to Ollama: {question}\")\n                response = client.chat(model=model, messages=messages)\n                logging.info(f\"Received response from Ollama:\\n{response}\")\n\n                if 'message' in response and 'content' in response['message']:\n                    question_score = response['message']['content']\n                    if question_score.isdigit():\n                        question_score = int(question_score)\n                        if 0 <= question_score <= 100:\n                            scores[f'question_{i}_score'].append(question_score)\n                        else:\n                            logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                            feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n                    else:\n                        logging.warning(f\"Question score {question_score} is not a valid integer.\")\n                        feedback += f\"Question {i} score is not a valid integer. \"\n                else:\n                    logging.error(f\"Invalid response from Ollama for question {i}: {response}\")\n                    feedback += f\"Invalid response from Ollama for question {i}. \"\n\n            messages.clear()  # Clear the message queue for the next iteration\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n            feedback += f\"Error during API call: {str(e)}. \"\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, number_of_questions + 1):\n        score = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_score'] = score\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nThe main changes in the updated code are:\n\n1. The `number_of_questions` variable is loaded from the YAML file under the `settings` section. It determines the number of questions to be assessed.\n\n2. The `assessment_questions` dictionary is loaded from the YAML file, which contains the questions as key-value pairs.\n\n3. The system prompt is generated dynamically using the `system_prompt_template` and the `assessment_questions` dictionary. It iterates over the questions and formats them into the prompt.\n\n4. The `get_scores_for_activity` function now uses the `number_of_questions` variable to create the `scores` dictionary with the appropriate keys.\n\n5. The `questions` list is created by extracting the values from the `assessment_questions` dictionary.\n\n6. The loop for iterating over the questions in the `get_scores_for_activity` function now uses `range(1, number_of_questions + 1)` to accommodate the dynamic number of questions.\n\n7. When processing each activity and assigning scores to the DataFrame, the loop now uses `range(1, number_of_questions + 1)` to iterate over the dynamic number of questions.\n\nWith these updates, the code can handle a dynamic number of questions specified in the YAML file and use the `average_iterations` variable to control the number of times to loop for creating averages.\n\nPlease make sure to update your YAML file with the new structure and additional variables as shown in your example.",
      "created_at": "2024-06-19T05:13:01.015249Z",
      "sequence_order": 41,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-19T05:13:01.015249Z",
            "stop_timestamp": "2024-06-19T05:13:01.015249Z",
            "flags": null,
            "type": "text",
            "text": " Here's the updated code that takes into account the dynamic number of questions and the `average_iterations` variable from the YAML file:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\naverage_iterations = config['settings']['average_iterations']\nassessment_rubric = config['settings']['assessment_rubric']\nnumber_of_questions = config['settings']['number_of_questions']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_questions = config['assessment_questions']\n\n# Generate the system prompt with dynamic questions\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    questions='\\n'.join(f\"{i}: {question}\" for i, question in assessment_questions.items())\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, number_of_questions + 1)}\n    feedback = \"\"\n\n    questions = list(assessment_questions.values())\n\n    for _ in range(average_iterations):\n        try:\n            client = ollama.Client()  # Reset the client to clear history\n\n            user_prompt = user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric) \\\n                                              .replace('{{activity_content}}', description)\n\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_prompt}\n            ]\n\n            logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n            logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt}\")\n\n            for i, question in enumerate(questions, start=1):\n                messages.append({'role': 'user', 'content': question})\n                logging.info(f\"Sending question to Ollama: {question}\")\n                response = client.chat(model=model, messages=messages)\n                logging.info(f\"Received response from Ollama:\\n{response}\")\n\n                if 'message' in response and 'content' in response['message']:\n                    question_score = response['message']['content']\n                    if question_score.isdigit():\n                        question_score = int(question_score)\n                        if 0 <= question_score <= 100:\n                            scores[f'question_{i}_score'].append(question_score)\n                        else:\n                            logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                            feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n                    else:\n                        logging.warning(f\"Question score {question_score} is not a valid integer.\")\n                        feedback += f\"Question {i} score is not a valid integer. \"\n                else:\n                    logging.error(f\"Invalid response from Ollama for question {i}: {response}\")\n                    feedback += f\"Invalid response from Ollama for question {i}. \"\n\n            messages.clear()  # Clear the message queue for the next iteration\n        except Exception as e:\n            logging.error(f\"Error during API call: {str(e)}\")\n            feedback += f\"Error during API call: {str(e)}. \"\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, number_of_questions + 1):\n        score = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_score'] = score\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nThe main changes in the updated code are:\n\n1. The `number_of_questions` variable is loaded from the YAML file under the `settings` section. It determines the number of questions to be assessed.\n\n2. The `assessment_questions` dictionary is loaded from the YAML file, which contains the questions as key-value pairs.\n\n3. The system prompt is generated dynamically using the `system_prompt_template` and the `assessment_questions` dictionary. It iterates over the questions and formats them into the prompt.\n\n4. The `get_scores_for_activity` function now uses the `number_of_questions` variable to create the `scores` dictionary with the appropriate keys.\n\n5. The `questions` list is created by extracting the values from the `assessment_questions` dictionary.\n\n6. The loop for iterating over the questions in the `get_scores_for_activity` function now uses `range(1, number_of_questions + 1)` to accommodate the dynamic number of questions.\n\n7. When processing each activity and assigning scores to the DataFrame, the loop now uses `range(1, number_of_questions + 1)` to iterate over the dynamic number of questions.\n\nWith these updates, the code can handle a dynamic number of questions specified in the YAML file and use the `average_iterations` variable to control the number of times to loop for creating averages.\n\nPlease make sure to update your YAML file with the new structure and additional variables as shown in your example.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-19T05:13:01.015249Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "a5392895-b690-4db4-af59-99270d9f33a9",
      "role": "human",
      "content": "instead of resetting the client can we make a message history queue that we clear like they do in this example:\n\nimport ollama\nmessages = []\ndef send(chat):\n  messages.append(\n    {\n      'role': 'user',\n      'content': chat,\n    }\n  )\n  stream = ollama.chat(model='mistral:instruct', \n    messages=messages,\n    stream=True,\n  )\n  response = \"\"\n  for chunk in stream:\n    part = chunk['message']['content']\n    print(part, end='', flush=True)\n    response = response + part\n  messages.append(\n    {\n      'role': 'assistant',\n      'content': response,\n    }\n  )\n  print(\"\")\nwhile True:\n    chat = input(\">>> \")\n    if chat == \"/exit\":\n        break\n    elif len(chat) > 0:\n        send(chat)",
      "created_at": "2024-06-19T05:35:48.801268Z",
      "sequence_order": 42,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-19T05:35:48.801268Z",
            "stop_timestamp": "2024-06-19T05:35:48.801268Z",
            "flags": null,
            "type": "text",
            "text": "instead of resetting the client can we make a message history queue that we clear like they do in this example:\n\nimport ollama\nmessages = []\ndef send(chat):\n  messages.append(\n    {\n      'role': 'user',\n      'content': chat,\n    }\n  )\n  stream = ollama.chat(model='mistral:instruct', \n    messages=messages,\n    stream=True,\n  )\n  response = \"\"\n  for chunk in stream:\n    part = chunk['message']['content']\n    print(part, end='', flush=True)\n    response = response + part\n  messages.append(\n    {\n      'role': 'assistant',\n      'content': response,\n    }\n  )\n  print(\"\")\nwhile True:\n    chat = input(\">>> \")\n    if chat == \"/exit\":\n        break\n    elif len(chat) > 0:\n        send(chat)",
            "citations": []
          }
        ],
        "updated_at": "2024-06-19T05:35:48.801268Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "3e6ca105-7cd1-4589-9094-f53fb197e7cf",
      "role": "assistant",
      "content": " Certainly! You can modify the code to use a message history queue instead of resetting the client. Here's the updated code:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\naverage_iterations = config['settings']['average_iterations']\nassessment_rubric = config['settings']['assessment_rubric']\nnumber_of_questions = config['settings']['number_of_questions']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_questions = config['assessment_questions']\n\n# Generate the system prompt with dynamic questions\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    questions='\\n'.join(f\"{i}: {question}\" for i, question in assessment_questions.items())\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef send_to_ollama(messages):\n    response = \"\"\n    try:\n        stream = client.chat(model=model, messages=messages, stream=True)\n        for chunk in stream:\n            part = chunk['message']['content']\n            print(part, end='', flush=True)\n            response += part\n        print(\"\")\n    except Exception as e:\n        logging.error(f\"Error during API call: {str(e)}\")\n    messages.append({'role': 'assistant', 'content': response})\n    return response\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, number_of_questions + 1)}\n    feedback = \"\"\n\n    questions = list(assessment_questions.values())\n\n    for _ in range(average_iterations):\n        messages = [\n            {'role': 'system', 'content': system_prompt},\n            {'role': 'user', 'content': user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric).replace('{{activity_content}}', description)}\n        ]\n\n        logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n        logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric).replace('{{activity_content}}', description)}\")\n\n        for i, question in enumerate(questions, start=1):\n            messages.append({'role': 'user', 'content': question})\n            logging.info(f\"Sending question to Ollama: {question}\")\n            response = send_to_ollama(messages)\n            logging.info(f\"Received response from Ollama:\\n{response}\")\n\n            question_score = response.strip()\n            if question_score.isdigit():\n                question_score = int(question_score)\n                if 0 <= question_score <= 100:\n                    scores[f'question_{i}_score'].append(question_score)\n                else:\n                    logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                    feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n            else:\n                logging.warning(f\"Question score {question_score} is not a valid integer.\")\n                feedback += f\"Question {i} score is not a valid integer. \"\n\n        messages.clear()  # Clear the message history queue for the next iteration\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, number_of_questions + 1):\n        score = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_score'] = score\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nThe main changes in the code are:\n\n1. A new function `send_to_ollama` is introduced, which takes the `messages` list as input. This function sends the messages to the Ollama API, streams the response, and appends the assistant's response to the `messages` list. It returns the response as a string.\n\n2. In the `get_scores_for_activity` function, instead of resetting the client for each iteration, a message history queue `messages` is created. The system prompt and user prompt are added to the `messages` list.\n\n3. For each question, the question is appended to the `messages` list, and the `send_to_ollama` function is called with the `messages` list.\n\n4. The response from the Ollama API is processed and stored in the `scores` dictionary as before.\n\n5. After processing all questions for an iteration, the `messages` list is cleared using `messages.clear()` to start fresh for the next iteration.\n\nWith these modifications, the code now uses a message history queue instead of resetting the client for each iteration. The `send_to_ollama` function handles the communication with the Ollama API and updates the message history queue accordingly.\n\nPlease note that this code assumes the Ollama API supports streaming responses. If the API doesn't support streaming, you may need to modify the `send_to_ollama` function accordingly.",
      "created_at": "2024-06-19T05:35:48.801268Z",
      "sequence_order": 43,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-19T05:35:48.801268Z",
            "stop_timestamp": "2024-06-19T05:35:48.801268Z",
            "flags": null,
            "type": "text",
            "text": " Certainly! You can modify the code to use a message history queue instead of resetting the client. Here's the updated code:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\naverage_iterations = config['settings']['average_iterations']\nassessment_rubric = config['settings']['assessment_rubric']\nnumber_of_questions = config['settings']['number_of_questions']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_questions = config['assessment_questions']\n\n# Generate the system prompt with dynamic questions\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    questions='\\n'.join(f\"{i}: {question}\" for i, question in assessment_questions.items())\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef send_to_ollama(messages):\n    response = \"\"\n    try:\n        stream = client.chat(model=model, messages=messages, stream=True)\n        for chunk in stream:\n            part = chunk['message']['content']\n            print(part, end='', flush=True)\n            response += part\n        print(\"\")\n    except Exception as e:\n        logging.error(f\"Error during API call: {str(e)}\")\n    messages.append({'role': 'assistant', 'content': response})\n    return response\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, number_of_questions + 1)}\n    feedback = \"\"\n\n    questions = list(assessment_questions.values())\n\n    for _ in range(average_iterations):\n        messages = [\n            {'role': 'system', 'content': system_prompt},\n            {'role': 'user', 'content': user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric).replace('{{activity_content}}', description)}\n        ]\n\n        logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n        logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric).replace('{{activity_content}}', description)}\")\n\n        for i, question in enumerate(questions, start=1):\n            messages.append({'role': 'user', 'content': question})\n            logging.info(f\"Sending question to Ollama: {question}\")\n            response = send_to_ollama(messages)\n            logging.info(f\"Received response from Ollama:\\n{response}\")\n\n            question_score = response.strip()\n            if question_score.isdigit():\n                question_score = int(question_score)\n                if 0 <= question_score <= 100:\n                    scores[f'question_{i}_score'].append(question_score)\n                else:\n                    logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                    feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n            else:\n                logging.warning(f\"Question score {question_score} is not a valid integer.\")\n                feedback += f\"Question {i} score is not a valid integer. \"\n\n        messages.clear()  # Clear the message history queue for the next iteration\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, number_of_questions + 1):\n        score = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_score'] = score\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nThe main changes in the code are:\n\n1. A new function `send_to_ollama` is introduced, which takes the `messages` list as input. This function sends the messages to the Ollama API, streams the response, and appends the assistant's response to the `messages` list. It returns the response as a string.\n\n2. In the `get_scores_for_activity` function, instead of resetting the client for each iteration, a message history queue `messages` is created. The system prompt and user prompt are added to the `messages` list.\n\n3. For each question, the question is appended to the `messages` list, and the `send_to_ollama` function is called with the `messages` list.\n\n4. The response from the Ollama API is processed and stored in the `scores` dictionary as before.\n\n5. After processing all questions for an iteration, the `messages` list is cleared using `messages.clear()` to start fresh for the next iteration.\n\nWith these modifications, the code now uses a message history queue instead of resetting the client for each iteration. The `send_to_ollama` function handles the communication with the Ollama API and updates the message history queue accordingly.\n\nPlease note that this code assumes the Ollama API supports streaming responses. If the API doesn't support streaming, you may need to modify the `send_to_ollama` function accordingly.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-19T05:35:48.801268Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "33e125d8-69ef-41c7-be9c-dfa5f073024a",
      "role": "human",
      "content": "if we're importing the assessment_questions variable doesn't it have a length, do we need to set the number of questions too?",
      "created_at": "2024-06-19T05:45:49.641572Z",
      "sequence_order": 44,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-19T05:45:49.641572Z",
            "stop_timestamp": "2024-06-19T05:45:49.641572Z",
            "flags": null,
            "type": "text",
            "text": "if we're importing the assessment_questions variable doesn't it have a length, do we need to set the number of questions too?",
            "citations": []
          }
        ],
        "updated_at": "2024-06-19T05:45:49.641572Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "6568174f-8adf-433c-9753-a37159435148",
      "role": "assistant",
      "content": " You're correct. If the `assessment_questions` variable is imported from the YAML file and it contains all the questions, there's no need to separately specify the number of questions in the YAML file. The code can dynamically determine the number of questions based on the length of the `assessment_questions` dictionary.\n\nHere's the updated YAML file without the `number_of_questions` variable:\n\n```yaml\nsettings:\n  model: llama3\n  average_iterations: 10\n  assessment_rubric: Clarity of Instructions\nprompts:\n  system_prompt: |\n    You are Afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online training entrepreneurship training program. \n    You will see the following sub-questions for {{assessment_rubric}}\n    and will be asked to score them from \n    {{questions}}\n    Your answer will be a score from 0, being very poor, to 100, for excellent. \n    When I ask you for the question's score, you will return only the score for that question as an integer and no other information or text.\n  user_prompt: |\n    Afif, your job is to assess the following activity against the {{assessment_rubric}}'s sub-questions.\n    After I send the activity, I will request one of the sub-questions. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\n    Activity follows in three brackets:[[[{{activity_content}}]]]\nassessment_questions:\n  1: Is there an outline?\n  2: Is the outline comprehensive? \n  3: Is the activity written clearly with no jargon?\n  4: Does the activity have actionable directions?\n  5: Are there any discrepancies in the agreement between subjects and verbs? \n  6: Do pronouns have clear and unambiguous antecedents throughout the passage\n  7: Are sentences concise without run-on sentences within the activity?\n```\n\nAnd here's the updated code that dynamically determines the number of questions based on the length of the `assessment_questions` dictionary:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\naverage_iterations = config['settings']['average_iterations']\nassessment_rubric = config['settings']['assessment_rubric']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_questions = config['assessment_questions']\nnumber_of_questions = len(assessment_questions)\n\n# Generate the system prompt with dynamic questions\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    questions='\\n'.join(f\"{i}: {question}\" for i, question in assessment_questions.items())\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef send_to_ollama(messages):\n    response = \"\"\n    try:\n        stream = client.chat(model=model, messages=messages, stream=True)\n        for chunk in stream:\n            part = chunk['message']['content']\n            print(part, end='', flush=True)\n            response += part\n        print(\"\")\n    except Exception as e:\n        logging.error(f\"Error during API call: {str(e)}\")\n    messages.append({'role': 'assistant', 'content': response})\n    return response\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, number_of_questions + 1)}\n    feedback = \"\"\n\n    questions = list(assessment_questions.values())\n\n    for _ in range(average_iterations):\n        messages = [\n            {'role': 'system', 'content': system_prompt},\n            {'role': 'user', 'content': user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric).replace('{{activity_content}}', description)}\n        ]\n\n        logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n        logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric).replace('{{activity_content}}', description)}\")\n\n        for i, question in enumerate(questions, start=1):\n            messages.append({'role': 'user', 'content': question})\n            logging.info(f\"Sending question to Ollama: {question}\")\n            response = send_to_ollama(messages)\n            logging.info(f\"Received response from Ollama:\\n{response}\")\n\n            question_score = response.strip()\n            if question_score.isdigit():\n                question_score = int(question_score)\n                if 0 <= question_score <= 100:\n                    scores[f'question_{i}_score'].append(question_score)\n                else:\n                    logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                    feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n            else:\n                logging.warning(f\"Question score {question_score} is not a valid integer.\")\n                feedback += f\"Question {i} score is not a valid integer. \"\n\n        messages.clear()  # Clear the message history queue for the next iteration\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, number_of_questions + 1):\n        score = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_score'] = score\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nIn this updated code:\n\n- The `number_of_questions` variable is determined dynamically by calculating the length of the `assessment_questions` dictionary using `len(assessment_questions)`.\n\n- The YAML file no longer includes the `number_of_questions` variable since it's not needed.\n\nWith these changes, the code will automatically adapt to the number of questions defined in the `assessment_questions` dictionary in the YAML file, without the need to explicitly specify the number of questions separately.",
      "created_at": "2024-06-19T05:45:49.641572Z",
      "sequence_order": 45,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-06-19T05:45:49.641572Z",
            "stop_timestamp": "2024-06-19T05:45:49.641572Z",
            "flags": null,
            "type": "text",
            "text": " You're correct. If the `assessment_questions` variable is imported from the YAML file and it contains all the questions, there's no need to separately specify the number of questions in the YAML file. The code can dynamically determine the number of questions based on the length of the `assessment_questions` dictionary.\n\nHere's the updated YAML file without the `number_of_questions` variable:\n\n```yaml\nsettings:\n  model: llama3\n  average_iterations: 10\n  assessment_rubric: Clarity of Instructions\nprompts:\n  system_prompt: |\n    You are Afif, a monitoring and evaluation expert working to help assess the clarity of activities for an online training entrepreneurship training program. \n    You will see the following sub-questions for {{assessment_rubric}}\n    and will be asked to score them from \n    {{questions}}\n    Your answer will be a score from 0, being very poor, to 100, for excellent. \n    When I ask you for the question's score, you will return only the score for that question as an integer and no other information or text.\n  user_prompt: |\n    Afif, your job is to assess the following activity against the {{assessment_rubric}}'s sub-questions.\n    After I send the activity, I will request one of the sub-questions. Give me the score and only the score as an integer with no additional information or text. Are you ready? respond with \"yes, or no\" only.\n    Activity follows in three brackets:[[[{{activity_content}}]]]\nassessment_questions:\n  1: Is there an outline?\n  2: Is the outline comprehensive? \n  3: Is the activity written clearly with no jargon?\n  4: Does the activity have actionable directions?\n  5: Are there any discrepancies in the agreement between subjects and verbs? \n  6: Do pronouns have clear and unambiguous antecedents throughout the passage\n  7: Are sentences concise without run-on sentences within the activity?\n```\n\nAnd here's the updated code that dynamically determines the number of questions based on the length of the `assessment_questions` dictionary:\n\n```python\nimport nltk\nimport pandas as pd\nimport textstat\nimport re\nfrom IPython.display import Image, display\nfrom bs4 import BeautifulSoup\nimport ollama\nimport yaml\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load settings from YAML file\nwith open('settings.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nmodel = config['settings']['model']\naverage_iterations = config['settings']['average_iterations']\nassessment_rubric = config['settings']['assessment_rubric']\nsystem_prompt_template = config['prompts']['system_prompt']\nuser_prompt_template = config['prompts']['user_prompt']\nassessment_questions = config['assessment_questions']\nnumber_of_questions = len(assessment_questions)\n\n# Generate the system prompt with dynamic questions\nsystem_prompt = system_prompt_template.format(\n    assessment_rubric=assessment_rubric,\n    questions='\\n'.join(f\"{i}: {question}\" for i, question in assessment_questions.items())\n)\n\n# Initialize Ollama client\nclient = ollama.Client()\n\n# Load the data with additional parameters to handle errors\ntry:\n    data = pd.read_csv('./activitiesJune18.csv')\nexcept pd.errors.ParserError as e:\n    logging.error(f\"Error reading the CSV file: {e}\")\n    raise\n\n# First remove all weird HTML to have a clean environment\ndef remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text(separator='\\n')\n\nif 'fullDescription' in data.columns:\n    data['description_without_html'] = data['fullDescription'].apply(lambda x: remove_html(str(x)))\n\ndef calculate_readability(text):\n    return textstat.flesch_reading_ease(text)\n\ndef check_appropriate_readability(score):\n    return 1 if 40 <= score <= 65 else 0\n\ndef check_apply_knowledge(text):\n    return 1 if \"Apply your knowledge!\" in text else 0\n\ndef check_worksheet(html_content):\n    return 1 if \"<strong>Worksheet</strong>\" in html_content else 0\n\ndef calculate_character_count_and_extract_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    text = soup.get_text()\n    return len(text), text\n\ndef check_character_range(count):\n    return 1 if 60 <= count[0] <= 300 else 0\n\n# Preprocess the data\ndata.fillna('', inplace=True)\ndata['description_without_html'] = data['description_without_html'].apply(remove_html)\ndata['full_description'] = data.apply(\n    lambda x: f\"{x['title']} {x['categories']} {x['labels']} {x['description_without_html']}\", axis=1\n)\n\ndata['readability_score'] = data['full_description'].apply(calculate_readability)\ndata['appropriate_readability_bool'] = data['readability_score'].apply(check_appropriate_readability)\n\ndata[['characters_count_total', 'description_without_html']] = data['full_description'].apply(\n    lambda html_content: pd.Series(calculate_character_count_and_extract_text(html_content))\n)\n\ndata['characters_count_total'] = data['full_description'].apply(calculate_character_count_and_extract_text)\ndata['60_to_300_characters_bool'] = data['characters_count_total'].apply(check_character_range)\n\ndata['has_apply_knowledge_bool'] = data['full_description'].apply(check_apply_knowledge)\ndata['has_worksheet_bool'] = data['full_description'].apply(check_worksheet)\n\ndef send_to_ollama(messages):\n    response = \"\"\n    try:\n        stream = client.chat(model=model, messages=messages, stream=True)\n        for chunk in stream:\n            part = chunk['message']['content']\n            print(part, end='', flush=True)\n            response += part\n        print(\"\")\n    except Exception as e:\n        logging.error(f\"Error during API call: {str(e)}\")\n    messages.append({'role': 'assistant', 'content': response})\n    return response\n\ndef get_scores_for_activity(description):\n    scores = {f'question_{i}_score': [] for i in range(1, number_of_questions + 1)}\n    feedback = \"\"\n\n    questions = list(assessment_questions.values())\n\n    for _ in range(average_iterations):\n        messages = [\n            {'role': 'system', 'content': system_prompt},\n            {'role': 'user', 'content': user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric).replace('{{activity_content}}', description)}\n        ]\n\n        logging.info(f\"Sending system prompt to Ollama:\\n{system_prompt}\")\n        logging.info(f\"Sending user prompt to Ollama:\\n{user_prompt_template.replace('{{assessment_rubric}}', assessment_rubric).replace('{{activity_content}}', description)}\")\n\n        for i, question in enumerate(questions, start=1):\n            messages.append({'role': 'user', 'content': question})\n            logging.info(f\"Sending question to Ollama: {question}\")\n            response = send_to_ollama(messages)\n            logging.info(f\"Received response from Ollama:\\n{response}\")\n\n            question_score = response.strip()\n            if question_score.isdigit():\n                question_score = int(question_score)\n                if 0 <= question_score <= 100:\n                    scores[f'question_{i}_score'].append(question_score)\n                else:\n                    logging.warning(f\"Question score {question_score} is outside the valid range of 0-100.\")\n                    feedback += f\"Question {i} score is outside the valid range of 0-100. \"\n            else:\n                logging.warning(f\"Question score {question_score} is not a valid integer.\")\n                feedback += f\"Question {i} score is not a valid integer. \"\n\n        messages.clear()  # Clear the message history queue for the next iteration\n\n    averaged_scores = {key: (sum(val) / len(val)) if val else 0 for key, val in scores.items()}\n    differences = {key: (max(val) - min(val)) if val else 0 for key, val in scores.items()}\n    return averaged_scores, differences, feedback.strip()\n\n# Process each activity\nsnapshot_interval = 100\nfor index, row in data.iterrows():\n    description = row['full_description']\n    averaged_scores, differences, feedback = get_scores_for_activity(description)\n    logging.info(f\"Scores for row {index}: {averaged_scores}\")\n    logging.info(f\"Differences for row {index}: {differences}\")\n    data.at[index, 'feedback'] = feedback\n    for q in range(1, number_of_questions + 1):\n        score = averaged_scores.get(f'question_{q}_score', 0)\n        data.at[index, f'question_{q}_score'] = score\n        data.at[index, f'question_{q}_difference'] = differences.get(f'question_{q}_score', 0)\n\n    # Save snapshot after every 100 activities\n    if (index + 1) % snapshot_interval == 0:\n        snapshot_file = f\"./june18-{index + 1}.csv\"\n        data.to_csv(snapshot_file, index=False)\n        logging.info(f\"Snapshot saved to {snapshot_file}\")\n\n# Save the final DataFrame to a new CSV file\ndata.to_csv(\"./updated_activities_with_clarity_instruct.csv\", index=False)\n```\n\nIn this updated code:\n\n- The `number_of_questions` variable is determined dynamically by calculating the length of the `assessment_questions` dictionary using `len(assessment_questions)`.\n\n- The YAML file no longer includes the `number_of_questions` variable since it's not needed.\n\nWith these changes, the code will automatically adapt to the number of questions defined in the `assessment_questions` dictionary in the YAML file, without the need to explicitly specify the number of questions separately.",
            "citations": []
          }
        ],
        "updated_at": "2024-06-19T05:45:49.641572Z",
        "attachments": [],
        "files": []
      }
    }
  ]
}