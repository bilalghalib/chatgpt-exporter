{
  "id": "745feff0-d834-4e26-801b-41fd51fdbe19",
  "source": "claude",
  "title": "Knowledge Extraction Service Review",
  "created_at": "2025-06-12T15:42:58.852545Z",
  "updated_at": "2025-06-12T15:55:12.909000Z",
  "summary": "",
  "message_count": 4,
  "messages": [
    {
      "id": "22a98990-f588-4441-a198-ab9a942ec501",
      "role": "human",
      "content": "review and critique the extraction service with 5 pros and 5 cons and find ways to reduce duplication and increase cleanliness and improve quality of extraction - how are the knowledge graphs leveraging the ontology? what're some valuable things we can do with this code?",
      "created_at": "2025-06-12T15:43:06.147060Z",
      "sequence_order": 0,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2025-06-12T15:43:06.142705Z",
            "stop_timestamp": "2025-06-12T15:43:06.142705Z",
            "flags": null,
            "type": "text",
            "text": "review and critique the extraction service with 5 pros and 5 cons and find ways to reduce duplication and increase cleanliness and improve quality of extraction - how are the knowledge graphs leveraging the ontology? what're some valuable things we can do with this code?",
            "citations": []
          }
        ],
        "updated_at": "2025-06-12T15:43:06.147060Z",
        "attachments": [
          {
            "file_name": "service.py",
            "file_size": 17400,
            "file_type": "text/x-python-script",
            "extracted_content": "\"\"\"\nUnified embedding service for Knowledge Garden\n\nThis service provides a standardized interface for generating embeddings\nacross all Knowledge Garden services. It supports multiple embedding models,\ncaching, and batch operations.\n\"\"\"\nimport logging\nimport os\nimport time\nimport numpy as np\nimport hashlib\nimport json\nfrom typing import List, Dict, Any, Optional, Union, Tuple\nfrom functools import lru_cache\nfrom pathlib import Path\nimport yaml\n\nlogger = logging.getLogger(__name__)\n\n\nclass EmbeddingService:\n    \"\"\"Service for generating and managing embeddings.\"\"\"\n    \n    def __init__(\n        self,\n        config_path: Optional[Path] = None,\n        model_name: Optional[str] = None,\n        cache_dir: Optional[Union[str, Path]] = None,\n        use_cache: bool = True,\n        batch_size: int = 64,\n    ):\n        \"\"\"Initialize the embedding service.\n        \n        Args:\n            config_path: Path to configuration file\n            model_name: Name of the embedding model (overrides config)\n            cache_dir: Directory for caching embeddings\n            use_cache: Whether to use caching\n            batch_size: Batch size for embedding generation\n        \"\"\"\n        # Load configuration\n        self.config = self._load_config(config_path)\n        \n        # Set up model and caching parameters\n        self.model_name = model_name or self._get_config_model_name()\n        self.use_cache = use_cache\n        self.batch_size = batch_size\n        \n        # Set up cache directory\n        cache_dir = cache_dir or self._get_config_cache_dir()\n        if isinstance(cache_dir, str):\n            cache_dir = Path(cache_dir)\n        self.cache_dir = cache_dir\n        \n        if use_cache and cache_dir:\n            os.makedirs(cache_dir, exist_ok=True)\n        \n        # Load model on demand\n        self._model = None\n        self._dimension = None\n        self._available = self._can_load_model()\n        \n        logger.info(f\"Embedding service initialized with model {self.model_name} (available: {self._available})\")\n    \n    def _load_config(self, config_path: Optional[Path]) -> Dict[str, Any]:\n        \"\"\"Load configuration with environment variable substitution.\n        \n        Args:\n            config_path: Path to configuration file\n            \n        Returns:\n            Configuration dictionary\n        \"\"\"\n        if not config_path:\n            # Try standard config locations\n            for path in [\n                Path(\"config/settings.yaml\"),\n                Path(\"config/unified_config.yaml\"),\n                Path(\"config/services.yaml\"),\n            ]:\n                if path.exists():\n                    config_path = path\n                    break\n        \n        if not config_path or not Path(config_path).exists():\n            logger.warning(f\"Config file not found at {config_path}, using empty config\")\n            return {}\n        \n        try:\n            with open(config_path) as f:\n                config_text = f.read()\n                \n            # Simple env var substitution\n            import re\n            def replace_env(match):\n                var_name, default = match.groups()\n                return os.getenv(var_name, default)\n            \n            config_text = re.sub(r'\\${(\\w+):-([^}]+)}', replace_env, config_text)\n            config_text = re.sub(r'\\${(\\w+)}', lambda m: os.getenv(m.group(1), ''), config_text)\n            \n            return yaml.safe_load(config_text)\n        except Exception as e:\n            logger.error(f\"Error loading config: {e}\")\n            return {}\n    \n    def _get_config_model_name(self) -> str:\n        \"\"\"Get model name from config or environment.\n        \n        Returns:\n            Model name\n        \"\"\"\n        # Check various config locations\n        model = (\n            self.config.get(\"services\", {}).get(\"extraction\", {}).get(\"embedding_model\") or\n            self.config.get(\"embedding\", {}).get(\"model\") or\n            os.environ.get(\"EMBEDDING_MODEL\") or\n            \"sentence-transformers/all-MiniLM-L6-v2\"  # Default\n        )\n        return model\n    \n    def _get_config_cache_dir(self) -> Optional[Path]:\n        \"\"\"Get cache directory from config or environment.\n        \n        Returns:\n            Cache directory path\n        \"\"\"\n        # Check various config locations\n        cache_dir = (\n            self.config.get(\"embedding\", {}).get(\"cache_dir\") or\n            os.environ.get(\"EMBEDDING_CACHE_DIR\") or\n            \"data/embeddings\"  # Default\n        )\n        return Path(cache_dir)\n    \n    def _can_load_model(self) -> bool:\n        \"\"\"Check if the model can be loaded.\n        \n        Returns:\n            True if the model can be loaded\n        \"\"\"\n        try:\n            # Check if sentence-transformers is installed\n            import importlib\n            if importlib.util.find_spec(\"sentence_transformers\") is None:\n                logger.warning(\"sentence-transformers package not installed\")\n                return False\n            return True\n        except Exception as e:\n            logger.warning(f\"Error checking for sentence-transformers: {e}\")\n            return False\n    \n    def _get_model(self):\n        \"\"\"Load the model on demand.\n        \n        Returns:\n            Loaded model\n        \"\"\"\n        if not self._model:\n            try:\n                from sentence_transformers import SentenceTransformer\n                self._model = SentenceTransformer(self.model_name)\n                self._dimension = self._model.get_sentence_embedding_dimension()\n                logger.info(f\"Loaded embedding model: {self.model_name} (dimension: {self._dimension})\")\n            except Exception as e:\n                logger.error(f\"Failed to load embedding model: {e}\")\n                raise\n        return self._model\n    \n    @property\n    def dimension(self) -> int:\n        \"\"\"Get the embedding dimension.\n        \n        Returns:\n            Embedding dimension\n        \"\"\"\n        if self._dimension is None:\n            if self._available:\n                # Load model to get dimension\n                self._get_model()\n            else:\n                # Default dimension\n                self._dimension = 384\n        return self._dimension\n    \n    @property\n    def available(self) -> bool:\n        \"\"\"Check if the embedding service is available.\n        \n        Returns:\n            True if available\n        \"\"\"\n        return self._available\n    \n    def _cache_key(self, text: str) -> str:\n        \"\"\"Generate a cache key for the text.\n        \n        Args:\n            text: Text to generate a cache key for\n            \n        Returns:\n            Cache key\n        \"\"\"\n        text_hash = hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n        model_hash = hashlib.md5(self.model_name.encode(\"utf-8\")).hexdigest()[:8]\n        return f\"{model_hash}_{text_hash}\"\n    \n    def _cache_path(self, key: str) -> Path:\n        \"\"\"Get the cache path for a key.\n        \n        Args:\n            key: Cache key\n            \n        Returns:\n            Cache path\n        \"\"\"\n        return Path(self.cache_dir) / f\"{key}.json\"\n    \n    def _load_from_cache(self, key: str) -> Optional[np.ndarray]:\n        \"\"\"Load embeddings from cache.\n        \n        Args:\n            key: Cache key\n            \n        Returns:\n            Loaded embedding or None if not found\n        \"\"\"\n        if not self.use_cache or not self.cache_dir:\n            return None\n        \n        cache_path = self._cache_path(key)\n        if cache_path.exists():\n            try:\n                with open(cache_path, \"r\") as f:\n                    data = json.load(f)\n                return np.array(data[\"embedding\"])\n            except Exception as e:\n                logger.warning(f\"Failed to load from cache: {e}\")\n        return None\n    \n    def _save_to_cache(self, key: str, embedding: np.ndarray):\n        \"\"\"Save embeddings to cache.\n        \n        Args:\n            key: Cache key\n            embedding: Embedding to save\n        \"\"\"\n        if not self.use_cache or not self.cache_dir:\n            return\n        \n        try:\n            cache_path = self._cache_path(key)\n            with open(cache_path, \"w\") as f:\n                json.dump({\n                    \"model\": self.model_name,\n                    \"dimension\": self.dimension,\n                    \"embedding\": embedding.tolist(),\n                    \"timestamp\": time.time(),\n                }, f)\n        except Exception as e:\n            logger.warning(f\"Failed to save to cache: {e}\")\n    \n    def generate_fallback(self, text: str) -> np.ndarray:\n        \"\"\"Generate a fallback embedding when the model is unavailable.\n        \n        Args:\n            text: Text to generate an embedding for\n            \n        Returns:\n            Random embedding with the correct dimension\n        \"\"\"\n        # Use a hash-based pseudo-random but deterministic embedding\n        hash_obj = hashlib.sha256(text.encode())\n        hash_bytes = hash_obj.digest()\n        \n        # Convert hash bytes to a seed for numpy\n        seed = int.from_bytes(hash_bytes[:8], byteorder=\"big\")\n        rng = np.random.RandomState(seed)\n        \n        # Generate a random embedding and normalize it\n        embedding = rng.normal(size=self.dimension)\n        return embedding / np.linalg.norm(embedding)\n    \n    def generate(self, text: str) -> np.ndarray:\n        \"\"\"Generate an embedding for text.\n        \n        Args:\n            text: Text to generate an embedding for\n            \n        Returns:\n            Embedding vector\n        \"\"\"\n        if not text.strip():\n            # Return zeros for empty text\n            return np.zeros(self.dimension)\n        \n        # Check cache first\n        if self.use_cache:\n            key = self._cache_key(text)\n            cached = self._load_from_cache(key)\n            if cached is not None:\n                logger.debug(\"Using cached embedding\")\n                return cached\n        \n        # Generate embedding\n        if not self._available:\n            logger.warning(\"Embedding model not available, using fallback\")\n            embedding = self.generate_fallback(text)\n        else:\n            try:\n                start_time = time.time()\n                model = self._get_model()\n                embedding = model.encode(text)\n                duration = time.time() - start_time\n                logger.debug(f\"Generated embedding in {duration:.2f}s\")\n            except Exception as e:\n                logger.error(f\"Failed to generate embedding: {e}\")\n                embedding = self.generate_fallback(text)\n        \n        # Cache the result\n        if self.use_cache:\n            self._save_to_cache(self._cache_key(text), embedding)\n        \n        return embedding\n    \n    def batch_generate(self, texts: List[str]) -> np.ndarray:\n        \"\"\"Generate embeddings for multiple texts.\n        \n        Args:\n            texts: Texts to generate embeddings for\n            \n        Returns:\n            Array of embeddings\n        \"\"\"\n        # Check which texts need embedding generation\n        to_generate = []\n        cached_results = {}\n        indices = []\n        \n        # Check cache first if enabled\n        if self.use_cache:\n            for i, text in enumerate(texts):\n                if not text.strip():\n                    # Skip empty text (will be zeros)\n                    cached_results[i] = np.zeros(self.dimension)\n                    continue\n                \n                key = self._cache_key(text)\n                cached = self._load_from_cache(key)\n                if cached is not None:\n                    cached_results[i] = cached\n                else:\n                    to_generate.append(text)\n                    indices.append(i)\n        else:\n            to_generate = texts\n            indices = list(range(len(texts)))\n        \n        # Generate new embeddings\n        new_embeddings = []\n        if to_generate:\n            if not self._available:\n                logger.warning(\"Embedding model not available, using fallback\")\n                new_embeddings = [self.generate_fallback(text) for text in to_generate]\n            else:\n                try:\n                    start_time = time.time()\n                    model = self._get_model()\n                    new_embeddings = model.encode(\n                        to_generate,\n                        batch_size=self.batch_size,\n                        show_progress_bar=False,\n                        convert_to_numpy=True\n                    )\n                    duration = time.time() - start_time\n                    logger.debug(f\"Generated {len(to_generate)} embeddings in {duration:.2f}s\")\n                except Exception as e:\n                    logger.error(f\"Failed to generate batch embeddings: {e}\")\n                    new_embeddings = [self.generate_fallback(text) for text in to_generate]\n            \n            # Cache the results\n            if self.use_cache:\n                for i, text in enumerate(to_generate):\n                    key = self._cache_key(text)\n                    self._save_to_cache(key, new_embeddings[i])\n        \n        # Combine cached and new embeddings\n        result = np.zeros((len(texts), self.dimension))\n        for i, embedding in cached_results.items():\n            result[i] = embedding\n        \n        for i, embedding_idx in enumerate(indices):\n            if i < len(new_embeddings):\n                result[embedding_idx] = new_embeddings[i]\n        \n        return result\n    \n    def similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n        \"\"\"Calculate cosine similarity between embeddings.\n        \n        Args:\n            embedding1: First embedding\n            embedding2: Second embedding\n            \n        Returns:\n            Cosine similarity (0-1)\n        \"\"\"\n        # Ensure embeddings are normalized\n        norm1 = np.linalg.norm(embedding1)\n        norm2 = np.linalg.norm(embedding2)\n        \n        if norm1 == 0 or norm2 == 0:\n            return 0.0\n            \n        return np.dot(embedding1, embedding2) / (norm1 * norm2)\n    \n    def cosine_similarities(self, query_embedding: np.ndarray, embeddings: np.ndarray) -> np.ndarray:\n        \"\"\"Calculate cosine similarities between a query and multiple embeddings.\n        \n        Args:\n            query_embedding: Query embedding\n            embeddings: Array of embeddings to compare against\n            \n        Returns:\n            Array of cosine similarities\n        \"\"\"\n        # Normalize query embedding\n        query_norm = np.linalg.norm(query_embedding)\n        if query_norm > 0:\n            query_embedding = query_embedding / query_norm\n            \n        # Calculate dot products\n        dots = np.dot(embeddings, query_embedding)\n        \n        # Calculate norms for embeddings\n        norms = np.linalg.norm(embeddings, axis=1)\n        \n        # Avoid division by zero\n        norms[norms == 0] = 1\n        \n        # Calculate similarities\n        similarities = dots / norms\n        \n        return similarities\n    \n    def search(\n        self, \n        query: str, \n        texts: List[str], \n        top_k: Optional[int] = None\n    ) -> List[Tuple[int, float, str]]:\n        \"\"\"Search for most similar texts to a query.\n        \n        Args:\n            query: Query text\n            texts: Texts to search\n            top_k: Number of results to return (None for all)\n            \n        Returns:\n            List of (index, similarity, text) tuples\n        \"\"\"\n        # Generate embeddings\n        query_embedding = self.generate(query)\n        embeddings = self.batch_generate(texts)\n        \n        # Calculate similarities\n        similarities = self.cosine_similarities(query_embedding, embeddings)\n        \n        # Create result tuples (index, similarity, text)\n        results = [(i, float(sim), texts[i]) for i, sim in enumerate(similarities)]\n        \n        # Sort by similarity (descending)\n        results.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top-k results\n        if top_k is not None:\n            results = results[:top_k]\n            \n        return results\n    \n    async def health_check(self) -> Dict[str, Any]:\n        \"\"\"Check if the embedding service is healthy.\n        \n        Returns:\n            Health information\n        \"\"\"\n        return {\n            \"status\": \"healthy\" if self._available else \"degraded\",\n            \"model\": self.model_name,\n            \"dimension\": self.dimension,\n            \"cache_enabled\": self.use_cache,\n            \"cache_dir\": str(self.cache_dir) if self.cache_dir else None,\n            \"fallback_available\": True,\n        }\n\n\n@lru_cache\ndef get_embedding_service(\n    model_name: Optional[str] = None,\n    cache_dir: Optional[str] = None,\n    config_path: Optional[str] = None,\n) -> EmbeddingService:\n    \"\"\"Get a cached embedding service instance.\n    \n    Args:\n        model_name: Name of the embedding model\n        cache_dir: Directory for caching embeddings\n        config_path: Path to configuration file\n        \n    Returns:\n        Embedding service instance\n    \"\"\"\n    _config_path = Path(config_path) if config_path else None\n    \n    return EmbeddingService(\n        config_path=_config_path,\n        model_name=model_name,\n        cache_dir=cache_dir if cache_dir else None,\n    )"
          },
          {
            "file_name": "llm_service.py",
            "file_size": 689,
            "file_type": "text/x-python-script",
            "extracted_content": "\"\"\"\nRe-export of the unified LLM service for backward compatibility.\n\"\"\"\nimport logging\nimport warnings\nfrom shared.services.llm.service import (\n    LLMService, \n    get_llm_service,\n    OllamaProvider, \n    AnthropicProvider, \n    TaskSize\n)\n\n# Display warning\nwarnings.warn(\n    \"Importing from shared.services.llm_service is deprecated. \"\n    \"Import from shared.services.llm.service instead.\",\n    DeprecationWarning,\n    stacklevel=2,\n)\n\nlogger = logging.getLogger(__name__)\nlogger.info(\"Using unified LLM service from shared.services.llm.service\")\n\n# Re-export for backward compatibility\n__all__ = [\"LLMService\", \"get_llm_service\", \"OllamaProvider\", \"AnthropicProvider\", \"TaskSize\"]"
          },
          {
            "file_name": "service.py",
            "file_size": 26717,
            "file_type": "text/x-python-script",
            "extracted_content": "\"\"\"\nUnified LLM service for Knowledge Garden\n\nThis service provides a unified interface for all LLM operations across the Knowledge Garden system.\nIt supports multiple providers (Anthropic, Ollama) and includes smart routing, fallbacks, retries,\nand caching capabilities.\n\"\"\"\nimport logging\nimport os\nimport json\nimport time\nimport httpx\nimport yaml\nfrom typing import Dict, List, Any, Optional, Union, Callable, Literal\nfrom enum import Enum\nfrom functools import lru_cache\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\n\nclass TaskSize(Enum):\n    \"\"\"Task size for routing to appropriate model.\"\"\"\n    SMALL = \"small\"\n    MEDIUM = \"medium\"\n    LARGE = \"large\"\n\n\nclass LLMService:\n    \"\"\"Unified LLM service with multi-provider support and smart routing.\"\"\"\n    \n    def __init__(\n        self,\n        config_path: Optional[Path] = None,\n        anthropic_api_key: Optional[str] = None,\n        default_model: str = \"claude-3-haiku-20240307\",\n        timeout: float = 60.0,\n        max_retries: int = 3,\n        fallback_handler: Optional[Callable] = None,\n        cache_enabled: bool = True,\n    ):\n        \"\"\"Initialize the LLM service.\n        \n        Args:\n            config_path: Path to configuration file\n            anthropic_api_key: API key for Anthropic (overrides config)\n            default_model: Default model to use\n            timeout: Timeout for requests in seconds\n            max_retries: Maximum number of retries for failed requests\n            fallback_handler: Function to call if all LLM attempts fail\n            cache_enabled: Whether to enable response caching\n        \"\"\"\n        # Load configuration\n        self.config = self._load_config(config_path)\n        \n        # Set up basic parameters\n        self.anthropic_api_key = anthropic_api_key or os.environ.get(\"ANTHROPIC_API_KEY\") or self.config.get(\"llm\", {}).get(\"providers\", {}).get(\"anthropic\", {}).get(\"api_key\")\n        self.default_model = default_model\n        self.timeout = timeout\n        self.max_retries = max_retries\n        self.fallback_handler = fallback_handler\n        self.cache_enabled = cache_enabled\n        \n        # Set up HTTP client\n        self.client = httpx.AsyncClient(timeout=timeout)\n        \n        # Initialize providers\n        self.providers = {}\n        self._initialize_providers()\n        \n        # Cache for responses\n        self._response_cache = {}\n        \n        logger.info(f\"Initialized LLM service with {len(self.providers)} providers\")\n    \n    def _load_config(self, config_path: Optional[Path]) -> Dict[str, Any]:\n        \"\"\"Load configuration with environment variable substitution.\n        \n        Args:\n            config_path: Path to configuration file\n            \n        Returns:\n            Configuration dictionary\n        \"\"\"\n        if not config_path:\n            # Try standard config locations\n            for path in [\n                Path(\"config/settings.yaml\"),\n                Path(\"config/unified_config.yaml\"),\n                Path(\"config/services.yaml\"),\n            ]:\n                if path.exists():\n                    config_path = path\n                    break\n        \n        if not config_path or not Path(config_path).exists():\n            logger.warning(f\"Config file not found at {config_path}, using empty config\")\n            return {}\n        \n        try:\n            with open(config_path) as f:\n                config_text = f.read()\n                \n            # Simple env var substitution\n            import re\n            def replace_env(match):\n                var_name, default = match.groups()\n                return os.getenv(var_name, default)\n            \n            config_text = re.sub(r'\\${(\\w+):-([^}]+)}', replace_env, config_text)\n            config_text = re.sub(r'\\${(\\w+)}', lambda m: os.getenv(m.group(1), ''), config_text)\n            \n            return yaml.safe_load(config_text)\n        except Exception as e:\n            logger.error(f\"Error loading config: {e}\")\n            return {}\n    \n    def _initialize_providers(self):\n        \"\"\"Initialize enabled providers.\"\"\"\n        llm_config = self.config.get(\"llm\", {})\n        \n        # Initialize Ollama if enabled\n        ollama_config = llm_config.get(\"providers\", {}).get(\"ollama\", {})\n        if ollama_config.get(\"enabled\", \"false\").lower() == \"true\":\n            try:\n                self.providers[\"ollama\"] = OllamaProvider(\n                    ollama_config.get(\"url\", \"http://localhost:11434\"),\n                    ollama_config.get(\"models\", {\"small\": \"mistral\", \"medium\": \"mixtral\"})\n                )\n                logger.info(\"Initialized Ollama provider\")\n            except Exception as e:\n                logger.error(f\"Failed to initialize Ollama provider: {e}\")\n        \n        # Initialize Anthropic if enabled\n        anthropic_config = llm_config.get(\"providers\", {}).get(\"anthropic\", {})\n        if (anthropic_config.get(\"enabled\", \"true\").lower() == \"true\" and \n            (self.anthropic_api_key or anthropic_config.get(\"api_key\"))):\n            try:\n                api_key = self.anthropic_api_key or anthropic_config.get(\"api_key\")\n                models = anthropic_config.get(\"models\", {\n                    \"small\": \"claude-3-haiku-20240307\",\n                    \"medium\": \"claude-3-sonnet-20240229\",\n                    \"large\": \"claude-3-opus-20240229\"\n                })\n                self.providers[\"anthropic\"] = AnthropicProvider(api_key, models)\n                logger.info(\"Initialized Anthropic provider\")\n            except Exception as e:\n                logger.error(f\"Failed to initialize Anthropic provider: {e}\")\n    \n    async def close(self):\n        \"\"\"Close the HTTP client.\"\"\"\n        await self.client.aclose()\n    \n    @property\n    def available(self) -> bool:\n        \"\"\"Check if any LLM provider is available.\"\"\"\n        return len(self.providers) > 0\n    \n    def _determine_task_size(self, task_type: str) -> TaskSize:\n        \"\"\"Determine task size based on task type.\n        \n        Args:\n            task_type: Type of task\n            \n        Returns:\n            Task size enum\n        \"\"\"\n        routing = self.config.get(\"llm\", {}).get(\"routing\", {})\n        \n        if task_type in routing.get(\"small_tasks\", []):\n            return TaskSize.SMALL\n        elif task_type in routing.get(\"medium_tasks\", []):\n            return TaskSize.MEDIUM\n        elif task_type in routing.get(\"large_tasks\", []):\n            return TaskSize.LARGE\n        else:\n            # Default based on heuristics\n            if any(word in task_type.lower() for word in [\"extract\", \"match\", \"simple\", \"validate\"]):\n                return TaskSize.SMALL\n            elif any(word in task_type.lower() for word in [\"summary\", \"analysis\", \"synthesis\", \"generate\"]):\n                return TaskSize.LARGE\n            else:\n                return TaskSize.MEDIUM\n    \n    def _select_provider_model(self, task_size: TaskSize) -> tuple:\n        \"\"\"Select best available provider and model for task size.\n        \n        Args:\n            task_size: Size of the task\n            \n        Returns:\n            (provider, model) tuple\n        \"\"\"\n        # Prefer Ollama for small tasks if available\n        if task_size == TaskSize.SMALL and \"ollama\" in self.providers:\n            provider = self.providers[\"ollama\"]\n            model = provider.models.get(\"small\", \"mistral\")\n            return provider, model\n        \n        # Use Anthropic for medium/large tasks if available\n        if \"anthropic\" in self.providers:\n            provider = self.providers[\"anthropic\"]\n            model = provider.models.get(task_size.value, \"claude-3-haiku-20240307\")\n            return provider, model\n        \n        # Fallback to any available provider\n        if self.providers:\n            provider_name = list(self.providers.keys())[0]\n            provider = self.providers[provider_name]\n            model = provider.models.get(task_size.value, list(provider.models.values())[0])\n            return provider, model\n        \n        return None, None\n    \n    def _cache_key(self, prompt: str, system_prompt: Optional[str], model: str, temperature: float) -> str:\n        \"\"\"Generate a cache key for the request.\n        \n        Args:\n            prompt: User prompt\n            system_prompt: System prompt\n            model: Model name\n            temperature: Temperature setting\n            \n        Returns:\n            Cache key\n        \"\"\"\n        import hashlib\n        data = f\"{prompt}|{system_prompt or ''}|{model}|{temperature}\"\n        return hashlib.md5(data.encode(\"utf-8\")).hexdigest()\n    \n    async def generate(\n        self, \n        prompt: str,\n        system_prompt: Optional[str] = None,\n        model: Optional[str] = None,\n        task_type: Optional[str] = None,\n        force_provider: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n        retries: Optional[int] = None,\n        use_cache: Optional[bool] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Generate text using the LLM.\n        \n        Args:\n            prompt: User prompt\n            system_prompt: Optional system prompt\n            model: Model to use (defaults to self.default_model)\n            task_type: Type of task (for routing)\n            force_provider: Force specific provider (ollama/anthropic)\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens to generate\n            retries: Number of retries (defaults to self.max_retries)\n            use_cache: Override cache setting\n            \n        Returns:\n            Response from the LLM\n            \n        Raises:\n            ValueError: If no LLM provider is available\n        \"\"\"\n        # Check if caching is enabled and response is in cache\n        use_cache = self.cache_enabled if use_cache is None else use_cache\n        if use_cache:\n            cache_key = self._cache_key(prompt, system_prompt, model or self.default_model, temperature)\n            if cache_key in self._response_cache:\n                logger.info(\"Using cached LLM response\")\n                return self._response_cache[cache_key]\n        \n        # Determine provider and model\n        if force_provider and force_provider in self.providers:\n            provider = self.providers[force_provider]\n            _model = model or list(provider.models.values())[0]\n        else:\n            # Smart routing\n            task_size = self._determine_task_size(task_type or \"general\")\n            provider, _model = self._select_provider_model(task_size)\n            \n            # Override model if specified\n            if model:\n                _model = model\n        \n        if not provider:\n            if self.fallback_handler:\n                logger.warning(\"No LLM provider available, using fallback handler\")\n                return await self.fallback_handler(prompt)\n            return self._mock_response(prompt)\n        \n        # Set retries\n        _retries = retries if retries is not None else self.max_retries\n        \n        # Try the request with retries\n        for attempt in range(_retries + 1):\n            try:\n                start_time = time.time()\n                \n                # Generate response\n                result = await provider.generate(\n                    prompt=prompt,\n                    system_prompt=system_prompt,\n                    model=_model,\n                    temperature=temperature,\n                    max_tokens=max_tokens,\n                )\n                \n                duration = time.time() - start_time\n                logger.info(f\"LLM request successful in {duration:.2f}s\")\n                \n                # Add metadata\n                result.update({\n                    \"duration\": duration,\n                    \"provider\": provider.__class__.__name__,\n                    \"model\": _model,\n                })\n                \n                # Cache the result\n                if use_cache:\n                    self._response_cache[cache_key] = result\n                \n                return result\n                \n            except Exception as e:\n                logger.error(f\"LLM request failed (attempt {attempt+1}/{_retries+1}): {e}\")\n                if attempt == _retries:\n                    if self.fallback_handler:\n                        logger.warning(\"All LLM attempts failed, using fallback handler\")\n                        return await self.fallback_handler(prompt)\n                    return self._mock_response(prompt)\n                \n                # Exponential backoff\n                await asyncio.sleep(2 ** attempt)\n    \n    def _mock_response(self, prompt: str) -> Dict[str, Any]:\n        \"\"\"Generate a mock response when no providers are available.\n        \n        Args:\n            prompt: Input prompt\n            \n        Returns:\n            Mock response\n        \"\"\"\n        logger.warning(\"No LLM providers available, using mock response\")\n        \n        # Check if prompt is asking for JSON\n        if \"JSON\" in prompt or \"json\" in prompt:\n            if \"presentation\" in prompt.lower():\n                text = self._mock_presentation_json()\n            elif \"diagram\" in prompt.lower():\n                text = self._mock_diagram_json()\n            else:\n                text = self._mock_generic_json()\n        else:\n            text = f\"This is a mock response from the LLM service. In production, this would be replaced with actual LLM-generated content based on your prompt: {prompt[:100]}...\"\n        \n        return {\n            \"text\": text,\n            \"model\": \"mock\",\n            \"provider\": \"mock\",\n            \"duration\": 0.0,\n            \"input_tokens\": 0,\n            \"output_tokens\": 0,\n        }\n    \n    def _mock_presentation_json(self) -> str:\n        \"\"\"Generate mock presentation JSON\"\"\"\n        presentation = {\n            \"title\": \"Data Analysis Workflow\",\n            \"slides\": [\n                {\n                    \"title\": \"Introduction\",\n                    \"content\": \"- Overview of the data analysis process\\n- Key stakeholders and responsibilities\\n- Timeline and deliverables\",\n                    \"visual_type\": \"diagram\",\n                    \"visual_description\": \"High-level overview of the data analysis workflow\",\n                    \"notes\": \"Introduce the purpose of the workflow and its importance to the organization.\"\n                },\n                {\n                    \"title\": \"Data Collection and Preparation\",\n                    \"content\": \"- Requirements gathering\\n- Data sourcing strategies\\n- Cleaning and validation techniques\",\n                    \"visual_type\": \"diagram\",\n                    \"visual_description\": \"Data preparation pipeline\",\n                    \"notes\": \"Emphasize the importance of quality data as the foundation of analysis.\"\n                },\n                {\n                    \"title\": \"Analysis and Modeling\",\n                    \"content\": \"- Exploratory data analysis\\n- Model selection and development\\n- Validation and testing\",\n                    \"visual_type\": \"chart\",\n                    \"visual_description\": \"Comparison of model performance metrics\",\n                    \"notes\": \"Discuss how different modeling approaches might be appropriate for different scenarios.\"\n                },\n                {\n                    \"title\": \"Reporting and Communication\",\n                    \"content\": \"- Visualization best practices\\n- Report generation\\n- Stakeholder presentations\",\n                    \"visual_type\": \"image\",\n                    \"visual_description\": \"Example of an effective data visualization\",\n                    \"notes\": \"Remind the audience that communication is critical for turning insights into action.\"\n                },\n                {\n                    \"title\": \"Implementation and Feedback\",\n                    \"content\": \"- Operationalizing insights\\n- Monitoring performance\\n- Gathering feedback for improvement\",\n                    \"visual_type\": \"diagram\",\n                    \"visual_description\": \"Feedback loop diagram\",\n                    \"notes\": \"Emphasize that this is an iterative process that improves over time.\"\n                }\n            ]\n        }\n        return json.dumps(presentation, indent=2)\n    \n    def _mock_diagram_json(self) -> str:\n        \"\"\"Generate mock diagram JSON\"\"\"\n        diagram = {\n            \"type\": \"flowchart\",\n            \"direction\": \"TD\",\n            \"nodes\": [\n                {\"id\": \"A\", \"label\": \"Start\", \"shape\": \"rounded\"},\n                {\"id\": \"B\", \"label\": \"Process Data\", \"shape\": \"box\"},\n                {\"id\": \"C\", \"label\": \"Decision Point\", \"shape\": \"diamond\"},\n                {\"id\": \"D\", \"label\": \"Action 1\", \"shape\": \"box\"},\n                {\"id\": \"E\", \"label\": \"Action 2\", \"shape\": \"box\"},\n                {\"id\": \"F\", \"label\": \"End\", \"shape\": \"rounded\"}\n            ],\n            \"edges\": [\n                {\"from\": \"A\", \"to\": \"B\"},\n                {\"from\": \"B\", \"to\": \"C\"},\n                {\"from\": \"C\", \"to\": \"D\", \"label\": \"Yes\"},\n                {\"from\": \"C\", \"to\": \"E\", \"label\": \"No\"},\n                {\"from\": \"D\", \"to\": \"F\"},\n                {\"from\": \"E\", \"to\": \"F\"}\n            ]\n        }\n        return json.dumps(diagram, indent=2)\n    \n    def _mock_generic_json(self) -> str:\n        \"\"\"Generate mock generic JSON\"\"\"\n        data = {\n            \"result\": \"success\",\n            \"message\": \"This is a mock JSON response\",\n            \"data\": {\n                \"item1\": \"value1\",\n                \"item2\": \"value2\",\n                \"item3\": [\"a\", \"b\", \"c\"]\n            }\n        }\n        return json.dumps(data, indent=2)\n    \n    async def extract_structured_data(\n        self,\n        content: str,\n        data_schema: Dict[str, Any],\n        instructions: str,\n        model: Optional[str] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Extract structured data from text using the LLM.\n        \n        Args:\n            content: Text to extract from\n            data_schema: JSON schema for the expected output\n            instructions: Instructions for the extraction\n            model: Model to use (defaults to self.default_model)\n            \n        Returns:\n            Structured data extracted from the text\n        \"\"\"\n        system_prompt = \"\"\"\n        You are a precise data extraction assistant. Extract structured information from the text according to the instructions and schema.\n        Your response should be VALID JSON matching the provided schema. Do not include any explanations or text outside of the JSON.\n        \"\"\"\n        \n        schema_str = json.dumps(data_schema, indent=2)\n        prompt = f\"\"\"\n        # Instructions\n        {instructions}\n        \n        # JSON Schema to Match\n        ```json\n        {schema_str}\n        ```\n        \n        # Text to Process\n        ```\n        {content}\n        ```\n        \n        Return a valid JSON object matching the schema.\n        \"\"\"\n        \n        result = await self.generate(\n            prompt=prompt,\n            system_prompt=system_prompt,\n            model=model,\n            task_type=\"structured_extraction\",\n            temperature=0.2,  # Low temperature for more deterministic output\n            max_tokens=2000,\n        )\n        \n        try:\n            # Extract JSON from the response\n            text = result[\"text\"]\n            # Remove any markdown code blocks if present\n            if \"```json\" in text:\n                text = text.split(\"```json\")[1].split(\"```\")[0].strip()\n            elif \"```\" in text:\n                text = text.split(\"```\")[1].split(\"```\")[0].strip()\n            \n            return json.loads(text)\n        except Exception as e:\n            logger.error(f\"Failed to parse structured data: {e}\")\n            logger.debug(f\"Raw response: {result['text']}\")\n            raise ValueError(f\"Failed to parse structured data from LLM response: {e}\")\n    \n    async def health_check(self) -> Dict[str, Any]:\n        \"\"\"Check the health of all LLM providers.\n        \n        Returns:\n            Health information\n        \"\"\"\n        providers_health = {}\n        \n        for name, provider in self.providers.items():\n            try:\n                health = await provider.health_check()\n                providers_health[name] = {\n                    \"status\": \"healthy\",\n                    \"models\": provider.models,\n                    \"details\": health\n                }\n            except Exception as e:\n                providers_health[name] = {\n                    \"status\": \"unhealthy\",\n                    \"error\": str(e)\n                }\n        \n        return {\n            \"status\": \"healthy\" if any(p[\"status\"] == \"healthy\" for p in providers_health.values()) else \"unhealthy\",\n            \"available\": self.available,\n            \"providers\": providers_health,\n            \"default_model\": self.default_model,\n            \"cache_enabled\": self.cache_enabled,\n            \"cache_size\": len(self._response_cache) if self.cache_enabled else 0,\n        }\n\n\nclass OllamaProvider:\n    \"\"\"Ollama provider implementation.\"\"\"\n    \n    def __init__(self, base_url: str, models: Dict[str, str]):\n        \"\"\"Initialize the Ollama provider.\n        \n        Args:\n            base_url: Base URL for Ollama API\n            models: Map of task size to model name\n        \"\"\"\n        self.base_url = base_url\n        self.models = models\n        \n    async def generate(\n        self, \n        prompt: str,\n        system_prompt: Optional[str] = None,\n        model: str = \"mistral\",\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n    ) -> Dict[str, Any]:\n        \"\"\"Generate using Ollama.\n        \n        Args:\n            prompt: User prompt\n            system_prompt: System prompt\n            model: Model to use\n            temperature: Temperature setting\n            max_tokens: Maximum tokens to generate\n            \n        Returns:\n            Generated text and metadata\n        \"\"\"\n        async with httpx.AsyncClient(timeout=60) as client:\n            # Prepare the request\n            payload = {\n                \"model\": model,\n                \"prompt\": prompt,\n                \"stream\": False,\n                \"temperature\": temperature,\n                \"num_predict\": max_tokens,\n            }\n            \n            if system_prompt:\n                payload[\"system\"] = system_prompt\n            \n            # Send the request\n            try:\n                response = await client.post(\n                    f\"{self.base_url}/api/generate\",\n                    json=payload\n                )\n                response.raise_for_status()\n                result = response.json()\n                \n                return {\n                    \"text\": result[\"response\"],\n                    \"input_tokens\": result.get(\"prompt_eval_count\", 0),\n                    \"output_tokens\": result.get(\"eval_count\", 0),\n                }\n            except Exception as e:\n                logger.error(f\"Ollama error: {e}\")\n                raise\n    \n    async def health_check(self) -> Dict[str, Any]:\n        \"\"\"Check if Ollama is healthy.\n        \n        Returns:\n            Health information\n        \"\"\"\n        try:\n            async with httpx.AsyncClient(timeout=5) as client:\n                response = await client.get(f\"{self.base_url}/api/version\")\n                response.raise_for_status()\n                return {\n                    \"version\": response.json().get(\"version\", \"unknown\"),\n                    \"url\": self.base_url,\n                }\n        except Exception as e:\n            logger.error(f\"Ollama health check failed: {e}\")\n            raise\n\n\nclass AnthropicProvider:\n    \"\"\"Anthropic provider implementation.\"\"\"\n    \n    def __init__(self, api_key: str, models: Dict[str, str]):\n        \"\"\"Initialize the Anthropic provider.\n        \n        Args:\n            api_key: Anthropic API key\n            models: Map of task size to model name\n        \"\"\"\n        self.api_key = api_key\n        self.models = models\n        self.base_url = \"https://api.anthropic.com/v1/messages\"\n        \n    async def generate(\n        self, \n        prompt: str,\n        system_prompt: Optional[str] = None,\n        model: str = \"claude-3-haiku-20240307\",\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n    ) -> Dict[str, Any]:\n        \"\"\"Generate using Anthropic.\n        \n        Args:\n            prompt: User prompt\n            system_prompt: System prompt\n            model: Model to use\n            temperature: Temperature setting\n            max_tokens: Maximum tokens to generate\n            \n        Returns:\n            Generated text and metadata\n        \"\"\"\n        headers = {\n            \"x-api-key\": self.api_key,\n            \"anthropic-version\": \"2023-06-01\",\n            \"content-type\": \"application/json\"\n        }\n        \n        # Prepare the request\n        data = {\n            \"model\": model,\n            \"max_tokens\": max_tokens,\n            \"temperature\": temperature,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        }\n        \n        if system_prompt:\n            data[\"system\"] = system_prompt\n        \n        # Send the request\n        async with httpx.AsyncClient(timeout=30) as client:\n            try:\n                response = await client.post(self.base_url, headers=headers, json=data)\n                response.raise_for_status()\n                result = response.json()\n                \n                return {\n                    \"text\": result[\"content\"][0][\"text\"],\n                    \"input_tokens\": result.get(\"usage\", {}).get(\"input_tokens\", 0),\n                    \"output_tokens\": result.get(\"usage\", {}).get(\"output_tokens\", 0),\n                }\n            except Exception as e:\n                logger.error(f\"Anthropic error: {e}\")\n                raise\n    \n    async def health_check(self) -> Dict[str, Any]:\n        \"\"\"Check if Anthropic is healthy.\n        \n        Returns:\n            Health information\n        \"\"\"\n        # Just check if we have an API key\n        return {\n            \"api_key_set\": bool(self.api_key),\n        }\n\n\n# Async utilities\nimport asyncio\n\n\n@lru_cache\ndef get_llm_service(config_path: Optional[str] = None) -> LLMService:\n    \"\"\"Get a cached LLM service instance.\n    \n    Args:\n        config_path: Path to configuration file\n        \n    Returns:\n        LLM service instance\n    \"\"\"\n    anthropic_api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n    config = config_path if config_path else None\n    \n    return LLMService(\n        config_path=Path(config) if config else None,\n        anthropic_api_key=anthropic_api_key\n    )"
          },
          {
            "file_name": "pattern_validation.yaml",
            "file_size": 8299,
            "file_type": "application/x-yaml",
            "extracted_content": "decision_type: pattern_validation\ntitle: Pattern Validation Decision\ndescription: Validates extracted patterns like CAPs, insights, questions, and tensions\n\n# Parliament members and their perspectives\nmembers:\n  - name: \"Pattern Recognizer\"\n    focus: \"Pattern structure and completeness\"\n    perspective: \"Evaluates if the extracted text properly fits the pattern structure\"\n    \n  - name: \"Meaning Analyst\"\n    focus: \"Semantic content and significance\"\n    perspective: \"Assesses the meaning and significance of the pattern content\"\n    \n  - name: \"Evidence Evaluator\"\n    focus: \"Evidence quality and context\"\n    perspective: \"Examines the quality of evidence and context for the pattern\"\n    \n  - name: \"Generalization Assessor\"\n    focus: \"Applicability and generalizability\"\n    perspective: \"Considers how generalizable and widely applicable the pattern is\"\n    \n  - name: \"Coherence Checker\"\n    focus: \"Internal coherence and consistency\"\n    perspective: \"Checks for logical coherence and consistency within the pattern\"\n\n# Full prompt template with improved structure\ntemplate: |\n  # Parliament Decision: Pattern Validation in Knowledge Garden\n\n  ## CONTEXT\n  You are the Parliament Decision Engine in the Knowledge Garden v2.0 system. Your task is to validate an extracted pattern. Knowledge Garden identifies several pattern types:\n  \n  1. **CAPs**: Contexts, Actions, Patterns (e.g., \"MOMENTS when feeling stuck are OPPORTUNITIES for creative thinking\")\n  2. **Insights**: Realizations or discoveries (e.g., \"I realized that prioritizing tasks by impact rather than urgency leads to better outcomes\")\n  3. **Questions**: Important or recurring questions (e.g., \"How might we balance productivity with mental well-being?\")\n  4. **Tensions**: Opposing concepts or polarities (e.g., \"There's a tension between structured planning and spontaneous creativity\")\n  \n  Pattern validation is critical because:\n  - Validated patterns become core elements in the knowledge graph\n  - They guide artifact generation and knowledge organization\n  - They help identify recurring themes across conversations\n\n  ## PATTERN TO VALIDATE\n\n  Pattern Type: {pattern_type}\n  Pattern Text: \"{pattern_text}\"\n  \n  Structured elements:\n  {structured_elements}\n  \n  Extraction confidence: {extraction_confidence}\n  \n  Sources:\n  {sources}\n  \n  Occurrence count: {occurrence_count}\n  \n  Related patterns:\n  {related_patterns}\n\n  ## EXPERT PANEL ANALYSIS\n\n  Each expert should analyze the pattern from their unique perspective:\n\n  1. Pattern Recognizer (focus on pattern structure and completeness):\n     - Evaluate if the text properly fits the claimed pattern type\n     - Check if all required structural elements are present\n     - Assess if the pattern is correctly formatted and complete\n\n  2. Meaning Analyst (focus on semantic content and significance):\n     - Assess the meaning and significance of the pattern\n     - Evaluate if the pattern captures an important insight or observation\n     - Consider the clarity and specificity of the expressed concept\n\n  3. Evidence Evaluator (focus on evidence quality and context):\n     - Examine the quality of evidence supporting this pattern\n     - Consider the contexts in which the pattern was observed\n     - Assess if there's sufficient evidence to validate the pattern\n\n  4. Generalization Assessor (focus on applicability and generalizability):\n     - Consider how generalizable the pattern is\n     - Assess if it applies broadly or only in specific contexts\n     - Evaluate if the pattern represents a recurring theme\n\n  5. Coherence Checker (focus on internal coherence and consistency):\n     - Check for logical coherence within the pattern\n     - Assess if the pattern elements relate to each other meaningfully\n     - Consider if there are internal contradictions or inconsistencies\n\n  ## PATTERN TYPE CRITERIA\n\n  ### CAP Pattern Criteria:\n  - Contains identifiable context, action, and pattern elements\n  - Context represents a specific situation or trigger\n  - Action represents a response or approach\n  - Forms a meaningful \"when X, then Y\" relationship\n  - Captures a repeatable pattern of behavior or observation\n\n  ### Insight Pattern Criteria:\n  - Represents a realization or discovery\n  - Contains substantive content beyond common knowledge\n  - Has explanatory or predictive value\n  - Represents a meaningful observation or conclusion\n  - Often starts with phrases like \"I realized,\" \"I noticed,\" or \"I discovered\"\n\n  ### Question Pattern Criteria:\n  - Poses a meaningful, non-rhetorical question\n  - Explores important themes or challenges\n  - Shows depth or complexity beyond simple queries\n  - Potentially recurring across contexts\n  - Often starts with \"How might,\" \"What if,\" or similar phrases\n\n  ### Tension Pattern Criteria:\n  - Identifies two opposing concepts, approaches, or values\n  - Shows the relationship between the opposing elements\n  - Represents a meaningful polarity or trade-off\n  - Captures a recurring challenge in balancing priorities\n  - Often contains phrases like \"tension between,\" \"balance,\" or \"trade-off\"\n\n  ## EXAMPLE DECISIONS\n\n  ### Example 1: CAP Pattern Validated\n  Pattern: \"MOMENTS when feeling overwhelmed are OPPORTUNITIES for practicing mindfulness\"\n  Decision: VALIDATED (Confidence: 0.95)\n  Reasoning: This perfectly follows the CAP structure with clear context (\"feeling overwhelmed\"), action (\"practicing mindfulness\"), and pattern connection. The relationship is meaningful and likely generalizable. Multiple sources confirm this pattern, and it forms a coherent insight about converting challenging states into growth opportunities.\n\n  ### Example 2: Insight Pattern Invalidated\n  Pattern: \"I realized that the weather is nice today\"\n  Decision: INVALIDATED (Confidence: 0.90)\n  Reasoning: While this follows the linguistic structure of an insight (\"I realized that...\"), the content is trivial, contextually limited, and lacks significance. It doesn't represent a meaningful realization with explanatory value. This is a simple observation rather than a true insight pattern worth tracking in the knowledge graph.\n\n  ### Example 3: Question Pattern Modified\n  Pattern: \"Should I go to the store?\"\n  Decision: MODIFIED (Confidence: 0.85)\n  Modified to: \"How might I optimize my shopping routine to save time and reduce stress?\"\n  Reasoning: The original question is too specific and lacks depth. However, the context suggests a broader theme about efficient shopping that could be valuable. The modified version captures a more generalizable question with wider applicability and deeper exploration potential.\n\n  ## EDGE CASES AND CONSIDERATIONS\n\n  - If a pattern is valid but poorly formulated, consider suggesting a clearer version\n  - If a pattern fits multiple types, prioritize the most appropriate classification\n  - Consider frequency of occurrence when evaluating significance\n  - Assess if the pattern represents personal preference or generalizable knowledge\n  - Balance precision (exact pattern as stated) with utility (valuable concept even if imperfectly stated)\n  - Consider cultural and contextual factors that might affect pattern interpretation\n\n  ## DELIBERATION AND FINAL DECISION\n\n  After each expert has provided their perspective, synthesize a final decision:\n\n  ```json\n  {\n    \"decision\": \"VALIDATED\", \"INVALIDATED\", or \"MODIFIED\",\n    \"confidence\": 0.0-1.0,\n    \"reasoning\": \"Detailed explanation of decision rationale...\",\n    \"modified_pattern\": \"If modified, the improved pattern formulation\",\n    \"suggested_connections\": [\"Optional related concepts or patterns this should connect to\"]\n  }\n  ```\n\n  Your decision should clearly state whether to validate, invalidate, or modify the pattern, with confidence level and detailed reasoning that references the pattern type criteria and expert perspectives.\n\n# Output validation schema\noutput_schema:\n  type: object\n  required:\n    - decision\n    - confidence\n    - reasoning\n  properties:\n    decision:\n      type: string\n      enum: [\"VALIDATED\", \"INVALIDATED\", \"MODIFIED\"]\n    confidence:\n      type: number\n      minimum: 0.0\n      maximum: 1.0\n    reasoning:\n      type: string\n      minLength: 50\n    modified_pattern:\n      type: string\n    suggested_connections:\n      type: array\n      items:\n        type: string"
          },
          {
            "file_name": "settings.py",
            "file_size": 7576,
            "file_type": "text/x-python-script",
            "extracted_content": "\"\"\"\nUnified configuration system for Knowledge Garden\n\nThis module provides a centralized configuration system that loads settings from\nYAML files, environment variables, and command-line arguments. It supports nested\nconfigurations, environment variable substitution, and validation using Pydantic.\n\"\"\"\nimport os\nimport re\nimport yaml\nimport logging\nfrom typing import Dict, Any, Optional, Union, List\nfrom pathlib import Path\nfrom pydantic import BaseSettings, Field, validator, root_validator\nimport argparse\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConfigSettings(BaseSettings):\n    \"\"\"Base settings model for all services.\n    \n    This model contains common settings shared across all services.\n    \"\"\"\n    # Project information\n    project_name: str = \"Knowledge Garden\"\n    project_version: str = \"3.0.0\"\n    \n    # Logging\n    log_level: str = Field(default=\"INFO\", env=\"LOG_LEVEL\")\n    \n    # Service ports\n    artifacts_port: int = Field(default=8002, env=\"ARTIFACTS_PORT\")\n    extraction_port: int = Field(default=8001, env=\"EXTRACTION_PORT\")\n    parliament_port: int = Field(default=8003, env=\"PARLIAMENT_PORT\")\n    \n    # Paths\n    artifacts_output_dir: Path = Field(default=Path(\"data/artifacts\"), env=\"ARTIFACTS_OUTPUT_DIR\")\n    embeddings_cache_dir: Path = Field(default=Path(\"data/embeddings\"), env=\"EMBEDDINGS_CACHE_DIR\")\n    \n    # LLM settings\n    anthropic_api_key: Optional[str] = Field(default=None, env=\"ANTHROPIC_API_KEY\")\n    anthropic_enabled: bool = Field(default=True, env=\"ANTHROPIC_ENABLED\")\n    ollama_enabled: bool = Field(default=False, env=\"OLLAMA_ENABLED\")\n    ollama_url: str = Field(default=\"http://localhost:11434\", env=\"OLLAMA_URL\")\n    \n    # Embedding settings\n    embedding_model: str = Field(\n        default=\"jinaai/jina-embeddings-v2-base-en\", \n        env=\"EMBEDDING_MODEL\"\n    )\n    \n    # Fuseki settings\n    fuseki_url: str = Field(default=\"http://localhost:3030\", env=\"FUSEKI_URL\")\n    fuseki_dataset: str = Field(default=\"kg\", env=\"FUSEKI_DATASET\")\n    fuseki_user: str = Field(default=\"admin\", env=\"FUSEKI_USER\")\n    fuseki_password: str = Field(default=\"admin\", env=\"FUSEKI_PASSWORD\")\n    \n    # Processing settings\n    batch_size: int = Field(default=64, env=\"BATCH_SIZE\")\n    concurrency_limit: int = Field(default=5, env=\"CONCURRENCY_LIMIT\")\n    cache_enabled: bool = Field(default=True, env=\"CACHE_ENABLED\")\n    \n    # Service mode\n    standalone: bool = Field(default=True, env=\"STANDALONE\")\n    \n    # Custom validators\n    @validator(\"artifacts_output_dir\", \"embeddings_cache_dir\", pre=True)\n    def validate_paths(cls, v):\n        \"\"\"Convert string paths to Path objects.\"\"\"\n        if isinstance(v, str):\n            return Path(v)\n        return v\n    \n    @root_validator\n    def check_paths_exist(cls, values):\n        \"\"\"Ensure paths exist.\"\"\"\n        for key, value in values.items():\n            if isinstance(value, Path) and key.endswith(\"_dir\"):\n                value.mkdir(parents=True, exist_ok=True)\n        return values\n    \n    class Config:\n        \"\"\"Pydantic config.\"\"\"\n        env_file = \".env\"\n        extra = \"allow\"  # Allow extra fields\n\n\ndef _parse_args() -> Dict[str, Any]:\n    \"\"\"Parse command-line arguments.\n    \n    Returns:\n        Dictionary of arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Knowledge Garden\")\n    \n    # Common arguments\n    parser.add_argument(\"--config\", help=\"Path to config file\")\n    parser.add_argument(\"--log-level\", help=\"Logging level\")\n    parser.add_argument(\"--anthropic-api-key\", help=\"Anthropic API key\")\n    parser.add_argument(\"--fuseki-url\", help=\"Fuseki server URL\")\n    parser.add_argument(\"--fuseki-dataset\", help=\"Fuseki dataset name\")\n    \n    # Parse arguments\n    args, _ = parser.parse_known_args()\n    \n    # Convert arguments to dictionary\n    return {k: v for k, v in vars(args).items() if v is not None}\n\n\ndef _load_yaml_config(config_path: Optional[Union[str, Path]] = None) -> Dict[str, Any]:\n    \"\"\"Load YAML configuration file with environment variable substitution.\n    \n    Args:\n        config_path: Path to configuration file\n        \n    Returns:\n        Configuration dictionary\n    \"\"\"\n    # If no config path is specified, try to find one\n    if not config_path:\n        for path in [\n            Path(\"config/settings.yaml\"),\n            Path(\"config/unified_config.yaml\"),\n            Path(\"config/services.yaml\"),\n            Path(\"/etc/knowledge_garden/config.yaml\"),\n        ]:\n            if path.exists():\n                config_path = path\n                break\n    \n    # If no config file is found, return empty dict\n    if not config_path or not Path(config_path).exists():\n        logger.warning(f\"Config file not found at {config_path}, using defaults\")\n        return {}\n    \n    try:\n        with open(config_path) as f:\n            config_text = f.read()\n            \n        # Environment variable substitution\n        def replace_env(match):\n            var_name, default = match.groups()\n            return os.getenv(var_name, default)\n        \n        config_text = re.sub(r'\\${(\\w+):-([^}]+)}', replace_env, config_text)\n        config_text = re.sub(r'\\${(\\w+)}', lambda m: os.getenv(m.group(1), ''), config_text)\n        \n        # Parse YAML\n        return yaml.safe_load(config_text)\n    except Exception as e:\n        logger.error(f\"Error loading config from {config_path}: {e}\")\n        return {}\n\n\ndef _flatten_dict(d: Dict[str, Any], parent_key: str = '', sep: str = '_') -> Dict[str, Any]:\n    \"\"\"Flatten a nested dictionary.\n    \n    Args:\n        d: Dictionary to flatten\n        parent_key: Parent key for nested dictionaries\n        sep: Separator for keys\n        \n    Returns:\n        Flattened dictionary\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(_flatten_dict(v, new_key, sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n\ndef load_config(config_path: Optional[Union[str, Path]] = None) -> ConfigSettings:\n    \"\"\"Load configuration from all sources.\n    \n    Args:\n        config_path: Path to configuration file\n        \n    Returns:\n        ConfigSettings object\n    \"\"\"\n    # Load configuration from YAML\n    yaml_config = _load_yaml_config(config_path)\n    \n    # Flatten nested dictionary\n    flat_config = _flatten_dict(yaml_config)\n    \n    # Parse command-line arguments\n    args_config = _parse_args()\n    \n    # Merge configurations (args override YAML)\n    merged_config = {**flat_config, **args_config}\n    \n    # Create settings object\n    settings = ConfigSettings(**merged_config)\n    \n    logger.debug(f\"Loaded configuration with {len(merged_config)} settings\")\n    return settings\n\n\n# Global settings instance\n_settings = None\n\n\ndef get_settings(config_path: Optional[Union[str, Path]] = None) -> ConfigSettings:\n    \"\"\"Get the global settings instance.\n    \n    Args:\n        config_path: Path to configuration file (only used if settings not initialized)\n        \n    Returns:\n        ConfigSettings object\n    \"\"\"\n    global _settings\n    if _settings is None:\n        _settings = load_config(config_path)\n    return _settings\n\n\ndef reload_settings(config_path: Optional[Union[str, Path]] = None) -> ConfigSettings:\n    \"\"\"Reload settings from all sources.\n    \n    Args:\n        config_path: Path to configuration file\n        \n    Returns:\n        Updated ConfigSettings object\n    \"\"\"\n    global _settings\n    _settings = load_config(config_path)\n    return _settings"
          },
          {
            "file_name": "unified_config.yaml",
            "file_size": 1800,
            "file_type": "application/x-yaml",
            "extracted_content": "# Unified configuration file for Knowledge Garden\n# This file replaces the multiple overlapping config files\n\n# Project information\nproject_name: Knowledge Garden\nproject_version: 3.0.0\n\n# Logging\nlog_level: ${LOG_LEVEL:-INFO}\n\n# Service information\nservices:\n  artifacts:\n    port: ${ARTIFACTS_PORT:-8002}\n    output_dir: data/artifacts\n    standalone: true\n    concurrency_limit: 5\n    enable_dependency_tracking: true\n    \n  extraction:\n    port: ${EXTRACTION_PORT:-8001}\n    batch_size: 64\n    standalone: true\n    \n  parliament:\n    port: ${PARLIAMENT_PORT:-8003}\n    cache_enabled: true\n    standalone: true\n\n# Fuseki settings\nfuseki:\n  url: ${FUSEKI_URL:-http://localhost:3030}\n  dataset: ${FUSEKI_DATASET:-kg}\n  user: ${FUSEKI_USER:-admin}\n  password: ${FUSEKI_PASSWORD:-admin}\n\n# LLM settings\nllm:\n  providers:\n    anthropic:\n      api_key: ${ANTHROPIC_API_KEY}\n      enabled: ${ANTHROPIC_ENABLED:-true}\n      models:\n        large: claude-3-opus-20240229\n        medium: claude-3-sonnet-20240229\n        small: claude-3-haiku-20240307\n    \n    ollama:\n      enabled: ${OLLAMA_ENABLED:-false}\n      url: ${OLLAMA_URL:-http://localhost:11434}\n      models:\n        medium: mixtral\n        small: mistral\n  \n  routing:\n    small_tasks:\n      - entity_extraction\n      - keyword_extraction\n      - pattern_matching\n    \n    medium_tasks:\n      - artifact_generation\n      - entity_resolution\n      - pattern_validation\n    \n    large_tasks:\n      - conversation_summary\n      - cross_document_analysis\n      - knowledge_synthesis\n\n# Embedding settings\nembedding:\n  model: ${EMBEDDING_MODEL:-jinaai/jina-embeddings-v2-base-en}\n  cache_dir: data/embeddings\n  batch_size: 64\n\n# Paths\npaths:\n  data: data\n  artifacts: data/artifacts\n  embeddings: data/embeddings\n  prompts: prompts\n  static: static"
          },
          {
            "file_name": "kg_core.ttl",
            "file_size": 8058,
            "file_type": "",
            "extracted_content": "@prefix kg: <urn:knowledgegarden:ontology#> .\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix owl: <http://www.w3.org/2002/07/owl#> .\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n@prefix prov: <http://www.w3.org/ns/prov#> .\n\n### Knowledge Garden Pattern-Enhanced Ontology ###\n<urn:knowledgegarden:ontology> a owl:Ontology ;\n    rdfs:label \"Knowledge Garden Pattern-Enhanced Ontology\" ;\n    rdfs:comment \"Ontology supporting three-phase extraction with cross-conversation analysis\" ;\n    owl:versionInfo \"3.0-three-phase\" .\n\n### Core Classes ###\n\nkg:Agent a owl:Class ;\n    rdfs:label \"Agent\" ;\n    rdfs:comment \"A person or system agent\" .\n\nkg:Conversation a owl:Class ;\n    rdfs:label \"Conversation\" ;\n    rdfs:comment \"A dialogue or interaction\" .\n\nkg:Concept a owl:Class ;\n    rdfs:label \"Concept\" ;\n    rdfs:comment \"An idea or framework discussed\" .\n\nkg:CAP a owl:Class ;\n    rdfs:label \"Constitutive Attentional Policy\" ;\n    rdfs:comment \"Behavioral pattern revealing what someone finds meaningful\" .\n\nkg:Value a owl:Class ;\n    rdfs:label \"Value\" ;\n    rdfs:comment \"Named cluster of related CAPs forming a source of meaning\" .\n\nkg:Question a owl:Class ;\n    rdfs:label \"Question\" ;\n    rdfs:comment \"Growth-oriented inquiry revealing edges of understanding\" .\n\nkg:Insight a owl:Class ;\n    rdfs:label \"Insight\" ;\n    rdfs:comment \"Transformative realization with before/after understanding\" .\n\nkg:Tension a owl:Class ;\n    rdfs:label \"Tension\" ;\n    rdfs:comment \"Meaningful polarity that reveals values and drives growth\" .\n\nkg:Claim a owl:Class ;\n    rdfs:label \"Claim\" ;\n    rdfs:comment \"An assertion open to evidence/debate\" .\n\nkg:Tag a owl:Class ;\n    rdfs:label \"Tag\" ;\n    rdfs:comment \"Thematic label for organizing content\" .\n\nkg:PatternAnalysis a owl:Class ;\n    rdfs:label \"Pattern Analysis\" ;\n    rdfs:comment \"Results from cross-conversation pattern detection\" .\n\n### Core Object Properties ###\n\n# Agent properties\nkg:hasCAP a owl:ObjectProperty ;\n    rdfs:domain kg:Agent ;\n    rdfs:range kg:CAP .\n\nkg:hasValue a owl:ObjectProperty ;\n    rdfs:domain kg:Agent ;\n    rdfs:range kg:Value .\n\nkg:hasAnalysis a owl:ObjectProperty ;\n    rdfs:domain kg:Agent ;\n    rdfs:range kg:PatternAnalysis .\n\nkg:participatedIn a owl:ObjectProperty ;\n    rdfs:domain kg:Agent ;\n    rdfs:range kg:Conversation .\n\n# Conversation properties\nkg:hasTag a owl:ObjectProperty ;\n    rdfs:domain kg:Conversation ;\n    rdfs:range kg:Tag .\n\nkg:revealsCAP a owl:ObjectProperty ;\n    rdfs:domain kg:Conversation ;\n    rdfs:range kg:CAP .\n\nkg:mentionsConcept a owl:ObjectProperty ;\n    rdfs:domain kg:Conversation ;\n    rdfs:range kg:Concept .\n\nkg:raisesQuestion a owl:ObjectProperty ;\n    rdfs:domain kg:Conversation ;\n    rdfs:range kg:Question .\n\nkg:leadsToInsight a owl:ObjectProperty ;\n    rdfs:domain kg:Conversation ;\n    rdfs:range kg:Insight .\n\nkg:identifiesTension a owl:ObjectProperty ;\n    rdfs:domain kg:Conversation ;\n    rdfs:range kg:Tension .\n\n# Value properties\nkg:constitutedBy a owl:ObjectProperty ;\n    rdfs:domain kg:Value ;\n    rdfs:range kg:CAP .\n\n# Concept relationships\nkg:relatesTo a owl:ObjectProperty .\nkg:contrastsWith a owl:ObjectProperty .\n\n# Tension properties\nkg:revealsValue a owl:ObjectProperty ;\n    rdfs:domain kg:Tension ;\n    rdfs:range kg:Value .\n\n# Provenance\nkg:extractedFrom a owl:ObjectProperty ;\n    rdfs:subPropertyOf prov:wasDerivedFrom .\n\n# Support/dispute for claims\nkg:supports a owl:ObjectProperty ;\n    rdfs:domain kg:Agent ;\n    rdfs:range kg:Claim .\n\nkg:disputes a owl:ObjectProperty ;\n    rdfs:domain kg:Agent ;\n    rdfs:range kg:Claim .\n\n### Core Datatype Properties ###\n\n# Temporal\nkg:extractedAt a owl:DatatypeProperty ;\n    rdfs:range xsd:dateTime .\n\nkg:analyzedAt a owl:DatatypeProperty ;\n    rdfs:range xsd:dateTime .\n\nkg:synthesizedAt a owl:DatatypeProperty ;\n    rdfs:range xsd:dateTime .\n\n# Conversation properties\nkg:theme a owl:DatatypeProperty ;\n    rdfs:domain kg:Conversation ;\n    rdfs:range xsd:string .\n\nkg:memorablePhrase a owl:DatatypeProperty ;\n    rdfs:domain kg:Conversation ;\n    rdfs:range xsd:string .\n\n# CAP properties\nkg:frequency a owl:DatatypeProperty ;\n    rdfs:domain kg:CAP ;\n    rdfs:range xsd:integer ;\n    rdfs:comment \"Overall frequency across all contexts\" .\n\nkg:localFrequency a owl:DatatypeProperty ;\n    rdfs:domain kg:CAP ;\n    rdfs:range xsd:integer ;\n    rdfs:comment \"Frequency within single conversation\" .\n\nkg:globalFrequency a owl:DatatypeProperty ;\n    rdfs:domain kg:CAP ;\n    rdfs:range xsd:integer ;\n    rdfs:comment \"Frequency across all conversations\" .\n\nkg:emotionalCharge a owl:DatatypeProperty ;\n    rdfs:domain kg:CAP ;\n    rdfs:range xsd:string .\n\nkg:behavioralIndicator a owl:DatatypeProperty ;\n    rdfs:domain kg:CAP ;\n    rdfs:range xsd:string .\n\nkg:candidateStatus a owl:DatatypeProperty ;\n    rdfs:domain kg:CAP ;\n    rdfs:range xsd:boolean ;\n    rdfs:comment \"Whether this CAP needs cross-conversation validation\" .\n\nkg:globalPattern a owl:DatatypeProperty ;\n    rdfs:domain kg:CAP ;\n    rdfs:range xsd:boolean ;\n    rdfs:comment \"Whether this CAP is validated across conversations\" .\n\n# Insight properties\nkg:beforeUnderstanding a owl:DatatypeProperty ;\n    rdfs:domain kg:Insight ;\n    rdfs:range xsd:string .\n\nkg:afterUnderstanding a owl:DatatypeProperty ;\n    rdfs:domain kg:Insight ;\n    rdfs:range xsd:string .\n\nkg:trigger a owl:DatatypeProperty ;\n    rdfs:domain kg:Insight ;\n    rdfs:range xsd:string .\n\nkg:implications a owl:DatatypeProperty ;\n    rdfs:domain kg:Insight ;\n    rdfs:range xsd:string .\n\nkg:insightType a owl:DatatypeProperty ;\n    rdfs:domain kg:Insight ;\n    rdfs:range xsd:string .\n\nkg:scope a owl:DatatypeProperty ;\n    rdfs:domain kg:Insight ;\n    rdfs:range xsd:string ;\n    rdfs:comment \"LOCAL or GLOBAL scope of insight\" .\n\n# Tension properties\nkg:pole1 a owl:DatatypeProperty ;\n    rdfs:domain kg:Tension ;\n    rdfs:range xsd:string .\n\nkg:pole2 a owl:DatatypeProperty ;\n    rdfs:domain kg:Tension ;\n    rdfs:range xsd:string .\n\nkg:description a owl:DatatypeProperty ;\n    rdfs:range xsd:string .\n\nkg:significance a owl:DatatypeProperty ;\n    rdfs:domain kg:Tension ;\n    rdfs:range xsd:string .\n\nkg:manifestsAs a owl:DatatypeProperty ;\n    rdfs:domain kg:Tension ;\n    rdfs:range xsd:string .\n\nkg:managementStrategy a owl:DatatypeProperty ;\n    rdfs:domain kg:Tension ;\n    rdfs:range xsd:string .\n\n# Question properties\nkg:questionCategory a owl:DatatypeProperty ;\n    rdfs:domain kg:Question ;\n    rdfs:range xsd:string .\n\nkg:reveals a owl:DatatypeProperty ;\n    rdfs:domain kg:Question ;\n    rdfs:range xsd:string .\n\nkg:growthEdge a owl:DatatypeProperty ;\n    rdfs:domain kg:Question ;\n    rdfs:range xsd:string .\n\nkg:persistence a owl:DatatypeProperty ;\n    rdfs:domain kg:Question ;\n    rdfs:range xsd:string ;\n    rdfs:comment \"CONTEXT-SPECIFIC or PERSISTENT across conversations\" .\n\n# Concept properties\nkg:definition a owl:DatatypeProperty ;\n    rdfs:domain kg:Concept ;\n    rdfs:range xsd:string .\n\nkg:isCoreConecpt a owl:DatatypeProperty ;\n    rdfs:domain kg:Concept ;\n    rdfs:range xsd:boolean ;\n    rdfs:comment \"Appears frequently across conversations\" .\n\nkg:likelyCore a owl:DatatypeProperty ;\n    rdfs:domain kg:Concept ;\n    rdfs:range xsd:boolean ;\n    rdfs:comment \"Predicted to be core based on single conversation\" .\n\n# Pattern Analysis properties\nkg:capCount a owl:DatatypeProperty ;\n    rdfs:domain kg:PatternAnalysis ;\n    rdfs:range xsd:integer .\n\nkg:coreConceptCount a owl:DatatypeProperty ;\n    rdfs:domain kg:PatternAnalysis ;\n    rdfs:range xsd:integer .\n\n# General properties\nkg:confidence a owl:DatatypeProperty ;\n    rdfs:range xsd:decimal .\n\nkg:evidence a owl:DatatypeProperty ;\n    rdfs:range xsd:string .\n\nkg:sourceText a owl:DatatypeProperty ;\n    rdfs:range xsd:string .\n\n### For Joe Edelman's Wisdom Graph ###\nkg:wiserThan a owl:TransitiveProperty ;\n    rdfs:domain kg:Value ;\n    rdfs:range kg:Value ;\n    rdfs:label \"wiser than\" ;\n    rdfs:comment \"Value A is a wisdom upgrade from Value B\" .\n"
          },
          {
            "file_name": "settings.yaml",
            "file_size": 1249,
            "file_type": "application/x-yaml",
            "extracted_content": "fuseki:\n  dataset: ${FUSEKI_DATASET:-kg}\n  password: ${FUSEKI_PASSWORD:-admin}\n  url: ${FUSEKI_URL:-http://localhost:3030}\n  user: ${FUSEKI_USER:-admin}\nllm:\n  providers:\n    anthropic:\n      api_key: ${ANTHROPIC_API_KEY}\n      enabled: ${ANTHROPIC_ENABLED:-true}\n      models:\n        large: claude-3-opus-20240229\n        medium: claude-3-sonnet-20240229\n        small: claude-3-haiku-20240307\n    ollama:\n      enabled: ${OLLAMA_ENABLED:-false}\n      models:\n        medium: mixtral\n        small: mistral\n      url: ${OLLAMA_URL:-http://localhost:11434}\n  routing:\n    large_tasks:\n    - conversation_summary\n    - cross_document_analysis\n    - knowledge_synthesis\n    medium_tasks:\n    - artifact_generation\n    - entity_resolution\n    - pattern_validation\n    small_tasks:\n    - entity_extraction\n    - keyword_extraction\n    - pattern_matching\nproject:\n  name: Knowledge Garden\n  version: 3.0.0\nservices:\n  artifacts:\n    output_dir: data/artifacts\n    port: ${ARTIFACTS_PORT:-8002}\n    standalone: true\n  extraction:\n    batch_size: 64\n    embedding_model: jinaai/jina-embeddings-v2-base-en\n    port: ${EXTRACTION_PORT:-8001}\n    standalone: true\n  parliament:\n    cache_enabled: true\n    port: ${PARLIAMENT_PORT:-8003}\n    standalone: true\n"
          },
          {
            "file_name": "config.py",
            "file_size": 3366,
            "file_type": "text/x-python-script",
            "extracted_content": "\"\"\"Unified configuration system for Knowledge Garden\"\"\"\nimport os\nimport yaml\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\n\nclass Config:\n    \"\"\"Unified configuration loader for Knowledge Garden\"\"\"\n    \n    def __init__(self, config_dir: Optional[Path] = None):\n        self.config_dir = config_dir or Path(__file__).parent.parent / \"config\"\n        self.config = self._load_all_configs()\n        \n    def _load_all_configs(self) -> Dict[str, Any]:\n        \"\"\"Load all YAML configs and merge with defaults\"\"\"\n        # Default configuration\n        config = {\n            \"project\": {\n                \"name\": \"Knowledge Garden\",\n                \"version\": \"2.0.0\",\n                \"data_dir\": Path(__file__).parent.parent / \"data\"\n            },\n            \"namespaces\": {\n                \"kg\": \"urn:knowledgegarden:ontology#\",\n                \"kgi\": \"urn:knowledgegarden:instance:\",\n            },\n            \"fuseki\": {\n                \"url\": os.getenv(\"KG_FUSEKI_URL\", \"http://localhost:3030\"),\n                \"dataset\": os.getenv(\"KG_FUSEKI_DATASET\", \"kg\"),\n                \"user\": os.getenv(\"KG_FUSEKI_USER\", \"admin\"),\n                \"password\": os.getenv(\"KG_FUSEKI_PASSWORD\", \"changeme\")\n            },\n            \"models\": {\n                \"embedding\": os.getenv(\"KG_EMBEDDING_MODEL\", \"jinaai/jina-embeddings-v2-base-en\"),\n                \"llm_fast\": os.getenv(\"KG_LLM_FAST\", \"claude-3-haiku-20240307\"),\n                \"llm_smart\": os.getenv(\"KG_LLM_SMART\", \"claude-3-sonnet-20240229\")\n            },\n            \"extraction\": {\n                \"batch_size\": int(os.getenv(\"KG_BATCH_SIZE\", \"10\")),\n                \"confidence_threshold\": float(os.getenv(\"KG_CONFIDENCE_THRESHOLD\", \"0.7\")),\n                \"similarity_threshold\": float(os.getenv(\"KG_SIMILARITY_THRESHOLD\", \"0.85\"))\n            }\n        }\n        \n        # Load YAML configs\n        if self.config_dir.exists():\n            for yaml_file in self.config_dir.glob(\"*.yaml\"):\n                try:\n                    with open(yaml_file, 'r') as f:\n                        yaml_config = yaml.safe_load(f)\n                        if yaml_config:\n                            # Merge into main config\n                            section = yaml_file.stem\n                            config[section] = yaml_config\n                except Exception as e:\n                    print(f\"Error loading {yaml_file}: {e}\")\n        \n        return config\n    \n    def get(self, key: str, default: Any = None) -> Any:\n        \"\"\"Get config value by dot-separated key\"\"\"\n        keys = key.split('.')\n        value = self.config\n        \n        for k in keys:\n            if isinstance(value, dict) and k in value:\n                value = value[k]\n            else:\n                return default\n        \n        return value\n    \n    def __getattr__(self, name: str) -> Any:\n        \"\"\"Allow attribute-style access\"\"\"\n        if name in self.config:\n            return self.config[name]\n        raise AttributeError(f\"Config has no attribute '{name}'\")\n\n# Global config instance\nconfig = Config()\n\n# Convenience exports\nPROJECT_ROOT = Path(__file__).parent.parent\nDATA_DIR = config.get(\"project.data_dir\", PROJECT_ROOT / \"data\")\nKG = config.get(\"namespaces.kg\")\nKGI = config.get(\"namespaces.kgi\")\nFUSEKI_URL = config.get(\"fuseki.url\")\nFUSEKI_DATASET = config.get(\"fuseki.dataset\")\n"
          },
          {
            "file_name": "config.yml",
            "file_size": 1017,
            "file_type": "application/x-yaml",
            "extracted_content": "# Knowledge Garden Configuration\n\n# Global settings\napp:\n  environment: development\n  debug: true\n\n# Fuseki connection\nfuseki:\n  url: http://fuseki:3030\n  dataset: kg\n  user: admin\n  password: admin\n\n# LLM settings\nllm:\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  enabled: true\n  max_tokens: 2000\n\n# Embedding settings\nembeddings:\n  model: sentence-transformers/all-MiniLM-L6-v2\n  dimension: 384\n  enabled: true\n  cache_enabled: true\n\n# Extraction service\nextraction:\n  host: 0.0.0.0\n  port: 8001\n  batch_size: 10\n  min_keyword_score: 0.1\n  min_pattern_confidence: 0.6\n  max_concepts_per_document: 50\n\n# Artifact service\nartifacts:\n  host: 0.0.0.0\n  port: 8002\n  static_dir: /app/data/artifacts/static\n  artifact_types:\n    - diagram\n    - summary\n    - framework\n    - insight_card\n    - practice\n    - report\n    - presentation\n    - image\n    - chart\n\n# Parliament service\nparliament:\n  host: 0.0.0.0\n  port: 8003\n  decisions_dir: /app/data/parliament/decisions\n  cache_dir: /app/data/parliament/cache"
          },
          {
            "file_name": "loader.py",
            "file_size": 8655,
            "file_type": "text/x-python-script",
            "extracted_content": "\"\"\"\nUnified prompt loader for Knowledge Garden\n\nThis module provides a centralized prompt loading system that looks for prompts\nin multiple locations and handles template variable substitution.\n\"\"\"\nimport logging\nimport os\nimport yaml\nfrom typing import Dict, Any, Optional, Union, List\nfrom pathlib import Path\nimport json\nimport jinja2\nfrom functools import lru_cache\n\nlogger = logging.getLogger(__name__)\n\n\nclass PromptLoader:\n    \"\"\"Unified prompt loader for Knowledge Garden\"\"\"\n    \n    def __init__(\n        self,\n        prompt_dirs: Optional[List[Union[str, Path]]] = None,\n        use_cache: bool = True,\n    ):\n        \"\"\"Initialize the prompt loader.\n        \n        Args:\n            prompt_dirs: List of directories to look for prompts\n            use_cache: Whether to cache loaded prompts\n        \"\"\"\n        self.prompt_dirs = []\n        \n        # Add default prompt directories\n        self._add_default_dirs()\n        \n        # Add user-specified directories\n        if prompt_dirs:\n            for dir_path in prompt_dirs:\n                if isinstance(dir_path, str):\n                    dir_path = Path(dir_path)\n                if dir_path.exists():\n                    self.prompt_dirs.append(dir_path)\n        \n        # Initialize Jinja environment\n        self.jinja_env = jinja2.Environment(\n            loader=jinja2.FileSystemLoader(self.prompt_dirs),\n            undefined=jinja2.StrictUndefined,\n            autoescape=False,\n        )\n        \n        # Cache for loaded prompts\n        self.use_cache = use_cache\n        self._prompt_cache = {}\n        \n        logger.info(f\"Prompt loader initialized with {len(self.prompt_dirs)} directories\")\n    \n    def _add_default_dirs(self):\n        \"\"\"Add default prompt directories.\"\"\"\n        # Check for shared prompts directory\n        shared_prompts = Path(__file__).parent\n        if shared_prompts.exists():\n            self.prompt_dirs.append(shared_prompts)\n        \n        # Check for service-specific prompts directories\n        for service_dir in [\"extraction\", \"parliament\", \"artifacts\"]:\n            service_prompts = Path(f\"services/{service_dir}/prompts\")\n            if service_prompts.exists():\n                self.prompt_dirs.append(service_prompts)\n        \n        # Check for prompts in the config directory\n        config_prompts = Path(\"config/prompts\")\n        if config_prompts.exists():\n            self.prompt_dirs.append(config_prompts)\n    \n    def _find_prompt_file(self, prompt_name: str) -> Optional[Path]:\n        \"\"\"Find a prompt file by name in prompt directories.\n        \n        Args:\n            prompt_name: Name of the prompt file (without extension)\n            \n        Returns:\n            Path to the prompt file, or None if not found\n        \"\"\"\n        # Check for full path with extension\n        if Path(prompt_name).exists():\n            return Path(prompt_name)\n        \n        # Check with .yaml extension\n        if not prompt_name.endswith((\".yaml\", \".yml\", \".json\")):\n            # Try different extensions\n            for ext in [\".yaml\", \".yml\", \".json\"]:\n                full_name = f\"{prompt_name}{ext}\"\n                \n                # Look in all prompt directories\n                for prompt_dir in self.prompt_dirs:\n                    prompt_path = prompt_dir / full_name\n                    if prompt_path.exists():\n                        return prompt_path\n        else:\n            # Name already has extension\n            for prompt_dir in self.prompt_dirs:\n                prompt_path = prompt_dir / prompt_name\n                if prompt_path.exists():\n                    return prompt_path\n        \n        return None\n    \n    def load_prompt(self, prompt_name: str) -> Dict[str, Any]:\n        \"\"\"Load a prompt by name.\n        \n        Args:\n            prompt_name: Name of the prompt file (without extension)\n            \n        Returns:\n            Prompt dictionary\n            \n        Raises:\n            FileNotFoundError: If prompt file not found\n        \"\"\"\n        # Check cache first\n        if self.use_cache and prompt_name in self._prompt_cache:\n            return self._prompt_cache[prompt_name]\n        \n        # Find prompt file\n        prompt_path = self._find_prompt_file(prompt_name)\n        if not prompt_path:\n            raise FileNotFoundError(f\"Prompt file '{prompt_name}' not found in prompt directories\")\n        \n        try:\n            # Load prompt file\n            with open(prompt_path) as f:\n                if prompt_path.suffix in [\".yaml\", \".yml\"]:\n                    prompt = yaml.safe_load(f)\n                elif prompt_path.suffix == \".json\":\n                    prompt = json.load(f)\n                else:\n                    # Default to YAML\n                    prompt = yaml.safe_load(f)\n            \n            # Cache prompt\n            if self.use_cache:\n                self._prompt_cache[prompt_name] = prompt\n            \n            return prompt\n        except Exception as e:\n            logger.error(f\"Error loading prompt '{prompt_name}': {e}\")\n            raise\n    \n    def render_template(\n        self,\n        prompt_name: str,\n        variables: Dict[str, Any],\n        template_field: str = \"template\",\n    ) -> str:\n        \"\"\"Render a prompt template with variables.\n        \n        Args:\n            prompt_name: Name of the prompt file (without extension)\n            variables: Dictionary of variables to substitute in the template\n            template_field: Field in the prompt dictionary containing the template\n            \n        Returns:\n            Rendered template\n            \n        Raises:\n            FileNotFoundError: If prompt file not found\n            KeyError: If template field not found in prompt\n        \"\"\"\n        # Load prompt\n        prompt = self.load_prompt(prompt_name)\n        \n        # Check for template field\n        if template_field not in prompt:\n            raise KeyError(f\"Template field '{template_field}' not found in prompt '{prompt_name}'\")\n        \n        # Get template\n        template_str = prompt[template_field]\n        \n        # Render template\n        try:\n            template = jinja2.Template(template_str)\n            return template.render(**variables)\n        except Exception as e:\n            logger.error(f\"Error rendering template '{prompt_name}': {e}\")\n            raise\n    \n    def get_decision_prompt(\n        self,\n        decision_type: str,\n        variables: Dict[str, Any],\n    ) -> str:\n        \"\"\"Render a parliament decision prompt.\n        \n        Args:\n            decision_type: Type of decision (pattern_validation, entity_merge, etc.)\n            variables: Dictionary of variables to substitute in the template\n            \n        Returns:\n            Rendered template\n            \n        Raises:\n            FileNotFoundError: If prompt file not found\n        \"\"\"\n        # Load prompt\n        prompt = self.load_prompt(decision_type)\n        \n        # Check for template field\n        if \"template\" not in prompt:\n            raise KeyError(f\"Template field not found in prompt '{decision_type}'\")\n        \n        # Get template\n        template_str = prompt[\"template\"]\n        \n        # Render template\n        try:\n            template = jinja2.Template(template_str)\n            return template.render(**variables)\n        except Exception as e:\n            logger.error(f\"Error rendering decision prompt '{decision_type}': {e}\")\n            raise\n    \n    def get_output_schema(\n        self,\n        prompt_name: str,\n    ) -> Dict[str, Any]:\n        \"\"\"Get the output schema for a prompt.\n        \n        Args:\n            prompt_name: Name of the prompt file (without extension)\n            \n        Returns:\n            Output schema dictionary\n            \n        Raises:\n            FileNotFoundError: If prompt file not found\n            KeyError: If output_schema field not found in prompt\n        \"\"\"\n        # Load prompt\n        prompt = self.load_prompt(prompt_name)\n        \n        # Check for output_schema field\n        if \"output_schema\" not in prompt:\n            raise KeyError(f\"Output schema field not found in prompt '{prompt_name}'\")\n        \n        return prompt[\"output_schema\"]\n\n\n@lru_cache\ndef get_prompt_loader(\n    prompt_dirs: Optional[List[str]] = None,\n    use_cache: bool = True,\n) -> PromptLoader:\n    \"\"\"Get a cached prompt loader.\n    \n    Args:\n        prompt_dirs: List of directories to look for prompts\n        use_cache: Whether to cache loaded prompts\n        \n    Returns:\n        PromptLoader instance\n    \"\"\"\n    return PromptLoader(prompt_dirs=prompt_dirs, use_cache=use_cache)"
          },
          {
            "file_name": "services.yaml",
            "file_size": 425,
            "file_type": "application/x-yaml",
            "extracted_content": "services:\n  artifacts:\n    output_dir: data/artifacts\n    port: 8002\n  extraction:\n    batch_size: 64\n    embedding_model: jinaai/jina-embeddings-v2-base-en\n    port: 8001\n  fuseki:\n    dataset: kg\n    password: admin\n    url: http://localhost:3030\n    user: admin\n  parliament:\n    cache_enabled: true\n    llm_model: claude-3-haiku-20240307\n    port: 8003\nshared:\n  anthropic_api_key: ${ANTHROPIC_API_KEY}\n  log_level: INFO\n"
          }
        ],
        "files": [
          {
            "file_name": "service.py"
          },
          {
            "file_name": "llm_service.py"
          },
          {
            "file_name": "service.py"
          },
          {
            "file_name": "pattern_validation.yaml"
          },
          {
            "file_name": "settings.py"
          },
          {
            "file_name": "unified_config.yaml"
          },
          {
            "file_name": "kg_core.ttl"
          },
          {
            "file_name": "settings.yaml"
          },
          {
            "file_name": "config.py"
          },
          {
            "file_name": "config.yml"
          },
          {
            "file_name": "loader.py"
          },
          {
            "file_name": "services.yaml"
          }
        ]
      }
    },
    {
      "id": "250df1f5-491a-4925-b672-d76fbd94fdf2",
      "role": "assistant",
      "content": "The user is asking me to review the Knowledge Garden extraction service and provide:\n1. 5 pros and 5 cons\n2. Ways to reduce duplication and increase cleanliness\n3. Ways to improve quality of extraction\n4. How knowledge graphs are leveraging the ontology\n5. Valuable things that can be done with this code\n\nLet me analyze the codebase systematically:\n\n1. **Architecture Analysis:**\n   - The service is structured with multiple layers: API, pipeline stages, models, services\n   - It uses a unified configuration system (shared/config)\n   - Has unified services for LLM, embeddings, and prompts\n   - Uses FastAPI for the API layer\n   - Has a pipeline architecture for extraction\n\n2. **Key Components:**\n   - Extraction pipeline with stages: document processing, embedding generation, quantization, concept extraction, clustering, topic modeling\n   - Pattern extraction for CAPs, insights, questions, tensions\n   - Entity extraction using spaCy\n   - Evidence tracking for provenance\n   - Integration with Fuseki triplestore\n   - Parliament service integration for entity resolution\n\n3. **Ontology Usage:**\n   Looking at kg_core.ttl, the ontology defines:\n   - Core classes: Agent, Conversation, Concept, CAP, Value, Question, Insight, Tension, Claim, Tag, PatternAnalysis\n   - Object properties: hasCAP, hasValue, revealsCAP, mentionsConcept, etc.\n   - Datatype properties: frequency, emotionalCharge, behavioralIndicator, etc.\n   - Support for three-phase extraction with cross-conversation analysis\n\nNow let me analyze the pros and cons:\n\n**Pros:**\n1. Well-structured pipeline architecture with clear separation of concerns\n2. Comprehensive pattern extraction (CAPs, insights, questions, tensions)\n3. Evidence tracking for provenance and knowledge validation\n4. Integration with unified services (LLM, embeddings) for consistency\n5. Support for clustering and topic modeling for knowledge organization\n \nThe system reveals critical architectural weaknesses that undermine its potential. Code redundancy plagues multiple service implementations, creating maintenance challenges. Error handling remains rudimentary, leaving the extraction process vulnerable to unexpected failures. The rich ontology remains largely untapped, suggesting significant untapped potential for semantic richness and knowledge representation.\n\nI'll focus on strategic improvements. Consolidating service implementations can dramatically reduce code complexity. Implementing robust error management with intelligent retry mechanisms would enhance system resilience. Deeper ontology integration could transform the extraction process, enabling more nuanced knowledge capture and cross-domain insights.\n\nThe current implementation hints at a powerful knowledge extraction framework, but requires systematic refinement to realize its full capabilities. Targeted optimizations in code structure, error handling, and ontological leverage could significantly elevate the system's effectiveness.Looking at the Knowledge Garden extraction service, I'll provide a comprehensive review focusing on its architecture, implementation quality, and potential improvements.\n\n## 5 Pros\n\n1. **Well-Architected Pipeline System**: The service uses a clean pipeline architecture with distinct stages (document processing  embedding generation  concept extraction  clustering  topic modeling), making it modular and extensible.\n\n2. **Comprehensive Pattern Recognition**: The pattern extractor identifies valuable conversational patterns including CAPs (Contexts/Actions/Patterns), insights, questions, and tensions - going beyond simple entity extraction to capture meaningful knowledge structures.\n\n3. **Evidence-Based Knowledge Provenance**: The evidence tracking system maintains detailed provenance for all extracted knowledge, linking back to source quotes and confidence scores, which is crucial for knowledge validation.\n\n4. **Unified Service Architecture**: The integration with shared services (LLM, embeddings, configuration) promotes consistency across the Knowledge Garden ecosystem and reduces service-specific implementations.\n\n5. **Advanced Knowledge Organization**: The clustering and topic modeling capabilities allow for discovering relationships between concepts across documents, enabling higher-level knowledge synthesis.\n\n## 5 Cons\n\n1. **Significant Code Duplication**: There's substantial duplication across services - multiple Fuseki clients, repeated configuration loading, and redundant service implementations (notice the deprecation warnings in several files).\n\n2. **Incomplete Implementation**: Many components are placeholders or mocks (e.g., `api/app.py` returns mock data, the extraction pipeline runner has minimal implementation).\n\n3. **Underutilized Ontology**: Despite having a rich ontology (`kg_core.ttl`) with sophisticated relationships and properties, the extraction service barely uses it - missing opportunities for semantic reasoning.\n\n4. **Weak Error Handling**: The service lacks robust error handling, retry mechanisms, and graceful degradation when dependencies (Fuseki, LLM services) are unavailable.\n\n5. **Limited Cross-Document Analysis**: While the architecture supports it, the actual implementation of cross-conversation pattern detection and knowledge synthesis is minimal.\n\n## Reducing Duplication & Increasing Cleanliness\n\n### 1. **Consolidate Service Clients**\n```python\n# Instead of multiple Fuseki implementations, use a single shared client\n# shared/services/fuseki/client.py\nclass FusekiClient:\n    \"\"\"Single, well-tested Fuseki client for all services\"\"\"\n    \n    def __init__(self, config: ConfigSettings):\n        self.endpoint = config.fuseki_url\n        self.dataset = config.fuseki_dataset\n        # Implement connection pooling, retries, circuit breakers\n```\n\n### 2. **Centralize Pattern Extraction**\n```python\n# shared/extraction/patterns.py\nclass UnifiedPatternExtractor:\n    \"\"\"Single source of truth for pattern extraction logic\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        self.extractors = {\n            'cap': CAPExtractor(),\n            'insight': InsightExtractor(),\n            'question': QuestionExtractor(),\n            'tension': TensionExtractor(),\n        }\n    \n    async def extract_all(self, text: str, context: Dict) -> ExtractedPatterns:\n        # Centralized extraction with context awareness\n        pass\n```\n\n### 3. **Implement Dependency Injection**\n```python\n# Use dependency injection to avoid repeated service initialization\nfrom fastapi import Depends\n\nclass ServiceRegistry:\n    \"\"\"Central registry for all services\"\"\"\n    \n    def __init__(self, config: ConfigSettings):\n        self.fuseki = FusekiClient(config)\n        self.llm = LLMService(config)\n        self.embeddings = EmbeddingService(config)\n        self.evidence_tracker = EvidenceTracker(self.fuseki)\n\n# In API endpoints\nasync def extract(request: ExtractionRequest, services: ServiceRegistry = Depends(get_services)):\n    # Use injected services\n    pass\n```\n\n## Improving Extraction Quality\n\n### 1. **Leverage the Ontology**\n```python\nclass OntologyAwareExtractor:\n    \"\"\"Extraction that uses the knowledge graph ontology\"\"\"\n    \n    async def extract_with_reasoning(self, text: str) -> KnowledgeGraph:\n        # Use ontology relationships\n        concepts = await self.extract_concepts(text)\n        \n        # Apply ontology rules\n        for concept in concepts:\n            if concept.type == \"CAP\":\n                # Link to Values using kg:constitutedBy\n                values = await self.infer_values(concept)\n                \n            if concept.type == \"Tension\":\n                # Use kg:revealsValue to find underlying values\n                revealed_values = await self.analyze_tension_values(concept)\n        \n        return self.build_knowledge_graph(concepts)\n```\n\n### 2. **Implement Proper Cross-Conversation Analysis**\n```python\nclass CrossConversationAnalyzer:\n    \"\"\"Analyze patterns across multiple conversations\"\"\"\n    \n    async def analyze_conversations(self, conversations: List[Conversation]) -> PatternAnalysis:\n        # Extract patterns from each conversation\n        all_patterns = []\n        for conv in conversations:\n            patterns = await self.extract_patterns(conv)\n            all_patterns.extend(patterns)\n        \n        # Find recurring themes\n        recurring_caps = self.find_recurring_patterns(all_patterns, min_frequency=3)\n        \n        # Identify value clusters\n        value_clusters = self.cluster_by_values(recurring_caps)\n        \n        # Build cross-conversation insights\n        return PatternAnalysis(\n            recurring_patterns=recurring_caps,\n            value_systems=value_clusters,\n            evolution_timeline=self.track_pattern_evolution(all_patterns)\n        )\n```\n\n### 3. **Add Validation and Quality Checks**\n```python\nclass ExtractionValidator:\n    \"\"\"Validate extracted knowledge using Parliament decisions\"\"\"\n    \n    async def validate_extraction(self, extracted: ExtractedKnowledge) -> ValidatedKnowledge:\n        # Use Parliament for validation\n        for pattern in extracted.patterns:\n            decision = await self.parliament.validate_pattern(pattern)\n            if decision.confidence < 0.7:\n                pattern.needs_review = True\n        \n        # Check for contradictions\n        conflicts = await self.detect_conflicts(extracted.claims)\n        for conflict in conflicts:\n            resolution = await self.parliament.resolve_conflict(conflict)\n            \n        return ValidatedKnowledge(extracted, validation_results)\n```\n\n## How Knowledge Graphs Should Leverage the Ontology\n\nCurrently, the ontology is underutilized. Here's how it should be used:\n\n### 1. **Semantic Relationship Mapping**\n```sparql\n# Use ontology properties to create rich connections\nINSERT DATA {\n    # When extracting a CAP, also infer Values\n    ?cap a kg:CAP ;\n         kg:frequency ?freq ;\n         kg:emotionalCharge ?charge .\n    \n    ?value a kg:Value ;\n           kg:constitutedBy ?cap .\n    \n    ?agent kg:hasCAP ?cap ;\n           kg:hasValue ?value .\n}\n```\n\n### 2. **Knowledge Tier Management**\n```python\n# Properly classify knowledge using the tier system\nasync def classify_knowledge_tier(self, knowledge_item):\n    if knowledge_item.source == \"personal_conversation\":\n        if knowledge_item.evidence_count > 5:\n            return \"DIGESTED\"  # Well-validated personal knowledge\n        return \"PERSONAL\"\n    elif knowledge_item.is_referenced:\n        return \"PERIPHERAL\"\n```\n\n### 3. **Pattern Evolution Tracking**\n```python\n# Track how patterns evolve over time using temporal properties\nasync def track_pattern_evolution(self, pattern_id: str):\n    query = \"\"\"\n    SELECT ?timestamp ?frequency ?confidence\n    WHERE {\n        ?pattern kg:patternId ?pattern_id ;\n                 kg:extractedAt ?timestamp ;\n                 kg:frequency ?frequency ;\n                 kg:confidence ?confidence .\n    }\n    ORDER BY ?timestamp\n    \"\"\"\n    # Analyze how patterns strengthen or weaken over time\n```\n\n## Valuable Things You Can Do With This Code\n\n### 1. **Personal Knowledge Assistant**\nBuild an AI assistant that truly understands your thinking patterns, values, and insights over time, creating a personalized knowledge companion that can remind you of your own insights when relevant.\n\n### 2. **Conversation Intelligence Platform**\nAnalyze team conversations to extract collective intelligence, identify recurring challenges, and surface insights that might otherwise be lost in the flow of discussion.\n\n### 3. **Learning Pattern Analyzer**\nTrack how understanding evolves over time by analyzing the progression of questions  insights  new questions, helping optimize learning paths.\n\n### 4. **Value Alignment Tool**\nUse the CAP  Value mapping to help individuals and teams understand their core values based on what they consistently pay attention to and act upon.\n\n### 5. **Knowledge Synthesis Engine**\nAutomatically generate comprehensive reports, summaries, and visualizations that synthesize knowledge from multiple conversations, creating a \"second brain\" that grows smarter over time.\n\nThe key is to move from simple extraction to true knowledge understanding by fully leveraging the ontology's semantic richness and the system's cross-conversation analysis capabilities.",
      "created_at": "2025-06-12T15:44:40.695073Z",
      "sequence_order": 1,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2025-06-12T15:43:12.224846Z",
            "stop_timestamp": "2025-06-12T15:44:06.921636Z",
            "flags": null,
            "type": "thinking",
            "thinking": "The user is asking me to review the Knowledge Garden extraction service and provide:\n1. 5 pros and 5 cons\n2. Ways to reduce duplication and increase cleanliness\n3. Ways to improve quality of extraction\n4. How knowledge graphs are leveraging the ontology\n5. Valuable things that can be done with this code\n\nLet me analyze the codebase systematically:\n\n1. **Architecture Analysis:**\n   - The service is structured with multiple layers: API, pipeline stages, models, services\n   - It uses a unified configuration system (shared/config)\n   - Has unified services for LLM, embeddings, and prompts\n   - Uses FastAPI for the API layer\n   - Has a pipeline architecture for extraction\n\n2. **Key Components:**\n   - Extraction pipeline with stages: document processing, embedding generation, quantization, concept extraction, clustering, topic modeling\n   - Pattern extraction for CAPs, insights, questions, tensions\n   - Entity extraction using spaCy\n   - Evidence tracking for provenance\n   - Integration with Fuseki triplestore\n   - Parliament service integration for entity resolution\n\n3. **Ontology Usage:**\n   Looking at kg_core.ttl, the ontology defines:\n   - Core classes: Agent, Conversation, Concept, CAP, Value, Question, Insight, Tension, Claim, Tag, PatternAnalysis\n   - Object properties: hasCAP, hasValue, revealsCAP, mentionsConcept, etc.\n   - Datatype properties: frequency, emotionalCharge, behavioralIndicator, etc.\n   - Support for three-phase extraction with cross-conversation analysis\n\nNow let me analyze the pros and cons:\n\n**Pros:**\n1. Well-structured pipeline architecture with clear separation of concerns\n2. Comprehensive pattern extraction (CAPs, insights, questions, tensions)\n3. Evidence tracking for provenance and knowledge validation\n4. Integration with unified services (LLM, embeddings) for consistency\n5. Support for clustering and topic modeling for knowledge organization\n \nThe system reveals critical architectural weaknesses that undermine its potential. Code redundancy plagues multiple service implementations, creating maintenance challenges. Error handling remains rudimentary, leaving the extraction process vulnerable to unexpected failures. The rich ontology remains largely untapped, suggesting significant untapped potential for semantic richness and knowledge representation.\n\nI'll focus on strategic improvements. Consolidating service implementations can dramatically reduce code complexity. Implementing robust error management with intelligent retry mechanisms would enhance system resilience. Deeper ontology integration could transform the extraction process, enabling more nuanced knowledge capture and cross-domain insights.\n\nThe current implementation hints at a powerful knowledge extraction framework, but requires systematic refinement to realize its full capabilities. Targeted optimizations in code structure, error handling, and ontological leverage could significantly elevate the system's effectiveness.",
            "summaries": [
              {
                "summary": "Thinking about evaluating extraction service performance, knowledge graph ontology, and code optimization strategies"
              },
              {
                "summary": "Reviewing extraction service, mapping pros, cons, improvements"
              },
              {
                "summary": "Dissecting extraction service's pros, cons, and improvement strategies"
              },
              {
                "summary": "Dissected extraction service, mapped pros, cons, and improvement strategies."
              }
            ],
            "cut_off": false
          },
          {
            "start_timestamp": "2025-06-12T15:44:03.157473Z",
            "stop_timestamp": "2025-06-12T15:44:31.676475Z",
            "flags": null,
            "type": "text",
            "text": "Looking at the Knowledge Garden extraction service, I'll provide a comprehensive review focusing on its architecture, implementation quality, and potential improvements.\n\n## 5 Pros\n\n1. **Well-Architected Pipeline System**: The service uses a clean pipeline architecture with distinct stages (document processing  embedding generation  concept extraction  clustering  topic modeling), making it modular and extensible.\n\n2. **Comprehensive Pattern Recognition**: The pattern extractor identifies valuable conversational patterns including CAPs (Contexts/Actions/Patterns), insights, questions, and tensions - going beyond simple entity extraction to capture meaningful knowledge structures.\n\n3. **Evidence-Based Knowledge Provenance**: The evidence tracking system maintains detailed provenance for all extracted knowledge, linking back to source quotes and confidence scores, which is crucial for knowledge validation.\n\n4. **Unified Service Architecture**: The integration with shared services (LLM, embeddings, configuration) promotes consistency across the Knowledge Garden ecosystem and reduces service-specific implementations.\n\n5. **Advanced Knowledge Organization**: The clustering and topic modeling capabilities allow for discovering relationships between concepts across documents, enabling higher-level knowledge synthesis.\n\n## 5 Cons\n\n1. **Significant Code Duplication**: There's substantial duplication across services - multiple Fuseki clients, repeated configuration loading, and redundant service implementations (notice the deprecation warnings in several files).\n\n2. **Incomplete Implementation**: Many components are placeholders or mocks (e.g., `api/app.py` returns mock data, the extraction pipeline runner has minimal implementation).\n\n3. **Underutilized Ontology**: Despite having a rich ontology (`kg_core.ttl`) with sophisticated relationships and properties, the extraction service barely uses it - missing opportunities for semantic reasoning.\n\n4. **Weak Error Handling**: The service lacks robust error handling, retry mechanisms, and graceful degradation when dependencies (Fuseki, LLM services) are unavailable.\n\n5. **Limited Cross-Document Analysis**: While the architecture supports it, the actual implementation of cross-conversation pattern detection and knowledge synthesis is minimal.\n\n## Reducing Duplication & Increasing Cleanliness\n\n### 1. **Consolidate Service Clients**\n```python\n# Instead of multiple Fuseki implementations, use a single shared client\n# shared/services/fuseki/client.py\nclass FusekiClient:\n    \"\"\"Single, well-tested Fuseki client for all services\"\"\"\n    \n    def __init__(self, config: ConfigSettings):\n        self.endpoint = config.fuseki_url\n        self.dataset = config.fuseki_dataset\n        # Implement connection pooling, retries, circuit breakers\n```\n\n### 2. **Centralize Pattern Extraction**\n```python\n# shared/extraction/patterns.py\nclass UnifiedPatternExtractor:\n    \"\"\"Single source of truth for pattern extraction logic\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        self.extractors = {\n            'cap': CAPExtractor(),\n            'insight': InsightExtractor(),\n            'question': QuestionExtractor(),\n            'tension': TensionExtractor(),\n        }\n    \n    async def extract_all(self, text: str, context: Dict) -> ExtractedPatterns:\n        # Centralized extraction with context awareness\n        pass\n```\n\n### 3. **Implement Dependency Injection**\n```python\n# Use dependency injection to avoid repeated service initialization\nfrom fastapi import Depends\n\nclass ServiceRegistry:\n    \"\"\"Central registry for all services\"\"\"\n    \n    def __init__(self, config: ConfigSettings):\n        self.fuseki = FusekiClient(config)\n        self.llm = LLMService(config)\n        self.embeddings = EmbeddingService(config)\n        self.evidence_tracker = EvidenceTracker(self.fuseki)\n\n# In API endpoints\nasync def extract(request: ExtractionRequest, services: ServiceRegistry = Depends(get_services)):\n    # Use injected services\n    pass\n```\n\n## Improving Extraction Quality\n\n### 1. **Leverage the Ontology**\n```python\nclass OntologyAwareExtractor:\n    \"\"\"Extraction that uses the knowledge graph ontology\"\"\"\n    \n    async def extract_with_reasoning(self, text: str) -> KnowledgeGraph:\n        # Use ontology relationships\n        concepts = await self.extract_concepts(text)\n        \n        # Apply ontology rules\n        for concept in concepts:\n            if concept.type == \"CAP\":\n                # Link to Values using kg:constitutedBy\n                values = await self.infer_values(concept)\n                \n            if concept.type == \"Tension\":\n                # Use kg:revealsValue to find underlying values\n                revealed_values = await self.analyze_tension_values(concept)\n        \n        return self.build_knowledge_graph(concepts)\n```\n\n### 2. **Implement Proper Cross-Conversation Analysis**\n```python\nclass CrossConversationAnalyzer:\n    \"\"\"Analyze patterns across multiple conversations\"\"\"\n    \n    async def analyze_conversations(self, conversations: List[Conversation]) -> PatternAnalysis:\n        # Extract patterns from each conversation\n        all_patterns = []\n        for conv in conversations:\n            patterns = await self.extract_patterns(conv)\n            all_patterns.extend(patterns)\n        \n        # Find recurring themes\n        recurring_caps = self.find_recurring_patterns(all_patterns, min_frequency=3)\n        \n        # Identify value clusters\n        value_clusters = self.cluster_by_values(recurring_caps)\n        \n        # Build cross-conversation insights\n        return PatternAnalysis(\n            recurring_patterns=recurring_caps,\n            value_systems=value_clusters,\n            evolution_timeline=self.track_pattern_evolution(all_patterns)\n        )\n```\n\n### 3. **Add Validation and Quality Checks**\n```python\nclass ExtractionValidator:\n    \"\"\"Validate extracted knowledge using Parliament decisions\"\"\"\n    \n    async def validate_extraction(self, extracted: ExtractedKnowledge) -> ValidatedKnowledge:\n        # Use Parliament for validation\n        for pattern in extracted.patterns:\n            decision = await self.parliament.validate_pattern(pattern)\n            if decision.confidence < 0.7:\n                pattern.needs_review = True\n        \n        # Check for contradictions\n        conflicts = await self.detect_conflicts(extracted.claims)\n        for conflict in conflicts:\n            resolution = await self.parliament.resolve_conflict(conflict)\n            \n        return ValidatedKnowledge(extracted, validation_results)\n```\n\n## How Knowledge Graphs Should Leverage the Ontology\n\nCurrently, the ontology is underutilized. Here's how it should be used:\n\n### 1. **Semantic Relationship Mapping**\n```sparql\n# Use ontology properties to create rich connections\nINSERT DATA {\n    # When extracting a CAP, also infer Values\n    ?cap a kg:CAP ;\n         kg:frequency ?freq ;\n         kg:emotionalCharge ?charge .\n    \n    ?value a kg:Value ;\n           kg:constitutedBy ?cap .\n    \n    ?agent kg:hasCAP ?cap ;\n           kg:hasValue ?value .\n}\n```\n\n### 2. **Knowledge Tier Management**\n```python\n# Properly classify knowledge using the tier system\nasync def classify_knowledge_tier(self, knowledge_item):\n    if knowledge_item.source == \"personal_conversation\":\n        if knowledge_item.evidence_count > 5:\n            return \"DIGESTED\"  # Well-validated personal knowledge\n        return \"PERSONAL\"\n    elif knowledge_item.is_referenced:\n        return \"PERIPHERAL\"\n```\n\n### 3. **Pattern Evolution Tracking**\n```python\n# Track how patterns evolve over time using temporal properties\nasync def track_pattern_evolution(self, pattern_id: str):\n    query = \"\"\"\n    SELECT ?timestamp ?frequency ?confidence\n    WHERE {\n        ?pattern kg:patternId ?pattern_id ;\n                 kg:extractedAt ?timestamp ;\n                 kg:frequency ?frequency ;\n                 kg:confidence ?confidence .\n    }\n    ORDER BY ?timestamp\n    \"\"\"\n    # Analyze how patterns strengthen or weaken over time\n```\n\n## Valuable Things You Can Do With This Code\n\n### 1. **Personal Knowledge Assistant**\nBuild an AI assistant that truly understands your thinking patterns, values, and insights over time, creating a personalized knowledge companion that can remind you of your own insights when relevant.\n\n### 2. **Conversation Intelligence Platform**\nAnalyze team conversations to extract collective intelligence, identify recurring challenges, and surface insights that might otherwise be lost in the flow of discussion.\n\n### 3. **Learning Pattern Analyzer**\nTrack how understanding evolves over time by analyzing the progression of questions  insights  new questions, helping optimize learning paths.\n\n### 4. **Value Alignment Tool**\nUse the CAP  Value mapping to help individuals and teams understand their core values based on what they consistently pay attention to and act upon.\n\n### 5. **Knowledge Synthesis Engine**\nAutomatically generate comprehensive reports, summaries, and visualizations that synthesize knowledge from multiple conversations, creating a \"second brain\" that grows smarter over time.\n\nThe key is to move from simple extraction to true knowledge understanding by fully leveraging the ontology's semantic richness and the system's cross-conversation analysis capabilities.",
            "citations": []
          }
        ],
        "updated_at": "2025-06-12T15:44:40.695073Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "1588b5c8-6929-4b59-9c9f-b2200ffb6236",
      "role": "human",
      "content": "does it make sense to put pattern extraction in shared when only the extractor will use it? how does that help? \nFor the rest of the improvements return a few scripts I can run in the root of this project that will inject, delete, and update the code in full so i can apply your recommendations \nIf you need to add things in shared, let me know, work to reduce code clutter and duplication and no need to be backward compatible \ncan you also give me a script that will print only the code as a text file from the folder i'min and under in all the subfolders with the title and location of all the files",
      "created_at": "2025-06-12T15:51:48.339248Z",
      "sequence_order": 2,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2025-06-12T15:51:48.334603Z",
            "stop_timestamp": "2025-06-12T15:51:48.334603Z",
            "flags": null,
            "type": "text",
            "text": "does it make sense to put pattern extraction in shared when only the extractor will use it? how does that help? \nFor the rest of the improvements return a few scripts I can run in the root of this project that will inject, delete, and update the code in full so i can apply your recommendations \nIf you need to add things in shared, let me know, work to reduce code clutter and duplication and no need to be backward compatible \ncan you also give me a script that will print only the code as a text file from the folder i'min and under in all the subfolders with the title and location of all the files",
            "citations": []
          }
        ],
        "updated_at": "2025-06-12T15:51:48.339248Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "cadbd0fb-4fa7-424e-9518-7c4c3e0b2b8a",
      "role": "assistant",
      "content": "The user makes a good point about pattern extraction - if only the extraction service uses it, it shouldn't go in shared. Let me reconsider the architecture.\n\nThey want:\n1. Scripts to implement the improvements\n2. A script to print all code files to a text file\n3. Focus on reducing duplication without worrying about backward compatibility\n\nLet me create practical scripts that will:\n1. Remove duplicate code\n2. Consolidate services\n3. Improve the extraction implementation\n4. Create a code printing utility\n\nI should focus on:\n- Removing the duplicate Fuseki service implementations\n- Removing the deprecated re-exports\n- Consolidating configuration\n- Improving the actual extraction implementation\n- Keeping pattern extraction in the extraction service where it belongsYou're absolutely right - pattern extraction is specific to the extraction service and shouldn't go in shared. Let me provide scripts to clean up the codebase and implement the improvements.\n\n## Script 1: Print All Code Files\n\n```bash\n#!/bin/bash\n# save as: print_code.sh\n\ncat << 'EOF' > print_all_code.py\n#!/usr/bin/env python3\nimport os\nimport sys\nfrom pathlib import Path\n\ndef print_code_files(root_dir=\".\"):\n    \"\"\"Print all code files with their paths\"\"\"\n    \n    # Extensions to include\n    code_extensions = {'.py', '.yaml', '.yml', '.sh', '.sql', '.ttl', '.md', '.txt', '.json'}\n    \n    # Directories to skip\n    skip_dirs = {'__pycache__', '.git', 'venv', 'env', '.env', 'node_modules', '.pytest_cache'}\n    \n    output_lines = []\n    \n    for root, dirs, files in os.walk(root_dir):\n        # Skip unwanted directories\n        dirs[:] = [d for d in dirs if d not in skip_dirs]\n        \n        for file in sorted(files):\n            file_path = Path(root) / file\n            \n            # Check if it's a code file\n            if file_path.suffix.lower() in code_extensions or file in ['Dockerfile', 'requirements.txt']:\n                rel_path = file_path.relative_to(root_dir)\n                \n                output_lines.append(f\"\\n{'='*80}\")\n                output_lines.append(f\"FILE: {rel_path}\")\n                output_lines.append(f\"{'='*80}\\n\")\n                \n                try:\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                        content = f.read()\n                        output_lines.append(content)\n                except Exception as e:\n                    output_lines.append(f\"ERROR reading file: {e}\")\n    \n    # Write to file\n    with open('all_code_output.txt', 'w', encoding='utf-8') as f:\n        f.write('\\n'.join(output_lines))\n    \n    print(f\"Code printed to all_code_output.txt\")\n    print(f\"Total files processed: {len([l for l in output_lines if l.startswith('FILE:')])}\")\n\nif __name__ == \"__main__\":\n    root = sys.argv[1] if len(sys.argv) > 1 else \".\"\n    print_code_files(root)\nEOF\n\nchmod +x print_all_code.py\npython3 print_all_code.py\n```\n\n## Script 2: Remove Duplication and Clean Up\n\n```bash\n#!/bin/bash\n# save as: cleanup_duplication.sh\n\ncat << 'EOF' > cleanup_duplication.py\n#!/usr/bin/env python3\nimport os\nimport shutil\n\ndef cleanup():\n    \"\"\"Remove duplicate code and deprecated files\"\"\"\n    \n    # Files to remove (deprecated re-exports and duplicates)\n    files_to_remove = [\n        # Remove deprecated re-export files\n        'services/extraction/config/prompt_loader.py',\n        'services/extraction/config/settings.py',\n        'shared/services/llm_service.py',  # deprecated re-export\n        \n        # Remove duplicate Fuseki implementations  \n        'services/extraction/extraction/services/fuseki_service.py',\n        'services/extraction/extraction/services/evidence_tracker.py',\n        \n        # Remove duplicate config files\n        'services/extraction/config/config.yml',\n        'services/extraction/config/settings.yaml',\n    ]\n    \n    for file_path in files_to_remove:\n        if os.path.exists(file_path):\n            os.remove(file_path)\n            print(f\"Removed: {file_path}\")\n    \n    # Create consolidated Fuseki service in shared if not exists\n    os.makedirs('shared/services/fuseki', exist_ok=True)\n    \n    print(\"\\nCleanup complete!\")\n\nif __name__ == \"__main__\":\n    cleanup()\nEOF\n\npython3 cleanup_duplication.py\n```\n\n## Script 3: Create Consolidated Fuseki Service\n\n```bash\n#!/bin/bash\n# save as: create_fuseki_service.sh\n\ncat << 'EOF' > create_fuseki_service.py\n#!/usr/bin/env python3\nimport os\n\n# Create the consolidated Fuseki service\nfuseki_service = '''\"\"\"Consolidated Fuseki service for all Knowledge Garden components.\"\"\"\nimport httpx\nimport logging\nfrom typing import Dict, List, Any, Optional\nimport json\nimport asyncio\nfrom urllib.parse import quote_plus\n\nlogger = logging.getLogger(__name__)\n\n\nclass FusekiClient:\n    \"\"\"Unified Fuseki client with connection pooling and retry logic.\"\"\"\n    \n    def __init__(\n        self, \n        endpoint: str,\n        username: Optional[str] = None,\n        password: Optional[str] = None,\n        timeout: float = 30.0,\n        max_retries: int = 3\n    ):\n        self.endpoint = endpoint.rstrip('/')\n        self.sparql_endpoint = f\"{self.endpoint}/sparql\"\n        self.update_endpoint = f\"{self.endpoint}/update\"\n        self.data_endpoint = f\"{self.endpoint}/data\"\n        \n        self.timeout = timeout\n        self.max_retries = max_retries\n        \n        # Setup authentication\n        self.auth = None\n        if username and password:\n            self.auth = httpx.BasicAuth(username, password)\n        \n        # Connection pool\n        self._client = httpx.AsyncClient(\n            timeout=timeout,\n            auth=self.auth,\n            limits=httpx.Limits(max_keepalive_connections=10, max_connections=20)\n        )\n        \n        logger.info(f\"Initialized Fuseki client for {endpoint}\")\n    \n    async def close(self):\n        \"\"\"Close the HTTP client.\"\"\"\n        await self._client.aclose()\n    \n    async def __aenter__(self):\n        return self\n    \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.close()\n    \n    async def health_check(self) -> Dict[str, Any]:\n        \"\"\"Check Fuseki health with retry logic.\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                # Try to ping the server\n                base_url = '/'.join(self.endpoint.split('/')[:-1])\n                response = await self._client.get(f\"{base_url}/$/ping\", timeout=5.0)\n                \n                if response.status_code == 200:\n                    # Test dataset access\n                    test_query = \"ASK { ?s ?p ?o }\"\n                    try:\n                        await self.query(test_query)\n                        return {\"status\": \"healthy\", \"message\": \"Fuseki operational\"}\n                    except:\n                        return {\"status\": \"degraded\", \"message\": \"Dataset inaccessible\"}\n                else:\n                    return {\"status\": \"unhealthy\", \"message\": f\"Status: {response.status_code}\"}\n                    \n            except Exception as e:\n                if attempt == self.max_retries - 1:\n                    logger.error(f\"Fuseki health check failed: {e}\")\n                    return {\"status\": \"unhealthy\", \"message\": str(e)}\n                await asyncio.sleep(2 ** attempt)\n    \n    async def query(self, sparql: str, timeout: Optional[float] = None) -> List[Dict[str, Any]]:\n        \"\"\"Execute SPARQL query with retry logic.\"\"\"\n        headers = {\n            \"Accept\": \"application/sparql-results+json\",\n            \"Content-Type\": \"application/x-www-form-urlencoded\"\n        }\n        \n        for attempt in range(self.max_retries):\n            try:\n                response = await self._client.post(\n                    self.sparql_endpoint,\n                    headers=headers,\n                    data={\"query\": sparql},\n                    timeout=timeout or self.timeout\n                )\n                response.raise_for_status()\n                \n                result = response.json()\n                return self._process_sparql_results(result)\n                \n            except httpx.HTTPStatusError as e:\n                logger.error(f\"SPARQL query failed (attempt {attempt + 1}): {e.response.text}\")\n                if attempt == self.max_retries - 1:\n                    raise\n            except Exception as e:\n                logger.error(f\"SPARQL query error (attempt {attempt + 1}): {e}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                    \n            await asyncio.sleep(2 ** attempt)\n    \n    async def update(self, sparql: str, timeout: Optional[float] = None) -> bool:\n        \"\"\"Execute SPARQL update with retry logic.\"\"\"\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        \n        for attempt in range(self.max_retries):\n            try:\n                response = await self._client.post(\n                    self.update_endpoint,\n                    headers=headers,\n                    data={\"update\": sparql},\n                    timeout=timeout or self.timeout\n                )\n                response.raise_for_status()\n                return True\n                \n            except httpx.HTTPStatusError as e:\n                logger.error(f\"SPARQL update failed (attempt {attempt + 1}): {e.response.text}\")\n                if attempt == self.max_retries - 1:\n                    raise\n            except Exception as e:\n                logger.error(f\"SPARQL update error (attempt {attempt + 1}): {e}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                    \n            await asyncio.sleep(2 ** attempt)\n    \n    def _process_sparql_results(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Process SPARQL JSON results into clean dictionaries.\"\"\"\n        if \"results\" not in result or \"bindings\" not in result[\"results\"]:\n            return []\n        \n        processed = []\n        for binding in result[\"results\"][\"bindings\"]:\n            row = {}\n            for var, value in binding.items():\n                if value[\"type\"] == \"uri\":\n                    row[var] = value[\"value\"]\n                elif value[\"type\"] == \"literal\":\n                    row[var] = value[\"value\"]\n                    if \"datatype\" in value:\n                        row[f\"{var}_datatype\"] = value[\"datatype\"]\n                    if \"xml:lang\" in value:\n                        row[f\"{var}_lang\"] = value[\"xml:lang\"]\n                elif value[\"type\"] == \"bnode\":\n                    row[var] = f\"_:{value['value']}\"\n                else:\n                    row[var] = value[\"value\"]\n            processed.append(row)\n        \n        return processed\n    \n    @staticmethod\n    def escape_literal(s: str) -> str:\n        \"\"\"Escape a string for use in SPARQL literals.\"\"\"\n        if s is None:\n            return \"\"\n        return str(s).replace('\\\\\\\\', '\\\\\\\\\\\\\\\\').replace('\"', '\\\\\\\\\"').replace('\\\\n', '\\\\\\\\n').replace('\\\\r', '\\\\\\\\r')\n\n\n# Singleton instance management\n_client_instances = {}\n\ndef get_fuseki_client(\n    endpoint: str,\n    username: Optional[str] = None,\n    password: Optional[str] = None\n) -> FusekiClient:\n    \"\"\"Get or create a Fuseki client instance.\"\"\"\n    key = (endpoint, username, password)\n    if key not in _client_instances:\n        _client_instances[key] = FusekiClient(endpoint, username, password)\n    return _client_instances[key]\n\nasync def close_all_clients():\n    \"\"\"Close all client instances.\"\"\"\n    for client in _client_instances.values():\n        await client.close()\n    _client_instances.clear()\n'''\n\n# Write the file\nos.makedirs('shared/services/fuseki', exist_ok=True)\nwith open('shared/services/fuseki/__init__.py', 'w') as f:\n    f.write('\"\"\"Fuseki service for Knowledge Garden.\"\"\"\\n')\n    f.write('from .client import FusekiClient, get_fuseki_client, close_all_clients\\n\\n')\n    f.write('__all__ = [\"FusekiClient\", \"get_fuseki_client\", \"close_all_clients\"]\\n')\n\nwith open('shared/services/fuseki/client.py', 'w') as f:\n    f.write(fuseki_service)\n\nprint(\"Created consolidated Fuseki service in shared/services/fuseki/\")\nEOF\n\npython3 create_fuseki_service.py\n```\n\n## Script 4: Improve Extraction Implementation\n\n```bash\n#!/bin/bash\n# save as: improve_extraction.sh\n\ncat << 'EOF' > improve_extraction.py\n#!/usr/bin/env python3\nimport os\n\n# Update the main extraction API to actually work\napi_app = '''\"\"\"FastAPI application for extraction service.\"\"\"\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nimport logging\nfrom typing import Dict, List, Any, Optional\nimport os\nfrom uuid import uuid4\n\nfrom extraction.models.api import (\n    ExtractionRequest, ExtractionResponse, BatchExtractionRequest, \n    BatchExtractionResponse, ConceptSearchRequest, ConceptSearchResponse\n)\nfrom extraction.pipeline.runner import ExtractionPipeline\nfrom extraction.services.evidence_tracker import EvidenceTracker\nfrom shared.services.fuseki import get_fuseki_client\nfrom shared.config.settings import get_settings\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize app\napp = FastAPI(\n    title=\"Knowledge Garden Extraction Service\",\n    version=\"3.0.0\",\n    description=\"Extract structured knowledge with embeddings and clustering\"\n)\n\n# Add CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Dependencies\ndef get_pipeline():\n    \"\"\"Get extraction pipeline instance.\"\"\"\n    settings = get_settings()\n    fuseki = get_fuseki_client(\n        f\"{settings.fuseki_url}/{settings.fuseki_dataset}\",\n        settings.fuseki_user,\n        settings.fuseki_password\n    )\n    evidence_tracker = EvidenceTracker(fuseki)\n    \n    config = {\n        \"document\": {\n            \"min_sentence_length\": 10,\n            \"max_sentence_length\": 500\n        },\n        \"embedding\": {\n            \"model\": settings.embedding_model,\n            \"batch_size\": settings.batch_size\n        },\n        \"extraction\": {\n            \"extract_entities\": True,\n            \"extract_caps\": True,\n            \"extract_values\": True,\n            \"extract_insights\": True,\n            \"extract_tensions\": True,\n            \"extract_questions\": True\n        },\n        \"clustering\": {\n            \"min_cluster_size\": 100,\n            \"min_samples\": 10,\n            \"use_umap\": True,\n            \"umap_components\": 5\n        }\n    }\n    \n    return ExtractionPipeline(config, evidence_tracker)\n\n# Health check\n@app.get(\"/health\")\nasync def health():\n    \"\"\"Health check endpoint.\"\"\"\n    try:\n        settings = get_settings()\n        fuseki = get_fuseki_client(\n            f\"{settings.fuseki_url}/{settings.fuseki_dataset}\",\n            settings.fuseki_user,\n            settings.fuseki_password\n        )\n        \n        fuseki_health = await fuseki.health_check()\n        \n        # Check embedding service\n        from shared.services.embeddings.service import get_embedding_service\n        embedding_service = get_embedding_service()\n        embedding_health = await embedding_service.health_check()\n        \n        return {\n            \"status\": \"healthy\" if fuseki_health[\"status\"] == \"healthy\" else \"degraded\",\n            \"service\": \"extraction\",\n            \"version\": \"3.0.0\",\n            \"dependencies\": {\n                \"fuseki\": fuseki_health,\n                \"embeddings\": embedding_health\n            }\n        }\n    except Exception as e:\n        logger.error(f\"Health check failed: {e}\")\n        return {\n            \"status\": \"unhealthy\",\n            \"service\": \"extraction\",\n            \"version\": \"3.0.0\",\n            \"error\": str(e)\n        }\n\n@app.post(\"/extract\", response_model=ExtractionResponse)\nasync def extract(\n    request: ExtractionRequest,\n    pipeline: ExtractionPipeline = Depends(get_pipeline)\n):\n    \"\"\"Extract knowledge from text.\"\"\"\n    try:\n        # Add request ID if not present\n        if not hasattr(request, 'id'):\n            request.id = uuid4()\n        \n        response = await pipeline.extract(request)\n        return response\n        \n    except Exception as e:\n        logger.error(f\"Extraction failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/extract/batch\", response_model=BatchExtractionResponse)\nasync def extract_batch(\n    request: BatchExtractionRequest,\n    pipeline: ExtractionPipeline = Depends(get_pipeline)\n):\n    \"\"\"Extract knowledge from multiple documents.\"\"\"\n    try:\n        # Add IDs to requests without them\n        for doc in request.documents:\n            if not hasattr(doc, 'id'):\n                doc.id = uuid4()\n        \n        response = await pipeline.extract_batch(\n            request.documents,\n            request.enable_cross_document_clustering\n        )\n        return response\n        \n    except Exception as e:\n        logger.error(f\"Batch extraction failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/search/concepts\", response_model=ConceptSearchResponse)\nasync def search_concepts(\n    request: ConceptSearchRequest,\n    pipeline: ExtractionPipeline = Depends(get_pipeline)\n):\n    \"\"\"Search for similar concepts.\"\"\"\n    try:\n        # Use embedding service to find similar concepts\n        from shared.services.embeddings.service import get_embedding_service\n        embedding_service = get_embedding_service()\n        \n        # This would need to be implemented with actual concept storage\n        # For now, return empty results\n        return ConceptSearchResponse(\n            query=request.query,\n            results=[],\n            total_found=0,\n            duration_ms=0.0\n        )\n        \n    except Exception as e:\n        logger.error(f\"Concept search failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8001)\n'''\n\n# Write the improved API\nwith open('services/extraction/api/app.py', 'w') as f:\n    f.write(api_app)\n\n# Create improved evidence tracker using shared Fuseki\nevidence_tracker = '''\"\"\"Evidence tracker using shared Fuseki service.\"\"\"\nimport logging\nfrom typing import Dict, List, Any, Optional\nfrom uuid import uuid4\nfrom datetime import datetime\n\nfrom shared.services.fuseki import FusekiClient\n\nlogger = logging.getLogger(__name__)\n\n\nclass EvidenceTracker:\n    \"\"\"Tracks provenance information for extracted knowledge.\"\"\"\n    \n    def __init__(self, fuseki_client: FusekiClient):\n        \"\"\"Initialize with shared Fuseki client.\"\"\"\n        self.fuseki = fuseki_client\n        self.prefix_map = {\n            \"kg\": \"urn:knowledgegarden:ontology#\",\n            \"kgi\": \"urn:knowledgegarden:instance#\",\n            \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\",\n            \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n            \"xsd\": \"http://www.w3.org/2001/XMLSchema#\",\n        }\n    \n    async def record_extraction_event(\n        self, \n        document_id: str, \n        metadata: Dict[str, Any]\n    ) -> str:\n        \"\"\"Record an extraction event.\"\"\"\n        event_id = f\"event_{uuid4().hex[:8]}\"\n        \n        # Build SPARQL update\n        prefixes = self._build_prefixes()\n        timestamp = datetime.now().isoformat()\n        \n        sparql = f\"\"\"\n        {prefixes}\n        \n        INSERT DATA {{\n            kgi:{event_id} a kg:ExtractionEvent ;\n                kg:timestamp \"{timestamp}\"^^xsd:dateTime ;\n                kg:documentId \"{document_id}\" ;\n                kg:extractedAt \"{timestamp}\"^^xsd:dateTime .\n        }}\n        \"\"\"\n        \n        try:\n            await self.fuseki.update(sparql)\n            logger.info(f\"Recorded extraction event: {event_id}\")\n            return event_id\n        except Exception as e:\n            logger.error(f\"Failed to record extraction event: {e}\")\n            # Return event ID anyway for resilience\n            return event_id\n    \n    async def record_concept(\n        self,\n        concept_id: str,\n        label: str,\n        concept_type: str,\n        document_id: str,\n        confidence: float,\n        properties: Optional[Dict[str, Any]] = None\n    ) -> bool:\n        \"\"\"Record a concept with evidence.\"\"\"\n        prefixes = self._build_prefixes()\n        timestamp = datetime.now().isoformat()\n        \n        # Build property statements\n        property_statements = []\n        if properties:\n            for key, value in properties.items():\n                if isinstance(value, str):\n                    prop_stmt = f'kg:{key} \"{self.fuseki.escape_literal(value)}\"'\n                elif isinstance(value, (int, float)):\n                    prop_stmt = f'kg:{key} {value}'\n                elif isinstance(value, bool):\n                    prop_stmt = f'kg:{key} {str(value).lower()}'\n                else:\n                    continue\n                property_statements.append(prop_stmt)\n        \n        props = \" ;\\\\n                \".join(property_statements) if property_statements else \"\"\n        if props:\n            props = \" ;\\\\n                \" + props\n        \n        sparql = f\"\"\"\n        {prefixes}\n        \n        INSERT DATA {{\n            kgi:{concept_id} a kg:Concept ;\n                rdfs:label \"{self.fuseki.escape_literal(label)}\" ;\n                kg:conceptType \"{concept_type}\" ;\n                kg:confidence {confidence} ;\n                kg:extractedFrom kgi:doc_{document_id} ;\n                kg:extractedAt \"{timestamp}\"^^xsd:dateTime{props} .\n        }}\n        \"\"\"\n        \n        try:\n            await self.fuseki.update(sparql)\n            logger.debug(f\"Recorded concept: {concept_id}\")\n            return True\n        except Exception as e:\n            logger.error(f\"Failed to record concept: {e}\")\n            return False\n    \n    async def store_evidence(self, evidence: Any, target_uri: str) -> bool:\n        \"\"\"Store evidence for a knowledge element.\"\"\"\n        # This would store the actual evidence\n        # Implementation depends on the Evidence model structure\n        return True\n    \n    def _build_prefixes(self) -> str:\n        \"\"\"Build SPARQL prefix declarations.\"\"\"\n        return \"\\\\n\".join([\n            f\"PREFIX {prefix}: <{uri}>\"\n            for prefix, uri in self.prefix_map.items()\n        ])\n'''\n\n# Write the improved evidence tracker\nos.makedirs('services/extraction/extraction/services', exist_ok=True)\nwith open('services/extraction/extraction/services/evidence_tracker.py', 'w') as f:\n    f.write(evidence_tracker)\n\nprint(\"Improved extraction implementation complete!\")\nEOF\n\npython3 improve_extraction.py\n```\n\n## Script 5: Update Imports to Use Shared Services\n\n```bash\n#!/bin/bash\n# save as: update_imports.sh\n\ncat << 'EOF' > update_imports.py\n#!/usr/bin/env python3\nimport os\nimport re\n\ndef update_imports():\n    \"\"\"Update all imports to use shared services\"\"\"\n    \n    # Files to update\n    files_to_update = []\n    \n    # Find all Python files\n    for root, dirs, files in os.walk('services/extraction'):\n        # Skip __pycache__ directories\n        dirs[:] = [d for d in dirs if d != '__pycache__']\n        \n        for file in files:\n            if file.endswith('.py'):\n                files_to_update.append(os.path.join(root, file))\n    \n    # Import replacements\n    replacements = [\n        # Fuseki imports\n        (r'from extraction\\.services\\.fuseki_service import FusekiService',\n         'from shared.services.fuseki import FusekiClient as FusekiService'),\n        (r'from extraction\\.pipeline\\.fuseki_service import FusekiService',\n         'from shared.services.fuseki import FusekiClient as FusekiService'),\n        (r'from services\\.fuseki import FusekiService',\n         'from shared.services.fuseki import FusekiClient as FusekiService'),\n        \n        # LLM imports\n        (r'from extraction\\.services\\.llm_service import LLMService',\n         'from shared.services.llm.service import LLMService'),\n        (r'from shared\\.services\\.llm_service import.*',\n         'from shared.services.llm.service import LLMService, get_llm_service'),\n        \n        # Config imports\n        (r'from extraction\\.config\\.settings import.*',\n         'from shared.config.settings import get_settings'),\n        (r'from services\\.extraction\\.config\\.settings import.*',\n         'from shared.config.settings import get_settings'),\n        \n        # Prompt loader imports\n        (r'from extraction\\.config\\.prompt_loader import.*',\n         'from shared.prompts.loader import get_prompt_loader'),\n        (r'from services\\.extraction\\.config\\.prompt_loader import.*',\n         'from shared.prompts.loader import get_prompt_loader'),\n    ]\n    \n    # Process each file\n    for file_path in files_to_update:\n        try:\n            with open(file_path, 'r') as f:\n                content = f.read()\n            \n            original_content = content\n            \n            # Apply replacements\n            for pattern, replacement in replacements:\n                content = re.sub(pattern, replacement, content)\n            \n            # Only write if changed\n            if content != original_content:\n                with open(file_path, 'w') as f:\n                    f.write(content)\n                print(f\"Updated imports in: {file_path}\")\n                \n        except Exception as e:\n            print(f\"Error updating {file_path}: {e}\")\n    \n    print(\"\\nImport updates complete!\")\n\nif __name__ == \"__main__\":\n    update_imports()\nEOF\n\npython3 update_imports.py\n```\n\n## Script 6: Add Ontology-Aware Extraction\n\n```bash\n#!/bin/bash\n# save as: add_ontology_extraction.sh\n\ncat << 'EOF' > add_ontology_extraction.py\n#!/usr/bin/env python3\nimport os\n\n# Create ontology-aware concept extractor\nontology_extractor = '''\"\"\"Ontology-aware concept extraction.\"\"\"\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom uuid import uuid4\n\nfrom extraction.pipeline.base import PipelineStage\nfrom extraction.models.documents import DocumentAnalysis\nfrom extraction.models.concepts import ExtractedConcept, ConceptType, EvidenceModel\nfrom extraction.extractors.pattern_extractor import PatternExtractor\nfrom extraction.extractors.entity_extractor import EntityExtractor\nfrom shared.services.llm.service import get_llm_service\n\nlogger = logging.getLogger(__name__)\n\n\nclass OntologyAwareConceptExtractor(PipelineStage[DocumentAnalysis, DocumentAnalysis]):\n    \"\"\"Extract concepts with ontology awareness.\"\"\"\n    \n    async def _initialize(self) -> None:\n        \"\"\"Initialize extractors and ontology knowledge.\"\"\"\n        self.pattern_extractor = PatternExtractor()\n        self.entity_extractor = EntityExtractor()\n        self.llm_service = get_llm_service()\n        \n        # Load ontology relationships\n        self.ontology_relationships = {\n            \"CAP\": {\n                \"creates\": [\"Value\"],\n                \"reveals\": [\"Pattern\", \"Behavior\"],\n                \"frequency_threshold\": 3  # CAPs need 3+ occurrences to become Values\n            },\n            \"Tension\": {\n                \"reveals\": [\"Value\"],\n                \"between\": [\"Concept\", \"Concept\"],\n                \"resolution\": [\"Strategy\", \"Practice\"]\n            },\n            \"Insight\": {\n                \"transforms\": [\"Understanding\"],\n                \"leads_to\": [\"Question\", \"Action\"],\n                \"requires\": [\"Evidence\", \"Reflection\"]\n            },\n            \"Question\": {\n                \"explores\": [\"Unknown\", \"Edge\"],\n                \"persistence\": [\"CONTEXT_SPECIFIC\", \"PERSISTENT\"],\n                \"depth\": [\"SURFACE\", \"DEEP\", \"EXISTENTIAL\"]\n            }\n        }\n        \n        # Extraction configuration\n        self.config = self.config or {}\n        self.extract_entities = self.config.get(\"extract_entities\", True)\n        self.extract_caps = self.config.get(\"extract_caps\", True)\n        self.extract_values = self.config.get(\"extract_values\", True)\n        self.extract_insights = self.config.get(\"extract_insights\", True)\n        self.extract_tensions = self.config.get(\"extract_tensions\", True)\n        self.extract_questions = self.config.get(\"extract_questions\", True)\n    \n    async def process(self, doc: DocumentAnalysis) -> DocumentAnalysis:\n        \"\"\"Extract concepts with ontology awareness.\"\"\"\n        \n        # Track CAP frequencies for Value inference\n        cap_frequencies = {}\n        \n        for para in doc.paragraphs:\n            for sent in para.sentences:\n                # Extract all patterns\n                patterns = self.pattern_extractor.extract_all(sent.text)\n                \n                # Process CAPs with Value inference\n                if self.extract_caps and patterns.get('caps'):\n                    para.has_caps = True\n                    for cap in patterns['caps']:\n                        cap_text = cap['pattern']\n                        \n                        # Track frequency\n                        cap_key = self._normalize_cap(cap_text)\n                        cap_frequencies[cap_key] = cap_frequencies.get(cap_key, 0) + 1\n                        \n                        # Create CAP concept\n                        concept = self._create_cap_concept(\n                            cap, sent, doc.document_uri\n                        )\n                        \n                        # Add ontology relationships\n                        concept.attributes[\"reveals\"] = \"behavioral_pattern\"\n                        concept.attributes[\"frequency\"] = cap_frequencies[cap_key]\n                        \n                        # Check if this CAP should create a Value\n                        if cap_frequencies[cap_key] >= self.ontology_relationships[\"CAP\"][\"frequency_threshold\"]:\n                            concept.attributes[\"creates_value\"] = True\n                            \n                            # Infer Value from recurring CAP\n                            if self.extract_values:\n                                value_concept = await self._infer_value_from_cap(\n                                    cap_text, cap_frequencies[cap_key], sent, doc\n                                )\n                                if value_concept:\n                                    sent.concepts.append(value_concept)\n                                    para.concepts.append(value_concept)\n                                    doc.concepts.append(value_concept)\n                        \n                        sent.concepts.append(concept)\n                        para.concepts.append(concept)\n                        doc.concepts.append(concept)\n                \n                # Process Tensions with Value revelation\n                if self.extract_tensions and patterns.get('tensions'):\n                    para.has_tensions = True\n                    for tension in patterns['tensions']:\n                        concept = self._create_tension_concept(\n                            tension, sent, doc.document_uri\n                        )\n                        \n                        # Analyze what values this tension reveals\n                        revealed_values = await self._analyze_tension_values(\n                            tension['pole1'], tension['pole2']\n                        )\n                        if revealed_values:\n                            concept.attributes[\"reveals_values\"] = revealed_values\n                        \n                        sent.concepts.append(concept)\n                        para.concepts.append(concept)\n                        doc.concepts.append(concept)\n                \n                # Process Insights with transformation tracking\n                if self.extract_insights and patterns.get('insights'):\n                    para.has_insights = True\n                    for insight in patterns['insights']:\n                        concept = self._create_insight_concept(\n                            insight, sent, doc.document_uri\n                        )\n                        \n                        # Analyze transformation\n                        transformation = await self._analyze_insight_transformation(\n                            insight['realization']\n                        )\n                        if transformation:\n                            concept.attributes[\"before_understanding\"] = transformation.get(\"before\", \"\")\n                            concept.attributes[\"after_understanding\"] = transformation.get(\"after\", \"\")\n                            concept.attributes[\"trigger\"] = transformation.get(\"trigger\", \"\")\n                        \n                        sent.concepts.append(concept)\n                        para.concepts.append(concept)\n                        doc.concepts.append(concept)\n                \n                # Process Questions with depth analysis\n                if self.extract_questions and patterns.get('questions'):\n                    for question in patterns['questions']:\n                        concept = self._create_question_concept(\n                            question, sent, doc.document_uri\n                        )\n                        \n                        # Analyze question depth and persistence\n                        analysis = await self._analyze_question_depth(\n                            question['text']\n                        )\n                        concept.attributes[\"depth\"] = analysis.get(\"depth\", \"SURFACE\")\n                        concept.attributes[\"persistence\"] = analysis.get(\"persistence\", \"CONTEXT_SPECIFIC\")\n                        concept.attributes[\"growth_edge\"] = analysis.get(\"growth_edge\", \"\")\n                        \n                        sent.concepts.append(concept)\n                        para.concepts.append(concept)\n                        doc.concepts.append(concept)\n        \n        # Update counts and calculate concept diversity\n        doc.total_concepts = len(doc.concepts)\n        for concept in doc.concepts:\n            concept_type = concept.concept_type\n            doc.concept_counts[concept_type] = doc.concept_counts.get(concept_type, 0) + 1\n        \n        # Calculate concept diversity based on type distribution\n        if doc.concepts:\n            type_counts = list(doc.concept_counts.values())\n            total = sum(type_counts)\n            if total > 0:\n                # Simple diversity metric: entropy of concept type distribution\n                entropy = 0\n                for count in type_counts:\n                    if count > 0:\n                        p = count / total\n                        entropy -= p * (p if p > 0 else 0)\n                doc.concept_diversity = min(entropy, 1.0)\n        \n        return doc\n    \n    def _normalize_cap(self, cap_text: str) -> str:\n        \"\"\"Normalize CAP text for frequency counting.\"\"\"\n        # Simple normalization - could be enhanced\n        return cap_text.lower().strip()\n    \n    def _create_cap_concept(\n        self, cap: Dict[str, Any], sent: Any, doc_uri: str\n    ) -> ExtractedConcept:\n        \"\"\"Create a CAP concept.\"\"\"\n        return ExtractedConcept(\n            uri=f\"urn:kg:cap:{uuid4()}\",\n            label=cap['pattern'][:100],\n            text=cap['pattern'],\n            concept_type=ConceptType.CAP,\n            cap_context=cap.get('context'),\n            cap_action=cap.get('action'),\n            source_sentence_id=sent.id,\n            source_document_uri=doc_uri,\n            evidence=EvidenceModel(\n                quote=cap['pattern'],\n                source_text=sent.text,\n                position=sent.position,\n                confidence=cap.get('confidence', 0.9)\n            ),\n            extraction_method=\"pattern\",\n            confidence=cap.get('confidence', 0.9)\n        )\n    \n    def _create_tension_concept(\n        self, tension: Dict[str, Any], sent: Any, doc_uri: str\n    ) -> ExtractedConcept:\n        \"\"\"Create a Tension concept.\"\"\"\n        return ExtractedConcept(\n            uri=f\"urn:kg:tension:{uuid4()}\",\n            label=f\"{tension['pole1']} vs {tension['pole2']}\",\n            text=tension.get('source_text', sent.text),\n            concept_type=ConceptType.TENSION,\n            tension_pole1=tension['pole1'],\n            tension_pole2=tension['pole2'],\n            source_sentence_id=sent.id,\n            source_document_uri=doc_uri,\n            evidence=EvidenceModel(\n                quote=tension.get('source_text', sent.text),\n                source_text=sent.text,\n                position=sent.position,\n                confidence=tension.get('confidence', 0.9)\n            ),\n            extraction_method=\"pattern\",\n            confidence=tension.get('confidence', 0.9)\n        )\n    \n    def _create_insight_concept(\n        self, insight: Dict[str, Any], sent: Any, doc_uri: str\n    ) -> ExtractedConcept:\n        \"\"\"Create an Insight concept.\"\"\"\n        return ExtractedConcept(\n            uri=f\"urn:kg:insight:{uuid4()}\",\n            label=insight['realization'][:100],\n            text=insight['realization'],\n            concept_type=ConceptType.INSIGHT,\n            source_sentence_id=sent.id,\n            source_document_uri=doc_uri,\n            evidence=EvidenceModel(\n                quote=insight['realization'],\n                source_text=sent.text,\n                position=sent.position,\n                confidence=insight.get('confidence', 0.9)\n            ),\n            extraction_method=\"pattern\",\n            confidence=insight.get('confidence', 0.9)\n        )\n    \n    def _create_question_concept(\n        self, question: Dict[str, Any], sent: Any, doc_uri: str\n    ) -> ExtractedConcept:\n        \"\"\"Create a Question concept.\"\"\"\n        return ExtractedConcept(\n            uri=f\"urn:kg:question:{uuid4()}\",\n            label=question['text'][:100],\n            text=question['text'],\n            concept_type=ConceptType.QUESTION,\n            source_sentence_id=sent.id,\n            source_document_uri=doc_uri,\n            evidence=EvidenceModel(\n                quote=question['text'],\n                source_text=sent.text,\n                position=sent.position,\n                confidence=question.get('confidence', 0.9)\n            ),\n            extraction_method=\"pattern\",\n            confidence=question.get('confidence', 0.9)\n        )\n    \n    async def _infer_value_from_cap(\n        self, cap_text: str, frequency: int, sent: Any, doc: DocumentAnalysis\n    ) -> Optional[ExtractedConcept]:\n        \"\"\"Infer a Value from a recurring CAP.\"\"\"\n        # Use LLM to infer the underlying value\n        prompt = f\"\"\"\n        A person has repeatedly expressed this pattern of attention/behavior:\n        \"{cap_text}\"\n        (Occurred {frequency} times)\n        \n        What core value or source of meaning does this pattern reveal?\n        Respond with just the value name (2-4 words).\n        \"\"\"\n        \n        try:\n            response = await self.llm_service.generate(\n                prompt=prompt,\n                temperature=0.3,\n                max_tokens=50\n            )\n            \n            value_name = response[\"text\"].strip()\n            \n            return ExtractedConcept(\n                uri=f\"urn:kg:value:{uuid4()}\",\n                label=value_name,\n                text=f\"Value inferred from pattern: {cap_text}\",\n                concept_type=ConceptType.VALUE,\n                source_sentence_id=sent.id,\n                source_document_uri=doc.document_uri,\n                evidence=EvidenceModel(\n                    quote=cap_text,\n                    source_text=f\"Inferred from {frequency} occurrences\",\n                    position=sent.position,\n                    confidence=0.8\n                ),\n                extraction_method=\"inference\",\n                confidence=0.8,\n                attributes={\n                    \"constituted_by\": cap_text,\n                    \"occurrence_count\": frequency\n                }\n            )\n        except Exception as e:\n            logger.error(f\"Failed to infer value: {e}\")\n            return None\n    \n    async def _analyze_tension_values(\n        self, pole1: str, pole2: str\n    ) -> Optional[List[str]]:\n        \"\"\"Analyze what values a tension reveals.\"\"\"\n        # This would use LLM or rules to infer values\n        # Simplified for now\n        return [f\"balance_between_{pole1}_and_{pole2}\"]\n    \n    async def _analyze_insight_transformation(\n        self, realization: str\n    ) -> Optional[Dict[str, str]]:\n        \"\"\"Analyze the transformation an insight represents.\"\"\"\n        # This would use LLM to understand before/after states\n        # Simplified for now\n        return {\n            \"before\": \"Previous understanding\",\n            \"after\": realization,\n            \"trigger\": \"Reflection\"\n        }\n    \n    async def _analyze_question_depth(\n        self, question: str\n    ) -> Dict[str, str]:\n        \"\"\"Analyze the depth and nature of a question.\"\"\"\n        # Simple heuristics - could use LLM\n        depth = \"SURFACE\"\n        if any(word in question.lower() for word in [\"why\", \"how\", \"meaning\", \"purpose\"]):\n            depth = \"DEEP\"\n        if any(word in question.lower() for word in [\"existence\", \"consciousness\", \"reality\"]):\n            depth = \"EXISTENTIAL\"\n        \n        persistence = \"CONTEXT_SPECIFIC\"\n        if depth in [\"DEEP\", \"EXISTENTIAL\"]:\n            persistence = \"PERSISTENT\"\n        \n        return {\n            \"depth\": depth,\n            \"persistence\": persistence,\n            \"growth_edge\": \"Understanding limits\"\n        }\n'''\n\n# Write the ontology-aware extractor\nextractor_path = 'services/extraction/extraction/pipeline/stages/ontology_concept_extractor.py'\nos.makedirs(os.path.dirname(extractor_path), exist_ok=True)\nwith open(extractor_path, 'w') as f:\n    f.write(ontology_extractor)\n\n# Update the pipeline to use the new extractor\npipeline_update = '''\n# In extraction/pipeline/runner.py, update the import:\n# from extraction.pipeline.stages.concept_extractor import ConceptExtractor\n# to:\n# from extraction.pipeline.stages.ontology_concept_extractor import OntologyAwareConceptExtractor as ConceptExtractor\n'''\n\nprint(f\"Created ontology-aware concept extractor at {extractor_path}\")\nprint(\"Update the pipeline runner to use OntologyAwareConceptExtractor\")\nEOF\n\npython3 add_ontology_extraction.py\n```\n\nRun these scripts in order:\n```bash\nchmod +x *.sh\n./print_code.sh              # Print all code to a file\n./cleanup_duplication.sh     # Remove duplicate files\n./create_fuseki_service.sh   # Create consolidated Fuseki service\n./improve_extraction.sh      # Improve extraction implementation\n./update_imports.sh          # Update imports to use shared services\n./add_ontology_extraction.sh # Add ontology-aware extraction\n```\n\nThese scripts will:\n1. Create a utility to print all code files\n2. Remove duplicate code and deprecated re-exports\n3. Create a single, robust Fuseki service in `shared/services/fuseki`\n4. Update the extraction API to actually work with proper dependency injection\n5. Update all imports to use the shared services\n6. Add ontology-aware extraction that leverages the knowledge graph structure\n\nThe key improvements:\n- Single Fuseki client with retry logic and connection pooling\n- Proper error handling and health checks\n- Ontology-aware extraction that infers Values from CAPs\n- Cleaner architecture without backward compatibility cruft\n- Pattern extraction stays in the extraction service where it belongs",
      "created_at": "2025-06-12T15:55:12.909000Z",
      "sequence_order": 3,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2025-06-12T15:51:51.050369Z",
            "stop_timestamp": "2025-06-12T15:51:56.815607Z",
            "flags": null,
            "type": "thinking",
            "thinking": "The user makes a good point about pattern extraction - if only the extraction service uses it, it shouldn't go in shared. Let me reconsider the architecture.\n\nThey want:\n1. Scripts to implement the improvements\n2. A script to print all code files to a text file\n3. Focus on reducing duplication without worrying about backward compatibility\n\nLet me create practical scripts that will:\n1. Remove duplicate code\n2. Consolidate services\n3. Improve the extraction implementation\n4. Create a code printing utility\n\nI should focus on:\n- Removing the duplicate Fuseki service implementations\n- Removing the deprecated re-exports\n- Consolidating configuration\n- Improving the actual extraction implementation\n- Keeping pattern extraction in the extraction service where it belongs",
            "summaries": [
              {
                "summary": "Thinking about extracting, modifying, and documenting project code structure"
              },
              {
                "summary": "Reconsidering architecture and preparing implementation scripts"
              },
              {
                "summary": "Crafting scripts for improvements and file extraction"
              },
              {
                "summary": "Strategized code refactoring and architectural improvements for extraction service."
              }
            ],
            "cut_off": false
          },
          {
            "start_timestamp": "2025-06-12T15:51:57.314975Z",
            "stop_timestamp": "2025-06-12T15:55:12.175748Z",
            "flags": null,
            "type": "text",
            "text": "You're absolutely right - pattern extraction is specific to the extraction service and shouldn't go in shared. Let me provide scripts to clean up the codebase and implement the improvements.\n\n## Script 1: Print All Code Files\n\n```bash\n#!/bin/bash\n# save as: print_code.sh\n\ncat << 'EOF' > print_all_code.py\n#!/usr/bin/env python3\nimport os\nimport sys\nfrom pathlib import Path\n\ndef print_code_files(root_dir=\".\"):\n    \"\"\"Print all code files with their paths\"\"\"\n    \n    # Extensions to include\n    code_extensions = {'.py', '.yaml', '.yml', '.sh', '.sql', '.ttl', '.md', '.txt', '.json'}\n    \n    # Directories to skip\n    skip_dirs = {'__pycache__', '.git', 'venv', 'env', '.env', 'node_modules', '.pytest_cache'}\n    \n    output_lines = []\n    \n    for root, dirs, files in os.walk(root_dir):\n        # Skip unwanted directories\n        dirs[:] = [d for d in dirs if d not in skip_dirs]\n        \n        for file in sorted(files):\n            file_path = Path(root) / file\n            \n            # Check if it's a code file\n            if file_path.suffix.lower() in code_extensions or file in ['Dockerfile', 'requirements.txt']:\n                rel_path = file_path.relative_to(root_dir)\n                \n                output_lines.append(f\"\\n{'='*80}\")\n                output_lines.append(f\"FILE: {rel_path}\")\n                output_lines.append(f\"{'='*80}\\n\")\n                \n                try:\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                        content = f.read()\n                        output_lines.append(content)\n                except Exception as e:\n                    output_lines.append(f\"ERROR reading file: {e}\")\n    \n    # Write to file\n    with open('all_code_output.txt', 'w', encoding='utf-8') as f:\n        f.write('\\n'.join(output_lines))\n    \n    print(f\"Code printed to all_code_output.txt\")\n    print(f\"Total files processed: {len([l for l in output_lines if l.startswith('FILE:')])}\")\n\nif __name__ == \"__main__\":\n    root = sys.argv[1] if len(sys.argv) > 1 else \".\"\n    print_code_files(root)\nEOF\n\nchmod +x print_all_code.py\npython3 print_all_code.py\n```\n\n## Script 2: Remove Duplication and Clean Up\n\n```bash\n#!/bin/bash\n# save as: cleanup_duplication.sh\n\ncat << 'EOF' > cleanup_duplication.py\n#!/usr/bin/env python3\nimport os\nimport shutil\n\ndef cleanup():\n    \"\"\"Remove duplicate code and deprecated files\"\"\"\n    \n    # Files to remove (deprecated re-exports and duplicates)\n    files_to_remove = [\n        # Remove deprecated re-export files\n        'services/extraction/config/prompt_loader.py',\n        'services/extraction/config/settings.py',\n        'shared/services/llm_service.py',  # deprecated re-export\n        \n        # Remove duplicate Fuseki implementations  \n        'services/extraction/extraction/services/fuseki_service.py',\n        'services/extraction/extraction/services/evidence_tracker.py',\n        \n        # Remove duplicate config files\n        'services/extraction/config/config.yml',\n        'services/extraction/config/settings.yaml',\n    ]\n    \n    for file_path in files_to_remove:\n        if os.path.exists(file_path):\n            os.remove(file_path)\n            print(f\"Removed: {file_path}\")\n    \n    # Create consolidated Fuseki service in shared if not exists\n    os.makedirs('shared/services/fuseki', exist_ok=True)\n    \n    print(\"\\nCleanup complete!\")\n\nif __name__ == \"__main__\":\n    cleanup()\nEOF\n\npython3 cleanup_duplication.py\n```\n\n## Script 3: Create Consolidated Fuseki Service\n\n```bash\n#!/bin/bash\n# save as: create_fuseki_service.sh\n\ncat << 'EOF' > create_fuseki_service.py\n#!/usr/bin/env python3\nimport os\n\n# Create the consolidated Fuseki service\nfuseki_service = '''\"\"\"Consolidated Fuseki service for all Knowledge Garden components.\"\"\"\nimport httpx\nimport logging\nfrom typing import Dict, List, Any, Optional\nimport json\nimport asyncio\nfrom urllib.parse import quote_plus\n\nlogger = logging.getLogger(__name__)\n\n\nclass FusekiClient:\n    \"\"\"Unified Fuseki client with connection pooling and retry logic.\"\"\"\n    \n    def __init__(\n        self, \n        endpoint: str,\n        username: Optional[str] = None,\n        password: Optional[str] = None,\n        timeout: float = 30.0,\n        max_retries: int = 3\n    ):\n        self.endpoint = endpoint.rstrip('/')\n        self.sparql_endpoint = f\"{self.endpoint}/sparql\"\n        self.update_endpoint = f\"{self.endpoint}/update\"\n        self.data_endpoint = f\"{self.endpoint}/data\"\n        \n        self.timeout = timeout\n        self.max_retries = max_retries\n        \n        # Setup authentication\n        self.auth = None\n        if username and password:\n            self.auth = httpx.BasicAuth(username, password)\n        \n        # Connection pool\n        self._client = httpx.AsyncClient(\n            timeout=timeout,\n            auth=self.auth,\n            limits=httpx.Limits(max_keepalive_connections=10, max_connections=20)\n        )\n        \n        logger.info(f\"Initialized Fuseki client for {endpoint}\")\n    \n    async def close(self):\n        \"\"\"Close the HTTP client.\"\"\"\n        await self._client.aclose()\n    \n    async def __aenter__(self):\n        return self\n    \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.close()\n    \n    async def health_check(self) -> Dict[str, Any]:\n        \"\"\"Check Fuseki health with retry logic.\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                # Try to ping the server\n                base_url = '/'.join(self.endpoint.split('/')[:-1])\n                response = await self._client.get(f\"{base_url}/$/ping\", timeout=5.0)\n                \n                if response.status_code == 200:\n                    # Test dataset access\n                    test_query = \"ASK { ?s ?p ?o }\"\n                    try:\n                        await self.query(test_query)\n                        return {\"status\": \"healthy\", \"message\": \"Fuseki operational\"}\n                    except:\n                        return {\"status\": \"degraded\", \"message\": \"Dataset inaccessible\"}\n                else:\n                    return {\"status\": \"unhealthy\", \"message\": f\"Status: {response.status_code}\"}\n                    \n            except Exception as e:\n                if attempt == self.max_retries - 1:\n                    logger.error(f\"Fuseki health check failed: {e}\")\n                    return {\"status\": \"unhealthy\", \"message\": str(e)}\n                await asyncio.sleep(2 ** attempt)\n    \n    async def query(self, sparql: str, timeout: Optional[float] = None) -> List[Dict[str, Any]]:\n        \"\"\"Execute SPARQL query with retry logic.\"\"\"\n        headers = {\n            \"Accept\": \"application/sparql-results+json\",\n            \"Content-Type\": \"application/x-www-form-urlencoded\"\n        }\n        \n        for attempt in range(self.max_retries):\n            try:\n                response = await self._client.post(\n                    self.sparql_endpoint,\n                    headers=headers,\n                    data={\"query\": sparql},\n                    timeout=timeout or self.timeout\n                )\n                response.raise_for_status()\n                \n                result = response.json()\n                return self._process_sparql_results(result)\n                \n            except httpx.HTTPStatusError as e:\n                logger.error(f\"SPARQL query failed (attempt {attempt + 1}): {e.response.text}\")\n                if attempt == self.max_retries - 1:\n                    raise\n            except Exception as e:\n                logger.error(f\"SPARQL query error (attempt {attempt + 1}): {e}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                    \n            await asyncio.sleep(2 ** attempt)\n    \n    async def update(self, sparql: str, timeout: Optional[float] = None) -> bool:\n        \"\"\"Execute SPARQL update with retry logic.\"\"\"\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        \n        for attempt in range(self.max_retries):\n            try:\n                response = await self._client.post(\n                    self.update_endpoint,\n                    headers=headers,\n                    data={\"update\": sparql},\n                    timeout=timeout or self.timeout\n                )\n                response.raise_for_status()\n                return True\n                \n            except httpx.HTTPStatusError as e:\n                logger.error(f\"SPARQL update failed (attempt {attempt + 1}): {e.response.text}\")\n                if attempt == self.max_retries - 1:\n                    raise\n            except Exception as e:\n                logger.error(f\"SPARQL update error (attempt {attempt + 1}): {e}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                    \n            await asyncio.sleep(2 ** attempt)\n    \n    def _process_sparql_results(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Process SPARQL JSON results into clean dictionaries.\"\"\"\n        if \"results\" not in result or \"bindings\" not in result[\"results\"]:\n            return []\n        \n        processed = []\n        for binding in result[\"results\"][\"bindings\"]:\n            row = {}\n            for var, value in binding.items():\n                if value[\"type\"] == \"uri\":\n                    row[var] = value[\"value\"]\n                elif value[\"type\"] == \"literal\":\n                    row[var] = value[\"value\"]\n                    if \"datatype\" in value:\n                        row[f\"{var}_datatype\"] = value[\"datatype\"]\n                    if \"xml:lang\" in value:\n                        row[f\"{var}_lang\"] = value[\"xml:lang\"]\n                elif value[\"type\"] == \"bnode\":\n                    row[var] = f\"_:{value['value']}\"\n                else:\n                    row[var] = value[\"value\"]\n            processed.append(row)\n        \n        return processed\n    \n    @staticmethod\n    def escape_literal(s: str) -> str:\n        \"\"\"Escape a string for use in SPARQL literals.\"\"\"\n        if s is None:\n            return \"\"\n        return str(s).replace('\\\\\\\\', '\\\\\\\\\\\\\\\\').replace('\"', '\\\\\\\\\"').replace('\\\\n', '\\\\\\\\n').replace('\\\\r', '\\\\\\\\r')\n\n\n# Singleton instance management\n_client_instances = {}\n\ndef get_fuseki_client(\n    endpoint: str,\n    username: Optional[str] = None,\n    password: Optional[str] = None\n) -> FusekiClient:\n    \"\"\"Get or create a Fuseki client instance.\"\"\"\n    key = (endpoint, username, password)\n    if key not in _client_instances:\n        _client_instances[key] = FusekiClient(endpoint, username, password)\n    return _client_instances[key]\n\nasync def close_all_clients():\n    \"\"\"Close all client instances.\"\"\"\n    for client in _client_instances.values():\n        await client.close()\n    _client_instances.clear()\n'''\n\n# Write the file\nos.makedirs('shared/services/fuseki', exist_ok=True)\nwith open('shared/services/fuseki/__init__.py', 'w') as f:\n    f.write('\"\"\"Fuseki service for Knowledge Garden.\"\"\"\\n')\n    f.write('from .client import FusekiClient, get_fuseki_client, close_all_clients\\n\\n')\n    f.write('__all__ = [\"FusekiClient\", \"get_fuseki_client\", \"close_all_clients\"]\\n')\n\nwith open('shared/services/fuseki/client.py', 'w') as f:\n    f.write(fuseki_service)\n\nprint(\"Created consolidated Fuseki service in shared/services/fuseki/\")\nEOF\n\npython3 create_fuseki_service.py\n```\n\n## Script 4: Improve Extraction Implementation\n\n```bash\n#!/bin/bash\n# save as: improve_extraction.sh\n\ncat << 'EOF' > improve_extraction.py\n#!/usr/bin/env python3\nimport os\n\n# Update the main extraction API to actually work\napi_app = '''\"\"\"FastAPI application for extraction service.\"\"\"\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nimport logging\nfrom typing import Dict, List, Any, Optional\nimport os\nfrom uuid import uuid4\n\nfrom extraction.models.api import (\n    ExtractionRequest, ExtractionResponse, BatchExtractionRequest, \n    BatchExtractionResponse, ConceptSearchRequest, ConceptSearchResponse\n)\nfrom extraction.pipeline.runner import ExtractionPipeline\nfrom extraction.services.evidence_tracker import EvidenceTracker\nfrom shared.services.fuseki import get_fuseki_client\nfrom shared.config.settings import get_settings\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize app\napp = FastAPI(\n    title=\"Knowledge Garden Extraction Service\",\n    version=\"3.0.0\",\n    description=\"Extract structured knowledge with embeddings and clustering\"\n)\n\n# Add CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Dependencies\ndef get_pipeline():\n    \"\"\"Get extraction pipeline instance.\"\"\"\n    settings = get_settings()\n    fuseki = get_fuseki_client(\n        f\"{settings.fuseki_url}/{settings.fuseki_dataset}\",\n        settings.fuseki_user,\n        settings.fuseki_password\n    )\n    evidence_tracker = EvidenceTracker(fuseki)\n    \n    config = {\n        \"document\": {\n            \"min_sentence_length\": 10,\n            \"max_sentence_length\": 500\n        },\n        \"embedding\": {\n            \"model\": settings.embedding_model,\n            \"batch_size\": settings.batch_size\n        },\n        \"extraction\": {\n            \"extract_entities\": True,\n            \"extract_caps\": True,\n            \"extract_values\": True,\n            \"extract_insights\": True,\n            \"extract_tensions\": True,\n            \"extract_questions\": True\n        },\n        \"clustering\": {\n            \"min_cluster_size\": 100,\n            \"min_samples\": 10,\n            \"use_umap\": True,\n            \"umap_components\": 5\n        }\n    }\n    \n    return ExtractionPipeline(config, evidence_tracker)\n\n# Health check\n@app.get(\"/health\")\nasync def health():\n    \"\"\"Health check endpoint.\"\"\"\n    try:\n        settings = get_settings()\n        fuseki = get_fuseki_client(\n            f\"{settings.fuseki_url}/{settings.fuseki_dataset}\",\n            settings.fuseki_user,\n            settings.fuseki_password\n        )\n        \n        fuseki_health = await fuseki.health_check()\n        \n        # Check embedding service\n        from shared.services.embeddings.service import get_embedding_service\n        embedding_service = get_embedding_service()\n        embedding_health = await embedding_service.health_check()\n        \n        return {\n            \"status\": \"healthy\" if fuseki_health[\"status\"] == \"healthy\" else \"degraded\",\n            \"service\": \"extraction\",\n            \"version\": \"3.0.0\",\n            \"dependencies\": {\n                \"fuseki\": fuseki_health,\n                \"embeddings\": embedding_health\n            }\n        }\n    except Exception as e:\n        logger.error(f\"Health check failed: {e}\")\n        return {\n            \"status\": \"unhealthy\",\n            \"service\": \"extraction\",\n            \"version\": \"3.0.0\",\n            \"error\": str(e)\n        }\n\n@app.post(\"/extract\", response_model=ExtractionResponse)\nasync def extract(\n    request: ExtractionRequest,\n    pipeline: ExtractionPipeline = Depends(get_pipeline)\n):\n    \"\"\"Extract knowledge from text.\"\"\"\n    try:\n        # Add request ID if not present\n        if not hasattr(request, 'id'):\n            request.id = uuid4()\n        \n        response = await pipeline.extract(request)\n        return response\n        \n    except Exception as e:\n        logger.error(f\"Extraction failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/extract/batch\", response_model=BatchExtractionResponse)\nasync def extract_batch(\n    request: BatchExtractionRequest,\n    pipeline: ExtractionPipeline = Depends(get_pipeline)\n):\n    \"\"\"Extract knowledge from multiple documents.\"\"\"\n    try:\n        # Add IDs to requests without them\n        for doc in request.documents:\n            if not hasattr(doc, 'id'):\n                doc.id = uuid4()\n        \n        response = await pipeline.extract_batch(\n            request.documents,\n            request.enable_cross_document_clustering\n        )\n        return response\n        \n    except Exception as e:\n        logger.error(f\"Batch extraction failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/search/concepts\", response_model=ConceptSearchResponse)\nasync def search_concepts(\n    request: ConceptSearchRequest,\n    pipeline: ExtractionPipeline = Depends(get_pipeline)\n):\n    \"\"\"Search for similar concepts.\"\"\"\n    try:\n        # Use embedding service to find similar concepts\n        from shared.services.embeddings.service import get_embedding_service\n        embedding_service = get_embedding_service()\n        \n        # This would need to be implemented with actual concept storage\n        # For now, return empty results\n        return ConceptSearchResponse(\n            query=request.query,\n            results=[],\n            total_found=0,\n            duration_ms=0.0\n        )\n        \n    except Exception as e:\n        logger.error(f\"Concept search failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8001)\n'''\n\n# Write the improved API\nwith open('services/extraction/api/app.py', 'w') as f:\n    f.write(api_app)\n\n# Create improved evidence tracker using shared Fuseki\nevidence_tracker = '''\"\"\"Evidence tracker using shared Fuseki service.\"\"\"\nimport logging\nfrom typing import Dict, List, Any, Optional\nfrom uuid import uuid4\nfrom datetime import datetime\n\nfrom shared.services.fuseki import FusekiClient\n\nlogger = logging.getLogger(__name__)\n\n\nclass EvidenceTracker:\n    \"\"\"Tracks provenance information for extracted knowledge.\"\"\"\n    \n    def __init__(self, fuseki_client: FusekiClient):\n        \"\"\"Initialize with shared Fuseki client.\"\"\"\n        self.fuseki = fuseki_client\n        self.prefix_map = {\n            \"kg\": \"urn:knowledgegarden:ontology#\",\n            \"kgi\": \"urn:knowledgegarden:instance#\",\n            \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\",\n            \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n            \"xsd\": \"http://www.w3.org/2001/XMLSchema#\",\n        }\n    \n    async def record_extraction_event(\n        self, \n        document_id: str, \n        metadata: Dict[str, Any]\n    ) -> str:\n        \"\"\"Record an extraction event.\"\"\"\n        event_id = f\"event_{uuid4().hex[:8]}\"\n        \n        # Build SPARQL update\n        prefixes = self._build_prefixes()\n        timestamp = datetime.now().isoformat()\n        \n        sparql = f\"\"\"\n        {prefixes}\n        \n        INSERT DATA {{\n            kgi:{event_id} a kg:ExtractionEvent ;\n                kg:timestamp \"{timestamp}\"^^xsd:dateTime ;\n                kg:documentId \"{document_id}\" ;\n                kg:extractedAt \"{timestamp}\"^^xsd:dateTime .\n        }}\n        \"\"\"\n        \n        try:\n            await self.fuseki.update(sparql)\n            logger.info(f\"Recorded extraction event: {event_id}\")\n            return event_id\n        except Exception as e:\n            logger.error(f\"Failed to record extraction event: {e}\")\n            # Return event ID anyway for resilience\n            return event_id\n    \n    async def record_concept(\n        self,\n        concept_id: str,\n        label: str,\n        concept_type: str,\n        document_id: str,\n        confidence: float,\n        properties: Optional[Dict[str, Any]] = None\n    ) -> bool:\n        \"\"\"Record a concept with evidence.\"\"\"\n        prefixes = self._build_prefixes()\n        timestamp = datetime.now().isoformat()\n        \n        # Build property statements\n        property_statements = []\n        if properties:\n            for key, value in properties.items():\n                if isinstance(value, str):\n                    prop_stmt = f'kg:{key} \"{self.fuseki.escape_literal(value)}\"'\n                elif isinstance(value, (int, float)):\n                    prop_stmt = f'kg:{key} {value}'\n                elif isinstance(value, bool):\n                    prop_stmt = f'kg:{key} {str(value).lower()}'\n                else:\n                    continue\n                property_statements.append(prop_stmt)\n        \n        props = \" ;\\\\n                \".join(property_statements) if property_statements else \"\"\n        if props:\n            props = \" ;\\\\n                \" + props\n        \n        sparql = f\"\"\"\n        {prefixes}\n        \n        INSERT DATA {{\n            kgi:{concept_id} a kg:Concept ;\n                rdfs:label \"{self.fuseki.escape_literal(label)}\" ;\n                kg:conceptType \"{concept_type}\" ;\n                kg:confidence {confidence} ;\n                kg:extractedFrom kgi:doc_{document_id} ;\n                kg:extractedAt \"{timestamp}\"^^xsd:dateTime{props} .\n        }}\n        \"\"\"\n        \n        try:\n            await self.fuseki.update(sparql)\n            logger.debug(f\"Recorded concept: {concept_id}\")\n            return True\n        except Exception as e:\n            logger.error(f\"Failed to record concept: {e}\")\n            return False\n    \n    async def store_evidence(self, evidence: Any, target_uri: str) -> bool:\n        \"\"\"Store evidence for a knowledge element.\"\"\"\n        # This would store the actual evidence\n        # Implementation depends on the Evidence model structure\n        return True\n    \n    def _build_prefixes(self) -> str:\n        \"\"\"Build SPARQL prefix declarations.\"\"\"\n        return \"\\\\n\".join([\n            f\"PREFIX {prefix}: <{uri}>\"\n            for prefix, uri in self.prefix_map.items()\n        ])\n'''\n\n# Write the improved evidence tracker\nos.makedirs('services/extraction/extraction/services', exist_ok=True)\nwith open('services/extraction/extraction/services/evidence_tracker.py', 'w') as f:\n    f.write(evidence_tracker)\n\nprint(\"Improved extraction implementation complete!\")\nEOF\n\npython3 improve_extraction.py\n```\n\n## Script 5: Update Imports to Use Shared Services\n\n```bash\n#!/bin/bash\n# save as: update_imports.sh\n\ncat << 'EOF' > update_imports.py\n#!/usr/bin/env python3\nimport os\nimport re\n\ndef update_imports():\n    \"\"\"Update all imports to use shared services\"\"\"\n    \n    # Files to update\n    files_to_update = []\n    \n    # Find all Python files\n    for root, dirs, files in os.walk('services/extraction'):\n        # Skip __pycache__ directories\n        dirs[:] = [d for d in dirs if d != '__pycache__']\n        \n        for file in files:\n            if file.endswith('.py'):\n                files_to_update.append(os.path.join(root, file))\n    \n    # Import replacements\n    replacements = [\n        # Fuseki imports\n        (r'from extraction\\.services\\.fuseki_service import FusekiService',\n         'from shared.services.fuseki import FusekiClient as FusekiService'),\n        (r'from extraction\\.pipeline\\.fuseki_service import FusekiService',\n         'from shared.services.fuseki import FusekiClient as FusekiService'),\n        (r'from services\\.fuseki import FusekiService',\n         'from shared.services.fuseki import FusekiClient as FusekiService'),\n        \n        # LLM imports\n        (r'from extraction\\.services\\.llm_service import LLMService',\n         'from shared.services.llm.service import LLMService'),\n        (r'from shared\\.services\\.llm_service import.*',\n         'from shared.services.llm.service import LLMService, get_llm_service'),\n        \n        # Config imports\n        (r'from extraction\\.config\\.settings import.*',\n         'from shared.config.settings import get_settings'),\n        (r'from services\\.extraction\\.config\\.settings import.*',\n         'from shared.config.settings import get_settings'),\n        \n        # Prompt loader imports\n        (r'from extraction\\.config\\.prompt_loader import.*',\n         'from shared.prompts.loader import get_prompt_loader'),\n        (r'from services\\.extraction\\.config\\.prompt_loader import.*',\n         'from shared.prompts.loader import get_prompt_loader'),\n    ]\n    \n    # Process each file\n    for file_path in files_to_update:\n        try:\n            with open(file_path, 'r') as f:\n                content = f.read()\n            \n            original_content = content\n            \n            # Apply replacements\n            for pattern, replacement in replacements:\n                content = re.sub(pattern, replacement, content)\n            \n            # Only write if changed\n            if content != original_content:\n                with open(file_path, 'w') as f:\n                    f.write(content)\n                print(f\"Updated imports in: {file_path}\")\n                \n        except Exception as e:\n            print(f\"Error updating {file_path}: {e}\")\n    \n    print(\"\\nImport updates complete!\")\n\nif __name__ == \"__main__\":\n    update_imports()\nEOF\n\npython3 update_imports.py\n```\n\n## Script 6: Add Ontology-Aware Extraction\n\n```bash\n#!/bin/bash\n# save as: add_ontology_extraction.sh\n\ncat << 'EOF' > add_ontology_extraction.py\n#!/usr/bin/env python3\nimport os\n\n# Create ontology-aware concept extractor\nontology_extractor = '''\"\"\"Ontology-aware concept extraction.\"\"\"\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom uuid import uuid4\n\nfrom extraction.pipeline.base import PipelineStage\nfrom extraction.models.documents import DocumentAnalysis\nfrom extraction.models.concepts import ExtractedConcept, ConceptType, EvidenceModel\nfrom extraction.extractors.pattern_extractor import PatternExtractor\nfrom extraction.extractors.entity_extractor import EntityExtractor\nfrom shared.services.llm.service import get_llm_service\n\nlogger = logging.getLogger(__name__)\n\n\nclass OntologyAwareConceptExtractor(PipelineStage[DocumentAnalysis, DocumentAnalysis]):\n    \"\"\"Extract concepts with ontology awareness.\"\"\"\n    \n    async def _initialize(self) -> None:\n        \"\"\"Initialize extractors and ontology knowledge.\"\"\"\n        self.pattern_extractor = PatternExtractor()\n        self.entity_extractor = EntityExtractor()\n        self.llm_service = get_llm_service()\n        \n        # Load ontology relationships\n        self.ontology_relationships = {\n            \"CAP\": {\n                \"creates\": [\"Value\"],\n                \"reveals\": [\"Pattern\", \"Behavior\"],\n                \"frequency_threshold\": 3  # CAPs need 3+ occurrences to become Values\n            },\n            \"Tension\": {\n                \"reveals\": [\"Value\"],\n                \"between\": [\"Concept\", \"Concept\"],\n                \"resolution\": [\"Strategy\", \"Practice\"]\n            },\n            \"Insight\": {\n                \"transforms\": [\"Understanding\"],\n                \"leads_to\": [\"Question\", \"Action\"],\n                \"requires\": [\"Evidence\", \"Reflection\"]\n            },\n            \"Question\": {\n                \"explores\": [\"Unknown\", \"Edge\"],\n                \"persistence\": [\"CONTEXT_SPECIFIC\", \"PERSISTENT\"],\n                \"depth\": [\"SURFACE\", \"DEEP\", \"EXISTENTIAL\"]\n            }\n        }\n        \n        # Extraction configuration\n        self.config = self.config or {}\n        self.extract_entities = self.config.get(\"extract_entities\", True)\n        self.extract_caps = self.config.get(\"extract_caps\", True)\n        self.extract_values = self.config.get(\"extract_values\", True)\n        self.extract_insights = self.config.get(\"extract_insights\", True)\n        self.extract_tensions = self.config.get(\"extract_tensions\", True)\n        self.extract_questions = self.config.get(\"extract_questions\", True)\n    \n    async def process(self, doc: DocumentAnalysis) -> DocumentAnalysis:\n        \"\"\"Extract concepts with ontology awareness.\"\"\"\n        \n        # Track CAP frequencies for Value inference\n        cap_frequencies = {}\n        \n        for para in doc.paragraphs:\n            for sent in para.sentences:\n                # Extract all patterns\n                patterns = self.pattern_extractor.extract_all(sent.text)\n                \n                # Process CAPs with Value inference\n                if self.extract_caps and patterns.get('caps'):\n                    para.has_caps = True\n                    for cap in patterns['caps']:\n                        cap_text = cap['pattern']\n                        \n                        # Track frequency\n                        cap_key = self._normalize_cap(cap_text)\n                        cap_frequencies[cap_key] = cap_frequencies.get(cap_key, 0) + 1\n                        \n                        # Create CAP concept\n                        concept = self._create_cap_concept(\n                            cap, sent, doc.document_uri\n                        )\n                        \n                        # Add ontology relationships\n                        concept.attributes[\"reveals\"] = \"behavioral_pattern\"\n                        concept.attributes[\"frequency\"] = cap_frequencies[cap_key]\n                        \n                        # Check if this CAP should create a Value\n                        if cap_frequencies[cap_key] >= self.ontology_relationships[\"CAP\"][\"frequency_threshold\"]:\n                            concept.attributes[\"creates_value\"] = True\n                            \n                            # Infer Value from recurring CAP\n                            if self.extract_values:\n                                value_concept = await self._infer_value_from_cap(\n                                    cap_text, cap_frequencies[cap_key], sent, doc\n                                )\n                                if value_concept:\n                                    sent.concepts.append(value_concept)\n                                    para.concepts.append(value_concept)\n                                    doc.concepts.append(value_concept)\n                        \n                        sent.concepts.append(concept)\n                        para.concepts.append(concept)\n                        doc.concepts.append(concept)\n                \n                # Process Tensions with Value revelation\n                if self.extract_tensions and patterns.get('tensions'):\n                    para.has_tensions = True\n                    for tension in patterns['tensions']:\n                        concept = self._create_tension_concept(\n                            tension, sent, doc.document_uri\n                        )\n                        \n                        # Analyze what values this tension reveals\n                        revealed_values = await self._analyze_tension_values(\n                            tension['pole1'], tension['pole2']\n                        )\n                        if revealed_values:\n                            concept.attributes[\"reveals_values\"] = revealed_values\n                        \n                        sent.concepts.append(concept)\n                        para.concepts.append(concept)\n                        doc.concepts.append(concept)\n                \n                # Process Insights with transformation tracking\n                if self.extract_insights and patterns.get('insights'):\n                    para.has_insights = True\n                    for insight in patterns['insights']:\n                        concept = self._create_insight_concept(\n                            insight, sent, doc.document_uri\n                        )\n                        \n                        # Analyze transformation\n                        transformation = await self._analyze_insight_transformation(\n                            insight['realization']\n                        )\n                        if transformation:\n                            concept.attributes[\"before_understanding\"] = transformation.get(\"before\", \"\")\n                            concept.attributes[\"after_understanding\"] = transformation.get(\"after\", \"\")\n                            concept.attributes[\"trigger\"] = transformation.get(\"trigger\", \"\")\n                        \n                        sent.concepts.append(concept)\n                        para.concepts.append(concept)\n                        doc.concepts.append(concept)\n                \n                # Process Questions with depth analysis\n                if self.extract_questions and patterns.get('questions'):\n                    for question in patterns['questions']:\n                        concept = self._create_question_concept(\n                            question, sent, doc.document_uri\n                        )\n                        \n                        # Analyze question depth and persistence\n                        analysis = await self._analyze_question_depth(\n                            question['text']\n                        )\n                        concept.attributes[\"depth\"] = analysis.get(\"depth\", \"SURFACE\")\n                        concept.attributes[\"persistence\"] = analysis.get(\"persistence\", \"CONTEXT_SPECIFIC\")\n                        concept.attributes[\"growth_edge\"] = analysis.get(\"growth_edge\", \"\")\n                        \n                        sent.concepts.append(concept)\n                        para.concepts.append(concept)\n                        doc.concepts.append(concept)\n        \n        # Update counts and calculate concept diversity\n        doc.total_concepts = len(doc.concepts)\n        for concept in doc.concepts:\n            concept_type = concept.concept_type\n            doc.concept_counts[concept_type] = doc.concept_counts.get(concept_type, 0) + 1\n        \n        # Calculate concept diversity based on type distribution\n        if doc.concepts:\n            type_counts = list(doc.concept_counts.values())\n            total = sum(type_counts)\n            if total > 0:\n                # Simple diversity metric: entropy of concept type distribution\n                entropy = 0\n                for count in type_counts:\n                    if count > 0:\n                        p = count / total\n                        entropy -= p * (p if p > 0 else 0)\n                doc.concept_diversity = min(entropy, 1.0)\n        \n        return doc\n    \n    def _normalize_cap(self, cap_text: str) -> str:\n        \"\"\"Normalize CAP text for frequency counting.\"\"\"\n        # Simple normalization - could be enhanced\n        return cap_text.lower().strip()\n    \n    def _create_cap_concept(\n        self, cap: Dict[str, Any], sent: Any, doc_uri: str\n    ) -> ExtractedConcept:\n        \"\"\"Create a CAP concept.\"\"\"\n        return ExtractedConcept(\n            uri=f\"urn:kg:cap:{uuid4()}\",\n            label=cap['pattern'][:100],\n            text=cap['pattern'],\n            concept_type=ConceptType.CAP,\n            cap_context=cap.get('context'),\n            cap_action=cap.get('action'),\n            source_sentence_id=sent.id,\n            source_document_uri=doc_uri,\n            evidence=EvidenceModel(\n                quote=cap['pattern'],\n                source_text=sent.text,\n                position=sent.position,\n                confidence=cap.get('confidence', 0.9)\n            ),\n            extraction_method=\"pattern\",\n            confidence=cap.get('confidence', 0.9)\n        )\n    \n    def _create_tension_concept(\n        self, tension: Dict[str, Any], sent: Any, doc_uri: str\n    ) -> ExtractedConcept:\n        \"\"\"Create a Tension concept.\"\"\"\n        return ExtractedConcept(\n            uri=f\"urn:kg:tension:{uuid4()}\",\n            label=f\"{tension['pole1']} vs {tension['pole2']}\",\n            text=tension.get('source_text', sent.text),\n            concept_type=ConceptType.TENSION,\n            tension_pole1=tension['pole1'],\n            tension_pole2=tension['pole2'],\n            source_sentence_id=sent.id,\n            source_document_uri=doc_uri,\n            evidence=EvidenceModel(\n                quote=tension.get('source_text', sent.text),\n                source_text=sent.text,\n                position=sent.position,\n                confidence=tension.get('confidence', 0.9)\n            ),\n            extraction_method=\"pattern\",\n            confidence=tension.get('confidence', 0.9)\n        )\n    \n    def _create_insight_concept(\n        self, insight: Dict[str, Any], sent: Any, doc_uri: str\n    ) -> ExtractedConcept:\n        \"\"\"Create an Insight concept.\"\"\"\n        return ExtractedConcept(\n            uri=f\"urn:kg:insight:{uuid4()}\",\n            label=insight['realization'][:100],\n            text=insight['realization'],\n            concept_type=ConceptType.INSIGHT,\n            source_sentence_id=sent.id,\n            source_document_uri=doc_uri,\n            evidence=EvidenceModel(\n                quote=insight['realization'],\n                source_text=sent.text,\n                position=sent.position,\n                confidence=insight.get('confidence', 0.9)\n            ),\n            extraction_method=\"pattern\",\n            confidence=insight.get('confidence', 0.9)\n        )\n    \n    def _create_question_concept(\n        self, question: Dict[str, Any], sent: Any, doc_uri: str\n    ) -> ExtractedConcept:\n        \"\"\"Create a Question concept.\"\"\"\n        return ExtractedConcept(\n            uri=f\"urn:kg:question:{uuid4()}\",\n            label=question['text'][:100],\n            text=question['text'],\n            concept_type=ConceptType.QUESTION,\n            source_sentence_id=sent.id,\n            source_document_uri=doc_uri,\n            evidence=EvidenceModel(\n                quote=question['text'],\n                source_text=sent.text,\n                position=sent.position,\n                confidence=question.get('confidence', 0.9)\n            ),\n            extraction_method=\"pattern\",\n            confidence=question.get('confidence', 0.9)\n        )\n    \n    async def _infer_value_from_cap(\n        self, cap_text: str, frequency: int, sent: Any, doc: DocumentAnalysis\n    ) -> Optional[ExtractedConcept]:\n        \"\"\"Infer a Value from a recurring CAP.\"\"\"\n        # Use LLM to infer the underlying value\n        prompt = f\"\"\"\n        A person has repeatedly expressed this pattern of attention/behavior:\n        \"{cap_text}\"\n        (Occurred {frequency} times)\n        \n        What core value or source of meaning does this pattern reveal?\n        Respond with just the value name (2-4 words).\n        \"\"\"\n        \n        try:\n            response = await self.llm_service.generate(\n                prompt=prompt,\n                temperature=0.3,\n                max_tokens=50\n            )\n            \n            value_name = response[\"text\"].strip()\n            \n            return ExtractedConcept(\n                uri=f\"urn:kg:value:{uuid4()}\",\n                label=value_name,\n                text=f\"Value inferred from pattern: {cap_text}\",\n                concept_type=ConceptType.VALUE,\n                source_sentence_id=sent.id,\n                source_document_uri=doc.document_uri,\n                evidence=EvidenceModel(\n                    quote=cap_text,\n                    source_text=f\"Inferred from {frequency} occurrences\",\n                    position=sent.position,\n                    confidence=0.8\n                ),\n                extraction_method=\"inference\",\n                confidence=0.8,\n                attributes={\n                    \"constituted_by\": cap_text,\n                    \"occurrence_count\": frequency\n                }\n            )\n        except Exception as e:\n            logger.error(f\"Failed to infer value: {e}\")\n            return None\n    \n    async def _analyze_tension_values(\n        self, pole1: str, pole2: str\n    ) -> Optional[List[str]]:\n        \"\"\"Analyze what values a tension reveals.\"\"\"\n        # This would use LLM or rules to infer values\n        # Simplified for now\n        return [f\"balance_between_{pole1}_and_{pole2}\"]\n    \n    async def _analyze_insight_transformation(\n        self, realization: str\n    ) -> Optional[Dict[str, str]]:\n        \"\"\"Analyze the transformation an insight represents.\"\"\"\n        # This would use LLM to understand before/after states\n        # Simplified for now\n        return {\n            \"before\": \"Previous understanding\",\n            \"after\": realization,\n            \"trigger\": \"Reflection\"\n        }\n    \n    async def _analyze_question_depth(\n        self, question: str\n    ) -> Dict[str, str]:\n        \"\"\"Analyze the depth and nature of a question.\"\"\"\n        # Simple heuristics - could use LLM\n        depth = \"SURFACE\"\n        if any(word in question.lower() for word in [\"why\", \"how\", \"meaning\", \"purpose\"]):\n            depth = \"DEEP\"\n        if any(word in question.lower() for word in [\"existence\", \"consciousness\", \"reality\"]):\n            depth = \"EXISTENTIAL\"\n        \n        persistence = \"CONTEXT_SPECIFIC\"\n        if depth in [\"DEEP\", \"EXISTENTIAL\"]:\n            persistence = \"PERSISTENT\"\n        \n        return {\n            \"depth\": depth,\n            \"persistence\": persistence,\n            \"growth_edge\": \"Understanding limits\"\n        }\n'''\n\n# Write the ontology-aware extractor\nextractor_path = 'services/extraction/extraction/pipeline/stages/ontology_concept_extractor.py'\nos.makedirs(os.path.dirname(extractor_path), exist_ok=True)\nwith open(extractor_path, 'w') as f:\n    f.write(ontology_extractor)\n\n# Update the pipeline to use the new extractor\npipeline_update = '''\n# In extraction/pipeline/runner.py, update the import:\n# from extraction.pipeline.stages.concept_extractor import ConceptExtractor\n# to:\n# from extraction.pipeline.stages.ontology_concept_extractor import OntologyAwareConceptExtractor as ConceptExtractor\n'''\n\nprint(f\"Created ontology-aware concept extractor at {extractor_path}\")\nprint(\"Update the pipeline runner to use OntologyAwareConceptExtractor\")\nEOF\n\npython3 add_ontology_extraction.py\n```\n\nRun these scripts in order:\n```bash\nchmod +x *.sh\n./print_code.sh              # Print all code to a file\n./cleanup_duplication.sh     # Remove duplicate files\n./create_fuseki_service.sh   # Create consolidated Fuseki service\n./improve_extraction.sh      # Improve extraction implementation\n./update_imports.sh          # Update imports to use shared services\n./add_ontology_extraction.sh # Add ontology-aware extraction\n```\n\nThese scripts will:\n1. Create a utility to print all code files\n2. Remove duplicate code and deprecated re-exports\n3. Create a single, robust Fuseki service in `shared/services/fuseki`\n4. Update the extraction API to actually work with proper dependency injection\n5. Update all imports to use the shared services\n6. Add ontology-aware extraction that leverages the knowledge graph structure\n\nThe key improvements:\n- Single Fuseki client with retry logic and connection pooling\n- Proper error handling and health checks\n- Ontology-aware extraction that infers Values from CAPs\n- Cleaner architecture without backward compatibility cruft\n- Pattern extraction stays in the extraction service where it belongs",
            "citations": []
          }
        ],
        "updated_at": "2025-06-12T15:55:12.909000Z",
        "attachments": [],
        "files": []
      }
    }
  ]
}