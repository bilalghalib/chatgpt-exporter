{
  "id": "afd32747-584d-4500-8fab-735702102c8a",
  "source": "chatgpt",
  "title": "Email Chat Assistance",
  "created_at": "2024-06-21T20:17:15.679081",
  "updated_at": "2024-06-21T21:40:31.168257",
  "summary": null,
  "message_count": 12,
  "messages": [
    {
      "id": "aaa23685-b737-4c87-986a-5c3dc601e0e0",
      "role": "user",
      "content": "from typing import Dict, Any\nimport asyncio\nimport json\nimport logging\nimport os\nimport re\nimport uuid\nimport mimetypes\nfrom celery import chain, states\nfrom celery.utils.log import get_task_logger\nfrom contextlib import asynccontextmanager\nfrom datetime import datetime\nfrom redis import Redis\nfrom graph_generator import generate_graph, save_graph_to_html\nfrom whisper_api import transcribe_audio\nfrom download_utils import download_file_from_brevo, save_file_content\nfrom email_utils import send_email\nfrom db_utils import get_task_metadata, save_task_metadata\nfrom airtable_utils import create_graph_record_async, update_status_async\nfrom PyPDF2 import PdfReader\nfrom chat_utils import save_chat_context, load_chat_context, extract_conversation_id, extract_message_from_email\nfrom ollama_utils import generate_ollama_response\nfrom celery_app import celery\n\nlogger = get_task_logger(__name__)\nREDIS_CLIENT = Redis(host='localhost', port=6379, db=0)\n\n@celery.task(bind=True)\ndef process_chat_task(self, email_data: Dict[str, Any]) -> Dict[str, Any]:\n    logger.info(f\"Starting process_chat_task for email: {email_data['From']['Address']}\")\n    \n    conversation_id = extract_conversation_id(email_data['Subject'])\n    context = load_chat_context(conversation_id)\n    new_message = extract_message_from_email(email_data)\n    \n    context.append({\"role\": \"user\", \"content\": new_message})\n    ollama_response = generate_ollama_response(context)\n    context.append({\"role\": \"assistant\", \"content\": ollama_response})\n    \n    save_chat_context(conversation_id, context)\n    \n    send_email(\n        email=email_data['From']['Address'],\n        subject=f\"Re: {email_data['Subject']}\",\n        content=ollama_response\n    )\n    \n    return {\"status\": \"completed\", \"conversation_id\": conversation_id}    \n    \nasync def extract_text_from_pdf(file_path):\n    logger.info(f\"Extracting text from PDF: {file_path}\")\n    text = \"\"\n    try:\n        with open(file_path, 'rb') as file:\n            pdf_reader = PdfReader(file)\n            for page in pdf_reader.pages:\n                text += page.extract_text()\n    except Exception as e:\n        logger.error(f\"Error extracting text from PDF: {str(e)}\")\n        raise\n    return text\n\n@celery.task(bind=True)\ndef process_graph_task(self, task_id, email=None):\n    task_id = str(task_id)\n    logger.info(f\"Starting process_graph_task with task_id: {task_id}\")\n    try:\n        task_data = get_task_metadata(task_id)\n        if task_data is None:\n            raise ValueError(f\"Task metadata not found for task_id: {task_id}\")\n\n        logger.info(f\"Retrieved task data: {task_data}\")\n\n        file_path = task_data['file_path']\n        logger.info(f\"File path from metadata: {file_path}\")\n\n        file_type, _ = mimetypes.guess_type(file_path)\n        logger.info(f\"Detected file type: {file_type}\")\n\n        graph_task_id = self.request.id\n        email_sender = email or task_data.get('email_sender')\n\n        logger.info(f\"Using current task ID: {graph_task_id} for process_graph_task\")\n        graph_output_dir = os.path.join('static', 'generated', f\"graph_{graph_task_id}\")\n        os.makedirs(graph_output_dir, exist_ok=True)\n\n        # Create Airtable record with status \"Processing\"\n        async def create_record():\n            record_id = await create_graph_record_async(task_id, email_sender)\n            return record_id\n\n        record_id = asyncio.run(create_record())\n        logger.info(f\"Airtable record created with ID: {record_id}\")\n\n        # Process different file types\n        if file_type == 'application/pdf':\n            extracted_text = asyncio.run(extract_text_from_pdf(file_path))\n            text_file_path = os.path.join(os.path.dirname(file_path), 'extracted_text.txt')\n            with open(text_file_path, 'w', encoding='utf-8') as f:\n                f.write(extracted_text)\n            file_path = text_file_path\n        elif file_type and file_type.startswith('audio/'):\n            transcription = asyncio.run(transcribe_audio(file_path))\n            transcription_path = os.path.join(os.path.dirname(file_path), 'transcription.txt')\n            with open(transcription_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            file_path = transcription_path\n\n        G = generate_graph(file_path, graph_output_dir)\n        save_graph_to_html(G, graph_output_dir)\n\n        logger.info(f\"Graph generated successfully for process_graph_task with task_id: {self.request.id}\")\n\n        result_dict = {\n            'graph_task': {\n                'task_id': graph_task_id,\n                'd3_url': f'/static/generated/graph_{graph_task_id}/index.html',\n                'pyvis_url': f'/static/generated/graph_{graph_task_id}/pyvis.html'\n            }\n        }\n\n        # Update Airtable record to \"Processing Completed\"\n        async def update_record():\n            await update_status_async(record_id, \"Processing Completed\")\n\n        asyncio.run(update_record())\n\n        # Trigger email task\n        task_data.update(result_dict)\n        send_email_task.delay(task_data)\n\n        self.update_state(state=states.SUCCESS, meta=result_dict)\n        return result_dict\n\n    except Exception as e:\n        logger.exception(f\"Error generating graph for process_graph_task with task_id: {self.request.id}\")\n        raise\n\ndef is_valid_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\n@celery.task(bind=True)\ndef transcribe_audio_task(self, task_id):\n    task_id = str(task_id)\n    task_data = get_task_metadata(task_id)\n    file_path = task_data['file_path']\n    download_task_id = task_data['download_task_id']\n\n    logger.info(f\"Starting transcribe_audio_task with download_task_id: {download_task_id}\")\n    output_dir = os.path.join('uploads', f\"tr_{download_task_id}\")\n    transcription_path = os.path.join(output_dir, 'transcription.txt')\n\n    if not os.path.exists(transcription_path):\n        async def run_transcription():\n            async with redis_lock(f\"transcribe_lock_{download_task_id}\") as acquired:\n                if acquired:\n                    try:\n                        logger.info(f\"Starting transcription for download task {download_task_id}\")\n                        logger.info(f\"File path: {file_path}\")\n                        logger.info(f\"File exists: {os.path.exists(file_path)}\")\n                        logger.info(f\"File size: {os.path.getsize(file_path)} bytes\")\n                        \n                        transcription = await transcribe_audio(file_path)\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(transcription)\n\n                        task_data.update({\n                            'transcription_data': transcription,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n\n                    except Exception as e:\n                        logger.error(f\"Error during transcription: {str(e)}\")\n                        logger.exception(\"Full traceback:\")\n                        # Fallback: create a text file with error message\n                        error_message = f\"Transcription failed. Error: {str(e)}\"\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(error_message)\n                        task_data.update({\n                            'transcription_data': error_message,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n                else:\n                    logger.info(f\"Another instance of the task is already running for download task {download_task_id}\")\n\n        try:\n            asyncio.run(run_transcription())\n        except Exception as e:\n            logger.error(f\"Error in run_transcription: {str(e)}\")\n            # Don't retry, we've already handled the error\n\n    result_dict = {'task_id': task_id}\n    logger.info(f\"transcribe_audio_task result_dict updated with transcription_path: {transcription_path}\")\n    logger.info(f\"Updated result_dict: {result_dict}\")\n    \n    self.update_state(state=states.SUCCESS, meta=result_dict)\n    return task_id\n        \n@celery.task(bind=True)\ndef download_and_process_attachment_task(self, file_url, email_sender, email_recipient, api_key, email_subject, data):\n    try:\n        download_task_id = str(uuid.uuid4())\n        logger.info(f\"Starting download_and_process_attachment_task with download_task_id: {download_task_id}\")\n\n        if 'Attachments' in data and data['Attachments']:\n            attachment_data = data['Attachments'][0]\n            attachment_name = attachment_data['Name']\n            attachment_type = attachment_data['ContentType']\n            download_token = attachment_data['DownloadToken']\n            \n            file_content = download_file_from_brevo(download_token, api_key)\n            file_path = os.path.join('uploads', f\"download_{download_task_id}\", attachment_name)\n            save_file_content(file_content, file_path)\n        else:\n            attachment_name = 'message.txt'\n            attachment_type = 'text/plain'\n            file_content = data.get('ExtractedMarkdownMessage', '')\n            file_path = os.path.join('uploads', f\"download_{download_task_id}\", attachment_name)\n            save_file_content(file_content.encode('utf-8'), file_path)\n        \n        metadata = {\n            'file_name': attachment_name,\n            'file_type': attachment_type,\n            'date_uploaded': datetime.now().strftime('%Y-%m-%d %H:%S'),\n            'download_task_id': download_task_id,\n            'downloaded_file_link': file_path,\n            'email_sender': email_sender,\n            'email_subject': email_subject,\n            'file_path': file_path\n        }\n        save_task_metadata(download_task_id, metadata)\n\n        valid_audio_formats = ['flac', 'm4a', 'mp3', 'mp4', 'mpeg', 'mpga', 'oga', 'ogg', 'wav', 'webm']\n        file_extension = os.path.splitext(attachment_name)[1].lower().strip('.')\n\n        if file_extension in valid_audio_formats:\n            task_chain = chain(transcribe_audio_task.s(download_task_id), process_graph_task.s())\n        else:\n            task_chain = process_graph_task.s(download_task_id)\n\n        task_result = task_chain.apply_async()\n        graph_task_id = task_result.id\n\n        return {'status': 'Processing', 'task_id': download_task_id}\n\n    except Exception as e:\n        self.update_state(\n            state=states.FAILURE,\n            meta={\n                'exc_type': type(e).__name__,\n                'exc_message': str(e)\n            }\n        )\n        raise self.retry(exc=e, countdown=5, max_retries=3)\n\n@celery.task(bind=True)\ndef send_email_task(self, task_data):\n    try:\n        graph_task_id = task_data.get('graph_task', {}).get('task_id')\n        email_sender = task_data['email_sender']\n        email_subject = task_data['email_subject']\n\n        if graph_task_id:\n            progress_url = f'https://www.txtray.com/progress/{graph_task_id}'\n        else:\n            progress_url = f'https://www.txtray.com/progress/{self.request.id}'\n\n        logger.info(f\"Email sender received in send_email_task: {email_sender}\")\n        logger.info(f\"Email subject received in send_email_task: {email_subject}\")\n\n        if not is_valid_email(email_sender):\n            raise ValueError(f\"Invalid email address: {email_sender}\")\n\n        logger.info(f\"Sending email with progress_url: {progress_url}\")\n        send_email(\n            email=email_sender,\n            subject=f\"Re: {email_subject}\",\n            content=f\"Your request has been processed. You can view the progress at: {progress_url}\"\n        )\n        logger.info(f\"Email sent successfully for task {self.request.id}\")\n\n        self.update_state(state=states.SUCCESS)\n\n    except Exception as e:\n        logger.exception(f\"Error sending email for task {self.request.id}\")\n        raise\nfrom flask import Flask, render_template, request, jsonify, url_for\nfrom celery import states\nimport os\nimport uuid\nimport traceback\nfrom dotenv import load_dotenv\nfrom datetime import datetime\nimport logging\nfrom redis import Redis\nfrom file_processor import FileProcessor\nimport tasks  # Import tasks module\nfrom db_utils import save_task_metadata\nfrom email_utils import send_email\nfrom flask_cors import CORS\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize logger\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\napp = Flask(__name__)\nCORS(app)  # Enable CORS\n\n# Initialize Redis client\nredis_client = Redis(host='localhost', port=6379, db=0)\n\n# Initialize FileProcessor\nfile_processor = FileProcessor(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), brevo_api_key=os.getenv(\"BREVO_API\"))\n\n# Simulate a database of authorized emails\nAUTHORIZED_EMAILS = {\"authorized@example.com\", \"testuser@example.com\"}\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Go to www.txt-ray.com to make graphs\"\n\n@app.route('/graphme', methods=['GET', 'POST'])\ndef graphme():\n    if request.method == 'GET':\n        return render_template('index.html')\n    try:\n        logger.info(\"Received POST request to /graphme\")\n        if 'file' not in request.files:\n            logger.error(\"No file part in the request\")\n            return jsonify({'error': 'No file part'}), 400\n        if 'email' not in request.form:\n            logger.error(\"No email in the request\")\n            return jsonify({'error': 'No email provided'}), 400\n\n        file = request.files['file']\n        email = request.form['email']\n\n        if file.filename == '':\n            logger.error(\"No selected file\")\n            return jsonify({'error': 'No selected file'}), 400\n\n        logger.info(f\"File received: {file.filename}\")\n        logger.info(f\"Email received: {email}\")\n\n        task_id = str(uuid.uuid4())\n        download_task_id = f\"download_{task_id}\"\n        original_filename = file.filename\n        \n        folder_path = os.path.join('uploads', download_task_id)\n        os.makedirs(folder_path, exist_ok=True)\n        \n        file_path = os.path.join(folder_path, original_filename)\n        file.save(file_path)\n        logger.info(f\"File saved to: {file_path}\")\n\n        metadata = {\n            'file_path': file_path,\n            'output_dir': folder_path,\n            'task_id': task_id,\n            'download_task_id': download_task_id,\n            'email_sender': email,\n            'original_filename': original_filename,\n            'date_uploaded': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'email_subject': f\"Your TxtRay Graph for {original_filename} is Ready\"\n        }\n        \n        save_task_metadata(task_id, metadata)\n        logger.info(f\"Metadata saved for task: {task_id}\")\n\n        process_task = tasks.process_graph_task.delay(task_id, email)  # Use tasks.process_graph_task here\n        logger.info(f\"Process task started with ID: {process_task.id}\")\n\n        response = {\n            'task_id': process_task.id,\n            'task_url': url_for('progress', task_id=process_task.id, _external=True)\n        }\n        logger.info(f\"Sending response: {response}\")\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error in /graphme route: {e}\\n{traceback.format_exc()}\")\n        return jsonify({'error': str(e)}), 500\n\n\n@app.route('/progress/<task_id>')\ndef progress(task_id):\n    return render_template('progress.html', task_id=task_id)\n\n@app.route('/status/<task_id>')\ndef task_status(task_id):\n    task = process_graph_task.AsyncResult(task_id)\n    \n    if task.state == states.PENDING:\n        response = {\n            'state': task.state,\n            'status': 'Processing...'\n        }\n    elif task.state == states.SUCCESS:\n        result_dict = task.result\n        graph_task_id = result_dict['graph_task']['task_id']\n        graph_task = process_graph_task.AsyncResult(graph_task_id)\n        if graph_task.state == states.SUCCESS:\n            response = {\n                'state': graph_task.state,\n                'status': 'Processing complete.',\n                'result': graph_task.result['graph_task']\n            }\n        else:\n            response = {\n                'state': graph_task.state,\n                'status': 'Processing...'\n            }\n    elif task.state == states.FAILURE:\n        response = {\n            'state': task.state,\n            'status': f'Error: {str(task.info)}'\n        }\n    else:\n        response = {\n            'state': task.state,\n            'status': str(task.info)\n        }\n    \n    return jsonify(response)\n    \n@app.route('/graphmeairtable', methods=['POST'])\ndef graphmeairtable():\n    try:\n        if request.is_json:\n            data = request.get_json()\n            if 'fileUrl' in data:\n                file_url = data['fileUrl']\n                task_id = str(uuid.uuid4())\n                file_path, error = download_file_from_url(file_url, 'file_from_url.txt')\n                if error:\n                    return jsonify({'error': error}), 400\n                output_dir = os.path.dirname(file_path)\n            elif 'fileContent' in data:\n                task_id = str(uuid.uuid4())\n                file_path, output_dir = save_file_content(data['fileContent'], 'file_from_content.txt', task_id)\n            else:\n                return jsonify({'error': 'No file URL or file content provided'}), 400\n        else:\n            return jsonify({'error': 'Invalid request format'}), 400\n        \n        metadata = {\n            'file_path': file_path,\n            'output_dir': output_dir,\n            'task_id': task_id\n        }\n        save_task_metadata(task_id, metadata)\n        \n        process_task = process_graph_task.delay(task_id)\n        return jsonify({'task_id': process_task.id, 'task_url': url_for('task_status', task_id=process_task.id, _external=True)})\n\n    except Exception as e:\n        logger.error(f\"Error in /graphmeairtable route: {e}\\n{traceback.format_exc()}\")\n        return jsonify({'error': 'Internal Server Error'}), 500\n\n\n@app.route('/update_subscription', methods=['POST'])\ndef update_subscription():\n    try:\n        data = request.get_json()\n        email = data['email']\n        subscription = data['subscription']\n        \n        # Log current Redis subscriptions related to authorized emails\n        logger.info(\"Current Redis subscriptions for authorized emails:\")\n        for key in redis_client.scan_iter(\"authorized_email_*\"):\n            logger.info(f\"{key.decode('utf-8')}: {redis_client.get(key).decode('utf-8')}\")\n        \n        # Save the email and subscription to Redis\n        redis_client.set(f\"authorized_email_{email}\", subscription)\n        \n        # Log updated Redis subscriptions related to authorized emails\n        logger.info(\"Updated Redis subscriptions for authorized emails:\")\n        for key in redis_client.scan_iter(\"authorized_email_*\"):\n            logger.info(f\"{key.decode('utf-8')}: {redis_client.get(key).decode('utf-8')}\")\n        \n        return jsonify({'message': 'Subscription updated successfully'}), 200\n\n    except Exception as e:\n        logger.error(f\"Error updating subscription: {e}\")\n        return jsonify({'error': 'Internal Server Error'}), 500\n\n@app.route('/webhook', methods=['POST'])\ndef respond():\n    app.logger.info('Webhook received: %s', request.json)\n    \n    try:\n        data = request.json['items'][0]\n        \n        email_sender = data['From']['Address']\n        email_recipient = data['To'][0]['Address']\n        email_subject = data['Subject']\n        \n        # Check if the sender's email is authorized\n        if email_sender not in AUTHORIZED_EMAILS and not redis_client.exists(f\"authorized_email_{email_sender}\"):\n            send_email(\n                email=email_sender,\n                subject=\"Unauthorized Access Attempt\",\n                content=\"You are not authorized to use this service. Please register at txt-ray.com and pay for a subscription to gain access.\"\n            )\n            return jsonify({'error': 'Unauthorized access'}), 403\n        \n        app.logger.info(f\"Authorized email: {email_sender}\")\n        \n        if email_recipient == 'chat@txtray.com':\n            app.logger.info(f\"Processing chat email from {email_sender}\")\n            task = process_chat_task.delay(data)\n        else:\n            app.logger.info(f\"Processing document email from {email_sender}\")\n            if 'Attachments' in data and len(data['Attachments']) > 0:\n                attachment = data['Attachments'][0]\n                file_url = f\"https://api.brevo.com/v3/inbound/attachments/{attachment['DownloadToken']}\"\n            else:\n                file_url = None\n            \n            api_key = os.getenv(\"BREVO_API\")\n            task = download_and_process_attachment_task.delay(file_url, email_sender, email_recipient, api_key, email_subject, data)\n        \n        return jsonify({'task_id': task.id, 'task_url': url_for('task_status', task_id=task.id, _external=True)}), 200\n    \n    except Exception as e:\n        app.logger.error(f\"Error processing webhook: {e}\")\n        return jsonify({'error': 'Webhook processing failed'}), 500\n@app.route('/whisper', methods=['POST'])\ndef whisper():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No selected file'}), 400\n\n    task_id = str(uuid.uuid4())\n    file_path, output_dir = save_file_content(file.read(), file.filename, task_id)\n    if file_path is None:\n        return jsonify({'error': 'File saving failed'}), 500\n\n    try:\n        metadata = {\n            'file_name': file.filename,\n            'date_uploaded': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'download_task_id': task_id,\n            'downloaded_file_link': file_path,\n            'file_path': file_path\n        }\n        save_task_metadata(task_id, metadata)\n\n        task_chain = chain(\n            transcribe_audio_task.s(task_id),\n            process_graph_task.s()\n        )\n        task_chain.apply_async()\n\n        return jsonify({'task_id': task_id})\n    except Exception as e:\n        app.logger.error(f\"Error starting transcription task: {e}\")\n        return jsonify({'error': 'Transcription task failed'}), 500\n        \n\nif __name__ == '__main__':\n    if not os.path.exists('uploads'):\n        os.makedirs('uploads')\n    if not os.path.exists('static/generated'):\n        os.makedirs('static/generated')\n    \n    app.run(debug=True, host='0.0.0.0')\nINFO:app:Sending response: {'task_id': 'f62ec983-1ef1-4ab1-affc-07902139632e', 'task_url': 'http://txtray.com/progress/f62ec983-1ef1-4ab1-affc-07902139632e'}\nERROR:app:Exception on /status/f62ec983-1ef1-4ab1-affc-07902139632e [GET]\nTraceback (most recent call last):\n  File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/flask_cors/extension.py\", line 178, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n  File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n  File \"/home/bilal/public_html/txtray.com/app.py\", line 108, in task_status\n    task = process_graph_task.AsyncResult(task_id)\nNameError: name 'process_graph_task' is not defined\n[2024-06-21 18:12:14,777: INFO/MainProcess] celery@server1.checkin.cards ready.\n[2024-06-21 18:12:25,163: ERROR/MainProcess] Received unregistered task of type 'tasks.process_graph_task'.\nThe message has been ignored and discarded.\n\nDid you remember to import the module containing this task?\nOr maybe you're using relative imports?\n\nPlease see\nhttps://docs.celeryq.dev/en/latest/internals/protocol.html\nfor more information.\n\nThe full contents of the message body was:\nb'[[\"8937955a-a4c9-4d29-b922-8f1d5f9dd003\", \"bilal@bloom.pm\"], {}, {\"callbacks\": null, \"errbacks\": null, \"chain\": null, \"chord\": null}]' (133b)\n\nThe full contents of the message headers:\n{'lang': 'py', 'task': 'tasks.process_graph_task', 'id': 'f62ec983-1ef1-4ab1-affc-07902139632e', 'shadow': None, 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'retries': 0, 'timelimit': [None, None], 'root_id': 'f62ec983-1ef1-4ab1-affc-07902139632e', 'parent_id': None, 'argsrepr': \"('8937955a-a4c9-4d29-b922-8f1d5f9dd003', 'bilal@bloom.pm')\", 'kwargsrepr': '{}', 'origin': 'gen992767@server1.checkin.cards', 'ignore_result': False, 'replaced_task_nesting': 0, 'stamped_headers': None, 'stamps': {}}\n\nThe delivery info for this task is:\n{'exchange': '', 'routing_key': 'celery'}\nTraceback (most recent call last):\n  File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/celery/worker/consumer/consumer.py\", line 659, in on_task_received\n    strategy = strategies[type_]\nKeyError: 'tasks.process_graph_task'\n",
      "created_at": "2024-06-21T20:17:15.686107",
      "sequence_order": 0,
      "parent_id": "b074b3d3-a02a-4428-88dc-81f2d2137490",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "b84633ee-0803-4771-aa43-e86c49affab4",
      "role": "assistant",
      "content": "Hello! We're here to help you improve your code and processes. Each of us will focus on different aspects to give you a comprehensive review and suggestions:\n\n**Josephine (White Hat):**\n\nThe code seems to be handling various tasks related to file processing, email communication, and task management using Celery. The main components include:\n1. `process_chat_task`: Processes chat-related tasks from email data.\n2. `extract_text_from_pdf`: Extracts text from PDF files.\n3. `process_graph_task`: Generates graphs from different file types.\n4. `transcribe_audio_task`: Transcribes audio files.\n5. `download_and_process_attachment_task`: Downloads and processes email attachments.\n6. `send_email_task`: Sends emails.\n7. Flask routes for handling web requests and task status.\n\nThere are a few issues to address:\n1. The error in the `task_status` route suggests that `process_graph_task` is not imported correctly.\n2. The error message from Celery indicates that the task `tasks.process_graph_task` is unregistered.\n\n**Prunella (Black Hat):**\n\nPotential problems and risks include:\n1. The `task_status` route failing due to a missing import can cause user confusion and disrupt the workflow.\n2. Unregistered Celery tasks mean tasks are not executed, leading to processing failures.\n3. Lack of error handling in some asynchronous functions could cause unhandled exceptions.\n4. Email validation in the `send_email_task` might not cover all edge cases.\n\n**Percy (Yellow Hat):**\n\nPositive aspects:\n1. The code is modular, with clear separation of tasks.\n2. Use of Celery for asynchronous task management is a good choice for scalability.\n3. The inclusion of logging is beneficial for debugging and monitoring.\n4. The use of Flask for web endpoints allows easy integration with other services.\n\n**Claude (Green Hat):**\n\nHere are some creative ideas for improvement:\n1. **Centralized Error Handling**: Implement a centralized error handling mechanism to catch and log all exceptions.\n2. **Dynamic Task Registration**: Ensure tasks are dynamically registered with Celery to avoid the unregistered task error.\n3. **Enhanced Email Validation**: Use a more robust email validation library like `email-validator` to handle edge cases.\n4. **File Type Detection**: Improve file type detection and handling for better flexibility and error resilience.\n\n**Cyril (Red Hat):**\n\nFrom a user perspective, encountering errors can be frustrating. Itâ€™s crucial to provide meaningful feedback and ensure smooth task execution. The overall user experience can be enhanced by ensuring tasks are processed correctly and errors are communicated effectively.\n\n**Quackers (Blue Hat):**\n\nHere's a comprehensive plan to address the identified issues and implement improvements:\n\n1. **Fix Import Issue**:\n   Ensure `process_graph_task` is imported in the Flask app.\n   ```python\n   from tasks import process_graph_task\n   ```\n\n2. **Register Celery Tasks**:\n   Ensure all Celery tasks are registered in the Celery app configuration.\n   ```python\n   app = Celery('tasks', broker='pyamqp://guest@localhost//')\n\n   app.conf.update(\n       task_routes={\n           'tasks.process_graph_task': {'queue': 'graph'},\n           'tasks.transcribe_audio_task': {'queue': 'audio'},\n           'tasks.send_email_task': {'queue': 'email'},\n       }\n   )\n   ```\n\n3. **Centralize Error Handling**:\n   Implement a centralized error handler to log and manage exceptions.\n   ```python\n   def handle_exception(exc):\n       logger.error(f\"Exception occurred: {exc}\")\n       return {\"error\": str(exc)}, 500\n\n   app.register_error_handler(Exception, handle_exception)\n   ```\n\n4. **Improve Email Validation**:\n   Use a robust library for email validation.\n   ```python\n   from email_validator import validate_email, EmailNotValidError\n\n   def is_valid_email(email):\n       try:\n           validate_email(email)\n           return True\n       except EmailNotValidError:\n           return False\n   ```\n\n5. **Enhanced File Type Detection**:\n   Use more reliable methods for detecting file types.\n   ```python\n   import magic\n\n   def get_file_type(file_path):\n       mime = magic.Magic(mime=True)\n       return mime.from_file(file_path)\n   ```\n\n6. **User Feedback**:\n   Improve user feedback for errors and processing status.\n   ```python\n   @app.route('/status/<task_id>')\n   def task_status(task_id):\n       task = process_graph_task.AsyncResult(task_id)\n       if task.state == states.PENDING:\n           response = {'state': task.state, 'status': 'Processing...'}\n       elif task.state == states.SUCCESS:\n           result_dict = task.result\n           response = {'state': task.state, 'status': 'Processing complete.', 'result': result_dict}\n       elif task.state == states.FAILURE:\n           response = {'state': task.state, 'status': f'Error: {str(task.info)}'}\n       else:\n           response = {'state': task.state, 'status': str(task.info)}\n       return jsonify(response)\n   ```\n\nBy addressing these points, the overall functionality and user experience of your system will be greatly enhanced. Let us know if you need any more specific details or further assistance!",
      "created_at": "2024-06-21T20:17:47.183482",
      "sequence_order": 1,
      "parent_id": "aaa23685-b737-4c87-986a-5c3dc601e0e0",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": true
      }
    },
    {
      "id": "aaa2e565-0dbf-40ca-9681-40ec5f10c1b1",
      "role": "user",
      "content": "celery_app.py\nfrom celery import Celery\n\ncelery = Celery('tasks', broker='redis://localhost:6379/0', backend='redis://localhost:6379/0')\n\ncelery.conf.update(\n    task_serializer='json',\n    accept_content=['json'],\n    result_serializer='json',\n    timezone='UTC',\n    enable_utc=True,\n)\n\ncelery_config.py\nfrom celery import Celery\n\nbroker_url = 'redis://localhost:6379/0'\nresult_backend = 'redis://localhost:6379/0'\ntask_serializer = 'json'\nresult_serializer = 'json'\naccept_content = ['json']\ntimezone = 'UTC'\nenable_utc = True\n\n\ndef make_celery(app):\n    celery = Celery(\n        app.import_name,\n        backend=app.config['CELERY_RESULT_BACKEND'],\n        broker=app.config['CELERY_BROKER_URL']\n    )\n    celery.conf.update(app.config)\n    TaskBase = celery.Task\n\n    class ContextTask(TaskBase):\n        def __call__(self, *args, **kwargs):\n            with app.app_context():\n                return TaskBase.__call__(self, *args, **kwargs)\n\n    celery.Task = ContextTask\n    return celeryfrom flask import Flask, render_template, request, jsonify, url_for\nfrom celery import states\nimport os\nimport uuid\nimport traceback\nfrom dotenv import load_dotenv\nfrom datetime import datetime\nimport logging\nfrom redis import Redis\nfrom file_processor import FileProcessor\nfrom db_utils import save_task_metadata\nfrom email_utils import send_email\nfrom flask_cors import CORS\nfrom tasks import process_graph_task\nimport tasks  # Import tasks module\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize logger\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\napp = Flask(__name__)\nCORS(app)  # Enable CORS\n\n# Initialize Redis client\nredis_client = Redis(host='localhost', port=6379, db=0)\n\n# Initialize FileProcessor\nfile_processor = FileProcessor(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), brevo_api_key=os.getenv(\"BREVO_API\"))\n\n# Simulate a database of authorized emails\nAUTHORIZED_EMAILS = {\"authorized@example.com\", \"testuser@example.com\"}\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Go to www.txt-ray.com to make graphs\"\n\n@app.route('/graphme', methods=['GET', 'POST'])\ndef graphme():\n    if request.method == 'GET':\n        return render_template('index.html')\n    try:\n        logger.info(\"Received POST request to /graphme\")\n        if 'file' not in request.files:\n            logger.error(\"No file part in the request\")\n            return jsonify({'error': 'No file part'}), 400\n        if 'email' not in request.form:\n            logger.error(\"No email in the request\")\n            return jsonify({'error': 'No email provided'}), 400\n\n        file = request.files['file']\n        email = request.form['email']\n\n        if file.filename == '':\n            logger.error(\"No selected file\")\n            return jsonify({'error': 'No selected file'}), 400\n\n        logger.info(f\"File received: {file.filename}\")\n        logger.info(f\"Email received: {email}\")\n\n        task_id = str(uuid.uuid4())\n        download_task_id = f\"download_{task_id}\"\n        original_filename = file.filename\n        \n        folder_path = os.path.join('uploads', download_task_id)\n        os.makedirs(folder_path, exist_ok=True)\n        \n        file_path = os.path.join(folder_path, original_filename)\n        file.save(file_path)\n        logger.info(f\"File saved to: {file_path}\")\n\n        metadata = {\n            'file_path': file_path,\n            'output_dir': folder_path,\n            'task_id': task_id,\n            'download_task_id': download_task_id,\n            'email_sender': email,\n            'original_filename': original_filename,\n            'date_uploaded': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'email_subject': f\"Your TxtRay Graph for {original_filename} is Ready\"\n        }\n        \n        save_task_metadata(task_id, metadata)\n        logger.info(f\"Metadata saved for task: {task_id}\")\n\n        process_task = tasks.process_graph_task.delay(task_id, email)  # Use tasks.process_graph_task here\n        logger.info(f\"Process task started with ID: {process_task.id}\")\n\n        response = {\n            'task_id': process_task.id,\n            'task_url': url_for('progress', task_id=process_task.id, _external=True)\n        }\n        logger.info(f\"Sending response: {response}\")\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error in /graphme route: {e}\\n{traceback.format_exc()}\")\n        return jsonify({'error': str(e)}), 500\n\n\n@app.route('/progress/<task_id>')\ndef progress(task_id):\n    return render_template('progress.html', task_id=task_id)\n\n@app.route('/status/<task_id>')\ndef task_status(task_id):\n    task = process_graph_task.AsyncResult(task_id)\n    \n    if task.state == states.PENDING:\n        response = {\n            'state': task.state,\n            'status': 'Processing...'\n        }\n    elif task.state == states.SUCCESS:\n        result_dict = task.result\n        graph_task_id = result_dict['graph_task']['task_id']\n        graph_task = process_graph_task.AsyncResult(graph_task_id)\n        if graph_task.state == states.SUCCESS:\n            response = {\n                'state': graph_task.state,\n                'status': 'Processing complete.',\n                'result': graph_task.result['graph_task']\n            }\n        else:\n            response = {\n                'state': graph_task.state,\n                'status': 'Processing...'\n            }\n    elif task.state == states.FAILURE:\n        response = {\n            'state': task.state,\n            'status': f'Error: {str(task.info)}'\n        }\n    else:\n        response = {\n            'state': task.state,\n            'status': str(task.info)\n        }\n    \n    return jsonify(response)\n    \n@app.route('/graphmeairtable', methods=['POST'])\ndef graphmeairtable():\n    try:\n        if request.is_json:\n            data = request.get_json()\n            if 'fileUrl' in data:\n                file_url = data['fileUrl']\n                task_id = str(uuid.uuid4())\n                file_path, error = download_file_from_url(file_url, 'file_from_url.txt')\n                if error:\n                    return jsonify({'error': error}), 400\n                output_dir = os.path.dirname(file_path)\n            elif 'fileContent' in data:\n                task_id = str(uuid.uuid4())\n                file_path, output_dir = save_file_content(data['fileContent'], 'file_from_content.txt', task_id)\n            else:\n                return jsonify({'error': 'No file URL or file content provided'}), 400\n        else:\n            return jsonify({'error': 'Invalid request format'}), 400\n        \n        metadata = {\n            'file_path': file_path,\n            'output_dir': output_dir,\n            'task_id': task_id\n        }\n        save_task_metadata(task_id, metadata)\n        \n        process_task = process_graph_task.delay(task_id)\n        return jsonify({'task_id': process_task.id, 'task_url': url_for('task_status', task_id=process_task.id, _external=True)})\n\n    except Exception as e:\n        logger.error(f\"Error in /graphmeairtable route: {e}\\n{traceback.format_exc()}\")\n        return jsonify({'error': 'Internal Server Error'}), 500\n\n\n@app.route('/update_subscription', methods=['POST'])\ndef update_subscription():\n    try:\n        data = request.get_json()\n        email = data['email']\n        subscription = data['subscription']\n        \n        # Log current Redis subscriptions related to authorized emails\n        logger.info(\"Current Redis subscriptions for authorized emails:\")\n        for key in redis_client.scan_iter(\"authorized_email_*\"):\n            logger.info(f\"{key.decode('utf-8')}: {redis_client.get(key).decode('utf-8')}\")\n        \n        # Save the email and subscription to Redis\n        redis_client.set(f\"authorized_email_{email}\", subscription)\n        \n        # Log updated Redis subscriptions related to authorized emails\n        logger.info(\"Updated Redis subscriptions for authorized emails:\")\n        for key in redis_client.scan_iter(\"authorized_email_*\"):\n            logger.info(f\"{key.decode('utf-8')}: {redis_client.get(key).decode('utf-8')}\")\n        \n        return jsonify({'message': 'Subscription updated successfully'}), 200\n\n    except Exception as e:\n        logger.error(f\"Error updating subscription: {e}\")\n        return jsonify({'error': 'Internal Server Error'}), 500\n\n@app.route('/webhook', methods=['POST'])\ndef respond():\n    app.logger.info('Webhook received: %s', request.json)\n    \n    try:\n        data = request.json['items'][0]\n        \n        email_sender = data['From']['Address']\n        email_recipient = data['To'][0]['Address']\n        email_subject = data['Subject']\n        \n        # Check if the sender's email is authorized\n        if email_sender not in AUTHORIZED_EMAILS and not redis_client.exists(f\"authorized_email_{email_sender}\"):\n            send_email(\n                email=email_sender,\n                subject=\"Unauthorized Access Attempt\",\n                content=\"You are not authorized to use this service. Please register at txt-ray.com and pay for a subscription to gain access.\"\n            )\n            return jsonify({'error': 'Unauthorized access'}), 403\n        \n        app.logger.info(f\"Authorized email: {email_sender}\")\n        \n        if email_recipient == 'chat@txtray.com':\n            app.logger.info(f\"Processing chat email from {email_sender}\")\n            task = process_chat_task.delay(data)\n        else:\n            app.logger.info(f\"Processing document email from {email_sender}\")\n            if 'Attachments' in data and len(data['Attachments']) > 0:\n                attachment = data['Attachments'][0]\n                file_url = f\"https://api.brevo.com/v3/inbound/attachments/{attachment['DownloadToken']}\"\n            else:\n                file_url = None\n            \n            api_key = os.getenv(\"BREVO_API\")\n            task = download_and_process_attachment_task.delay(file_url, email_sender, email_recipient, api_key, email_subject, data)\n        \n        return jsonify({'task_id': task.id, 'task_url': url_for('task_status', task_id=task.id, _external=True)}), 200\n    \n    except Exception as e:\n        app.logger.error(f\"Error processing webhook: {e}\")\n        return jsonify({'error': 'Webhook processing failed'}), 500\n@app.route('/whisper', methods=['POST'])\ndef whisper():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No selected file'}), 400\n\n    task_id = str(uuid.uuid4())\n    file_path, output_dir = save_file_content(file.read(), file.filename, task_id)\n    if file_path is None:\n        return jsonify({'error': 'File saving failed'}), 500\n\n    try:\n        metadata = {\n            'file_name': file.filename,\n            'date_uploaded': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'download_task_id': task_id,\n            'downloaded_file_link': file_path,\n            'file_path': file_path\n        }\n        save_task_metadata(task_id, metadata)\n\n        task_chain = chain(\n            transcribe_audio_task.s(task_id),\n            process_graph_task.s()\n        )\n        task_chain.apply_async()\n\n        return jsonify({'task_id': task_id})\n    except Exception as e:\n        app.logger.error(f\"Error starting transcription task: {e}\")\n        return jsonify({'error': 'Transcription task failed'}), 500\n        \n\nif __name__ == '__main__':\n    if not os.path.exists('uploads'):\n        os.makedirs('uploads')\n    if not os.path.exists('static/generated'):\n        os.makedirs('static/generated')\n    \n    app.run(debug=True, host='0.0.0.0')\nfrom typing import Dict, Any\nimport asyncio\nimport json\nimport logging\nimport os\nimport re\nimport uuid\nimport mimetypes\nfrom celery import chain, states\nfrom celery.utils.log import get_task_logger\nfrom contextlib import asynccontextmanager\nfrom datetime import datetime\nfrom redis import Redis\nfrom graph_generator import generate_graph, save_graph_to_html\nfrom whisper_api import transcribe_audio\nfrom download_utils import download_file_from_brevo, save_file_content\nfrom email_utils import send_email\nfrom db_utils import get_task_metadata, save_task_metadata\nfrom airtable_utils import create_graph_record_async, update_status_async\nfrom PyPDF2 import PdfReader\nfrom chat_utils import save_chat_context, load_chat_context, extract_conversation_id, extract_message_from_email\nfrom ollama_utils import generate_ollama_response\nfrom celery_app import celery\n\nlogger = get_task_logger(__name__)\nREDIS_CLIENT = Redis(host='localhost', port=6379, db=0)\n\n@celery.task(bind=True)\ndef process_chat_task(self, email_data: Dict[str, Any]) -> Dict[str, Any]:\n    logger.info(f\"Starting process_chat_task for email: {email_data['From']['Address']}\")\n    \n    conversation_id = extract_conversation_id(email_data['Subject'])\n    context = load_chat_context(conversation_id)\n    new_message = extract_message_from_email(email_data)\n    \n    context.append({\"role\": \"user\", \"content\": new_message})\n    ollama_response = generate_ollama_response(context)\n    context.append({\"role\": \"assistant\", \"content\": ollama_response})\n    \n    save_chat_context(conversation_id, context)\n    \n    send_email(\n        email=email_data['From']['Address'],\n        subject=f\"Re: {email_data['Subject']}\",\n        content=ollama_response\n    )\n    \n    return {\"status\": \"completed\", \"conversation_id\": conversation_id}    \n    \nasync def extract_text_from_pdf(file_path):\n    logger.info(f\"Extracting text from PDF: {file_path}\")\n    text = \"\"\n    try:\n        with open(file_path, 'rb') as file:\n            pdf_reader = PdfReader(file)\n            for page in pdf_reader.pages:\n                text += page.extract_text()\n    except Exception as e:\n        logger.error(f\"Error extracting text from PDF: {str(e)}\")\n        raise\n    return text\n\n@celery.task(bind=True)\ndef process_graph_task(self, task_id, email=None):\n    task_id = str(task_id)\n    logger.info(f\"Starting process_graph_task with task_id: {task_id}\")\n    try:\n        task_data = get_task_metadata(task_id)\n        if task_data is None:\n            raise ValueError(f\"Task metadata not found for task_id: {task_id}\")\n\n        logger.info(f\"Retrieved task data: {task_data}\")\n\n        file_path = task_data['file_path']\n        logger.info(f\"File path from metadata: {file_path}\")\n\n        file_type, _ = mimetypes.guess_type(file_path)\n        logger.info(f\"Detected file type: {file_type}\")\n\n        graph_task_id = self.request.id\n        email_sender = email or task_data.get('email_sender')\n\n        logger.info(f\"Using current task ID: {graph_task_id} for process_graph_task\")\n        graph_output_dir = os.path.join('static', 'generated', f\"graph_{graph_task_id}\")\n        os.makedirs(graph_output_dir, exist_ok=True)\n\n        # Create Airtable record with status \"Processing\"\n        async def create_record():\n            record_id = await create_graph_record_async(task_id, email_sender)\n            return record_id\n\n        record_id = asyncio.run(create_record())\n        logger.info(f\"Airtable record created with ID: {record_id}\")\n\n        # Process different file types\n        if file_type == 'application/pdf':\n            extracted_text = asyncio.run(extract_text_from_pdf(file_path))\n            text_file_path = os.path.join(os.path.dirname(file_path), 'extracted_text.txt')\n            with open(text_file_path, 'w', encoding='utf-8') as f:\n                f.write(extracted_text)\n            file_path = text_file_path\n        elif file_type and file_type.startswith('audio/'):\n            transcription = asyncio.run(transcribe_audio(file_path))\n            transcription_path = os.path.join(os.path.dirname(file_path), 'transcription.txt')\n            with open(transcription_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            file_path = transcription_path\n\n        G = generate_graph(file_path, graph_output_dir)\n        save_graph_to_html(G, graph_output_dir)\n\n        logger.info(f\"Graph generated successfully for process_graph_task with task_id: {self.request.id}\")\n\n        result_dict = {\n            'graph_task': {\n                'task_id': graph_task_id,\n                'd3_url': f'/static/generated/graph_{graph_task_id}/index.html',\n                'pyvis_url': f'/static/generated/graph_{graph_task_id}/pyvis.html'\n            }\n        }\n\n        # Update Airtable record to \"Processing Completed\"\n        async def update_record():\n            await update_status_async(record_id, \"Processing Completed\")\n\n        asyncio.run(update_record())\n\n        # Trigger email task\n        task_data.update(result_dict)\n        send_email_task.delay(task_data)\n\n        self.update_state(state=states.SUCCESS, meta=result_dict)\n        return result_dict\n\n    except Exception as e:\n        logger.exception(f\"Error generating graph for process_graph_task with task_id: {self.request.id}\")\n        raise\n\ndef is_valid_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\n@celery.task(bind=True)\ndef transcribe_audio_task(self, task_id):\n    task_id = str(task_id)\n    task_data = get_task_metadata(task_id)\n    file_path = task_data['file_path']\n    download_task_id = task_data['download_task_id']\n\n    logger.info(f\"Starting transcribe_audio_task with download_task_id: {download_task_id}\")\n    output_dir = os.path.join('uploads', f\"tr_{download_task_id}\")\n    transcription_path = os.path.join(output_dir, 'transcription.txt')\n\n    if not os.path.exists(transcription_path):\n        async def run_transcription():\n            async with redis_lock(f\"transcribe_lock_{download_task_id}\") as acquired:\n                if acquired:\n                    try:\n                        logger.info(f\"Starting transcription for download task {download_task_id}\")\n                        logger.info(f\"File path: {file_path}\")\n                        logger.info(f\"File exists: {os.path.exists(file_path)}\")\n                        logger.info(f\"File size: {os.path.getsize(file_path)} bytes\")\n                        \n                        transcription = await transcribe_audio(file_path)\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(transcription)\n\n                        task_data.update({\n                            'transcription_data': transcription,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n\n                    except Exception as e:\n                        logger.error(f\"Error during transcription: {str(e)}\")\n                        logger.exception(\"Full traceback:\")\n                        # Fallback: create a text file with error message\n                        error_message = f\"Transcription failed. Error: {str(e)}\"\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(error_message)\n                        task_data.update({\n                            'transcription_data': error_message,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n                else:\n                    logger.info(f\"Another instance of the task is already running for download task {download_task_id}\")\n\n        try:\n            asyncio.run(run_transcription())\n        except Exception as e:\n            logger.error(f\"Error in run_transcription: {str(e)}\")\n            # Don't retry, we've already handled the error\n\n    result_dict = {'task_id': task_id}\n    logger.info(f\"transcribe_audio_task result_dict updated with transcription_path: {transcription_path}\")\n    logger.info(f\"Updated result_dict: {result_dict}\")\n    \n    self.update_state(state=states.SUCCESS, meta=result_dict)\n    return task_id\n        \n@celery.task(bind=True)\ndef download_and_process_attachment_task(self, file_url, email_sender, email_recipient, api_key, email_subject, data):\n    try:\n        download_task_id = str(uuid.uuid4())\n        logger.info(f\"Starting download_and_process_attachment_task with download_task_id: {download_task_id}\")\n\n        if 'Attachments' in data and data['Attachments']:\n            attachment_data = data['Attachments'][0]\n            attachment_name = attachment_data['Name']\n            attachment_type = attachment_data['ContentType']\n            download_token = attachment_data['DownloadToken']\n            \n            file_content = download_file_from_brevo(download_token, api_key)\n            file_path = os.path.join('uploads', f\"download_{download_task_id}\", attachment_name)\n            save_file_content(file_content, file_path)\n        else:\n            attachment_name = 'message.txt'\n            attachment_type = 'text/plain'\n            file_content = data.get('ExtractedMarkdownMessage', '')\n            file_path = os.path.join('uploads', f\"download_{download_task_id}\", attachment_name)\n            save_file_content(file_content.encode('utf-8'), file_path)\n        \n        metadata = {\n            'file_name': attachment_name,\n            'file_type': attachment_type,\n            'date_uploaded': datetime.now().strftime('%Y-%m-%d %H:%S'),\n            'download_task_id': download_task_id,\n            'downloaded_file_link': file_path,\n            'email_sender': email_sender,\n            'email_subject': email_subject,\n            'file_path': file_path\n        }\n        save_task_metadata(download_task_id, metadata)\n\n        valid_audio_formats = ['flac', 'm4a', 'mp3', 'mp4', 'mpeg', 'mpga', 'oga', 'ogg', 'wav', 'webm']\n        file_extension = os.path.splitext(attachment_name)[1].lower().strip('.')\n\n        if file_extension in valid_audio_formats:\n            task_chain = chain(transcribe_audio_task.s(download_task_id), process_graph_task.s())\n        else:\n            task_chain = process_graph_task.s(download_task_id)\n\n        task_result = task_chain.apply_async()\n        graph_task_id = task_result.id\n\n        return {'status': 'Processing', 'task_id': download_task_id}\n\n    except Exception as e:\n        self.update_state(\n            state=states.FAILURE,\n            meta={\n                'exc_type': type(e).__name__,\n                'exc_message': str(e)\n            }\n        )\n        raise self.retry(exc=e, countdown=5, max_retries=3)\n\n@celery.task(bind=True)\ndef send_email_task(self, task_data):\n    try:\n        graph_task_id = task_data.get('graph_task', {}).get('task_id')\n        email_sender = task_data['email_sender']\n        email_subject = task_data['email_subject']\n\n        if graph_task_id:\n            progress_url = f'https://www.txtray.com/progress/{graph_task_id}'\n        else:\n            progress_url = f'https://www.txtray.com/progress/{self.request.id}'\n\n        logger.info(f\"Email sender received in send_email_task: {email_sender}\")\n        logger.info(f\"Email subject received in send_email_task: {email_subject}\")\n\n        if not is_valid_email(email_sender):\n            raise ValueError(f\"Invalid email address: {email_sender}\")\n\n        logger.info(f\"Sending email with progress_url: {progress_url}\")\n        send_email(\n            email=email_sender,\n            subject=f\"Re: {email_subject}\",\n            content=f\"Your request has been processed. You can view the progress at: {progress_url}\"\n        )\n        logger.info(f\"Email sent successfully for task {self.request.id}\")\n\n        self.update_state(state=states.SUCCESS)\n\n    except Exception as e:\n        logger.exception(f\"Error sending email for task {self.request.id}\")\n        raise\n\n\nwhere do i register celery tasks, think step by step and find best practices to streamline my celery config set up",
      "created_at": "2024-06-21T20:20:48.578636",
      "sequence_order": 2,
      "parent_id": "b84633ee-0803-4771-aa43-e86c49affab4",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "81e14cdb-af82-4938-b946-8ed634e4cd27",
      "role": "assistant",
      "content": "To register Celery tasks and streamline your Celery configuration, you should follow best practices to ensure tasks are correctly imported and registered. Here is a step-by-step guide to achieve this:\n\n### Step 1: Celery Configuration\n\nYou should have a single, central configuration file for your Celery app. You have a good start with `celery_app.py` and `celery_config.py`, but let's streamline it.\n\n#### `celery_config.py`\nThis file contains configuration settings. You can keep it as it is:\n\n```python\n# celery_config.py\nfrom celery import Celery\n\nbroker_url = 'redis://localhost:6379/0'\nresult_backend = 'redis://localhost:6379/0'\ntask_serializer = 'json'\nresult_serializer = 'json'\naccept_content = ['json']\ntimezone = 'UTC'\nenable_utc = True\n\ndef make_celery(app):\n    celery = Celery(\n        app.import_name,\n        backend=app.config['CELERY_RESULT_BACKEND'],\n        broker=app.config['CELERY_BROKER_URL']\n    )\n    celery.conf.update(app.config)\n    TaskBase = celery.Task\n\n    class ContextTask(TaskBase):\n        def __call__(self, *args, **kwargs):\n            with app.app_context():\n                return TaskBase.__call__(self, *args, **kwargs)\n\n    celery.Task = ContextTask\n    return celery\n```\n\n### Step 2: Initialize Celery in Your Flask App\n\nInitialize Celery with your Flask app and load the configuration.\n\n#### `celery_app.py`\nUpdate `celery_app.py` to ensure it initializes the Celery instance correctly:\n\n```python\n# celery_app.py\nfrom celery import Celery\nfrom celery_config import make_celery\n\ncelery = Celery('tasks', broker='redis://localhost:6379/0', backend='redis://localhost:6379/0')\n\ncelery.conf.update(\n    task_serializer='json',\n    accept_content=['json'],\n    result_serializer='json',\n    timezone='UTC',\n    enable_utc=True,\n)\n```\n\n### Step 3: Create a Celery Instance with Flask\n\nYou need to ensure that the Celery instance is correctly set up with your Flask app. Modify your Flask app initialization to use the `make_celery` function from `celery_config.py`.\n\n#### `app.py`\n\n```python\n# app.py\nfrom flask import Flask, render_template, request, jsonify, url_for\nimport os\nimport uuid\nimport traceback\nfrom datetime import datetime\nimport logging\nfrom dotenv import load_dotenv\nfrom redis import Redis\nfrom file_processor import FileProcessor\nfrom db_utils import save_task_metadata\nfrom email_utils import send_email\nfrom celery_app import celery  # Import celery instance here\nfrom celery_config import make_celery  # Import make_celery function here\nimport tasks  # Import tasks module\nfrom flask_cors import CORS\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize logger\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\n# Initialize Flask app\napp = Flask(__name__)\nCORS(app)  # Enable CORS\n\n# Update Flask config with Celery settings\napp.config.update(\n    CELERY_BROKER_URL='redis://localhost:6379/0',\n    CELERY_RESULT_BACKEND='redis://localhost:6379/0'\n)\n\n# Initialize Celery\ncelery = make_celery(app)\n\n# Initialize Redis client\nredis_client = Redis(host='localhost', port=6379, db=0)\n\n# Initialize FileProcessor\nfile_processor = FileProcessor(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), brevo_api_key=os.getenv(\"BREVO_API\"))\n\n# Simulate a database of authorized emails\nAUTHORIZED_EMAILS = {\"authorized@example.com\", \"testuser@example.com\"}\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Go to www.txt-ray.com to make graphs\"\n\n# ... [other routes] ...\n\nif __name__ == '__main__':\n    if not os.path.exists('uploads'):\n        os.makedirs('uploads')\n    if not os.path.exists('static/generated'):\n        os.makedirs('static/generated')\n    \n    app.run(debug=True, host='0.0.0.0')\n```\n\n### Step 4: Define and Register Your Celery Tasks\n\nEnsure that your Celery tasks are defined in a separate module and imported correctly.\n\n#### `tasks.py`\n\n```python\n# tasks.py\nfrom celery_app import celery\nfrom typing import Dict, Any\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom redis import Redis\nfrom graph_generator import generate_graph, save_graph_to_html\nfrom whisper_api import transcribe_audio\nfrom download_utils import download_file_from_brevo, save_file_content\nfrom email_utils import send_email\nfrom db_utils import get_task_metadata, save_task_metadata\nfrom airtable_utils import create_graph_record_async, update_status_async\nfrom PyPDF2 import PdfReader\nfrom chat_utils import save_chat_context, load_chat_context, extract_conversation_id, extract_message_from_email\nfrom ollama_utils import generate_ollama_response\n\nlogger = logging.getLogger(__name__)\nREDIS_CLIENT = Redis(host='localhost', port=6379, db=0)\n\n@celery.task(bind=True)\ndef process_chat_task(self, email_data: Dict[str, Any]) -> Dict[str, Any]:\n    # Task implementation\n\n@celery.task(bind=True)\ndef process_graph_task(self, task_id, email=None):\n    # Task implementation\n\n@celery.task(bind=True)\ndef transcribe_audio_task(self, task_id):\n    # Task implementation\n\n@celery.task(bind=True)\ndef download_and_process_attachment_task(self, file_url, email_sender, email_recipient, api_key, email_subject, data):\n    # Task implementation\n\n@celery.task(bind=True)\ndef send_email_task(self, task_data):\n    # Task implementation\n```\n\n### Step 5: Ensure Tasks are Registered and Imported\n\nMake sure that tasks are imported correctly in your Flask app and the Celery worker. Here is an updated part of your Flask app:\n\n#### `app.py`\n\n```python\n# app.py\nfrom flask import Flask, render_template, request, jsonify, url_for\nfrom celery import states\nimport os\nimport uuid\nimport traceback\nfrom dotenv import load_dotenv\nfrom datetime import datetime\nimport logging\nfrom redis import Redis\nfrom file_processor import FileProcessor\nfrom db_utils import save_task_metadata\nfrom email_utils import send_email\nfrom celery_app import celery  # Import celery instance here\nfrom celery_config import make_celery  # Import make_celery function here\nimport tasks  # Import tasks module to ensure tasks are registered\nfrom flask_cors import CORS\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize logger\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\n# Initialize Flask app\napp = Flask(__name__)\nCORS(app)  # Enable CORS\n\n# Update Flask config with Celery settings\napp.config.update(\n    CELERY_BROKER_URL='redis://localhost:6379/0',\n    CELERY_RESULT_BACKEND='redis://localhost:6379/0'\n)\n\n# Initialize Celery\ncelery = make_celery(app)\n\n# Initialize Redis client\nredis_client = Redis(host='localhost', port=6379, db=0)\n\n# Initialize FileProcessor\nfile_processor = FileProcessor(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), brevo_api_key=os.getenv(\"BREVO_API\"))\n\n# Simulate a database of authorized emails\nAUTHORIZED_EMAILS = {\"authorized@example.com\", \"testuser@example.com\"}\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Go to www.txt-ray.com to make graphs\"\n\n# ... [other routes] ...\n\nif __name__ == '__main__':\n    if not os.path.exists('uploads'):\n        os.makedirs('uploads')\n    if not os.path.exists('static/generated'):\n        os.makedirs('static/generated')\n    \n    app.run(debug=True, host='0.0.0.0')\n```\n\n### Step 6: Ensure Celery Worker Loads the Tasks\n\nMake sure your Celery worker is started in the same context where the tasks are defined and the application is initialized:\n\n```sh\ncelery -A celery_app.celery worker --loglevel=info\n```\n\nThis command ensures that the Celery worker picks up the tasks defined in your `tasks.py` module.\n\nBy following these steps, you ensure that Celery tasks are correctly defined, registered, and managed within your Flask application, leading to a more streamlined and robust configuration.",
      "created_at": "2024-06-21T20:21:39.424567",
      "sequence_order": 3,
      "parent_id": "aaa2e565-0dbf-40ca-9681-40ec5f10c1b1",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": true
      }
    },
    {
      "id": "aaa201fe-5245-4a60-b07a-580d7eda476a",
      "role": "user",
      "content": "The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\n{\"status\": \"OpenAIClient initialized successfully.\"}\nINFO:app:Received POST request to /graphme\nINFO:app:File received: rec.pdf\nINFO:app:Email received: bg@bilalghalib.com\nINFO:app:File saved to: uploads/download_b43aa87c-99ed-4ced-9b70-d92ff4dd2a3c/rec.pdf\nINFO:app:Metadata saved for task: b43aa87c-99ed-4ced-9b70-d92ff4dd2a3c\nINFO:app:Process task started with ID: 70b4b7f2-9fef-47b2-8d33-bdc61f7e4bbe\nINFO:app:Sending response: {'task_id': '70b4b7f2-9fef-47b2-8d33-bdc61f7e4bbe', 'task_url': 'http://txtray.com/progress/70b4b7f2-9fef-47b2-8d33-bdc61f7e4bbe'}\nERROR:app:Exception on /status/70b4b7f2-9fef-47b2-8d33-bdc61f7e4bbe [GET]\nTraceback (most recent call last):\n  File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/flask_cors/extension.py\", line 178, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n  File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n  File \"/home/bilal/public_html/txtray.com/app.py\", line 121, in task_status\n    task = process_graph_task.AsyncResult(task_id)\nNameError: name 'process_graph_task' is not defined\n\n\n# tasks.py\nfrom celery_app import celery\nfrom typing import Dict, Any\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom redis import Redis\nfrom graph_generator import generate_graph, save_graph_to_html\nfrom whisper_api import transcribe_audio\nfrom download_utils import download_file_from_brevo, save_file_content\nfrom email_utils import send_email\nfrom db_utils import get_task_metadata, save_task_metadata\nfrom airtable_utils import create_graph_record_async, update_status_async\nfrom PyPDF2 import PdfReader\nfrom chat_utils import save_chat_context, load_chat_context, extract_conversation_id, extract_message_from_email\nfrom ollama_utils import generate_ollama_response\n\nlogger = logging.getLogger(__name__)\nREDIS_CLIENT = Redis(host='localhost', port=6379, db=0)\n\n@celery.task(bind=True)\ndef process_chat_task(self, email_data: Dict[str, Any]) -> Dict[str, Any]:\n    logger.info(f\"Starting process_chat_task for email: {email_data['From']['Address']}\")\n    \n    conversation_id = extract_conversation_id(email_data['Subject'])\n    context = load_chat_context(conversation_id)\n    new_message = extract_message_from_email(email_data)\n    \n    context.append({\"role\": \"user\", \"content\": new_message})\n    ollama_response = generate_ollama_response(context)\n    context.append({\"role\": \"assistant\", \"content\": ollama_response})\n    \n    save_chat_context(conversation_id, context)\n    \n    send_email(\n        email=email_data['From']['Address'],\n        subject=f\"Re: {email_data['Subject']}\",\n        content=ollama_response\n    )\n    \n    return {\"status\": \"completed\", \"conversation_id\": conversation_id}    \n    \nasync def extract_text_from_pdf(file_path):\n    logger.info(f\"Extracting text from PDF: {file_path}\")\n    text = \"\"\n    try:\n        with open(file_path, 'rb') as file:\n            pdf_reader = PdfReader(file)\n            for page in pdf_reader.pages:\n                text += page.extract_text()\n    except Exception as e:\n        logger.error(f\"Error extracting text from PDF: {str(e)}\")\n        raise\n    return text\n\n@celery.task(bind=True)\ndef process_graph_task(self, task_id, email=None):\n    task_id = str(task_id)\n    logger.info(f\"Starting process_graph_task with task_id: {task_id}\")\n    try:\n        task_data = get_task_metadata(task_id)\n        if task_data is None:\n            raise ValueError(f\"Task metadata not found for task_id: {task_id}\")\n\n        logger.info(f\"Retrieved task data: {task_data}\")\n\n        file_path = task_data['file_path']\n        logger.info(f\"File path from metadata: {file_path}\")\n\n        file_type, _ = mimetypes.guess_type(file_path)\n        logger.info(f\"Detected file type: {file_type}\")\n\n        graph_task_id = self.request.id\n        email_sender = email or task_data.get('email_sender')\n\n        logger.info(f\"Using current task ID: {graph_task_id} for process_graph_task\")\n        graph_output_dir = os.path.join('static', 'generated', f\"graph_{graph_task_id}\")\n        os.makedirs(graph_output_dir, exist_ok=True)\n\n        # Create Airtable record with status \"Processing\"\n        async def create_record():\n            record_id = await create_graph_record_async(task_id, email_sender)\n            return record_id\n\n        record_id = asyncio.run(create_record())\n        logger.info(f\"Airtable record created with ID: {record_id}\")\n\n        # Process different file types\n        if file_type == 'application/pdf':\n            extracted_text = asyncio.run(extract_text_from_pdf(file_path))\n            text_file_path = os.path.join(os.path.dirname(file_path), 'extracted_text.txt')\n            with open(text_file_path, 'w', encoding='utf-8') as f:\n                f.write(extracted_text)\n            file_path = text_file_path\n        elif file_type and file_type.startswith('audio/'):\n            transcription = asyncio.run(transcribe_audio(file_path))\n            transcription_path = os.path.join(os.path.dirname(file_path), 'transcription.txt')\n            with open(transcription_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            file_path = transcription_path\n\n        G = generate_graph(file_path, graph_output_dir)\n        save_graph_to_html(G, graph_output_dir)\n\n        logger.info(f\"Graph generated successfully for process_graph_task with task_id: {self.request.id}\")\n\n        result_dict = {\n            'graph_task': {\n                'task_id': graph_task_id,\n                'd3_url': f'/static/generated/graph_{graph_task_id}/index.html',\n                'pyvis_url': f'/static/generated/graph_{graph_task_id}/pyvis.html'\n            }\n        }\n\n        # Update Airtable record to \"Processing Completed\"\n        async def update_record():\n            await update_status_async(record_id, \"Processing Completed\")\n\n        asyncio.run(update_record())\n\n        # Trigger email task\n        task_data.update(result_dict)\n        send_email_task.delay(task_data)\n\n        self.update_state(state=states.SUCCESS, meta=result_dict)\n        return result_dict\n\n    except Exception as e:\n        logger.exception(f\"Error generating graph for process_graph_task with task_id: {self.request.id}\")\n        raise\n\ndef is_valid_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\n@celery.task(bind=True)\ndef transcribe_audio_task(self, task_id):\n    task_id = str(task_id)\n    task_data = get_task_metadata(task_id)\n    file_path = task_data['file_path']\n    download_task_id = task_data['download_task_id']\n\n    logger.info(f\"Starting transcribe_audio_task with download_task_id: {download_task_id}\")\n    output_dir = os.path.join('uploads', f\"tr_{download_task_id}\")\n    transcription_path = os.path.join(output_dir, 'transcription.txt')\n\n    if not os.path.exists(transcription_path):\n        async def run_transcription():\n            async with redis_lock(f\"transcribe_lock_{download_task_id}\") as acquired:\n                if acquired:\n                    try:\n                        logger.info(f\"Starting transcription for download task {download_task_id}\")\n                        logger.info(f\"File path: {file_path}\")\n                        logger.info(f\"File exists: {os.path.exists(file_path)}\")\n                        logger.info(f\"File size: {os.path.getsize(file_path)} bytes\")\n                        \n                        transcription = await transcribe_audio(file_path)\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(transcription)\n\n                        task_data.update({\n                            'transcription_data': transcription,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n\n                    except Exception as e:\n                        logger.error(f\"Error during transcription: {str(e)}\")\n                        logger.exception(\"Full traceback:\")\n                        # Fallback: create a text file with error message\n                        error_message = f\"Transcription failed. Error: {str(e)}\"\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(error_message)\n                        task_data.update({\n                            'transcription_data': error_message,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n                else:\n                    logger.info(f\"Another instance of the task is already running for download task {download_task_id}\")\n\n        try:\n            asyncio.run(run_transcription())\n        except Exception as e:\n            logger.error(f\"Error in run_transcription: {str(e)}\")\n            # Don't retry, we've already handled the error\n\n    result_dict = {'task_id': task_id}\n    logger.info(f\"transcribe_audio_task result_dict updated with transcription_path: {transcription_path}\")\n    logger.info(f\"Updated result_dict: {result_dict}\")\n    \n    self.update_state(state=states.SUCCESS, meta=result_dict)\n    return task_id\n        \n@celery.task(bind=True)\ndef download_and_process_attachment_task(self, file_url, email_sender, email_recipient, api_key, email_subject, data):\n    try:\n        download_task_id = str(uuid.uuid4())\n        logger.info(f\"Starting download_and_process_attachment_task with download_task_id: {download_task_id}\")\n\n        if 'Attachments' in data and data['Attachments']:\n            attachment_data = data['Attachments'][0]\n            attachment_name = attachment_data['Name']\n            attachment_type = attachment_data['ContentType']\n            download_token = attachment_data['DownloadToken']\n            \n            file_content = download_file_from_brevo(download_token, api_key)\n            file_path = os.path.join('uploads', f\"download_{download_task_id}\", attachment_name)\n            save_file_content(file_content, file_path)\n        else:\n            attachment_name = 'message.txt'\n            attachment_type = 'text/plain'\n            file_content = data.get('ExtractedMarkdownMessage', '')\n            file_path = os.path.join('uploads', f\"download_{download_task_id}\", attachment_name)\n            save_file_content(file_content.encode('utf-8'), file_path)\n        \n        metadata = {\n            'file_name': attachment_name,\n            'file_type': attachment_type,\n            'date_uploaded': datetime.now().strftime('%Y-%m-%d %H:%S'),\n            'download_task_id': download_task_id,\n            'downloaded_file_link': file_path,\n            'email_sender': email_sender,\n            'email_subject': email_subject,\n            'file_path': file_path\n        }\n        save_task_metadata(download_task_id, metadata)\n\n        valid_audio_formats = ['flac', 'm4a', 'mp3', 'mp4', 'mpeg', 'mpga', 'oga', 'ogg', 'wav', 'webm']\n        file_extension = os.path.splitext(attachment_name)[1].lower().strip('.')\n\n        if file_extension in valid_audio_formats:\n            task_chain = chain(transcribe_audio_task.s(download_task_id), process_graph_task.s())\n        else:\n            task_chain = process_graph_task.s(download_task_id)\n\n        task_result = task_chain.apply_async()\n        graph_task_id = task_result.id\n\n        return {'status': 'Processing', 'task_id': download_task_id}\n\n    except Exception as e:\n        self.update_state(\n            state=states.FAILURE,\n            meta={\n                'exc_type': type(e).__name__,\n                'exc_message': str(e)\n            }\n        )\n        raise self.retry(exc=e, countdown=5, max_retries=3)\n\n@celery.task(bind=True)\ndef send_email_task(self, task_data):\n    try:\n        graph_task_id = task_data.get('graph_task', {}).get('task_id')\n        email_sender = task_data['email_sender']\n        email_subject = task_data['email_subject']\n\n        if graph_task_id:\n            progress_url = f'https://www.txtray.com/progress/{graph_task_id}'\n        else:\n            progress_url = f'https://www.txtray.com/progress/{self.request.id}'\n\n        logger.info(f\"Email sender received in send_email_task: {email_sender}\")\n        logger.info(f\"Email subject received in send_email_task: {email_subject}\")\n\n        if not is_valid_email(email_sender):\n            raise ValueError(f\"Invalid email address: {email_sender}\")\n\n        logger.info(f\"Sending email with progress_url: {progress_url}\")\n        send_email(\n            email=email_sender,\n            subject=f\"Re: {email_subject}\",\n            content=f\"Your request has been processed. You can view the progress at: {progress_url}\"\n        )\n        logger.info(f\"Email sent successfully for task {self.request.id}\")\n\n        self.update_state(state=states.SUCCESS)\n\n    except Exception as e:\n        logger.exception(f\"Error sending email for task {self.request.id}\")\n        raise\n# app.py\nfrom flask import Flask, render_template, request, jsonify, url_for\nfrom celery import states\nimport os\nimport uuid\nimport traceback\nfrom dotenv import load_dotenv\nfrom datetime import datetime\nimport logging\nfrom redis import Redis\nfrom file_processor import FileProcessor\nfrom db_utils import save_task_metadata\nfrom email_utils import send_email\nfrom celery_app import celery  # Import celery instance here\nfrom celery_config import make_celery  # Import make_celery function here\nimport tasks  # Import tasks module to ensure tasks are registered\nfrom flask_cors import CORS\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize logger\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\n# Initialize Flask app\napp = Flask(__name__)\nCORS(app)  # Enable CORS\n\n# Update Flask config with Celery settings\napp.config.update(\n    CELERY_BROKER_URL='redis://localhost:6379/0',\n    CELERY_RESULT_BACKEND='redis://localhost:6379/0'\n)\n\n# Initialize Celery\ncelery = make_celery(app)\n\n# Initialize Redis client\nredis_client = Redis(host='localhost', port=6379, db=0)\n\n# Initialize FileProcessor\nfile_processor = FileProcessor(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), brevo_api_key=os.getenv(\"BREVO_API\"))\n\n# Simulate a database of authorized emails\nAUTHORIZED_EMAILS = {\"authorized@example.com\", \"testuser@example.com\"}\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Go to www.txt-ray.com to make graphs\"\n\n@app.route('/graphme', methods=['GET', 'POST'])\ndef graphme():\n    if request.method == 'GET':\n        return render_template('index.html')\n    try:\n        logger.info(\"Received POST request to /graphme\")\n        if 'file' not in request.files:\n            logger.error(\"No file part in the request\")\n            return jsonify({'error': 'No file part'}), 400\n        if 'email' not in request.form:\n            logger.error(\"No email in the request\")\n            return jsonify({'error': 'No email provided'}), 400\n\n        file = request.files['file']\n        email = request.form['email']\n\n        if file.filename == '':\n            logger.error(\"No selected file\")\n            return jsonify({'error': 'No selected file'}), 400\n\n        logger.info(f\"File received: {file.filename}\")\n        logger.info(f\"Email received: {email}\")\n\n        task_id = str(uuid.uuid4())\n        download_task_id = f\"download_{task_id}\"\n        original_filename = file.filename\n        \n        folder_path = os.path.join('uploads', download_task_id)\n        os.makedirs(folder_path, exist_ok=True)\n        \n        file_path = os.path.join(folder_path, original_filename)\n        file.save(file_path)\n        logger.info(f\"File saved to: {file_path}\")\n\n        metadata = {\n            'file_path': file_path,\n            'output_dir': folder_path,\n            'task_id': task_id,\n            'download_task_id': download_task_id,\n            'email_sender': email,\n            'original_filename': original_filename,\n            'date_uploaded': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'email_subject': f\"Your TxtRay Graph for {original_filename} is Ready\"\n        }\n        \n        save_task_metadata(task_id, metadata)\n        logger.info(f\"Metadata saved for task: {task_id}\")\n\n        process_task = tasks.process_graph_task.delay(task_id, email)  # Use tasks.process_graph_task here\n        logger.info(f\"Process task started with ID: {process_task.id}\")\n\n        response = {\n            'task_id': process_task.id,\n            'task_url': url_for('progress', task_id=process_task.id, _external=True)\n        }\n        logger.info(f\"Sending response: {response}\")\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error in /graphme route: {e}\\n{traceback.format_exc()}\")\n        return jsonify({'error': str(e)}), 500\n\n\n@app.route('/progress/<task_id>')\ndef progress(task_id):\n    return render_template('progress.html', task_id=task_id)\n\n@app.route('/status/<task_id>')\ndef task_status(task_id):\n    task = process_graph_task.AsyncResult(task_id)\n    \n    if task.state == states.PENDING:\n        response = {\n            'state': task.state,\n            'status': 'Processing...'\n        }\n    elif task.state == states.SUCCESS:\n        result_dict = task.result\n        graph_task_id = result_dict['graph_task']['task_id']\n        graph_task = process_graph_task.AsyncResult(graph_task_id)\n        if graph_task.state == states.SUCCESS:\n            response = {\n                'state': graph_task.state,\n                'status': 'Processing complete.',\n                'result': graph_task.result['graph_task']\n            }\n        else:\n            response = {\n                'state': graph_task.state,\n                'status': 'Processing...'\n            }\n    elif task.state == states.FAILURE:\n        response = {\n            'state': task.state,\n            'status': f'Error: {str(task.info)}'\n        }\n    else:\n        response = {\n            'state': task.state,\n            'status': str(task.info)\n        }\n    \n    return jsonify(response)\n    \n@app.route('/graphmeairtable', methods=['POST'])\ndef graphmeairtable():\n    try:\n        if request.is_json:\n            data = request.get_json()\n            if 'fileUrl' in data:\n                file_url = data['fileUrl']\n                task_id = str(uuid.uuid4())\n                file_path, error = download_file_from_url(file_url, 'file_from_url.txt')\n                if error:\n                    return jsonify({'error': error}), 400\n                output_dir = os.path.dirname(file_path)\n            elif 'fileContent' in data:\n                task_id = str(uuid.uuid4())\n                file_path, output_dir = save_file_content(data['fileContent'], 'file_from_content.txt', task_id)\n            else:\n                return jsonify({'error': 'No file URL or file content provided'}), 400\n        else:\n            return jsonify({'error': 'Invalid request format'}), 400\n        \n        metadata = {\n            'file_path': file_path,\n            'output_dir': output_dir,\n            'task_id': task_id\n        }\n        save_task_metadata(task_id, metadata)\n        \n        process_task = process_graph_task.delay(task_id)\n        return jsonify({'task_id': process_task.id, 'task_url': url_for('task_status', task_id=process_task.id, _external=True)})\n\n    except Exception as e:\n        logger.error(f\"Error in /graphmeairtable route: {e}\\n{traceback.format_exc()}\")\n        return jsonify({'error': 'Internal Server Error'}), 500\n\n\n@app.route('/update_subscription', methods=['POST'])\ndef update_subscription():\n    try:\n        data = request.get_json()\n        email = data['email']\n        subscription = data['subscription']\n        \n        # Log current Redis subscriptions related to authorized emails\n        logger.info(\"Current Redis subscriptions for authorized emails:\")\n        for key in redis_client.scan_iter(\"authorized_email_*\"):\n            logger.info(f\"{key.decode('utf-8')}: {redis_client.get(key).decode('utf-8')}\")\n        \n        # Save the email and subscription to Redis\n        redis_client.set(f\"authorized_email_{email}\", subscription)\n        \n        # Log updated Redis subscriptions related to authorized emails\n        logger.info(\"Updated Redis subscriptions for authorized emails:\")\n        for key in redis_client.scan_iter(\"authorized_email_*\"):\n            logger.info(f\"{key.decode('utf-8')}: {redis_client.get(key).decode('utf-8')}\")\n        \n        return jsonify({'message': 'Subscription updated successfully'}), 200\n\n    except Exception as e:\n        logger.error(f\"Error updating subscription: {e}\")\n        return jsonify({'error': 'Internal Server Error'}), 500\n\n@app.route('/webhook', methods=['POST'])\ndef respond():\n    app.logger.info('Webhook received: %s', request.json)\n    \n    try:\n        data = request.json['items'][0]\n        \n        email_sender = data['From']['Address']\n        email_recipient = data['To'][0]['Address']\n        email_subject = data['Subject']\n        \n        # Check if the sender's email is authorized\n        if email_sender not in AUTHORIZED_EMAILS and not redis_client.exists(f\"authorized_email_{email_sender}\"):\n            send_email(\n                email=email_sender,\n                subject=\"Unauthorized Access Attempt\",\n                content=\"You are not authorized to use this service. Please register at txt-ray.com and pay for a subscription to gain access.\"\n            )\n            return jsonify({'error': 'Unauthorized access'}), 403\n        \n        app.logger.info(f\"Authorized email: {email_sender}\")\n        \n        if email_recipient == 'chat@txtray.com':\n            app.logger.info(f\"Processing chat email from {email_sender}\")\n            task = process_chat_task.delay(data)\n        else:\n            app.logger.info(f\"Processing document email from {email_sender}\")\n            if 'Attachments' in data and len(data['Attachments']) > 0:\n                attachment = data['Attachments'][0]\n                file_url = f\"https://api.brevo.com/v3/inbound/attachments/{attachment['DownloadToken']}\"\n            else:\n                file_url = None\n            \n            api_key = os.getenv(\"BREVO_API\")\n            task = download_and_process_attachment_task.delay(file_url, email_sender, email_recipient, api_key, email_subject, data)\n        \n        return jsonify({'task_id': task.id, 'task_url': url_for('task_status', task_id=task.id, _external=True)}), 200\n    \n    except Exception as e:\n        app.logger.error(f\"Error processing webhook: {e}\")\n        return jsonify({'error': 'Webhook processing failed'}), 500\n@app.route('/whisper', methods=['POST'])\ndef whisper():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No selected file'}), 400\n\n    task_id = str(uuid.uuid4())\n    file_path, output_dir = save_file_content(file.read(), file.filename, task_id)\n    if file_path is None:\n        return jsonify({'error': 'File saving failed'}), 500\n\n    try:\n        metadata = {\n            'file_name': file.filename,\n            'date_uploaded': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'download_task_id': task_id,\n            'downloaded_file_link': file_path,\n            'file_path': file_path\n        }\n        save_task_metadata(task_id, metadata)\n\n        task_chain = chain(\n            transcribe_audio_task.s(task_id),\n            process_graph_task.s()\n        )\n        task_chain.apply_async()\n\n        return jsonify({'task_id': task_id})\n    except Exception as e:\n        app.logger.error(f\"Error starting transcription task: {e}\")\n        return jsonify({'error': 'Transcription task failed'}), 500\n        \n\nif __name__ == '__main__':\n    if not os.path.exists('uploads'):\n        os.makedirs('uploads')\n    if not os.path.exists('static/generated'):\n        os.makedirs('static/generated')\n    \n    app.run(debug=True, host='0.0.0.0')",
      "created_at": "2024-06-21T20:31:18.636730",
      "sequence_order": 4,
      "parent_id": "81e14cdb-af82-4938-b946-8ed634e4cd27",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "cae72691-f9a3-4738-aea0-2d72de413204",
      "role": "assistant",
      "content": "It appears that the main issue is with the import and registration of the Celery tasks. To ensure that tasks are properly registered and can be recognized by both the Celery worker and the Flask app, follow these steps:\n\n### Step 1: Centralize Task Definitions\n\nMake sure all your Celery tasks are defined in a single module. This makes it easier to manage and import tasks. You already have a `tasks.py` file, so ensure all tasks are included there.\n\n#### `tasks.py`\n\n```python\n# tasks.py\nfrom celery_app import celery\nfrom typing import Dict, Any\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom redis import Redis\nfrom graph_generator import generate_graph, save_graph_to_html\nfrom whisper_api import transcribe_audio\nfrom download_utils import download_file_from_brevo, save_file_content\nfrom email_utils import send_email\nfrom db_utils import get_task_metadata, save_task_metadata\nfrom airtable_utils import create_graph_record_async, update_status_async\nfrom PyPDF2 import PdfReader\nfrom chat_utils import save_chat_context, load_chat_context, extract_conversation_id, extract_message_from_email\nfrom ollama_utils import generate_ollama_response\n\nlogger = logging.getLogger(__name__)\nREDIS_CLIENT = Redis(host='localhost', port=6379, db=0)\n\n@celery.task(bind=True)\ndef process_chat_task(self, email_data: Dict[str, Any]) -> Dict[str, Any]:\n    # Task implementation\n\nasync def extract_text_from_pdf(file_path):\n    # Function implementation\n\n@celery.task(bind=True)\ndef process_graph_task(self, task_id, email=None):\n    # Task implementation\n\ndef is_valid_email(email):\n    # Function implementation\n\n@celery.task(bind=True)\ndef transcribe_audio_task(self, task_id):\n    # Task implementation\n\n@celery.task(bind=True)\ndef download_and_process_attachment_task(self, file_url, email_sender, email_recipient, api_key, email_subject, data):\n    # Task implementation\n\n@celery.task(bind=True)\ndef send_email_task(self, task_data):\n    # Task implementation\n```\n\n### Step 2: Ensure Tasks are Imported in Flask App\n\nIn your Flask app (`app.py`), ensure you import the tasks module to register the tasks when the app starts.\n\n#### `app.py`\n\n```python\n# app.py\nfrom flask import Flask, render_template, request, jsonify, url_for\nfrom celery import states\nimport os\nimport uuid\nimport traceback\nfrom datetime import datetime\nimport logging\nfrom dotenv import load_dotenv\nfrom redis import Redis\nfrom file_processor import FileProcessor\nfrom db_utils import save_task_metadata\nfrom email_utils import send_email\nfrom celery_app import celery  # Import celery instance here\nfrom celery_config import make_celery  # Import make_celery function here\nimport tasks  # Import tasks module to ensure tasks are registered\nfrom flask_cors import CORS\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize logger\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\n# Initialize Flask app\napp = Flask(__name__)\nCORS(app)  # Enable CORS\n\n# Update Flask config with Celery settings\napp.config.update(\n    CELERY_BROKER_URL='redis://localhost:6379/0',\n    CELERY_RESULT_BACKEND='redis://localhost:6379/0'\n)\n\n# Initialize Celery\ncelery = make_celery(app)\n\n# Initialize Redis client\nredis_client = Redis(host='localhost', port=6379, db=0)\n\n# Initialize FileProcessor\nfile_processor = FileProcessor(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), brevo_api_key=os.getenv(\"BREVO_API\"))\n\n# Simulate a database of authorized emails\nAUTHORIZED_EMAILS = {\"authorized@example.com\", \"testuser@example.com\"}\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Go to www.txt-ray.com to make graphs\"\n\n@app.route('/graphme', methods=['GET', 'POST'])\ndef graphme():\n    if request.method == 'GET':\n        return render_template('index.html')\n    try:\n        logger.info(\"Received POST request to /graphme\")\n        if 'file' not in request.files:\n            logger.error(\"No file part in the request\")\n            return jsonify({'error': 'No file part'}), 400\n        if 'email' not in request.form:\n            logger.error(\"No email in the request\")\n            return jsonify({'error': 'No email provided'}), 400\n\n        file = request.files['file']\n        email = request.form['email']\n\n        if file.filename == '':\n            logger.error(\"No selected file\")\n            return jsonify({'error': 'No selected file'}), 400\n\n        logger.info(f\"File received: {file.filename}\")\n        logger.info(f\"Email received: {email}\")\n\n        task_id = str(uuid.uuid4())\n        download_task_id = f\"download_{task_id}\"\n        original_filename = file.filename\n        \n        folder_path = os.path.join('uploads', download_task_id)\n        os.makedirs(folder_path, exist_ok=True)\n        \n        file_path = os.path.join(folder_path, original_filename)\n        file.save(file_path)\n        logger.info(f\"File saved to: {file_path}\")\n\n        metadata = {\n            'file_path': file_path,\n            'output_dir': folder_path,\n            'task_id': task_id,\n            'download_task_id': download_task_id,\n            'email_sender': email,\n            'original_filename': original_filename,\n            'date_uploaded': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'email_subject': f\"Your TxtRay Graph for {original_filename} is Ready\"\n        }\n        \n        save_task_metadata(task_id, metadata)\n        logger.info(f\"Metadata saved for task: {task_id}\")\n\n        process_task = tasks.process_graph_task.delay(task_id, email)\n        logger.info(f\"Process task started with ID: {process_task.id}\")\n\n        response = {\n            'task_id': process_task.id,\n            'task_url': url_for('progress', task_id=process_task.id, _external=True)\n        }\n        logger.info(f\"Sending response: {response}\")\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error in /graphme route: {e}\\n{traceback.format_exc()}\")\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/progress/<task_id>')\ndef progress(task_id):\n    return render_template('progress.html', task_id=task_id)\n\n@app.route('/status/<task_id>')\ndef task_status(task_id):\n    task = tasks.process_graph_task.AsyncResult(task_id)\n    \n    if task.state == states.PENDING:\n        response = {\n            'state': task.state,\n            'status': 'Processing...'\n        }\n    elif task.state == states.SUCCESS:\n        result_dict = task.result\n        graph_task_id = result_dict['graph_task']['task_id']\n        graph_task = tasks.process_graph_task.AsyncResult(graph_task_id)\n        if graph_task.state == states.SUCCESS:\n            response = {\n                'state': graph_task.state,\n                'status': 'Processing complete.',\n                'result': graph_task.result['graph_task']\n            }\n        else:\n            response = {\n                'state': graph_task.state,\n                'status': 'Processing...'\n            }\n    elif task.state == states.FAILURE:\n        response = {\n            'state': task.state,\n            'status': f'Error: {str(task.info)}'\n        }\n    else:\n        response = {\n            'state': task.state,\n            'status': str(task.info)\n        }\n    \n    return jsonify(response)\n\n@app.route('/graphmeairtable', methods=['POST'])\ndef graphmeairtable():\n    try:\n        if request.is_json:\n            data = request.get_json()\n            if 'fileUrl' in data:\n                file_url = data['fileUrl']\n                task_id = str(uuid.uuid4())\n                file_path, error = download_file_from_url(file_url, 'file_from_url.txt')\n                if error:\n                    return jsonify({'error': error}), 400\n                output_dir = os.path.dirname(file_path)\n            elif 'fileContent' in data:\n                task_id = str(uuid.uuid4())\n                file_path, output_dir = save_file_content(data['fileContent'], 'file_from_content.txt', task_id)\n            else:\n                return jsonify({'error': 'No file URL or file content provided'}), 400\n        else:\n            return jsonify({'error': 'Invalid request format'}), 400\n        \n        metadata = {\n            'file_path': file_path,\n            'output_dir': output_dir,\n            'task_id': task_id\n        }\n        save_task_metadata(task_id, metadata)\n        \n        process_task = tasks.process_graph_task.delay(task_id)\n        return jsonify({'task_id': process_task.id, 'task_url': url_for('task_status', task_id=process_task.id, _external=True)})\n\n    except Exception as e:\n        logger.error(f\"Error in /graphmeairtable route: {e}\\n{traceback.format_exc()}\")\n        return jsonify({'error': 'Internal Server Error'}), 500\n\n@app.route('/update_subscription', methods=['POST'])\ndef update_subscription():\n    try:\n        data = request.get_json()\n        email = data['email']\n        subscription = data['subscription']\n        \n        # Log current Redis subscriptions related to authorized emails\n        logger.info(\"Current Redis subscriptions for authorized emails:\")\n        for key in redis_client.scan_iter(\"authorized_email_*\"):\n            logger.info(f\"{key.decode('utf-8')}: {redis_client.get(key).decode('utf-8')}\")\n        \n        # Save the email and subscription to Redis\n        redis_client.set(f\"authorized_email_{email}\", subscription)\n        \n        # Log updated Redis",
      "created_at": "2024-06-21T20:32:24.033218",
      "sequence_order": 5,
      "parent_id": "aaa201fe-5245-4a60-b07a-580d7eda476a",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": false
      }
    },
    {
      "id": "aaa2838c-da04-4294-b24a-e3c1850a875e",
      "role": "user",
      "content": "can we use the openai endpoint for chat responses instead of ollama?\n# tasks.py\nimport mimetypes \nfrom celery import states \nfrom celery_app import celery\nfrom typing import Dict, Any\nimport asyncio\nimport logging\nimport uuid\nfrom datetime import datetime\nfrom redis import Redis\nfrom graph_generator import generate_graph, save_graph_to_html\nfrom whisper_api import transcribe_audio\nfrom download_utils import download_file_from_brevo, save_file_content\nfrom db_utils import get_task_metadata, save_task_metadata\nfrom airtable_utils import create_graph_record_async, update_status_async\nfrom PyPDF2 import PdfReader\nfrom chat_utils import save_chat_context, load_chat_context, extract_conversation_id, extract_message_from_email\nfrom ollama_utils import generate_ollama_response\nfrom email_utils import send_email  # Import send_email_task\nimport os  \nimport re\n\nlogger = logging.getLogger(__name__)\nREDIS_CLIENT = Redis(host='localhost', port=6379, db=0)\n\n@celery.task(bind=True)\ndef process_chat_task(self, email_data: Dict[str, Any]) -> Dict[str, Any]:\n    logger.info(f\"Starting process_chat_task for email: {email_data['From']['Address']}\")\n    \n    conversation_id = extract_conversation_id(email_data['Subject'])\n    context = load_chat_context(conversation_id)\n    new_message = extract_message_from_email(email_data)\n    \n    context.append({\"role\": \"user\", \"content\": new_message})\n    ollama_response = generate_ollama_response(context)\n    context.append({\"role\": \"assistant\", \"content\": ollama_response})\n    \n    save_chat_context(conversation_id, context)\n    \n    send_email(\n        email=email_data['From']['Address'],\n        subject=f\"Re: {email_data['Subject']}\",\n        content=ollama_response\n    )\n    \n    return {\"status\": \"completed\", \"conversation_id\": conversation_id}    \n    \nasync def extract_text_from_pdf(file_path):\n    logger.info(f\"Extracting text from PDF: {file_path}\")\n    text = \"\"\n    try:\n        with open(file_path, 'rb') as file:\n            pdf_reader = PdfReader(file)\n            for page in pdf_reader.pages:\n                text += page.extract_text()\n    except Exception as e:\n        logger.error(f\"Error extracting text from PDF: {str(e)}\")\n        raise\n    return text\n\n@celery.task(bind=True)\ndef process_graph_task(self, task_id, email=None):\n    task_id = str(task_id)\n    logger.info(f\"Starting process_graph_task with task_id: {task_id}\")\n    try:\n        task_data = get_task_metadata(task_id)\n        if task_data is None:\n            raise ValueError(f\"Task metadata not found for task_id: {task_id}\")\n\n        logger.info(f\"Retrieved task data: {task_data}\")\n\n        file_path = task_data['file_path']\n        logger.info(f\"File path from metadata: {file_path}\")\n\n        file_type, _ = mimetypes.guess_type(file_path)\n        logger.info(f\"Detected file type: {file_type}\")\n\n        graph_task_id = self.request.id\n        email_sender = email or task_data.get('email_sender')\n\n        logger.info(f\"Using current task ID: {graph_task_id} for process_graph_task\")\n        graph_output_dir = os.path.join('static', 'generated', f\"graph_{graph_task_id}\")\n        os.makedirs(graph_output_dir, exist_ok=True)\n\n        # Create Airtable record with status \"Processing\"\n        async def create_record():\n            record_id = await create_graph_record_async(task_id, email_sender)\n            return record_id\n\n        record_id = asyncio.run(create_record())\n        logger.info(f\"Airtable record created with ID: {record_id}\")\n\n        # Process different file types\n        if file_type == 'application/pdf':\n            extracted_text = asyncio.run(extract_text_from_pdf(file_path))\n            text_file_path = os.path.join(os.path.dirname(file_path), 'extracted_text.txt')\n            with open(text_file_path, 'w', encoding='utf-8') as f:\n                f.write(extracted_text)\n            file_path = text_file_path\n        elif file_type and file_type.startswith('audio/'):\n            transcription = asyncio.run(transcribe_audio(file_path))\n            transcription_path = os.path.join(os.path.dirname(file_path), 'transcription.txt')\n            with open(transcription_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            file_path = transcription_path\n\n        G = generate_graph(file_path, graph_output_dir)\n        save_graph_to_html(G, graph_output_dir)\n\n        logger.info(f\"Graph generated successfully for process_graph_task with task_id: {self.request.id}\")\n\n        result_dict = {\n            'graph_task': {\n                'task_id': graph_task_id,\n                'd3_url': f'/static/generated/graph_{graph_task_id}/index.html',\n                'pyvis_url': f'/static/generated/graph_{graph_task_id}/pyvis.html'\n            }\n        }\n\n        # Update Airtable record to \"Processing Completed\"\n        async def update_record():\n            await update_status_async(record_id, \"Processing Completed\")\n\n        asyncio.run(update_record())\n\n        # Trigger email task\n        task_data.update(result_dict)\n        send_email_task.delay(task_data)\n\n        self.update_state(state=states.SUCCESS, meta=result_dict)\n        return result_dict\n\n    except Exception as e:\n        logger.exception(f\"Error generating graph for process_graph_task with task_id: {self.request.id}\")\n        raise\n\ndef is_valid_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\n@celery.task(bind=True)\ndef transcribe_audio_task(self, task_id):\n    task_id = str(task_id)\n    task_data = get_task_metadata(task_id)\n    file_path = task_data['file_path']\n    download_task_id = task_data['download_task_id']\n\n    logger.info(f\"Starting transcribe_audio_task with download_task_id: {download_task_id}\")\n    output_dir = os.path.join('uploads', f\"tr_{download_task_id}\")\n    transcription_path = os.path.join(output_dir, 'transcription.txt')\n\n    if not os.path.exists(transcription_path):\n        async def run_transcription():\n            async with redis_lock(f\"transcribe_lock_{download_task_id}\") as acquired:\n                if acquired:\n                    try:\n                        logger.info(f\"Starting transcription for download task {download_task_id}\")\n                        logger.info(f\"File path: {file_path}\")\n                        logger.info(f\"File exists: {os.path.exists(file_path)}\")\n                        logger.info(f\"File size: {os.path.getsize(file_path)} bytes\")\n                        \n                        transcription = await transcribe_audio(file_path)\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(transcription)\n\n                        task_data.update({\n                            'transcription_data': transcription,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n\n                    except Exception as e:\n                        logger.error(f\"Error during transcription: {str(e)}\")\n                        logger.exception(\"Full traceback:\")\n                        # Fallback: create a text file with error message\n                        error_message = f\"Transcription failed. Error: {str(e)}\"\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(error_message)\n                        task_data.update({\n                            'transcription_data': error_message,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n                else:\n                    logger.info(f\"Another instance of the task is already running for download task {download_task_id}\")\n\n        try:\n            asyncio.run(run_transcription())\n        except Exception as e:\n            logger.error(f\"Error in run_transcription: {str(e)}\")\n            # Don't retry, we've already handled the error\n\n    result_dict = {'task_id': task_id}\n    logger.info(f\"transcribe_audio_task result_dict updated with transcription_path: {transcription_path}\")\n    logger.info(f\"Updated result_dict: {result_dict}\")\n    \n    self.update_state(state=states.SUCCESS, meta=result_dict)\n    return task_id\n        \n@celery.task(bind=True)\ndef download_and_process_attachment_task(self, file_url, email_sender, email_recipient, api_key, email_subject, data):\n    try:\n        download_task_id = str(uuid.uuid4())\n        logger.info(f\"Starting download_and_process_attachment_task with download_task_id: {download_task_id}\")\n\n        if 'Attachments' in data and data['Attachments']:\n            attachment_data = data['Attachments'][0]\n            attachment_name = attachment_data['Name']\n            attachment_type = attachment_data['ContentType']\n            download_token = attachment_data['DownloadToken']\n            \n            file_content = download_file_from_brevo(download_token, api_key)\n            file_path = os.path.join('uploads', f\"download_{download_task_id}\", attachment_name)\n            save_file_content(file_content, file_path)\n        else:\n            attachment_name = 'message.txt'\n            attachment_type = 'text/plain'\n            file_content = data.get('ExtractedMarkdownMessage', '')\n            file_path = os.path.join('uploads', f\"download_{download_task_id}\", attachment_name)\n            save_file_content(file_content.encode('utf-8'), file_path)\n        \n        metadata = {\n            'file_name': attachment_name,\n            'file_type': attachment_type,\n            'date_uploaded': datetime.now().strftime('%Y-%m-%d %H:%S'),\n            'download_task_id': download_task_id,\n            'downloaded_file_link': file_path,\n            'email_sender': email_sender,\n            'email_subject': email_subject,\n            'file_path': file_path\n        }\n        save_task_metadata(download_task_id, metadata)\n\n        valid_audio_formats = ['flac', 'm4a', 'mp3', 'mp4', 'mpeg', 'mpga', 'oga', 'ogg', 'wav', 'webm']\n        file_extension = os.path.splitext(attachment_name)[1].lower().strip('.')\n\n        if file_extension in valid_audio_formats:\n            task_chain = chain(transcribe_audio_task.s(download_task_id), process_graph_task.s())\n        else:\n            task_chain = process_graph_task.s(download_task_id)\n\n        task_result = task_chain.apply_async()\n        graph_task_id = task_result.id\n\n        return {'status': 'Processing', 'task_id': download_task_id}\n\n    except Exception as e:\n        self.update_state(\n            state=states.FAILURE,\n            meta={\n                'exc_type': type(e).__name__,\n                'exc_message': str(e)\n            }\n        )\n        raise self.retry(exc=e, countdown=5, max_retries=3)\n\n@celery.task(bind=True)\ndef send_email_task(self, task_data):\n    try:\n        graph_task_id = task_data.get('graph_task', {}).get('task_id')\n        email_sender = task_data['email_sender']\n        email_subject = task_data['email_subject']\n\n        if graph_task_id:\n            progress_url = f'https://www.txtray.com/progress/{graph_task_id}'\n        else:\n            progress_url = f'https://www.txtray.com/progress/{self.request.id}'\n\n        logger.info(f\"Email sender received in send_email_task: {email_sender}\")\n        logger.info(f\"Email subject received in send_email_task: {email_subject}\")\n\n        if not is_valid_email(email_sender):\n            raise ValueError(f\"Invalid email address: {email_sender}\")\n\n        logger.info(f\"Sending email with progress_url: {progress_url}\")\n        send_email(\n            email=email_sender,\n            subject=f\"Re: {email_subject}\",\n            content=f\"Your request has been processed. You can view the progress at: {progress_url}\"\n        )\n        logger.info(f\"Email sent successfully for task {self.request.id}\")\n\n        self.update_state(state=states.SUCCESS)\n\n    except Exception as e:\n        logger.exception(f\"Error sending email for task {self.request.id}\")\n        raise\nhere's a demo:\nHow Assistants work Beta\nThe Assistants API is designed to help developers build powerful AI assistants capable of performing a variety of tasks.\n\nThe Assistants API is in beta and we are actively working on adding more functionality. Share your feedback in our Developer Forum!\nAssistants can call OpenAIâ€™s models with specific instructions to tune their personality and capabilities.\nAssistants can access multiple tools in parallel. These can be both OpenAI-hosted tools â€” like code_interpreter and file_search â€” or tools you build / host (via function calling).\nAssistants can access persistent Threads. Threads simplify AI application development by storing message history and truncating it when the conversation gets too long for the modelâ€™s context length. You create a Thread once, and simply append Messages to it as your users reply.\nAssistants can access files in several formats â€” either as part of their creation or as part of Threads between Assistants and users. When using tools, Assistants can also create files (e.g., images, spreadsheets, etc) and cite files they reference in the Messages they create.\nObjects\nAssistants object architecture diagram\n\nOBJECT\tWHAT IT REPRESENTS\nAssistant\tPurpose-built AI that uses OpenAIâ€™s models and calls tools\nThread\tA conversation session between an Assistant and a user. Threads store Messages and automatically handle truncation to fit content into a modelâ€™s context.\nMessage\tA message created by an Assistant or a user. Messages can include text, images, and other files. Messages stored as a list on the Thread.\nRun\tAn invocation of an Assistant on a Thread. The Assistant uses its configuration and the Threadâ€™s Messages to perform tasks by calling models and tools. As part of a Run, the Assistant appends Messages to the Thread.\nRun Step\tA detailed list of steps the Assistant took as part of a Run. An Assistant can call tools or create Messages during its run. Examining Run Steps allows you to introspect how the Assistant is getting to its final results.\nCreating Assistants\nWe recommend using OpenAIâ€™s latest models with the Assistants API for best results and maximum compatibility with tools.\nTo get started, creating an Assistant only requires specifying the model to use. But you can further customize the behavior of the Assistant:\n\nUse the instructions parameter to guide the personality of the Assistant and define its goals. Instructions are similar to system messages in the Chat Completions API.\nUse the tools parameter to give the Assistant access to up to 128 tools. You can give it access to OpenAI-hosted tools like code_interpreter and file_search, or call a third-party tools via a function calling.\nUse the tool_resources parameter to give the tools like code_interpreter and file_search access to files. Files are uploaded using the File upload endpoint and must have the purpose set to assistants to be used with this API.\nFor example, to create an Assistant that can create data visualization based on a .csv file, first upload a file.\n\npython\n\nfile = client.files.create(\n  file=open(\"revenue-forecast.csv\", \"rb\"),\n  purpose='assistants'\n)\nThen, create the Assistant with the code_interpreter tool enabled and provide the file as a resource to the tool.\n\npython\n\nassistant = client.beta.assistants.create(\n  name=\"Data visualizer\",\n  description=\"You are great at creating beautiful data visualizations. You analyze data present in .csv files, understand trends, and come up with data visualizations relevant to those trends. You also share a brief text summary of the trends observed.\",\n  model=\"gpt-4o\",\n  tools=[{\"type\": \"code_interpreter\"}],\n  tool_resources={\n    \"code_interpreter\": {\n      \"file_ids\": [file.id]\n    }\n  }\n)\nYou can attach a maximum of 20 files to code_interpreter and 10,000 files to file_search (using vector_store objects).\n\nEach file can be at most 512 MB in size and have a maximum of 5,000,000 tokens. By default, the size of all the files uploaded by your organization cannot exceed 100 GB, but you can reach out to our support team to increase this limit.\n\nManaging Threads and Messages\nThreads and Messages represent a conversation session between an Assistant and a user. There is no limit to the number of Messages you can store in a Thread. Once the size of the Messages exceeds the context window of the model, the Thread will attempt to smartly truncate messages, before fully dropping the ones it considers the least important.\n\nYou can create a Thread with an initial list of Messages like this:\n\npython\n\nthread = client.beta.threads.create(\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": \"Create 3 data visualizations based on the trends in this file.\",\n      \"attachments\": [\n        {\n          \"file_id\": file.id,\n          \"tools\": [{\"type\": \"code_interpreter\"}]\n        }\n      ]\n    }\n  ]\n)\nMessages can contain text, images, or file attachment. Message attachments are helper methods that add files to a thread's tool_resources. You can also choose to add files to the thread.tool_resources directly.\n\nCreating image input content\nMessage content can contain either external image URLs or File IDs uploaded via the File API. Only models with Vision support can accept image input. Supported image content types include png, jpg, gif, and webp. When creating image files, pass purpose=\"vision\" to allow you to later download and display the input content. Currently, there is a 100GB limit per organization and 10GB for user in organization. Please contact us to request a limit increase.\n\nTools cannot access image content unless specified. To pass image files to Code Interpreter, add the file ID in the message attachments list to allow the tool to read and analyze the input. Image URLs cannot be downloaded in Code Interpreter today.\n\npython\n\nfile = client.files.create(\n  file=open(\"myimage.png\", \"rb\"),\n  purpose=\"vision\"\n)\nthread = client.beta.threads.create(\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"What is the difference between these images?\"\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\"url\": \"https://example.com/image.png\"}\n        },\n        {\n          \"type\": \"image_file\",\n          \"image_file\": {\"file_id\": file.id}\n        },\n      ],\n    }\n  ]\n)\nLow or high fidelity image understanding\nBy controlling the detail parameter, which has three options, low, high, or auto, you have control over how the model processes the image and generates its textual understanding.\n\nlow will enable the \"low res\" mode. The model will receive a low-res 512px x 512px version of the image, and represent the image with a budget of 85 tokens. This allows the API to return faster responses and consume fewer input tokens for use cases that do not require high detail.\nhigh will enable \"high res\" mode, which first allows the model to see the low res image and then creates detailed crops of input images based on the input image size. Use the pricing calculator to see token counts for various image sizes.\npython\n\nthread = client.beta.threads.create(\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"What is this an image of?\"\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": \"https://example.com/image.png\",\n            \"detail\": \"high\"\n          }\n        },\n      ],\n    }\n  ]\n)\nContext window management\nThe Assistants API automatically manages the truncation to ensure it stays within the model's maximum context length. You can customize this behavior by specifying the maximum tokens you'd like a run to utilize and/or the maximum number of recent messages you'd like to include in a run.\n\nMax Completion and Max Prompt Tokens\nTo control the token usage in a single Run, set max_prompt_tokens and max_completion_tokens when creating the Run. These limits apply to the total number of tokens used in all completions throughout the Run's lifecycle.\n\nFor example, initiating a Run with max_prompt_tokens set to 500 and max_completion_tokens set to 1000 means the first completion will truncate the thread to 500 tokens and cap the output at 1000 tokens. If only 200 prompt tokens and 300 completion tokens are used in the first completion, the second completion will have available limits of 300 prompt tokens and 700 completion tokens.\n\nIf a completion reaches the max_completion_tokens limit, the Run will terminate with a status of incomplete, and details will be provided in the incomplete_details field of the Run object.\n\nWhen using the File Search tool, we recommend setting the max_prompt_tokens to no less than 20,000. For longer conversations or multiple interactions with File Search, consider increasing this limit to 50,000, or ideally, removing the max_prompt_tokens limits altogether to get the highest quality results.\nTruncation Strategy\nYou may also specify a truncation strategy to control how your thread should be rendered into the model's context window. Using a truncation strategy of type auto will use OpenAI's default truncation strategy. Using a truncation strategy of type last_messages will allow you to specify the number of the most recent messages to include in the context window.\n\nMessage annotations\nMessages created by Assistants may contain annotations within the content array of the object. Annotations provide information around how you should annotate the text in the Message.\n\nThere are two types of Annotations:\n\nfile_citation: File citations are created by the file_search tool and define references to a specific file that was uploaded and used by the Assistant to generate the response.\nfile_path: File path annotations are created by the code_interpreter tool and contain references to the files generated by the tool.\nWhen annotations are present in the Message object, you'll see illegible model-generated substrings in the text that you should replace with the annotations. These strings may look something like ã€13â€ sourceã€‘ or sandbox:/mnt/data/file.csv. Hereâ€™s an example python code snippet that replaces these strings with information present in the annotations.\n\npython\n\n# Retrieve the message object\nmessage = client.beta.threads.messages.retrieve(\n  thread_id=\"...\",\n  message_id=\"...\"\n)\n# Extract the message content\nmessage_content = message.content[0].text\nannotations = message_content.annotations\ncitations = []\n# Iterate over the annotations and add footnotes\nfor index, annotation in enumerate(annotations):\n    # Replace the text with a footnote\n    message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n    # Gather citations based on annotation attributes\n    if (file_citation := getattr(annotation, 'file_citation', None)):\n        cited_file = client.files.retrieve(file_citation.file_id)\n        citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n    elif (file_path := getattr(annotation, 'file_path', None)):\n        cited_file = client.files.retrieve(file_path.file_id)\n        citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n        # Note: File download functionality not implemented above for brevity\n# Add footnotes to the end of the message before displaying to user\nmessage_content.value += '\\n' + '\\n'.join(citations)\nRuns and Run Steps\nWhen you have all the context you need from your user in the Thread, you can run the Thread with an Assistant of your choice.\n\npython\n\nrun = client.beta.threads.runs.create(\n  thread_id=thread.id,\n  assistant_id=assistant.id\n)\nBy default, a Run will use the model and tools configuration specified in Assistant object, but you can override most of these when creating the Run for added flexibility:\n\npython\n\nrun = client.beta.threads.runs.create(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  model=\"gpt-4o\",\n  instructions=\"New instructions that override the Assistant instructions\",\n  tools=[{\"type\": \"code_interpreter\"}, {\"type\": \"file_search\"}]\n)\nNote: tool_resources associated with the Assistant cannot be overridden during Run creation. You must use the modify Assistant endpoint to do this.\n\nRun lifecycle\nRun objects can have multiple statuses.\n\nRun lifecycle - diagram showing possible status transitions\n\nSTATUS\tDEFINITION\nqueued\tWhen Runs are first created or when you complete the required_action, they are moved to a queued status. They should almost immediately move to in_progress.\nin_progress\tWhile in_progress, the Assistant uses the model and tools to perform steps. You can view progress being made by the Run by examining the Run Steps.\ncompleted\tThe Run successfully completed! You can now view all Messages the Assistant added to the Thread, and all the steps the Run took. You can also continue the conversation by adding more user Messages to the Thread and creating another Run.\nrequires_action\tWhen using the Function calling tool, the Run will move to a required_action state once the model determines the names and arguments of the functions to be called. You must then run those functions and submit the outputs before the run proceeds. If the outputs are not provided before the expires_at timestamp passes (roughly 10 mins past creation), the run will move to an expired status.\nexpired\tThis happens when the function calling outputs were not submitted before expires_at and the run expires. Additionally, if the runs take too long to execute and go beyond the time stated in expires_at, our systems will expire the run.\ncancelling\tYou can attempt to cancel an in_progress run using the Cancel Run endpoint. Once the attempt to cancel succeeds, status of the Run moves to cancelled. Cancellation is attempted but not guaranteed.\ncancelled\tRun was successfully cancelled.\nfailed\tYou can view the reason for the failure by looking at the last_error object in the Run. The timestamp for the failure will be recorded under failed_at.\nincomplete\tRun ended due to max_prompt_tokens or max_completion_tokens reached. You can view the specific reason by looking at the incomplete_details object in the Run.\nPolling for updates\nIf you are not using streaming, in order to keep the status of your run up to date, you will have to periodically retrieve the Run object. You can check the status of the run each time you retrieve the object to determine what your application should do next.\n\nYou can optionally use Polling Helpers in our Node and Python SDKs to help you with this. These helpers will automatically poll the Run object for you and return the Run object when it's in a terminal state.\n\nThread locks\nWhen a Run is in_progress and not in a terminal state, the Thread is locked. This means that:\n\nNew Messages cannot be added to the Thread.\nNew Runs cannot be created on the Thread.\nRun steps\nRun steps lifecycle - diagram showing possible status transitions\n\nRun step statuses have the same meaning as Run statuses.\n\nMost of the interesting detail in the Run Step object lives in the step_details field. There can be two types of step details:\n\nmessage_creation: This Run Step is created when the Assistant creates a Message on the Thread.\ntool_calls: This Run Step is created when the Assistant calls a tool. Details around this are covered in the relevant sections of the Tools guide.\nData access guidance\nCurrently, Assistants, Threads, Messages, and Vector Stores created via the API are scoped to the Project they're created in. As such, any person with API key access to that Project is able to read or write Assistants, Threads, Messages, and Runs in the Project.\n\nWe strongly recommend the following data access controls:\n\nImplement authorization. Before performing reads or writes on Assistants, Threads, Messages, and Vector Stores, ensure that the end-user is authorized to do so. For example, store in your database the object IDs that the end-user has access to, and check it before fetching the object ID with the API.\nRestrict API key access. Carefully consider who in your organization should have API keys and be part of a Project. Periodically audit this list. API keys enable a wide range of operations including reading and modifying sensitive information, such as Messages and Files.\nCreate separate accounts. Consider creating separate Projects for different applications in order to isolate data across multiple applications.\nNext\nNow that you have explored how Assistants work, the next step is to explore Assistant Tools which covers topics like Function calling, File Search, and Code Interpreter.\n\nPlease use the simplest version, we just need a converation with no tools\n\n",
      "created_at": "2024-06-21T21:23:02.738586",
      "sequence_order": 6,
      "parent_id": "cae72691-f9a3-4738-aea0-2d72de413204",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "1a0c41dc-187e-4212-883e-233843d3b87a",
      "role": "assistant",
      "content": "To integrate the OpenAI API for chat responses instead of using Ollama, we'll make a few adjustments to your `tasks.py` to use the OpenAI API. \n\nHere's how to do it:\n\n### Step 1: Update the Environment Variables\nMake sure you have the OpenAI API key set in your environment variables. Add the following line to your `.env` file:\n```\nOPENAI_API_KEY=your_openai_api_key\n```\n\n### Step 2: Update `tasks.py` to Use OpenAI API\nReplace the `generate_ollama_response` function with OpenAI's chat completion API. Below is the updated code for `tasks.py`:\n\n```python\n# tasks.py\nimport mimetypes\nfrom celery import states\nfrom celery_app import celery\nfrom typing import Dict, Any\nimport asyncio\nimport logging\nimport uuid\nfrom datetime import datetime\nfrom redis import Redis\nfrom graph_generator import generate_graph, save_graph_to_html\nfrom whisper_api import transcribe_audio\nfrom download_utils import download_file_from_brevo, save_file_content\nfrom db_utils import get_task_metadata, save_task_metadata\nfrom airtable_utils import create_graph_record_async, update_status_async\nfrom PyPDF2 import PdfReader\nfrom chat_utils import save_chat_context, load_chat_context, extract_conversation_id, extract_message_from_email\nfrom email_utils import send_email\nimport os\nimport re\nimport openai\n\nlogger = logging.getLogger(__name__)\nREDIS_CLIENT = Redis(host='localhost', port=6379, db=0)\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ndef generate_openai_response(context):\n    messages = [{\"role\": message[\"role\"], \"content\": message[\"content\"]} for message in context]\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        messages=messages\n    )\n    return response['choices'][0]['message']['content']\n\n@celery.task(bind=True)\ndef process_chat_task(self, email_data: Dict[str, Any]) -> Dict[str, Any]:\n    logger.info(f\"Starting process_chat_task for email: {email_data['From']['Address']}\")\n    \n    conversation_id = extract_conversation_id(email_data['Subject'])\n    context = load_chat_context(conversation_id)\n    new_message = extract_message_from_email(email_data)\n    \n    context.append({\"role\": \"user\", \"content\": new_message})\n    openai_response = generate_openai_response(context)\n    context.append({\"role\": \"assistant\", \"content\": openai_response})\n    \n    save_chat_context(conversation_id, context)\n    \n    send_email(\n        email=email_data['From']['Address'],\n        subject=f\"Re: {email_data['Subject']}\",\n        content=openai_response\n    )\n    \n    return {\"status\": \"completed\", \"conversation_id\": conversation_id}\n\nasync def extract_text_from_pdf(file_path):\n    logger.info(f\"Extracting text from PDF: {file_path}\")\n    text = \"\"\n    try:\n        with open(file_path, 'rb') as file:\n            pdf_reader = PdfReader(file)\n            for page in pdf_reader.pages:\n                text += page.extract_text()\n    except Exception as e:\n        logger.error(f\"Error extracting text from PDF: {str(e)}\")\n        raise\n    return text\n\n@celery.task(bind=True)\ndef process_graph_task(self, task_id, email=None):\n    task_id = str(task_id)\n    logger.info(f\"Starting process_graph_task with task_id: {task_id}\")\n    try:\n        task_data = get_task_metadata(task_id)\n        if task_data is None:\n            raise ValueError(f\"Task metadata not found for task_id: {task_id}\")\n\n        logger.info(f\"Retrieved task data: {task_data}\")\n\n        file_path = task_data['file_path']\n        logger.info(f\"File path from metadata: {file_path}\")\n\n        file_type, _ = mimetypes.guess_type(file_path)\n        logger.info(f\"Detected file type: {file_type}\")\n\n        graph_task_id = self.request.id\n        email_sender = email or task_data.get('email_sender')\n\n        logger.info(f\"Using current task ID: {graph_task_id} for process_graph_task\")\n        graph_output_dir = os.path.join('static', 'generated', f\"graph_{graph_task_id}\")\n        os.makedirs(graph_output_dir, exist_ok=True)\n\n        # Create Airtable record with status \"Processing\"\n        async def create_record():\n            record_id = await create_graph_record_async(task_id, email_sender)\n            return record_id\n\n        record_id = asyncio.run(create_record())\n        logger.info(f\"Airtable record created with ID: {record_id}\")\n\n        # Process different file types\n        if file_type == 'application/pdf':\n            extracted_text = asyncio.run(extract_text_from_pdf(file_path))\n            text_file_path = os.path.join(os.path.dirname(file_path), 'extracted_text.txt')\n            with open(text_file_path, 'w', encoding='utf-8') as f:\n                f.write(extracted_text)\n            file_path = text_file_path\n        elif file_type and file_type.startswith('audio/'):\n            transcription = asyncio.run(transcribe_audio(file_path))\n            transcription_path = os.path.join(os.path.dirname(file_path), 'transcription.txt')\n            with open(transcription_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            file_path = transcription_path\n\n        G = generate_graph(file_path, graph_output_dir)\n        save_graph_to_html(G, graph_output_dir)\n\n        logger.info(f\"Graph generated successfully for process_graph_task with task_id: {self.request.id}\")\n\n        result_dict = {\n            'graph_task': {\n                'task_id': graph_task_id,\n                'd3_url': f'/static/generated/graph_{graph_task_id}/index.html',\n                'pyvis_url': f'/static/generated/graph_{graph_task_id}/pyvis.html'\n            }\n        }\n\n        # Update Airtable record to \"Processing Completed\"\n        async def update_record():\n            await update_status_async(record_id, \"Processing Completed\")\n\n        asyncio.run(update_record())\n\n        # Trigger email task\n        task_data.update(result_dict)\n        send_email_task.delay(task_data)\n\n        self.update_state(state=states.SUCCESS, meta=result_dict)\n        return result_dict\n\n    except Exception as e:\n        logger.exception(f\"Error generating graph for process_graph_task with task_id: {self.request.id}\")\n        raise\n\ndef is_valid_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\n@celery.task(bind=True)\ndef transcribe_audio_task(self, task_id):\n    task_id = str(task_id)\n    task_data = get_task_metadata(task_id)\n    file_path = task_data['file_path']\n    download_task_id = task_data['download_task_id']\n\n    logger.info(f\"Starting transcribe_audio_task with download_task_id: {download_task_id}\")\n    output_dir = os.path.join('uploads', f\"tr_{download_task_id}\")\n    transcription_path = os.path.join(output_dir, 'transcription.txt')\n\n    if not os.path.exists(transcription_path):\n        async def run_transcription():\n            async with redis_lock(f\"transcribe_lock_{download_task_id}\") as acquired:\n                if acquired:\n                    try:\n                        logger.info(f\"Starting transcription for download task {download_task_id}\")\n                        logger.info(f\"File path: {file_path}\")\n                        logger.info(f\"File exists: {os.path.exists(file_path)}\")\n                        logger.info(f\"File size: {os.path.getsize(file_path)} bytes\")\n                        \n                        transcription = await transcribe_audio(file_path)\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(transcription)\n\n                        task_data.update({\n                            'transcription_data': transcription,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n\n                    except Exception as e:\n                        logger.error(f\"Error during transcription: {str(e)}\")\n                        logger.exception(\"Full traceback:\")\n                        # Fallback: create a text file with error message\n                        error_message = f\"Transcription failed. Error: {str(e)}\"\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(error_message)\n                        task_data.update({\n                            'transcription_data': error_message,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n                else:\n                    logger.info(f\"Another instance of the task is already running for download task {download_task_id}\")\n\n        try:\n            asyncio.run(run_transcription())\n        except Exception as e:\n            logger.error(f\"Error in run_transcription: {str(e)}\")\n            # Don't retry, we've already handled the error\n\n    result_dict = {'task_id': task_id}\n    logger.info(f\"transcribe_audio_task result_dict updated with transcription_path: {transcription_path}\")\n    logger.info(f\"Updated result_dict: {result_dict}\")\n    \n    self.update_state(state=states.SUCCESS, meta=result_dict)\n    return task_id\n        \n@celery.task(bind=True)\ndef download_and_process_attachment_task(self, file_url, email_sender, email_recipient, api_key, email_subject",
      "created_at": "2024-06-21T21:24:01.741326",
      "sequence_order": 7,
      "parent_id": "aaa2838c-da04-4294-b24a-e3c1850a875e",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": false
      }
    },
    {
      "id": "c630bf03-7bb4-48e7-b860-dd6927d53e2b",
      "role": "assistant",
      "content": "Sure! To use the OpenAI API for generating chat responses instead of Ollama, you'll need to make a few changes to your `tasks.py` file. Here's how you can do it:\n\n1. **Install the OpenAI Python package**:\n   If you haven't already installed the OpenAI Python package, you can install it using pip:\n   ```sh\n   pip install openai\n   ```\n\n2. **Update the `process_chat_task` to use the OpenAI API**:\n   Modify your `process_chat_task` function to make a call to the OpenAI API instead of Ollama. You'll need to replace the `generate_ollama_response` function with a function that calls the OpenAI API.\n\nHere's the updated `tasks.py` file:\n\n```python\nimport mimetypes\nfrom celery import states\nfrom celery_app import celery\nfrom typing import Dict, Any\nimport asyncio\nimport logging\nimport uuid\nfrom datetime import datetime\nfrom redis import Redis\nfrom graph_generator import generate_graph, save_graph_to_html\nfrom whisper_api import transcribe_audio\nfrom download_utils import download_file_from_brevo, save_file_content\nfrom db_utils import get_task_metadata, save_task_metadata\nfrom airtable_utils import create_graph_record_async, update_status_async\nfrom PyPDF2 import PdfReader\nfrom chat_utils import save_chat_context, load_chat_context, extract_conversation_id, extract_message_from_email\nimport openai\nfrom email_utils import send_email  # Import send_email_task\nimport os\nimport re\n\nlogger = logging.getLogger(__name__)\nREDIS_CLIENT = Redis(host='localhost', port=6379, db=0)\n\n# Set your OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ndef generate_openai_response(context):\n    try:\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",  # or \"gpt-3.5-turbo\"\n            messages=context\n        )\n        return response.choices[0].message['content']\n    except Exception as e:\n        logger.error(f\"Error generating OpenAI response: {str(e)}\")\n        raise\n\n@celery.task(bind=True)\ndef process_chat_task(self, email_data: Dict[str, Any]) -> Dict[str, Any]:\n    logger.info(f\"Starting process_chat_task for email: {email_data['From']['Address']}\")\n    \n    conversation_id = extract_conversation_id(email_data['Subject'])\n    context = load_chat_context(conversation_id)\n    new_message = extract_message_from_email(email_data)\n    \n    context.append({\"role\": \"user\", \"content\": new_message})\n    openai_response = generate_openai_response(context)\n    context.append({\"role\": \"assistant\", \"content\": openai_response})\n    \n    save_chat_context(conversation_id, context)\n    \n    send_email(\n        email=email_data['From']['Address'],\n        subject=f\"Re: {email_data['Subject']}\",\n        content=openai_response\n    )\n    \n    return {\"status\": \"completed\", \"conversation_id\": conversation_id}\n\nasync def extract_text_from_pdf(file_path):\n    logger.info(f\"Extracting text from PDF: {file_path}\")\n    text = \"\"\n    try:\n        with open(file_path, 'rb') as file:\n            pdf_reader = PdfReader(file)\n            for page in pdf_reader.pages:\n                text += page.extract_text()\n    except Exception as e:\n        logger.error(f\"Error extracting text from PDF: {str(e)}\")\n        raise\n    return text\n\n@celery.task(bind=True)\ndef process_graph_task(self, task_id, email=None):\n    task_id = str(task_id)\n    logger.info(f\"Starting process_graph_task with task_id: {task_id}\")\n    try:\n        task_data = get_task_metadata(task_id)\n        if task_data is None:\n            raise ValueError(f\"Task metadata not found for task_id: {task_id}\")\n\n        logger.info(f\"Retrieved task data: {task_data}\")\n\n        file_path = task_data['file_path']\n        logger.info(f\"File path from metadata: {file_path}\")\n\n        file_type, _ = mimetypes.guess_type(file_path)\n        logger.info(f\"Detected file type: {file_type}\")\n\n        graph_task_id = self.request.id\n        email_sender = email or task_data.get('email_sender')\n\n        logger.info(f\"Using current task ID: {graph_task_id} for process_graph_task\")\n        graph_output_dir = os.path.join('static', 'generated', f\"graph_{graph_task_id}\")\n        os.makedirs(graph_output_dir, exist_ok=True)\n\n        # Create Airtable record with status \"Processing\"\n        async def create_record():\n            record_id = await create_graph_record_async(task_id, email_sender)\n            return record_id\n\n        record_id = asyncio.run(create_record())\n        logger.info(f\"Airtable record created with ID: {record_id}\")\n\n        # Process different file types\n        if file_type == 'application/pdf':\n            extracted_text = asyncio.run(extract_text_from_pdf(file_path))\n            text_file_path = os.path.join(os.path.dirname(file_path), 'extracted_text.txt')\n            with open(text_file_path, 'w', encoding='utf-8') as f:\n                f.write(extracted_text)\n            file_path = text_file_path\n        elif file_type and file_type.startswith('audio/'):\n            transcription = asyncio.run(transcribe_audio(file_path))\n            transcription_path = os.path.join(os.path.dirname(file_path), 'transcription.txt')\n            with open(transcription_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            file_path = transcription_path\n\n        G = generate_graph(file_path, graph_output_dir)\n        save_graph_to_html(G, graph_output_dir)\n\n        logger.info(f\"Graph generated successfully for process_graph_task with task_id: {self.request.id}\")\n\n        result_dict = {\n            'graph_task': {\n                'task_id': graph_task_id,\n                'd3_url': f'/static/generated/graph_{graph_task_id}/index.html',\n                'pyvis_url': f'/static/generated/graph_{graph_task_id}/pyvis.html'\n            }\n        }\n\n        # Update Airtable record to \"Processing Completed\"\n        async def update_record():\n            await update_status_async(record_id, \"Processing Completed\")\n\n        asyncio.run(update_record())\n\n        # Trigger email task\n        task_data.update(result_dict)\n        send_email_task.delay(task_data)\n\n        self.update_state(state=states.SUCCESS, meta=result_dict)\n        return result_dict\n\n    except Exception as e:\n        logger.exception(f\"Error generating graph for process_graph_task with task_id: {self.request.id}\")\n        raise\n\ndef is_valid_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\n@celery.task(bind=True)\ndef transcribe_audio_task(self, task_id):\n    task_id = str(task_id)\n    task_data = get_task_metadata(task_id)\n    file_path = task_data['file_path']\n    download_task_id = task_data['download_task_id']\n\n    logger.info(f\"Starting transcribe_audio_task with download_task_id: {download_task_id}\")\n    output_dir = os.path.join('uploads', f\"tr_{download_task_id}\")\n    transcription_path = os.path.join(output_dir, 'transcription.txt')\n\n    if not os.path.exists(transcription_path):\n        async def run_transcription():\n            async with redis_lock(f\"transcribe_lock_{download_task_id}\") as acquired:\n                if acquired:\n                    try:\n                        logger.info(f\"Starting transcription for download task {download_task_id}\")\n                        logger.info(f\"File path: {file_path}\")\n                        logger.info(f\"File exists: {os.path.exists(file_path)}\")\n                        logger.info(f\"File size: {os.path.getsize(file_path)} bytes\")\n                        \n                        transcription = await transcribe_audio(file_path)\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(transcription)\n\n                        task_data.update({\n                            'transcription_data': transcription,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n\n                    except Exception as e:\n                        logger.error(f\"Error during transcription: {str(e)}\")\n                        logger.exception(\"Full traceback:\")\n                        # Fallback: create a text file with error message\n                        error_message = f\"Transcription failed. Error: {str(e)}\"\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(error_message)\n                        task_data.update({\n                            'transcription_data': error_message,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n                else:\n                    logger.info(f\"Another instance of the task is already running for download task {download_task_id}\")\n\n        try:\n            asyncio.run(run_transcription())\n        except Exception as e:\n            logger.error(f\"Error in run_transcription: {str(e)}\")\n            # Don't retry, we've already handled the error\n\n    result_dict = {'task_id': task_id}\n    logger.info(f\"transcribe_audio_task result_dict updated with transcription_path: {transcription_path}\")\n    logger.info(f\"Updated result_dict: {result_dict}\")\n    \n    self",
      "created_at": "2024-06-21T21:26:23.910212",
      "sequence_order": 8,
      "parent_id": "aaa2838c-da04-4294-b24a-e3c1850a875e",
      "metadata": {
        "status": "finished_partial_completion",
        "weight": 1,
        "end_turn": false
      }
    },
    {
      "id": "8f463d7f-9a94-44d2-aa2a-7ba0a3a090b9",
      "role": "assistant",
      "content": ".update_state(state=states.SUCCESS, meta=result_dict)\n    return task_id\n        \n@celery.task(bind=True)\ndef download_and_process_attachment_task(self, file_url, email_sender, email_recipient, api_key, email_subject, data):\n    try:\n        download_task_id = str(uuid.uuid4())\n        logger.info(f\"Starting download_and_process_attachment_task with download_task_id: {download_task_id}\")\n\n        if 'Attachments' in data and data['Attachments']:\n            attachment_data = data['Attachments'][0]\n            attachment_name = attachment_data['Name']\n            attachment_type = attachment_data['ContentType']\n            download_token = attachment_data['DownloadToken']\n            \n            file_content = download_file_from_brevo(download_token, api_key)\n            file_path = os.path.join('uploads', f\"download_{download_task_id}\", attachment_name)\n            save_file_content(file_content, file_path)\n        else:\n            attachment_name = 'message.txt'\n            attachment_type = 'text/plain'\n            file_content = data.get('ExtractedMarkdownMessage', '')\n            file_path = os.path.join('uploads', f\"download_{download_task_id}\", attachment_name)\n            save_file_content(file_content.encode('utf-8'), file_path)\n        \n        metadata = {\n            'file_name': attachment_name,\n            'file_type': attachment_type,\n            'date_uploaded': datetime.now().strftime('%Y-%m-%d %H:%S'),\n            'download_task_id': download_task_id,\n            'downloaded_file_link': file_path,\n            'email_sender': email_sender,\n            'email_subject': email_subject,\n            'file_path': file_path\n        }\n        save_task_metadata(download_task_id, metadata)\n\n        valid_audio_formats = ['flac', 'm4a', 'mp3', 'mp4', 'mpeg', 'mpga', 'oga', 'ogg', 'wav', 'webm']\n        file_extension = os.path.splitext(attachment_name)[1].lower().strip('.')\n\n        if file_extension in valid_audio_formats:\n            task_chain = chain(transcribe_audio_task.s(download_task_id), process_graph_task.s())\n        else:\n            task_chain = process_graph_task.s(download_task_id)\n\n        task_result = task_chain.apply_async()\n        graph_task_id = task_result.id\n\n        return {'status': 'Processing', 'task_id': download_task_id}\n\n    except Exception as e:\n        self.update_state(\n            state=states.FAILURE,\n            meta={\n                'exc_type': type(e).__name__,\n                'exc_message': str(e)\n            }\n        )\n        raise self.retry(exc=e, countdown=5, max_retries=3)\n\n@celery.task(bind=True)\ndef send_email_task(self, task_data):\n    try:\n        graph_task_id = task_data.get('graph_task', {}).get('task_id')\n        email_sender = task_data['email_sender']\n        email_subject = task_data['email_subject']\n\n        if graph_task_id:\n            progress_url = f'https://www.txtray.com/progress/{graph_task_id}'\n        else:\n            progress_url = f'https://www.txtray.com/progress/{self.request.id}'\n\n        logger.info(f\"Email sender received in send_email_task: {email_sender}\")\n        logger.info(f\"Email subject received in send_email_task: {email_subject}\")\n\n        if not is_valid_email(email_sender):\n            raise ValueError(f\"Invalid email address: {email_sender}\")\n\n        logger.info(f\"Sending email with progress_url: {progress_url}\")\n        send_email(\n            email=email_sender,\n            subject=f\"Re: {email_subject}\",\n            content=f\"Your request has been processed. You can view the progress at: {progress_url}\"\n        )\n        logger.info(f\"Email sent successfully for task {self.request.id}\")\n\n        self.update_state(state=states.SUCCESS)\n\n    except Exception as e:\n        logger.exception(f\"Error sending email for task {self.request.id}\")\n        raise\n```\n\nIn this updated `tasks.py`, the `generate_openai_response` function is defined to call the OpenAI API and generate a response based on the context. The `process_chat_task` function is modified to use this new function instead of `generate_ollama_response`.\n\n### Key Points:\n- **OpenAI API Key**: Make sure your OpenAI API key is set in your environment variables (`OPENAI_API_KEY`).\n- **Model Choice**: You can choose between `gpt-4` and `gpt-3.5-turbo` based on your requirements and availability.\n\nThis setup will enable your Celery task to generate chat responses using the OpenAI API.",
      "created_at": "2024-06-21T21:26:56.453608",
      "sequence_order": 9,
      "parent_id": "c630bf03-7bb4-48e7-b860-dd6927d53e2b",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": true
      }
    },
    {
      "id": "aaa2c10d-26bb-4fd3-bad0-af44a4dfd5c2",
      "role": "user",
      "content": "     CGroup: /system.slice/celery.service\n             â”œâ”€1006713 /home/bilal/public_html/txtray.com/cgi-bin/venvcgi/bin/python3 /home/bilal/public_html/txtray.com/cgi-bin/venvcgi/bin/celery -A tasks worker --loglevel=info\n             â”œâ”€1006716 /home/bilal/public_html/txtray.com/cgi-bin/venvcgi/bin/python3 /home/bilal/public_html/txtray.com/cgi-bin/venvcgi/bin/celery -A tasks worker --loglevel=info\n             â””â”€1006717 /home/bilal/public_html/txtray.com/cgi-bin/venvcgi/bin/python3 /home/bilal/public_html/txtray.com/cgi-bin/venvcgi/bin/celery -A tasks worker --loglevel=info\n\nJun 21 19:36:35 server1.checkin.cards celery[1006717]:     R = retval = fun(*args, **kwargs)\nJun 21 19:36:35 server1.checkin.cards celery[1006717]:   File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/celery/app/trace.py\", line 736, in __protected_call__\nJun 21 19:36:35 server1.checkin.cards celery[1006717]:     return self.run(*args, **kwargs)\nJun 21 19:36:35 server1.checkin.cards celery[1006717]:   File \"/home/bilal/public_html/txtray.com/tasks.py\", line 279, in download_and_process_attachment_task\nJun 21 19:36:35 server1.checkin.cards celery[1006717]:     raise self.retry(exc=e, countdown=5, max_retries=3)\nJun 21 19:36:35 server1.checkin.cards celery[1006717]:   File \"/home/bilal/public_html/txtray.com/cgi-bin/venvcgi/lib/python3.10/site-packages/celery/app/task.py\", line 736, in retry\nJun 21 19:36:35 server1.checkin.cards celery[1006717]:     raise_with_context(exc)\nJun 21 19:36:35 server1.checkin.cards celery[1006717]:   File \"/home/bilal/public_html/txtray.com/tasks.py\", line 262, in download_and_process_attachment_task\nJun 21 19:36:35 server1.checkin.cards celery[1006717]:     task_chain = chain(transcribe_audio_task.s(download_task_id), process_graph_task.s())\nJun 21 19:36:35 server1.checkin.cards celery[1006717]: NameError: name 'chain' is not defined\n",
      "created_at": "2024-06-21T21:39:33.373433",
      "sequence_order": 10,
      "parent_id": "8f463d7f-9a94-44d2-aa2a-7ba0a3a090b9",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "24e0d5e8-4a9d-40af-9ab2-75109a440ba7",
      "role": "assistant",
      "content": "The error message `NameError: name 'chain' is not defined` indicates that the `chain` function from Celery is not imported in your `tasks.py` file. To fix this, ensure that you import `chain` from `celery` at the top of your `tasks.py` file.\n\nHere's how you can update your `tasks.py` file to include the necessary import:\n\n```python\nimport mimetypes\nfrom celery import states, chain  # Add chain import here\nfrom celery_app import celery\nfrom typing import Dict, Any\nimport asyncio\nimport logging\nimport uuid\nfrom datetime import datetime\nfrom redis import Redis\nfrom graph_generator import generate_graph, save_graph_to_html\nfrom whisper_api import transcribe_audio\nfrom download_utils import download_file_from_brevo, save_file_content\nfrom db_utils import get_task_metadata, save_task_metadata\nfrom airtable_utils import create_graph_record_async, update_status_async\nfrom PyPDF2 import PdfReader\nfrom chat_utils import save_chat_context, load_chat_context, extract_conversation_id, extract_message_from_email\nimport openai\nfrom email_utils import send_email  # Import send_email_task\nimport os\nimport re\n\nlogger = logging.getLogger(__name__)\nREDIS_CLIENT = Redis(host='localhost', port=6379, db=0)\n\n# Set your OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ndef generate_openai_response(context):\n    try:\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",  # or \"gpt-3.5-turbo\"\n            messages=context\n        )\n        return response.choices[0].message['content']\n    except Exception as e:\n        logger.error(f\"Error generating OpenAI response: {str(e)}\")\n        raise\n\n@celery.task(bind=True)\ndef process_chat_task(self, email_data: Dict[str, Any]) -> Dict[str, Any]:\n    logger.info(f\"Starting process_chat_task for email: {email_data['From']['Address']}\")\n    \n    conversation_id = extract_conversation_id(email_data['Subject'])\n    context = load_chat_context(conversation_id)\n    new_message = extract_message_from_email(email_data)\n    \n    context.append({\"role\": \"user\", \"content\": new_message})\n    openai_response = generate_openai_response(context)\n    context.append({\"role\": \"assistant\", \"content\": openai_response})\n    \n    save_chat_context(conversation_id, context)\n    \n    send_email(\n        email=email_data['From']['Address'],\n        subject=f\"Re: {email_data['Subject']}\",\n        content=openai_response\n    )\n    \n    return {\"status\": \"completed\", \"conversation_id\": conversation_id}\n\nasync def extract_text_from_pdf(file_path):\n    logger.info(f\"Extracting text from PDF: {file_path}\")\n    text = \"\"\n    try:\n        with open(file_path, 'rb') as file:\n            pdf_reader = PdfReader(file)\n            for page in pdf_reader.pages:\n                text += page.extract_text()\n    except Exception as e:\n        logger.error(f\"Error extracting text from PDF: {str(e)}\")\n        raise\n    return text\n\n@celery.task(bind=True)\ndef process_graph_task(self, task_id, email=None):\n    task_id = str(task_id)\n    logger.info(f\"Starting process_graph_task with task_id: {task_id}\")\n    try:\n        task_data = get_task_metadata(task_id)\n        if task_data is None:\n            raise ValueError(f\"Task metadata not found for task_id: {task_id}\")\n\n        logger.info(f\"Retrieved task data: {task_data}\")\n\n        file_path = task_data['file_path']\n        logger.info(f\"File path from metadata: {file_path}\")\n\n        file_type, _ = mimetypes.guess_type(file_path)\n        logger.info(f\"Detected file type: {file_type}\")\n\n        graph_task_id = self.request.id\n        email_sender = email or task_data.get('email_sender')\n\n        logger.info(f\"Using current task ID: {graph_task_id} for process_graph_task\")\n        graph_output_dir = os.path.join('static', 'generated', f\"graph_{graph_task_id}\")\n        os.makedirs(graph_output_dir, exist_ok=True)\n\n        # Create Airtable record with status \"Processing\"\n        async def create_record():\n            record_id = await create_graph_record_async(task_id, email_sender)\n            return record_id\n\n        record_id = asyncio.run(create_record())\n        logger.info(f\"Airtable record created with ID: {record_id}\")\n\n        # Process different file types\n        if file_type == 'application/pdf':\n            extracted_text = asyncio.run(extract_text_from_pdf(file_path))\n            text_file_path = os.path.join(os.path.dirname(file_path), 'extracted_text.txt')\n            with open(text_file_path, 'w', encoding='utf-8') as f:\n                f.write(extracted_text)\n            file_path = text_file_path\n        elif file_type and file_type.startswith('audio/'):\n            transcription = asyncio.run(transcribe_audio(file_path))\n            transcription_path = os.path.join(os.path.dirname(file_path), 'transcription.txt')\n            with open(transcription_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            file_path = transcription_path\n\n        G = generate_graph(file_path, graph_output_dir)\n        save_graph_to_html(G, graph_output_dir)\n\n        logger.info(f\"Graph generated successfully for process_graph_task with task_id: {self.request.id}\")\n\n        result_dict = {\n            'graph_task': {\n                'task_id': graph_task_id,\n                'd3_url': f'/static/generated/graph_{graph_task_id}/index.html',\n                'pyvis_url': f'/static/generated/graph_{graph_task_id}/pyvis.html'\n            }\n        }\n\n        # Update Airtable record to \"Processing Completed\"\n        async def update_record():\n            await update_status_async(record_id, \"Processing Completed\")\n\n        asyncio.run(update_record())\n\n        # Trigger email task\n        task_data.update(result_dict)\n        send_email_task.delay(task_data)\n\n        self.update_state(state=states.SUCCESS, meta=result_dict)\n        return result_dict\n\n    except Exception as e:\n        logger.exception(f\"Error generating graph for process_graph_task with task_id: {self.request.id}\")\n        raise\n\ndef is_valid_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\n@celery.task(bind=True)\ndef transcribe_audio_task(self, task_id):\n    task_id = str(task_id)\n    task_data = get_task_metadata(task_id)\n    file_path = task_data['file_path']\n    download_task_id = task_data['download_task_id']\n\n    logger.info(f\"Starting transcribe_audio_task with download_task_id: {download_task_id}\")\n    output_dir = os.path.join('uploads', f\"tr_{download_task_id}\")\n    transcription_path = os.path.join(output_dir, 'transcription.txt')\n\n    if not os.path.exists(transcription_path):\n        async def run_transcription():\n            async with redis_lock(f\"transcribe_lock_{download_task_id}\") as acquired:\n                if acquired:\n                    try:\n                        logger.info(f\"Starting transcription for download task {download_task_id}\")\n                        logger.info(f\"File path: {file_path}\")\n                        logger.info(f\"File exists: {os.path.exists(file_path)}\")\n                        logger.info(f\"File size: {os.path.getsize(file_path)} bytes\")\n                        \n                        transcription = await transcribe_audio(file_path)\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(transcription)\n\n                        task_data.update({\n                            'transcription_data': transcription,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n\n                    except Exception as e:\n                        logger.error(f\"Error during transcription: {str(e)}\")\n                        logger.exception(\"Full traceback:\")\n                        # Fallback: create a text file with error message\n                        error_message = f\"Transcription failed. Error: {str(e)}\"\n                        os.makedirs(output_dir, exist_ok=True)\n                        with open(transcription_path, 'w') as f:\n                            f.write(error_message)\n                        task_data.update({\n                            'transcription_data': error_message,\n                            'transcription_task_id': self.request.id,\n                            'transcription_file_link': transcription_path,\n                            'transcription_created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                            'file_path': transcription_path\n                        })\n                        save_task_metadata(task_id, task_data)\n                else:\n                    logger.info(f\"Another instance of the task is already running for download task {download_task_id}\")\n\n        try:\n            asyncio.run(run_transcription())\n        except Exception as e:\n            logger.error(f\"Error in run_transcription: {str(e)}\")\n            # Don't retry, we've already handled the error\n\n    result_dict = {'task_id': task_id}\n    logger.info(f\"transcribe_audio_task result_dict updated with transcription_path: {transcription_path}\")\n    logger.info(f\"Updated result_dict: {result_dict}\")\n    \n    self.update_state(state=states.SUCCESS, meta=result_dict)\n    return task_id\n\n@celery.task(bind=True)\ndef download_and_process_attachment_task(self, file_url, email_sender, email_recipient, api_key, email_subject, data):\n    try:\n        download_task_id = str(uuid.uuid4())\n        logger.info(f\"",
      "created_at": "2024-06-21T21:40:30.947176",
      "sequence_order": 11,
      "parent_id": "aaa2c10d-26bb-4fd3-bad0-af44a4dfd5c2",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": false
      }
    }
  ]
}