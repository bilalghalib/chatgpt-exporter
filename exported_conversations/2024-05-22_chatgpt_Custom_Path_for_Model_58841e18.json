{
  "id": "58841e18-bdf8-45da-b642-8ab7bc63fb06",
  "source": "chatgpt",
  "title": "Custom Path for Model",
  "created_at": "2024-05-22T23:09:12.794680",
  "updated_at": "2024-05-22T23:09:32.314443",
  "summary": null,
  "message_count": 3,
  "messages": [
    {
      "id": "aaa26da8-a4e1-4d2c-ae41-e86cc2da16ce",
      "role": "user",
      "content": "i want to use airllm, you can see it's info here:\nSkip to content\nNavigation Menu\nlyogavin\n/\nAnima\n \nType / to search\n \nCode\nIssues\n57\nPull requests\n2\nDiscussions\nActions\nProjects\nSecurity\nInsights\nFiles\n\n\nt\nair_llm\nairllm\nexamples\ntests\n\nLICENSE\n\nREADME.md\n\n__init__.py\n\ninference_example.py\n\nsetup.py\nanima_100k\nassets\ndata\neval\nexamples\nrlhf\nscripts\ntraining\n\n.gitignore\n\nLICENSE\n\nREADME.md\n\nREADME_en.md\n\nanima_logo.png\n\nrequirements.txt\nBreadcrumbsAnima\n/air_llm/\nDirectory actionsAdd fileMore options\nLatest commit\nlyogavin\nlyogavin\nUpdate README.md\nd48b2aa\n Â· \nlast month\nHistory\nBreadcrumbsAnima\n/air_llm/\nFolders and files\nName\tLast commit message\tLast commit date\nparent directory\n..\n\nairllm\nsupport delete_original model files\n5 months ago\n\nexamples\nfix automodel on macos\n5 months ago\n\ntests\nfix automodel on macos\n5 months ago\n\nLICENSE\nadd airllm\n6 months ago\n\nREADME.md\nUpdate README.md\nlast month\n\n__init__.py\nfix test import\n5 months ago\n\ninference_example.py\nadd airllm\n6 months ago\n\nsetup.py\nsupport delete_original model files\n5 months ago\nREADME.md\nairllm_logo\n\nQuickstart | Configurations | MacOS | Example notebooks | FAQ\n\nAirLLM optimizes inference memory usage, allowing 70B large language models to run inference on a single 4GB GPU card. No quantization, distillation, pruning or other model compression techniques that would result in degraded model performance are needed.\n\nAirLLMä¼˜åŒ–inferenceå†…å­˜ï¼Œ4GBå•å¡GPUå¯ä»¥è¿è¡Œ70Bå¤§è¯­è¨€æ¨¡å‹æ¨ç†ã€‚ä¸éœ€è¦ä»»ä½•æŸå¤±æ¨¡å‹æ€§èƒ½çš„é‡åŒ–å’Œè’¸é¦ï¼Œå‰ªæç­‰æ¨¡å‹å‹ç¼©ã€‚\n\nUpdates\n\n[2024/04/20] AirLLM supports Llama3 natively already. Run Llama3 70B on 4GB single GPU.\n\nAirLLMå¤©ç„¶æ”¯æŒLlama3 70Bã€‚4GBæ˜¾å­˜è¿è¡ŒLlama3 70Bå¤§æ¨¡å‹ã€‚\n\n[2023/12/25] v2.8.2: Support MacOS running 70B large language models.\n\næ”¯æŒè‹¹æœç³»ç»Ÿè¿è¡Œ70Bå¤§æ¨¡å‹ï¼\n\n[2023/12/20] v2.7: Support AirLLMMixtral.\n\n[2023/12/20] v2.6: Added AutoModel, automatically detect model type, no need to provide model class to initialize model.\n\næä¾›AuoModelï¼Œè‡ªåŠ¨æ ¹æ®repoå‚æ•°æ£€æµ‹æ¨¡å‹ç±»å‹ï¼Œè‡ªåŠ¨åˆå§‹åŒ–æ¨¡å‹ã€‚\n\n[2023/12/18] v2.5: added prefetching to overlap the model loading and compute. 10% speed improvement.\n\n[2023/12/03] added support of ChatGLM, QWen, Baichuan, Mistral, InternLM!\n\næ”¯æŒChatGLM, QWEN, Baichuan, Mistral, InternLM!\n\n[2023/12/02] added support for safetensors. Now support all top 10 models in open llm leaderboard.\n\næ”¯æŒsafetensorç³»åˆ—æ¨¡å‹ï¼Œç°åœ¨open llm leaderboardå‰10çš„æ¨¡å‹éƒ½å·²ç»æ”¯æŒã€‚\n\n[2023/12/01] airllm 2.0. Support compressions: 3x run time speed up!\n\nairllm2.0ã€‚æ”¯æŒæ¨¡å‹å‹ç¼©ï¼Œé€Ÿåº¦æå‡3å€ã€‚\n\n[2023/11/20] airllm Initial verion!\n\nairllmå‘å¸ƒã€‚\n\nTable of Contents\n\nQuick start\nModel Compression\nConfigurations\nRun on MacOS\nExample notebooks\nSupported Models\nAcknowledgement\nFAQ\nQuickstart\n\n1. Install package\n\nFirst, install the airllm pip package.\n\né¦–å…ˆå®‰è£…airllmåŒ…ã€‚\n\npip install airllm\nå¦‚æœæ‰¾ä¸åˆ°packageï¼Œå¯èƒ½æ˜¯å› ä¸ºé»˜è®¤çš„é•œåƒé—®é¢˜ã€‚å¯ä»¥å°è¯•åˆ¶å®šåŸå§‹é•œåƒï¼š\n\npip install -i https://pypi.org/simple/ airllm\n2. Inference\n\nThen, initialize AirLLMLlama2, pass in the huggingface repo ID of the model being used, or the local path, and inference can be performed similar to a regular transformer model.\n\nç„¶åï¼Œåˆå§‹åŒ–AirLLMLlama2ï¼Œä¼ å…¥æ‰€ä½¿ç”¨æ¨¡å‹çš„huggingface repo IDï¼Œæˆ–è€…æœ¬åœ°è·¯å¾„å³å¯ç±»ä¼¼äºæ™®é€šçš„transformeræ¨¡å‹è¿›è¡Œæ¨ç†ã€‚\n\n(You can can also specify the path to save the splitted layered model through layer_shards_saving_path when init AirLLMLlama2.\n\nå¦‚æœéœ€è¦æŒ‡å®šå¦å¤–çš„è·¯å¾„æ¥å­˜å‚¨åˆ†å±‚çš„æ¨¡å‹å¯ä»¥åœ¨åˆå§‹åŒ–AirLLMLlama2æ˜¯ä¼ å…¥å‚æ•°ï¼šlayer_shards_saving_pathã€‚)\n\nfrom airllm import AutoModel\n\nMAX_LENGTH = 128\n# could use hugging face model repo id:\nmodel = AutoModel.from_pretrained(\"garage-bAInd/Platypus2-70B-instruct\")\n\n# or use model's local path...\n#model = AutoModel.from_pretrained(\"/home/ubuntu/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/b585e74bcaae02e52665d9ac6d23f4d0dbc81a0f\")\n\ninput_text = [\n        'What is the capital of United States?',\n        #'I like',\n    ]\n\ninput_tokens = model.tokenizer(input_text,\n    return_tensors=\"pt\", \n    return_attention_mask=False, \n    truncation=True, \n    max_length=MAX_LENGTH, \n    padding=False)\n           \ngeneration_output = model.generate(\n    input_tokens['input_ids'].cuda(), \n    max_new_tokens=20,\n    use_cache=True,\n    return_dict_in_generate=True)\n\noutput = model.tokenizer.decode(generation_output.sequences[0])\n\nprint(output)\nNote: During inference, the original model will first be decomposed and saved layer-wise. Please ensure there is sufficient disk space in the huggingface cache directory.\n\næ³¨æ„ï¼šæ¨ç†è¿‡ç¨‹ä¼šé¦–å…ˆå°†åŸå§‹æ¨¡å‹æŒ‰å±‚åˆ†æ‹†ï¼Œè½¬å­˜ã€‚è¯·ä¿è¯huggingface cacheç›®å½•æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ã€‚\n\nModel Compression - 3x Inference Speed Up!\n\nWe just added model compression based on block-wise quantization based model compression. Which can further speed up the inference speed for up to 3x , with almost ignorable accuracy loss! (see more performance evaluation and why we use block-wise quantization in this paper)\n\næˆ‘ä»¬å¢åŠ äº†åŸºäºblock-wise quantizationçš„æ¨¡å‹å‹ç¼©ï¼Œæ¨ç†é€Ÿåº¦æå‡3å€å‡ ä¹æ²¡æœ‰ç²¾åº¦æŸå¤±ã€‚ç²¾åº¦è¯„æµ‹å¯ä»¥å‚è€ƒæ­¤paperï¼šthis paper\n\nspeed_improvement\n\nHow to enable model compression speed up:\n\nStep 1. make sure you have bitsandbytes installed by pip install -U bitsandbytes \nStep 2. make sure airllm verion later than 2.0.0: pip install -U airllm\nStep 3. when initialize the model, passing the argument compression ('4bit' or '8bit'):\nmodel = AutoModel.from_pretrained(\"garage-bAInd/Platypus2-70B-instruct\",\n                     compression='4bit' # specify '8bit' for 8-bit block-wise quantization \n                    )\nWhat is the differences between model compression and quantization?\n\nQuantization normally needs to quantize both weights and activations to really speed things up. Which makes it harder to maintain accuracy and avoid the impact of outliers in all kinds of inputs.\n\nWhile in our case the bottleneck is mainly at the disk loading, we only need to make the model loading size smaller. So we get to only quantize the weights part, which is easier to ensure the accuracy.\n\nConfigurations\n\nWhen initialize the model, we support the following configurations:\n\nåˆå§‹åŒ–modelçš„æ—¶å€™ï¼Œå¯ä»¥æŒ‡å®šä»¥ä¸‹çš„é…ç½®å‚æ•°ï¼š\n\ncompression: supported options: 4bit, 8bit for 4-bit or 8-bit block-wise quantization, or by default None for no compression\nprofiling_mode: supported options: True to output time consumptions or by default False\nlayer_shards_saving_path: optionally another path to save the splitted model\nhf_token: huggingface token can be provided here if downloading gated models like: meta-llama/Llama-2-7b-hf\nprefetching: prefetching to overlap the model loading and compute. By default turned on. For now only AirLLMLlama2 supports this.\ndelete_original: if you don't have too much disk space, you can set delete_original to true to delete the original downloaded hugging face model, only keep the transformed one to save half of the disk space.\nMacOS\n\nJust install airllm and run the code the same as on linux. See more in Quick Start.\n\nmake sure you installed mlx and torch\nyou probabaly need to install python native see more here\nonly Apple silicon is supported\nExample python notebook\n\nExample Python Notebook\n\nExample colabs here:\n\n Open In Colab\nSupported Models\n\nHF open llm leaderboard top models\n\nIncluding but not limited to the following: (Most of the open models are based on llama2, so should be supported by default)\n\n@12/01/23\n\nRank\tModel\tSupported\tModel Class\n1\tTigerResearch/tigerbot-70b-chat-v2\tâœ…\tAirLLMLlama2\n2\tupstage/SOLAR-0-70b-16bit\tâœ…\tAirLLMLlama2\n3\tICBU-NPU/FashionGPT-70B-V1.1\tâœ…\tAirLLMLlama2\n4\tsequelbox/StellarBright\tâœ…\tAirLLMLlama2\n5\tbhenrym14/platypus-yi-34b\tâœ…\tAirLLMLlama2\n6\tMayaPH/GodziLLa2-70B\tâœ…\tAirLLMLlama2\n7\t01-ai/Yi-34B\tâœ…\tAirLLMLlama2\n8\tgarage-bAInd/Platypus2-70B-instruct\tâœ…\tAirLLMLlama2\n9\tjondurbin/airoboros-l2-70b-2.2.1\tâœ…\tAirLLMLlama2\n10\tchargoddard/Yi-34B-Llama\tâœ…\tAirLLMLlama2\nï¼Ÿ\tmistralai/Mistral-7B-Instruct-v0.1\tâœ…\tAirLLMMistral\nï¼Ÿ\tmistralai/Mixtral-8x7B-v0.1\tâœ…\tAirLLMMixtral\nopencompass leaderboard top models\n\nIncluding but not limited to the following: (Most of the open models are based on llama2, so should be supported by default)\n\n@12/01/23\n\nRank\tModel\tSupported\tModel Class\n1\tGPT-4\tclosed.aiğŸ˜“\tN/A\n2\tTigerResearch/tigerbot-70b-chat-v2\tâœ…\tAirLLMLlama2\n3\tTHUDM/chatglm3-6b-base\tâœ…\tAirLLMChatGLM\n4\tQwen/Qwen-14B\tâœ…\tAirLLMQWen\n5\t01-ai/Yi-34B\tâœ…\tAirLLMLlama2\n6\tChatGPT\tclosed.aiğŸ˜“\tN/A\n7\tOrionStarAI/OrionStar-Yi-34B-Chat\tâœ…\tAirLLMLlama2\n8\tQwen/Qwen-14B-Chat\tâœ…\tAirLLMQWen\n9\tDuxiaoman-DI/XuanYuan-70B\tâœ…\tAirLLMLlama2\n10\tinternlm/internlm-20b\tâœ…\tAirLLMInternLM\n26\tbaichuan-inc/Baichuan2-13B-Chat\tâœ…\tAirLLMBaichuan\nexample of other models (ChatGLM, QWen, Baichuan, Mistral, etc):\n\nTo request other model support: here\n\nAcknowledgement\n\nA lot of the code are based on SimJeg's great work in the Kaggle exam competition. Big shoutout to SimJeg:\n\nGitHub account @SimJeg, the code on Kaggle, the associated discussion.\n\nFAQ\n\n1. MetadataIncompleteBuffer\n\nsafetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer\n\nIf you run into this error, most possible cause is you run out of disk space. The process of splitting model is very disk-consuming. See this. You may need to extend your disk space, clear huggingface .cache and rerun.\n\nå¦‚æœä½ ç¢°åˆ°è¿™ä¸ªerrorï¼Œå¾ˆæœ‰å¯èƒ½æ˜¯ç©ºé—´ä¸è¶³ã€‚å¯ä»¥å‚è€ƒä¸€ä¸‹è¿™ä¸ª å¯èƒ½éœ€è¦æ‰©å¤§ç¡¬ç›˜ç©ºé—´ï¼Œåˆ é™¤huggingfaceçš„.cacheï¼Œç„¶åé‡æ–°runã€‚\n\n2. ValueError: max() arg is an empty sequence\n\nMost likely you are loading QWen or ChatGLM model with Llama2 class. Try the following:\n\nFor QWen model:\n\nfrom airllm import AutoModel #<----- instead of AirLLMLlama2\nAutoModel.from_pretrained(...)\nFor ChatGLM model:\n\nfrom airllm import AutoModel #<----- instead of AirLLMLlama2\nAutoModel.from_pretrained(...)\n3. 401 Client Error....Repo model ... is gated.\n\nSome models are gated models, needs huggingface api token. You can provide hf_token:\n\nmodel = AutoModel.from_pretrained(\"meta-llama/Llama-2-7b-hf\", #hf_token='HF_API_TOKEN')\n4. ValueError: Asking to pad but the tokenizer does not have a padding token.\n\nSome model's tokenizer doesn't have padding token, so you can set a padding token or simply turn the padding config off:\n\ninput_tokens = model.tokenizer(input_text,\n   return_tensors=\"pt\", \n   return_attention_mask=False, \n   truncation=True, \n   max_length=MAX_LENGTH, \n   padding=False  #<-----------   turn off padding \n)\nCiting AirLLM\n\nIf you find AirLLM useful in your research and wish to cite it, please use the following BibTex entry:\n\n@software{airllm2023,\n  author = {Gavin Li},\n  title = {AirLLM: scaling large language models on low-end commodity computers},\n  url = {https://github.com/lyogavin/Anima/tree/main/air_llm},\n  version = {0.0},\n  year = {2023},\n}\nContribution\n\nWelcome contribution, ideas and discussions!\n\nIf you find it useful, please â­ or buy me a coffee! ğŸ™\n\n\"Buy Me A Coffee\"\n \n\nAnd a n example here:\nfrom airllm import AirLLMLlama2\n\nMAX_LENGTH = 128\n# could use hugging face model repo id:\nmodel = AirLLMLlama2(\"garage-bAInd/Platypus2-70B-instruct\")\n\n# or use model's local path...\n#model = AirLLMLlama2(\"/home/ubuntu/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/b585e74bcaae02e52665d9ac6d23f4d0dbc81a0f\")\n\ninput_text = [\n        'What is the capital of United States?',\n        #'I like',\n    ]\n\ninput_tokens = model.tokenizer(input_text,\n    return_tensors=\"pt\", \n    return_attention_mask=False, \n    truncation=True, \n    max_length=MAX_LENGTH, \n    padding=True)\n           \ngeneration_output = model.generate(\n    input_tokens['input_ids'].cuda(), \n    max_new_tokens=2,\n    use_cache=True,\n    return_dict_in_generate=True)\n\noutput = model.tokenizer.decode(generation_output.sequences[0])\n\nprint(output)\n\nThe problem is that it saves the model on my local drive how can i select an alternative source to download it?",
      "created_at": "2024-05-22T23:09:12.804583",
      "sequence_order": 0,
      "parent_id": "1e4a0740-fc31-48c6-8d4b-2f18f0016f8a",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "69535c53-5ef4-473d-a8fb-26982bbb2603",
      "role": "assistant",
      "content": "",
      "created_at": "2024-05-22T23:09:30.494330",
      "sequence_order": 1,
      "parent_id": "aaa26da8-a4e1-4d2c-ae41-e86cc2da16ce",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "bf69ad31-66ea-4928-8c20-fba986b78985",
      "role": "assistant",
      "content": "### Josephine (White Hat)\n\nTo select an alternative source for downloading the model using AirLLM, you need to specify the `layer_shards_saving_path` parameter when initializing the model. This will allow you to direct the saved layers to a different directory.\n\n### Claude (Green Hat)\n\nYou can achieve this by adjusting your code to specify a custom path for the `layer_shards_saving_path`. Hereâ€™s how you can modify your code to set an alternative path:\n\n```python\nfrom airllm import AirLLMLlama2\n\nMAX_LENGTH = 128\n# Define the custom path for saving the model layers\ncustom_path = \"/your/custom/path/for/model/layers\"\n\n# Initialize the model with the custom path\nmodel = AirLLMLlama2(\"garage-bAInd/Platypus2-70B-instruct\", layer_shards_saving_path=custom_path)\n\n# or use model's local path...\n# model = AirLLMLlama2(\"/home/ubuntu/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/b585e74bcaae02e52665d9ac6d23f4d0dbc81a0f\", layer_shards_saving_path=custom_path)\n\ninput_text = [\n        'What is the capital of United States?',\n        #'I like',\n    ]\n\ninput_tokens = model.tokenizer(input_text,\n    return_tensors=\"pt\", \n    return_attention_mask=False, \n    truncation=True, \n    max_length=MAX_LENGTH, \n    padding=True)\n           \ngeneration_output = model.generate(\n    input_tokens['input_ids'].cuda(), \n    max_new_tokens=20,\n    use_cache=True,\n    return_dict_in_generate=True)\n\noutput = model.tokenizer.decode(generation_output.sequences[0])\n\nprint(output)\n```\n\n### Prunella (Black Hat)\n\nEnsure that the custom path you specify has sufficient disk space to handle the model's layer-wise decomposition and saving. Also, verify that the path is accessible and has the necessary permissions.\n\n### Percy (Yellow Hat)\n\nUsing a custom path can help you manage your disk space better, especially if your primary storage is limited. It also allows you to organize your files more efficiently.\n\n### Cyril (Red Hat)\n\nSetting an alternative path might alleviate any anxiety about running out of disk space, ensuring that the process runs smoothly without interruptions.\n\n### Quackers (Blue Hat)\n\nBy synthesizing all perspectives, it's clear that specifying a custom path for saving the model layers is a practical and efficient solution to manage disk space and ensure a smooth inference process with AirLLM.\n\nIf you encounter any issues or need further customization, feel free to ask for more assistance!",
      "created_at": "2024-05-22T23:09:30.494666",
      "sequence_order": 2,
      "parent_id": "69535c53-5ef4-473d-a8fb-26982bbb2603",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": true
      }
    }
  ]
}