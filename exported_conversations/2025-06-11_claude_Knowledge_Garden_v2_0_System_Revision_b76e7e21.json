{
  "id": "b76e7e21-6d63-4a87-b6f7-2d36cc54ce1c",
  "source": "claude",
  "title": "Knowledge Garden v2.0 System Revision",
  "created_at": "2025-06-11T14:01:03.354297Z",
  "updated_at": "2025-06-11T14:10:04.024525Z",
  "summary": "",
  "message_count": 2,
  "messages": [
    {
      "id": "acf530ed-d5ce-4c61-a6a6-9352c4b65a18",
      "role": "human",
      "content": "review the strategy, the implementation and the improvement requests and return any files fixed in full. Identify any core issues and fix them and return the file in full",
      "created_at": "2025-06-11T14:04:09.809174Z",
      "sequence_order": 0,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2025-06-11T14:04:09.804933Z",
            "stop_timestamp": "2025-06-11T14:04:09.804933Z",
            "flags": null,
            "type": "text",
            "text": "review the strategy, the implementation and the improvement requests and return any files fixed in full. Identify any core issues and fix them and return the file in full",
            "citations": []
          }
        ],
        "updated_at": "2025-06-11T14:04:09.809174Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 16631,
            "file_type": "txt",
            "extracted_content": "Knowledge Garden v2.0 - Implementation Plan\nExecutive Summary\nKnowledge Garden v2.0 transforms the existing extraction system into a hybrid semantic-ontological knowledge cultivation platform. We'll leverage embeddings for discovery, LLMs for reasoning, and a Parliament decision engine for complex knowledge operations. The system maintains backward compatibility while adding sophisticated knowledge evolution capabilities.\n\nProduct Requirements Document (PRD)\nVision\nCreate a living knowledge system that distinguishes between personal insights, digested external knowledge, and peripheral references while enabling semantic discovery, automated artifact generation, and collaborative decision-making through an AI parliament.\nCore Principles\n\nHybrid Intelligence: Embeddings for discovery, LLMs for validation\nKnowledge Provenance: Clear attribution of source, confidence, and reasoning\nLiving Evolution: Knowledge can merge, update, and grow\nModular Architecture: Each component can be improved independently\nPractical Implementation: Build on existing code, avoid over-engineering\n\nKey Features\n1. Enhanced Extraction Pipeline\n\nReplace regex patterns with sentence-level LLM extraction\nGenerate and store embeddings for all extracted entities\nMaintain evidence chains with source quotes (1-3 sentences)\nEntity resolution using embedding similarity\n\n2. Three-Tier Knowledge Model\nPersonalKnowledge: Core insights from user conversations\nDigestedKnowledge: Accepted external content with validation\nPeripheralKnowledge: Citations, references, \"see also\" items\n3. Parliament Decision Engine\n\nSingle Claude prompt with extended thinking\nDynamically generates appropriate parliament members\nHandles: merge candidates, relationship validation, artifact design\nReturns structured decisions with confidence and reasoning\n\n4. Artifact Dependency System\n\nTrack artifact lineage (CSV → Chart → Slide)\nMark dependents as \"stale\" when sources update\nMaintain version history\nBundle related artifacts as meta-artifacts\n\n5. Embedding-Powered Features\n\nDetect merge candidates (similarity > 0.85)\nFind semantic neighbors for blindspot detection\nCluster related concepts for artifact suggestions\nEnable future spatial visualization\n\n\nTechnical Architecture\nSystem Components\npythonknowledge_garden/\n├── extraction/\n│   ├── sentence_extractor.py      # New: Sentence-level extraction\n│   ├── embedding_generator.py     # New: Jina embeddings\n│   ├── entity_resolver.py         # New: Similarity-based resolution\n│   └── evidence_tracker.py        # New: Source quote management\n├── parliament/\n│   ├── decision_engine.py         # New: Parliament coordinator\n│   ├── prompts/                   # New: Decision-specific prompts\n│   └── decision_registry.py       # New: Track all decisions\n├── knowledge/\n│   ├── three_tier_model.py        # New: Personal/Digested/Peripheral\n│   ├── merge_manager.py           # New: Handle entity merging\n│   └── provenance_tracker.py      # New: Source attribution\n├── artifacts/\n│   ├── dependency_graph.py        # New: Track artifact relationships\n│   ├── version_manager.py         # New: Handle stale/versions\n│   └── meta_artifacts.py          # New: Bundle related artifacts\n└── ontology/\n    ├── living_ontology.py         # New: Versioned schema\n    └── validators.py              # New: Constraint checking\nData Flow\n1. EXTRACTION FLOW\nRaw Text → Sentence Splitter → For each sentence:\n   ├─ Generate embedding (Jina)\n   ├─ Extract triples (Local LLM)\n   ├─ Store evidence quote\n   └─ Assign knowledge tier\n\n2. RESOLUTION FLOW  \nNew entities → Embedding similarity check → If similar:\n   ├─ Create merge candidate\n   ├─ Parliament decision (if complex)\n   └─ Update or create entity\n\n3. ARTIFACT FLOW\nContent + Request → Analyze semantic neighborhood →\n   ├─ Suggest artifact types\n   ├─ Generate base artifacts\n   ├─ Track dependencies\n   └─ Bundle if composite\n\nImplementation File List\nPhase 1: Core Extraction Enhancement\npython# knowledge_garden/extraction/sentence_extractor.py\nclass SentenceExtractor:\n    \"\"\"Extract knowledge from individual sentences using LLM\"\"\"\n    def __init__(self, llm_service, embedding_service):\n        self.llm = llm_service\n        self.embedder = embedding_service\n    \n    async def extract_sentence(self, sentence: str, context: Dict) -> ExtractedSentence:\n        # Generate embedding\n        embedding = await self.embedder.encode(sentence)\n        \n        # Extract triples\n        prompt = SENTENCE_EXTRACTION_PROMPT.format(\n            sentence=sentence,\n            context=context\n        )\n        response = await self.llm.generate(prompt)\n        triples = self._parse_triples(response)\n        \n        return ExtractedSentence(\n            text=sentence,\n            embedding=embedding,\n            triples=triples,\n            confidence=self._calculate_confidence(response)\n        )\n\n# knowledge_garden/extraction/embedding_generator.py  \nclass EmbeddingGenerator:\n    \"\"\"Generate and manage embeddings using Jina\"\"\"\n    def __init__(self, model_name='jinaai/jina-embeddings-v2-base-en'):\n        self.model = SentenceTransformer(model_name)\n        \n    async def encode(self, text: str) -> np.ndarray:\n        return self.model.encode(text, convert_to_numpy=True)\n    \n    def find_similar(self, embedding: np.ndarray, \n                    stored_embeddings: Dict[str, np.ndarray], \n                    threshold: float = 0.85) -> List[Tuple[str, float]]:\n        similarities = {}\n        for entity_id, stored_emb in stored_embeddings.items():\n            similarity = cosine_similarity(embedding, stored_emb)\n            if similarity > threshold:\n                similarities[entity_id] = similarity\n        return sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n\n# knowledge_garden/extraction/entity_resolver.py\nclass EntityResolver:\n    \"\"\"Resolve entities using embeddings and LLM validation\"\"\"\n    def __init__(self, embedding_generator, parliament_engine):\n        self.embedder = embedding_generator\n        self.parliament = parliament_engine\n        \n    async def resolve_entity(self, new_entity: Entity, \n                           existing_entities: List[Entity]) -> ResolutionResult:\n        # Find similar entities using embeddings\n        candidates = self.embedder.find_similar(\n            new_entity.embedding,\n            {e.id: e.embedding for e in existing_entities}\n        )\n        \n        if not candidates:\n            return ResolutionResult(action=\"create_new\", entity=new_entity)\n        \n        # Simple case: very high similarity\n        if candidates[0][1] > 0.95:\n            return ResolutionResult(\n                action=\"merge\",\n                target_id=candidates[0][0],\n                confidence=candidates[0][1]\n            )\n        \n        # Complex case: use Parliament\n        decision = await self.parliament.decide(\n            decision_type=\"entity_merge\",\n            context={\n                \"new_entity\": new_entity,\n                \"candidates\": candidates[:3]\n            }\n        )\n        \n        return decision\nPhase 2: Parliament Decision Engine\npython# knowledge_garden/parliament/decision_engine.py\nclass ParliamentEngine:\n    \"\"\"Coordinate parliament decisions using extended Claude thinking\"\"\"\n    \n    PARLIAMENT_SYSTEM_PROMPT = \"\"\"\n    You are a Parliament coordinator for a knowledge management system.\n    Your role is to convene appropriate experts and facilitate decisions.\n    \n    For the given decision type and context:\n    1. First, identify 3-5 relevant parliament members based on the decision\n    2. Have each member analyze the situation from their perspective\n    3. Facilitate a discussion between members\n    4. Synthesize a final decision with confidence and reasoning\n    \n    Use <parliament_thinking> tags for the deliberation process.\n    Return a final JSON decision outside the thinking tags.\n    \"\"\"\n    \n    async def decide(self, decision_type: str, context: Dict) -> Decision:\n        prompt = self._build_prompt(decision_type, context)\n        \n        response = await self.llm.generate(\n            prompt,\n            system=self.PARLIAMENT_SYSTEM_PROMPT,\n            temperature=0.3,\n            use_extended_thinking=True\n        )\n        \n        return self._parse_decision(response)\n    \n    def _build_prompt(self, decision_type: str, context: Dict) -> str:\n        templates = {\n            \"entity_merge\": \"\"\"\n            Decision needed: Should '{new_entity}' be merged with any of these candidates?\n            \n            Candidates:\n            {candidates}\n            \n            Consider: semantic similarity, context, precision needs\n            \"\"\",\n            \n            \"artifact_design\": \"\"\"\n            Content to transform: {content_summary}\n            \n            Potential artifacts: {artifact_options}\n            \n            Consider: content type, user intent, dependencies\n            \"\"\",\n            \n            \"relationship_validation\": \"\"\"\n            Proposed relationship: {subject} - {predicate} - {object}\n            \n            Existing knowledge context: {context}\n            \n            Consider: ontology constraints, semantic validity, confidence\n            \"\"\"\n        }\n        \n        return templates[decision_type].format(**context)\n\n# knowledge_garden/parliament/prompts/merge_decision.yaml\ndecision_type: entity_merge\nmembers:\n  - name: \"Precision Expert\"\n    focus: \"Exact naming and disambiguation\"\n  - name: \"Context Analyst\"  \n    focus: \"Usage patterns and semantic context\"\n  - name: \"Knowledge Curator\"\n    focus: \"Long-term knowledge graph health\"\noutput_schema:\n  action: \"merge|keep_separate|request_user\"\n  confidence: float\n  reasoning: str\n  evidence: List[str]\nPhase 3: Knowledge Tier Management\npython# knowledge_garden/knowledge/three_tier_model.py\nclass KnowledgeTier(Enum):\n    PERSONAL = \"personal\"      # From user conversations\n    DIGESTED = \"digested\"      # Validated external\n    PERIPHERAL = \"peripheral\"  # References, citations\n\nclass TieredKnowledgeManager:\n    \"\"\"Manage three-tier knowledge model\"\"\"\n    \n    def classify_source(self, source: Source) -> KnowledgeTier:\n        if source.type == \"conversation\" and source.participant == \"user\":\n            return KnowledgeTier.PERSONAL\n        elif source.type == \"external\" and source.validated:\n            return KnowledgeTier.DIGESTED\n        else:\n            return KnowledgeTier.PERIPHERAL\n    \n    def add_triple(self, triple: Triple, source: Source, evidence: Evidence):\n        tier = self.classify_source(source)\n        \n        # Add tier-specific metadata\n        triple.metadata.update({\n            \"tier\": tier.value,\n            \"source_uri\": source.uri,\n            \"evidence_quote\": evidence.quote[:200],  # Max 1-3 sentences\n            \"extraction_confidence\": evidence.confidence,\n            \"extracted_at\": datetime.now()\n        })\n        \n        # Store in appropriate graph\n        graph_uri = self._get_graph_uri(tier, source.user_id)\n        self.fuseki.insert_triple(triple, graph_uri)\nPhase 4: Artifact Dependencies\npython# knowledge_garden/artifacts/dependency_graph.py\nclass ArtifactDependencyGraph:\n    \"\"\"Track artifact relationships and staleness\"\"\"\n    \n    def __init__(self):\n        self.graph = nx.DiGraph()\n    \n    def add_artifact(self, artifact: Artifact, dependencies: List[str]):\n        self.graph.add_node(artifact.id, artifact=artifact)\n        \n        for dep_id in dependencies:\n            self.graph.add_edge(dep_id, artifact.id)\n    \n    def mark_stale(self, artifact_id: str):\n        \"\"\"Mark artifact and all dependents as stale\"\"\"\n        stale_artifacts = [artifact_id]\n        \n        # Find all descendants\n        descendants = nx.descendants(self.graph, artifact_id)\n        stale_artifacts.extend(descendants)\n        \n        # Update metadata\n        for node_id in stale_artifacts:\n            self.graph.nodes[node_id]['stale'] = True\n            self.graph.nodes[node_id]['stale_since'] = datetime.now()\n        \n        return stale_artifacts\n    \n    def get_regeneration_order(self, artifact_id: str) -> List[str]:\n        \"\"\"Get topological order for regenerating stale artifacts\"\"\"\n        subgraph = self.graph.subgraph(\n            nx.ancestors(self.graph, artifact_id) | {artifact_id}\n        )\n        return list(nx.topological_sort(subgraph))\n\n# knowledge_garden/artifacts/meta_artifacts.py\nclass MetaArtifact:\n    \"\"\"Bundle related artifacts with metadata\"\"\"\n    \n    def __init__(self, artifact_type: str, title: str):\n        self.id = f\"meta_{uuid.uuid4().hex[:8]}\"\n        self.type = artifact_type\n        self.title = title\n        self.artifacts = []\n        self.summary = None\n        \n    def add_artifact(self, artifact: Artifact, role: str):\n        self.artifacts.append({\n            \"artifact_id\": artifact.id,\n            \"role\": role,  # e.g., \"data\", \"visualization\", \"slide\"\n            \"added_at\": datetime.now()\n        })\n    \n    def generate_summary(self, llm_service) -> str:\n        \"\"\"Generate a summary of the bundled artifacts\"\"\"\n        prompt = f\"\"\"\n        Summarize this collection of artifacts:\n        \n        Title: {self.title}\n        Type: {self.type}\n        \n        Artifacts:\n        {json.dumps(self.artifacts, indent=2)}\n        \n        Create a 2-3 sentence summary of what this collection represents.\n        \"\"\"\n        \n        self.summary = llm_service.generate(prompt)\n        return self.summary\nPhase 5: Configuration Files\nyaml# knowledge_garden/config/extraction.yaml\nextraction:\n  sentence_model: \"jinaai/jina-embeddings-v2-base-en\"\n  embedding_dimension: 768\n  batch_size: 64\n  \n  similarity_thresholds:\n    exact_match: 0.95\n    merge_candidate: 0.85\n    potentially_related: 0.70\n    \n  evidence:\n    max_quote_length: 200  # characters\n    context_window: 1      # sentences before/after\n\n# knowledge_garden/config/parliament.yaml  \nparliament:\n  llm_model: \"claude-3-sonnet\"\n  temperature: 0.3\n  use_extended_thinking: true\n  \n  decision_types:\n    - entity_merge\n    - relationship_validation\n    - artifact_selection\n    - knowledge_tier_assignment\n    \n  complexity_triggers:\n    confidence_below: 0.7\n    multiple_candidates: true\n    cross_tier_merge: true\n\n# knowledge_garden/config/artifacts.yaml\nartifacts:\n  storage_path: \"data/artifacts\"\n  \n  dependency_tracking:\n    enabled: true\n    auto_mark_stale: true\n    version_retention: 5\n    \n  meta_artifacts:\n    auto_bundle_threshold: 3\n    generate_summaries: true\n\nMigration Strategy\nPhase 1 (Week 1-2): Extraction Enhancement\n\nImplement SentenceExtractor alongside existing PatternExtractor\nAdd EmbeddingGenerator using Jina\nStore embeddings in knowledge graph as kg:embedding property\nTest on sample conversations\n\nPhase 2 (Week 3-4): Parliament Integration\n\nBuild ParliamentEngine with basic decision types\nCreate prompt templates for each decision\nIntegrate with entity resolution\nAdd decision logging to knowledge graph\n\nPhase 3 (Week 5-6): Knowledge Tiers\n\nImplement three-tier classification\nUpdate Fuseki queries to filter by tier\nAdd source attribution to all triples\nCreate tier-specific visualizations\n\nPhase 4 (Week 7-8): Artifact Evolution\n\nBuild dependency graph system\nImplement staleness tracking\nAdd version management\nCreate meta-artifact bundling\n\n\nKey Implementation Notes\n\nBackward Compatibility: All new features extend existing ones. Old data remains valid.\nEmbedding Storage: Store as base64-encoded strings in RDF:\nturtle:entity123 kg:embedding \"W0FiY2RlZi4uLg==...\"^^xsd:base64Binary .\n\nParliament Decisions: Store in knowledge graph for auditability:\nturtle:decision456 a kg:ParliamentDecision ;\n    kg:decisionType \"entity_merge\" ;\n    kg:confidence 0.85 ;\n    kg:reasoning \"High semantic similarity...\" .\n\nEvidence Quotes: Extract 1-3 complete sentences, store with pointer:\nturtle:triple789 kg:evidence [\n    kg:quote \"The user stated that neural networks...\" ;\n    kg:sourceDocument :doc123 ;\n    kg:characterOffset 1524 ;\n] .\n\nStale Artifacts: Use RDF properties for tracking:\nturtle:artifact999 kg:stale true ;\n    kg:staleSince \"2024-01-15T10:30:00\"^^xsd:dateTime ;\n    kg:staleReason :artifact888 .\n\n\nThis design balances ambition with practicality, building on your existing codebase while adding the key innovations from our discussion. The modular approach means each component can be developed and tested independently."
          },
          {
            "file_name": "paste-2.txt",
            "file_size": 13850,
            "file_type": "txt",
            "extracted_content": "# Knowledge Garden Extraction System: Improvement Recommendations\n\n## 1. Prompt Design Improvements (Current Score: 2.8/5)\n\nThe LLM prompts are currently the weakest part of the system and need significant enhancement:\n\n### Current Issues\n- Lack of contextual information about the knowledge garden system\n- Missing few-shot examples to guide the model\n- Insufficient error handling guidance\n- Limited reasoning guidance for complex decisions\n- Basic output format specifications\n\n### Specific Improvements\n1. **Standardize Prompt Structure** - All prompts should follow a consistent format:\n   ```yaml\n   # Header section with metadata\n   name: entity_resolution_prompt\n   version: 1.0\n   description: Resolves whether two entities refer to the same underlying concept\n   \n   # System context section\n   system_context: |\n     You are a knowledge graph entity resolution expert for the Knowledge Garden system.\n     Knowledge Garden extracts entities from conversations and builds a connected knowledge graph.\n     Your task is to determine if two entity mentions refer to the same underlying concept.\n     \n   # Input schema section\n   input_schema:\n     entity1:\n       label: str  # Entity 1 text label\n       type: str   # Entity 1 type classification\n       context: str  # Context where Entity 1 was found\n     entity2:\n       label: str  # Entity 2 text label\n       type: str   # Entity 2 type classification\n       context: str  # Context where Entity 2 was found\n     similarity_score: float  # Vector similarity between entities (0-1)\n   \n   # Examples section\n   examples:\n     - input:\n         entity1:\n           label: \"Claude AI\"\n           type: \"Product\"\n           context: \"Users can access Claude AI through the website.\"\n         entity2:\n           label: \"Claude\"\n           type: \"Product\"\n           context: \"Many prefer Claude over other AI assistants.\"\n         similarity_score: 0.87\n       output:\n         decision: \"merge\"\n         confidence: 0.95\n         reasoning: |\n           Both entities refer to the same AI product. \"Claude AI\" and \"Claude\" \n           are the same product with a slight naming variation. The high similarity\n           score and matching type support this conclusion.\n     \n     - input:\n         entity1:\n           label: \"Python\"\n           type: \"Programming_Language\"\n           context: \"Python is easy to learn.\"\n         entity2:\n           label: \"Python\"\n           type: \"Animal\"\n           context: \"The python coiled around the branch.\"\n         similarity_score: 0.92\n       output:\n         decision: \"keep_separate\"\n         confidence: 0.98\n         reasoning: |\n           Despite identical labels and high similarity score, these are completely\n           different concepts. One is a programming language, the other is a snake.\n           The context makes it clear these should not be merged.\n   \n   # Reasoning guidance section\n   reasoning_guidance: |\n     Follow these steps to reach a conclusion:\n     1. Compare entity labels - Are they identical, similar, or different?\n     2. Compare entity types - Do they belong to compatible categories?\n     3. Examine context - Do the contexts suggest the same or different concepts?\n     4. Consider similarity score - Scores above 0.85 suggest potential matches\n     5. Check for common abbreviations or alternative names\n     6. Identify any contradictory information that would prevent merging\n     7. Determine confidence in your decision based on available evidence\n   \n   # Error handling section\n   error_handling: |\n     If input is incomplete:\n     - Missing labels: Return decision=\"insufficient_data\"\n     - Missing types: Proceed but reduce confidence by 0.2\n     - Missing context: Proceed but reduce confidence by 0.1\n     - Invalid similarity score: Use 0.5 as default and note in reasoning\n     \n     If inputs are contradictory or ambiguous:\n     - When in doubt, choose \"keep_separate\" with appropriate confidence\n     - If information suggests both possibilities equally, return confidence=0.5\n   \n   # Output schema section\n   output_schema:\n     decision:\n       type: enum\n       values: [\"merge\", \"keep_separate\", \"insufficient_data\"]\n       description: Whether to merge the entities or keep them separate\n     confidence:\n       type: float\n       range: [0.0, 1.0]\n       description: Confidence in the decision (0-1)\n     reasoning:\n       type: str\n       description: Step-by-step reasoning for the decision\n   \n   # Prompt template\n   template: |\n     <task>\n     You are a knowledge graph entity resolution expert. Determine if these two entities\n     refer to the same underlying concept and should be merged or kept separate.\n     </task>\n     \n     <context>\n     Entity 1: \"{{entity1.label}}\" (Type: {{entity1.type}})\n     Context: {{entity1.context}}\n     \n     Entity 2: \"{{entity2.label}}\" (Type: {{entity2.type}})\n     Context: {{entity2.context}}\n     \n     Vector Similarity Score: {{similarity_score}}\n     </context>\n     \n     <instructions>\n     Analyze both entities and determine if they refer to the same concept.\n     Follow this reasoning process:\n     1. Compare entity labels\n     2. Compare entity types\n     3. Examine contexts\n     4. Consider the similarity score\n     5. Check for abbreviations or alternative names\n     6. Identify any contradictions\n     7. Make a decision with confidence\n     \n     Return your analysis in this JSON format:\n     {\n       \"decision\": \"merge\" or \"keep_separate\",\n       \"confidence\": 0.0-1.0,\n       \"reasoning\": \"Your step-by-step reasoning\"\n     }\n     </instructions>\n   ```\n\n2. **Add Detailed System Context** - Each prompt should include comprehensive context:\n   - Explain the role of the specific extractor in the Knowledge Garden system\n   - Define key terminology used in the system\n   - Explain the expected input and output format\n   - Describe how the output will be used in downstream processing\n\n3. **Provide Rich Few-Shot Examples** - Include 3-5 diverse examples for each prompt:\n   - Cover both common and edge cases\n   - Show expected reasoning process\n   - Include examples with different confidence levels\n   - Demonstrate proper error handling\n\n4. **Add Explicit Reasoning Steps** - Guide the model's reasoning process:\n   - Break down complex decisions into clear steps\n   - Specify decision criteria for each step\n   - Encourage explicit weighing of evidence\n   - Provide framework for confidence scoring\n\n5. **Enhance Output Format Specifications** - Use clear JSON schemas:\n   - Define all fields with types and descriptions\n   - Provide validation criteria for outputs\n   - Support confidence scores for each extracted element\n   - Include fields for reasoning and provenance\n\n6. **Improve Error Handling Guidance** - Add robust error handling:\n   - Define fallback procedures for incomplete or contradictory inputs\n   - Specify confidence reduction factors for uncertain cases\n   - Include strategies for dealing with ambiguous language\n   - Provide guidelines for handling unexpected input formats\n\n## 2. Testing Infrastructure Improvements (Current Score: 1.3/5)\n\nThe system lacks proper testing infrastructure:\n\n### Current Issues\n- Missing unit tests for core components\n- No integration tests for component interactions\n- Lack of test data for extraction validation\n- No continuous integration setup\n- Missing performance benchmarks\n\n### Specific Improvements\n\n1. **Create Comprehensive Unit Test Suite**\n   - Implement unit tests for each module (>80% coverage)\n   - Test all public methods with various input scenarios\n   - Include edge cases and error conditions\n   - Add the following files:\n     - `tests/extraction/test_embedding_generator.py`\n     - `tests/extraction/test_entity_resolver.py` \n     - `tests/extraction/test_evidence_tracker.py`\n     - `tests/extraction/test_sentence_extractor.py`\n     - `tests/extraction/test_pipeline.py`\n     - `tests/extraction/test_llm_extractor.py`\n\n2. **Develop Integration Tests**\n   - Test interactions between components\n   - Create end-to-end workflow tests\n   - Add mock services for external dependencies\n   - Add the following files:\n     - `tests/integration/test_extraction_workflow.py`\n     - `tests/integration/test_knowledge_tiers.py`\n     - `tests/integration/test_artifact_generation.py`\n\n3. **Create Test Dataset**\n   - Develop a standard dataset with known patterns\n   - Include diverse conversation types\n   - Provide expected extraction outputs\n   - Support both normal and edge cases\n   - Add to `tests/data/test_conversations.json`\n\n4. **Setup Continuous Integration**\n   - Create GitHub Actions workflow\n   - Run tests on pull requests and merges\n   - Track code coverage over time\n   - Add to `.github/workflows/test.yml`\n\n5. **Add Performance Testing**\n   - Benchmark extraction components\n   - Track memory usage and execution time\n   - Compare against baselines\n   - Test with various dataset sizes\n   - Add to `tests/performance/benchmarks.py`\n\n6. **Implement Test Fixtures**\n   - Create reusable test fixtures\n   - Mock external dependencies (Fuseki, Anthropic)\n   - Generate synthetic test data\n   - Add to `tests/conftest.py`\n\n## 3. Configuration System Improvements (Current Score: 3.3/5)\n\n### Current Issues\n- Limited validation for configuration parameters\n- Insufficient documentation of configuration options\n- No schema for configuration validation\n\n### Specific Improvements\n\n1. **Add Schema Validation**\n   - Create JSON schemas for all configuration files\n   - Validate configuration at startup\n   - Provide descriptive error messages\n   - Add to `knowledge_garden/core/config_validator.py`\n\n2. **Enhance Configuration Documentation**\n   - Add detailed descriptions for each parameter\n   - Explain impact of configuration changes\n   - Include recommended values\n   - Add to `knowledge_garden/config/README.md`\n\n3. **Add Configuration Examples**\n   - Provide sample configurations for common scenarios\n   - Include production and development setups\n   - Document common adjustments\n   - Add to `knowledge_garden/config/examples/`\n\n4. **Implement Default Configuration**\n   - Provide sensible defaults for all parameters\n   - Allow partial configuration overrides\n   - Add hierarchical configuration loading\n   - Add to `knowledge_garden/core/config.py`\n\n## 4. Feature and Functionality Improvements\n\n1. **Enhance Pattern Extraction**\n   - Add more pattern types beyond CAPs, questions, insights, and tensions\n   - Improve pattern detection accuracy with more sophisticated regex\n   - Add pattern validation and confidence scoring\n   - Implement in `knowledge_garden/extraction/pattern_extractor.py`\n\n2. **Improve Entity Resolution**\n   - Add attribute-based matching for entities\n   - Implement context-aware resolution strategies\n   - Add hierarchical entity relationships\n   - Implement in `knowledge_garden/extraction/entity_resolver.py`\n\n3. **Add Cross-Document Coreference Resolution**\n   - Resolve entity references across different conversations\n   - Track entity evolution over time\n   - Merge entity profiles as more information is discovered\n   - Add to `knowledge_garden/extraction/coreference_resolver.py`\n\n4. **Implement Knowledge Graph Enrichment**\n   - Add automatic relationship inference\n   - Include fact verification from external sources\n   - Support knowledge graph completion\n   - Add to `knowledge_garden/knowledge/graph_enrichment.py`\n\n## 5. UI and Visualization Improvements\n\n1. **Create Interactive Knowledge Graph Viewer**\n   - Implement D3.js-based interactive visualization\n   - Support filtering and focusing on subgraphs\n   - Enable exploration of entity relationships\n   - Add to `knowledge_garden/ui/graph_viewer.py`\n\n2. **Add Knowledge Dashboard**\n   - Create summary visualizations of extracted knowledge\n   - Track knowledge growth over time\n   - Show statistics by entity and relationship types\n   - Add to `knowledge_garden/ui/dashboard.py`\n\n3. **Implement Search Interface**\n   - Add full-text search for knowledge elements\n   - Support faceted search by entity type and attributes\n   - Enable relationship-based querying\n   - Add to `knowledge_garden/ui/search_interface.py`\n\n## 6. Performance and Scalability Improvements\n\n1. **Optimize Embedding Generation**\n   - Implement batch processing for embeddings\n   - Add caching for frequently accessed embeddings\n   - Support model quantization for faster inference\n   - Improve in `knowledge_garden/extraction/embedding_generator.py`\n\n2. **Enhance Database Performance**\n   - Add indexing for common query patterns\n   - Implement connection pooling for Fuseki\n   - Support asynchronous batch operations\n   - Improve in `knowledge_garden/core/fuseki_client.py`\n\n3. **Add Parallel Processing**\n   - Implement multiprocessing for extraction\n   - Add worker pool for conversation processing\n   - Support distributed extraction for large datasets\n   - Add to `knowledge_garden/infrastructure/parallelization.py`\n\n## Implementation Priority\n\n### Immediate Priorities (Next 2 Weeks)\n1. Improve prompts following template above\n2. Add basic unit tests for critical components\n3. Implement configuration validation\n\n### Medium-Term (2-6 Weeks)\n1. Develop comprehensive test suite\n2. Build standard test dataset\n3. Add interactive knowledge graph visualization\n4. Enhance pattern extraction capabilities\n\n### Long-Term (6+ Weeks)\n1. Implement cross-document coreference resolution\n2. Add knowledge graph enrichment\n3. Create full search and dashboard interface\n4. Optimize for performance and scale\n\n## Conclusion\n\nBy systematically addressing these improvements, we can significantly enhance the Knowledge Garden system's extraction quality, reliability, and usability. The most critical areas to focus on first are the prompt designs and testing infrastructure, as these directly impact extraction quality and system reliability."
          },
          {
            "file_name": "knowledge_garden_improvement_code.py",
            "file_size": 276110,
            "file_type": "text/x-python-script",
            "extracted_content": "# Knowledge Garden Extraction - Improvement-Focused Code Export\n# Generated: 2025-06-11 16:02:21\n# Project: Knowledge Garden Extraction\n# Files: 28\n\n# Table of Contents by Improvement Area\n\n\n## Configuration System\n# 1. config/artifacts.yaml - Artifact Configuration for Knowledge Garden v2.0\n# 2. config/extraction.yaml - Extraction configuration\n# 3. config/knowledge_tiers.yaml - Knowledge Tier Management Configuration for Knowledge Garden v2.0\n# 4. config/parliament.yaml - Parliament Decision Engine Configuration for Knowledge Garden v2.0\n# 5. core/config.py - Central configuration for Knowledge Garden\n# 6. core/config_validator.py - Configuration validation module for Knowledge Garden v2.0\n\n## Extraction Pipeline\n# 7. extraction/embedding_generator.py - Embedding generation and similarity search for knowledge garden\n# 8. extraction/entity_resolver.py - Entity resolution using embeddings and LLM validation\n# 9. extraction/evidence_tracker.py - Evidence tracking for knowledge provenance\n# 10. extraction/pattern_extractor.py - Pattern Extractor for extracting CAPs, questions, insights, and tensions\n# 11. extraction/pipeline.py - Knowledge extraction pipeline for processing text data\n# 12. extraction/sentence_extractor.py - Sentence-level extraction with LLM for knowledge triples\n# 13. process_conversations.py - Process conversations file for knowledge extraction\n\n## Models\n# 14. models/evidence.py - Evidence model for tracking provenance of extracted knowledge\n# 15. models/extracted_sentence.py - Model for extracted sentences with embeddings\n# 16. models/triple.py - Triple data model for RDF\n\n## Prompt Design\n# 17. extraction/llm_extractor.py - LLM-based Extractor for enhanced extraction\n# 18. extraction/prompt_manager.py - Prompt management for LLM-based extraction\n# 19. extraction/prompts/__init__.py - Prompt templates for LLM-based extraction\n# 20. extraction/prompts/entity_resolution.yaml - YAML configuration file\n# 21. extraction/prompts/sentence_extraction.yaml - YAML configuration file\n# 22. parliament/prompts/entity_merge.yaml - YAML configuration file\n# 23. parliament/prompts/knowledge_tier.yaml - YAML configuration file\n# 24. prompt_examples.md - Knowledge Garden v2.0 Prompt Assessment\n\n## Testing Infrastructure\n# 25. assessment.py - Assessment script for Knowledge Garden v2.0\n# 26. prompt_assessment.py - Prompt Quality Assessment for Knowledge Garden v2.0\n# 27. run_assessment.py - Simple assessment script for Knowledge Garden extraction without dependencies\n# 28. test_extraction.py - Test script for Knowledge Garden extraction\n\n\n\n\n#================================================================================\n# IMPROVEMENT AREA: Configuration System\n#================================================================================\n\n\n#=============================================================================\n# File: config/artifacts.yaml\n# Description: Artifact Configuration for Knowledge Garden v2.0\n# Improvement Area: Configuration System\n#=============================================================================\n# Artifact Configuration for Knowledge Garden v2.0\n\n# Dependency Graph settings\ndependency_graph:\n  # Enable persistent storage of dependency graph in Fuseki\n  persistent: true\n  \n  # Auto-update dependencies when artifacts are modified\n  auto_update: true\n  \n  # Maximum depth for dependency traversal (prevents excessive cascading)\n  max_depth: 10\n  \n  # Dependency types to include in graph\n  include_dependency_types:\n    - data\n    - knowledge\n    - visual\n    - computational\n  \n  # Graph visualization settings\n  visualization:\n    enabled: true\n    output_format: \"svg\"\n    layout: \"dot\"  # dot, neato, fdp, sfdp, twopi, circo\n    node_size: 10\n    edge_width: 1.5\n    include_labels: true\n\n# Version Manager settings\nversion_manager:\n  # Enable versioning for artifacts\n  enabled: true\n  \n  # Keep specific number of versions in history (0 = unlimited)\n  keep_versions: 10\n  \n  # Enable automatic versioning on changes\n  auto_version: true\n  \n  # Minimum change threshold for creating a new version (percentage)\n  # Only applicable for content-based artifacts\n  min_change_threshold: 5\n  \n  # Compression for version storage\n  compress_versions: true\n  \n  # Content diff algorithm for comparing versions\n  diff_algorithm: \"unified\"  # unified, context, ndiff, html\n  \n  # Include metadata in version history\n  include_metadata: true\n\n# Meta-Artifact settings\nmeta_artifacts:\n  # Maximum number of artifacts in a single meta-artifact\n  max_artifacts: 100\n  \n  # Enable relationship inference between artifacts\n  infer_relationships: true\n  \n  # Relationship inference confidence threshold (0-100)\n  relationship_confidence: 75\n  \n  # Default types to create for collections\n  default_types:\n    - collection\n    - report\n    - framework\n  \n  # Auto-organization settings\n  auto_organize:\n    enabled: true\n    min_similarity: 0.8\n    suggestion_threshold: 3  # Minimum artifacts to suggest organization\n    max_suggestions: 5       # Maximum suggestions to generate\n\n# Artifact Storage settings\nstorage:\n  # Base path for artifact storage\n  base_path: \"data/artifacts\"\n  \n  # Directory structure by artifact type\n  directories:\n    summaries: \"summaries\"\n    diagrams: \"diagrams\"\n    frameworks: \"frameworks\"\n    reports: \"reports\"\n    presentations: \"presentations\"\n    insights: \"insights\"\n    collections: \"collections\"\n    meta: \"meta\"\n  \n  # File naming convention\n  file_naming: \"{id}_{type}_{timestamp}\"\n  \n  # File formats for different artifact types\n  formats:\n    summary: \"md\"\n    diagram: \"svg\"\n    framework: \"json\"\n    report: \"pdf\"\n    presentation: \"pptx\"\n    insight: \"md\"\n    collection: \"json\"\n    meta: \"json\"\n\n# Staleness settings\nstaleness:\n  # Enable staleness tracking for artifacts\n  enabled: true\n  \n  # Check frequency (in hours)\n  check_frequency: 24\n  \n  # Auto-refresh stale artifacts\n  auto_refresh: false\n  \n  # Notification settings for stale artifacts\n  notifications:\n    enabled: true\n    channels:\n      - email\n      - system\n    threshold: 72  # Hours before notification\n    \n  # Prioritization for regeneration\n  prioritization:\n    strategy: \"impact\"  # impact, age, usage\n    max_concurrent: 5\n\n# Artifact export settings\nexport:\n  # Enable artifact export functionality\n  enabled: true\n  \n  # Available export formats\n  formats:\n    - json\n    - pdf\n    - md\n    - html\n    - pptx\n  \n  # Export naming convention\n  naming: \"{title}_{date}\"\n  \n  # Include metadata in exports\n  include_metadata: true\n  \n  # Include provenance information\n  include_provenance: true\n  \n  # Enable batch export\n  batch_export: true\n\n# Integration settings\nintegrations:\n  # Google Workspace integration\n  google:\n    enabled: false\n    docs_enabled: true\n    slides_enabled: true\n    sheets_enabled: false\n    \n  # Notion integration\n  notion:\n    enabled: false\n    database_sync: true\n    \n  # Miro integration\n  miro:\n    enabled: false\n    \n  # Local filesystem integration\n  filesystem:\n    enabled: true\n    watch_directories: true\n    \n  # GitHub integration\n  github:\n    enabled: false\n    repo_sync: false\n\n\n#=============================================================================\n# File: config/extraction.yaml\n# Description: Extraction configuration\n# Improvement Area: Configuration System\n#=============================================================================\n# Extraction configuration\n\n# Embedding settings\nembedding:\n  model: \"jinaai/jina-embeddings-v2-base-en\"\n  dimension: 768\n  batch_size: 64\n  device: \"auto\"  # \"auto\", \"cpu\", \"cuda\"\n  cache_dir: \"data/embeddings\"\n\n# Similarity thresholds\nsimilarity:\n  exact_match: 0.95\n  merge_candidate: 0.85\n  potentially_related: 0.70\n\n# Evidence settings\nevidence:\n  max_quote_length: 200  # characters\n  context_window: 1      # sentences before/after\n  max_batch_size: 100    # maximum number of evidence records to store in one batch\n\n# Sentence extraction\nsentence_extraction:\n  min_sentence_length: 5\n  max_sentence_length: 500\n  context_window: 1\n  batch_size: 10\n  \n  # Extraction modes:\n  # - \"all\": Extract from all sentences\n  # - \"filtered\": Only extract from sentences that pass relevance filter\n  # - \"sampled\": Extract from random sample of sentences\n  mode: \"filtered\"\n  \n  # Filter settings (for \"filtered\" mode)\n  relevance_threshold: 0.5\n  entity_density_threshold: 2  # min entities per sentence\n  \n  # Sampling settings (for \"sampled\" mode)\n  sample_rate: 0.2\n  min_samples: 5\n  \n# Entity resolution\nentity_resolution:\n  auto_merge_threshold: 0.95  # Confidence above which to automatically merge\n  parliament_threshold: 0.7   # Confidence below which to use parliament\n  \n  # When true, complex cases go to parliament\n  use_parliament_for_complex: true\n  \n  # Complexity indicators\n  complexity_indicators:\n    multiple_candidates: true\n    conflicting_types: true\n    ambiguous_attributes: true\n    cross_tier_merge: true\n\n# LLM settings\nllm:\n  default_model: \"claude-3-haiku-20240307\"\n  timeout: 30  # seconds\n  max_retries: 3\n  \n  # Models to use for different tasks\n  sentence_extraction_model: \"claude-3-haiku-20240307\"\n  entity_resolution_model: \"claude-3-haiku-20240307\"\n  complex_resolution_model: \"claude-3-sonnet-20240229\"\n\n\n#=============================================================================\n# File: config/knowledge_tiers.yaml\n# Description: Knowledge Tier Management Configuration for Knowledge Garden v2.0\n# Improvement Area: Configuration System\n#=============================================================================\n# Knowledge Tier Management Configuration for Knowledge Garden v2.0\n\n# Core tier settings\ncore:\n  # Enable tiered knowledge management\n  enabled: true\n  \n  # Default tier for new knowledge\n  default_tier: \"personal\"\n  \n  # Automatic tier promotion\n  auto_promote: true\n  \n  # Minimum evidence required for tier promotion\n  min_evidence: 2\n  \n  # Time-based tier aging (demote if not accessed)\n  tier_aging:\n    enabled: false\n    personal_to_peripheral_days: 90\n    digested_to_personal_days: 180\n  \n  # Notification on tier changes\n  notify_tier_changes: true\n\n# Tier definitions\ntiers:\n  # Personal tier - From user conversations\n  personal:\n    name: \"Personal\"\n    description: \"Knowledge from user conversations and personal insights\"\n    confidence_threshold: 0.6\n    evidence_required: 1\n    visibility: \"private\"\n    color: \"#7CB9E8\"  # Blue\n    \n    # Personal tier specific settings\n    settings:\n      allow_contradictions: true\n      private_by_default: true\n      require_source: false\n      allow_incomplete: true\n  \n  # Digested tier - Validated external knowledge\n  digested:\n    name: \"Digested\"\n    description: \"Validated external knowledge integrated into the knowledge base\"\n    confidence_threshold: 0.8\n    evidence_required: 2\n    visibility: \"shared\"\n    color: \"#50C878\"  # Green\n    \n    # Digested tier specific settings\n    settings:\n      allow_contradictions: false\n      private_by_default: false\n      require_source: true\n      allow_incomplete: false\n      verification_required: true\n  \n  # Peripheral tier - References, citations\n  peripheral:\n    name: \"Peripheral\"\n    description: \"Referenced or cited knowledge, not fully integrated\"\n    confidence_threshold: 0.5\n    evidence_required: 1\n    visibility: \"referenced\"\n    color: \"#D3D3D3\"  # Light gray\n    \n    # Peripheral tier specific settings\n    settings:\n      allow_contradictions: true\n      private_by_default: false\n      require_source: true\n      allow_incomplete: true\n      auto_prune: true\n      prune_after_days: 120\n\n# Promotion rules\npromotion:\n  # Personal to Digested promotion\n  personal_to_digested:\n    enabled: true\n    \n    # Criteria for promotion\n    criteria:\n      min_confidence: 0.8\n      min_evidence: 3\n      min_references: 2\n      min_usage_count: 5\n      parliament_approval: true\n    \n    # Automatic promotion check frequency\n    check_frequency_days: 14\n    \n    # Evidence weighting factors\n    evidence_weights:\n      user_confirmation: 2.0\n      external_validation: 1.5\n      consistent_extraction: 1.0\n      high_confidence: 0.8\n  \n  # Peripheral to Personal promotion\n  peripheral_to_personal:\n    enabled: true\n    \n    # Criteria for promotion\n    criteria:\n      min_confidence: 0.7\n      min_references: 1\n      min_usage_count: 3\n      parliament_approval: false\n    \n    # Automatic promotion check frequency\n    check_frequency_days: 7\n    \n    # Evidence weighting factors\n    evidence_weights:\n      user_reference: 1.5\n      consistent_appearance: 1.0\n      related_to_personal: 0.8\n\n# Demotion rules\ndemotion:\n  # Digested to Personal demotion\n  digested_to_personal:\n    enabled: true\n    \n    # Criteria for demotion\n    criteria:\n      confidence_below: 0.7\n      contradictions_found: true\n      evidence_invalidated: true\n      parliament_decision: true\n    \n    # Automatic demotion check frequency\n    check_frequency_days: 30\n  \n  # Personal to Peripheral demotion\n  personal_to_peripheral:\n    enabled: true\n    \n    # Criteria for demotion\n    criteria:\n      confidence_below: 0.5\n      usage_count_below: 2\n      days_since_last_use: 90\n      parliament_decision: false\n    \n    # Automatic demotion check frequency\n    check_frequency_days: 30\n\n# Merge management\nmerging:\n  # Enable entity merging\n  enabled: true\n  \n  # Default merge behavior\n  default_behavior: \"conservative\"  # conservative, aggressive, balanced\n  \n  # Similarity threshold for automatic merge consideration\n  similarity_threshold: 0.85\n  \n  # Require parliament approval for merges\n  require_parliament: true\n  \n  # Parliament threshold override by tier\n  parliament_threshold_override:\n    personal: 0.7\n    digested: 0.9\n    peripheral: 0.8\n  \n  # Merge conflict resolution\n  conflict_resolution:\n    strategy: \"parliament\"  # parliament, newer, higher_confidence\n    preserve_evidence: true\n    create_audit_trail: true\n  \n  # Post-merge verification\n  post_merge_verification: true\n\n# Provenance tracking\nprovenance:\n  # Enable provenance tracking\n  enabled: true\n  \n  # Include provenance in knowledge exports\n  include_in_exports: true\n  \n  # Track confidence scores in provenance\n  track_confidence: true\n  \n  # Track extraction method\n  track_extraction_method: true\n  \n  # Track source information\n  track_source: true\n  \n  # Track timestamp information\n  track_timestamps: true\n  \n  # Provenance visualization\n  visualization:\n    enabled: true\n    format: \"graph\"  # graph, table, timeline\n    include_in_artifacts: true\n\n# Knowledge analytics\nanalytics:\n  # Enable knowledge analytics\n  enabled: true\n  \n  # Track tier distribution\n  track_tier_distribution: true\n  \n  # Track promotion/demotion rates\n  track_tier_changes: true\n  \n  # Track confidence distribution\n  track_confidence: true\n  \n  # Track evidence distribution\n  track_evidence: true\n  \n  # Track knowledge usage\n  track_usage: true\n  \n  # Analytics reporting\n  reporting:\n    enabled: true\n    frequency: \"monthly\"  # daily, weekly, monthly\n    format: \"dashboard\"  # dashboard, report, email\n    include_visualizations: true\n\n# Export/import settings\ntransfer:\n  # Enable knowledge export\n  export:\n    enabled: true\n    formats:\n      - json\n      - rdf\n      - csv\n    include_provenance: true\n    include_tier_info: true\n  \n  # Enable knowledge import\n  import:\n    enabled: true\n    default_tier: \"peripheral\"\n    validation_required: true\n    parliament_approval: true\n    formats:\n      - json\n      - rdf\n      - csv\n\n# Privacy and sharing\nprivacy:\n  # Default privacy level by tier\n  default_privacy:\n    personal: \"private\"\n    digested: \"shared\"\n    peripheral: \"public\"\n  \n  # Require explicit sharing for personal tier\n  require_explicit_sharing: true\n  \n  # Anonymize personal information in exports\n  anonymize_exports: true\n  \n  # Privacy preservation in merges\n  preserve_privacy_in_merges: true\n  \n  # Privacy level inheritance in promotions\n  inherit_privacy_level: false\n\n\n#=============================================================================\n# File: config/parliament.yaml\n# Description: Parliament Decision Engine Configuration for Knowledge Garden v2.0\n# Improvement Area: Configuration System\n#=============================================================================\n# Parliament Decision Engine Configuration for Knowledge Garden v2.0\n\n# Core Parliament settings\ncore:\n  # Enable Parliament decision engine\n  enabled: true\n  \n  # Default mode for Parliament operation\n  default_mode: \"cooperative\"  # cooperative, adversarial, divergent\n  \n  # Number of perspectives to include in deliberation\n  num_perspectives: 5\n  \n  # Number of rounds in deliberation\n  num_rounds: 3\n  \n  # Time limit per round (in seconds)\n  time_limit: 120\n  \n  # Consensus threshold (percentage of agreement needed)\n  consensus_threshold: 80\n  \n  # Fallback strategy when consensus not reached\n  fallback_strategy: \"majority\"  # majority, weighted, chairperson\n  \n  # Enable automatic summarization of deliberation\n  summarize: true\n  \n  # Cache parliament decisions\n  cache_decisions: true\n  \n  # Cache lifetime (in hours)\n  cache_lifetime: 24\n\n# Parliament members and roles\nmembers:\n  # Default roles in parliament\n  default_roles:\n    - role: \"Precision Expert\"\n      weight: 1.0\n      focus: \"Exact naming and disambiguation\"\n      perspective: \"Focuses on precise language and avoids over-merging distinct concepts\"\n    \n    - role: \"Pattern Recognizer\"\n      weight: 1.0\n      focus: \"Identifying relationships and similarities\"\n      perspective: \"Identifies connections between concepts and recognizes similarities\"\n    \n    - role: \"Domain Expert\"\n      weight: 1.2\n      focus: \"Deep domain understanding\"\n      perspective: \"Evaluates decisions based on domain-specific knowledge and norms\"\n    \n    - role: \"Skeptic\"\n      weight: 0.8\n      focus: \"Critical evaluation and evidence\"\n      perspective: \"Questions assumptions and requires evidence for claims\"\n    \n    - role: \"Pragmatist\"\n      weight: 1.0\n      focus: \"Practical outcomes and usability\"\n      perspective: \"Evaluates decisions based on practical utility and user experience\"\n  \n  # Parliament chairperson (serves as tiebreaker)\n  chairperson: \"Domain Expert\"\n  \n  # Custom roles (domain-specific)\n  custom_roles:\n    - role: \"Technical Expert\"\n      weight: 1.1\n      focus: \"Technical accuracy and feasibility\"\n      perspective: \"Evaluates technical aspects of decisions\"\n      domains: [\"software\", \"engineering\", \"architecture\"]\n    \n    - role: \"User Advocate\"\n      weight: 1.0\n      focus: \"User needs and experience\"\n      perspective: \"Represents user perspectives and needs\"\n      domains: [\"design\", \"product\", \"user_experience\"]\n    \n    - role: \"Historical Context Expert\"\n      weight: 0.9\n      focus: \"Historical patterns and precedents\"\n      perspective: \"Provides historical context for decisions\"\n      domains: [\"history\", \"evolution\", \"context\"]\n\n# Decision types configuration\ndecision_types:\n  # Entity merging decisions\n  entity_merge:\n    enabled: true\n    required_roles:\n      - \"Precision Expert\"\n      - \"Pattern Recognizer\"\n    confidence_threshold: 0.85\n    max_decision_time: 60\n    prompt_template: \"parliament/prompts/entity_merge.yaml\"\n  \n  # Relation validation decisions\n  relation_validation:\n    enabled: true\n    required_roles:\n      - \"Domain Expert\"\n      - \"Skeptic\"\n    confidence_threshold: 0.75\n    max_decision_time: 45\n    prompt_template: \"parliament/prompts/relation_validation.yaml\"\n  \n  # Knowledge tier classification\n  tier_classification:\n    enabled: true\n    required_roles:\n      - \"Domain Expert\"\n      - \"Pragmatist\"\n    confidence_threshold: 0.7\n    max_decision_time: 30\n    prompt_template: \"parliament/prompts/tier_classification.yaml\"\n  \n  # Conflict resolution\n  conflict_resolution:\n    enabled: true\n    required_roles:\n      - \"Domain Expert\"\n      - \"Skeptic\"\n      - \"Pragmatist\"\n    confidence_threshold: 0.9\n    max_decision_time: 90\n    prompt_template: \"parliament/prompts/conflict_resolution.yaml\"\n  \n  # Pattern validation\n  pattern_validation:\n    enabled: true\n    required_roles:\n      - \"Pattern Recognizer\"\n      - \"Skeptic\"\n    confidence_threshold: 0.8\n    max_decision_time: 45\n    prompt_template: \"parliament/prompts/pattern_validation.yaml\"\n\n# LLM configuration for Parliament\nllm:\n  # Default LLM model to use\n  model: \"claude-3-haiku-20240307\"\n  \n  # Alternative models\n  alternative_models:\n    - \"claude-3-sonnet-20240229\"\n    - \"claude-3-opus-20240229\"\n  \n  # Temperature for generation\n  temperature: 0.7\n  \n  # Max tokens to generate\n  max_tokens: 4000\n  \n  # Top-p sampling\n  top_p: 0.9\n  \n  # Prompt optimization\n  prompt_optimization:\n    enabled: true\n    cache_prompts: true\n  \n  # Fallback strategy if primary model fails\n  fallback_strategy: \"alternative_model\"  # alternative_model, reduced_complexity, cached_response\n  \n  # Enable trace logging for LLM calls\n  trace_logging: true\n\n# Provenance tracking\nprovenance:\n  # Enable provenance tracking for decisions\n  enabled: true\n  \n  # Include full deliberation in provenance\n  include_deliberation: true\n  \n  # Include member perspectives in provenance\n  include_perspectives: true\n  \n  # Link decisions in knowledge graph\n  link_in_graph: true\n  \n  # Provenance format\n  format: \"structured\"  # structured, narrative, minimal\n\n# Decision registry\nregistry:\n  # Enable decision registry\n  enabled: true\n  \n  # Store decisions in Fuseki\n  store_in_fuseki: true\n  \n  # Local backup of decisions\n  local_backup: true\n  \n  # Backup directory\n  backup_dir: \"data/parliament/decisions\"\n  \n  # Include timestamps in registry\n  include_timestamps: true\n  \n  # Track decision impacts\n  track_impacts: true\n  \n  # Decision indexing for faster retrieval\n  indexing:\n    enabled: true\n    index_type: \"hybrid\"  # text, semantic, hybrid\n    update_frequency: \"realtime\"  # realtime, hourly, daily\n\n# Performance monitoring\nmonitoring:\n  # Enable performance monitoring\n  enabled: true\n  \n  # Metrics to track\n  metrics:\n    - decision_time\n    - consensus_rate\n    - decision_confidence\n    - impact_score\n  \n  # Performance logging\n  logging:\n    enabled: true\n    log_level: \"info\"\n    log_file: \"logs/parliament_performance.log\"\n  \n  # Alerting for performance issues\n  alerting:\n    enabled: false\n    threshold: 0.7\n    \n  # Performance optimization\n  optimization:\n    enabled: true\n    auto_adjust: true\n    learning_rate: 0.1\n\n\n#=============================================================================\n# File: core/config.py\n# Description: Central configuration for Knowledge Garden\n# Improvement Area: Configuration System\n#=============================================================================\n\"\"\"Central configuration for Knowledge Garden\"\"\"\nfrom pathlib import Path\nfrom rdflib import Namespace\n\n# Paths\nPROJECT_ROOT = Path(__file__).parent.parent\nDATA_DIR = PROJECT_ROOT / \"data\"\nONTOLOGY_DIR = PROJECT_ROOT / \"ontologies\"\n\n# Namespaces\nKG = Namespace(\"urn:knowledgegarden:ontology#\")\nKGI = Namespace(\"urn:knowledgegarden:instance:\")  # Updated to match legacy format\nKGU = Namespace(\"urn:knowledgegarden:user:\")  # From legacy config\nKGG = Namespace(\"urn:knowledgegarden:graph:\")  # From legacy config\n\n# Model configurations\nANTHROPIC_MODEL_FAST = \"claude-3-haiku-20240307\"\nANTHROPIC_MODEL_SMART = \"claude-3-sonnet-20240229\"\nOPENAI_MODEL_FAST = \"gpt-3.5-turbo\"\nOPENAI_MODEL_SMART = \"gpt-4\"\n\n# Fuseki configuration\nFUSEKI_URL = \"http://localhost:3030\"\nFUSEKI_DATASET = \"kg\"\nDEFAULT_FUSEKI_URL = f\"{FUSEKI_URL}/{FUSEKI_DATASET}\"  # For legacy compatibility\nFUSEKI_UPDATE = f\"{DEFAULT_FUSEKI_URL}/update\"  # For legacy compatibility\nFUSEKI_QUERY = f\"{DEFAULT_FUSEKI_URL}/query\"  # For legacy compatibility\n\n# Credentials (used by legacy client)\nFUSEKI_USER = \"admin\"\nFUSEKI_PASSWORD = \"changeme\"\n\n# Processing Configuration\nDEFAULT_BATCH_SIZE = 20\nBATCH_SIZE = 10\nMIN_PATTERN_FREQUENCY = 2\nCONFIDENCE_THRESHOLD = 0.7\nDEFAULT_USER_ID = \"bilal\"\n\n# Artifact types - Detailed format\nARTIFACT_TYPES_DETAILED = {\n    'diagram': ['mermaid', 'graphviz'],\n    'framework': ['markdown', 'yaml'],\n    'insight_card': ['markdown'],\n    'practice': ['markdown'],\n    'presentation': ['marp']\n}\n\n# Artifact types - List format\nARTIFACT_TYPES = [\n    \"diagram\",\n    \"framework\", \n    \"summary\",\n    \"practice\",\n    \"insight_card\",\n    \"presentation\",\n    \"report\",\n    \"image\",\n    \"chart\"\n]\n\n# NLP models\nSPACY_MODEL = \"en_core_web_sm\"\n\n# Keyword extraction\nMAX_KEYWORDS = 10\nKEYWORD_MIN_LENGTH = 3\n\n# Extraction patterns\nCAP_PATTERNS = [\n    r\"MOMENTS? when[^.]+OPPORTUNIT(?:Y|IES) for[^.]+\",\n    r\"moments? when[^.]+opportunit(?:y|ies) for[^.]+\",\n    r\"I (?:pay attention to|notice|focus on|attend to)[^.]+when[^.]+\"\n]\n\n\n#=============================================================================\n# File: core/config_validator.py\n# Description: Configuration validation module for Knowledge Garden v2.0\n# Improvement Area: Configuration System\n#=============================================================================\n\"\"\"\nConfiguration validation module for Knowledge Garden v2.0\n\nThis module provides functionality to validate configuration files against JSON schemas,\nensuring that all required fields are present and have correct types and values.\n\"\"\"\n\nimport os\nimport json\nimport logging\nimport jsonschema\nimport yaml\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\n# Define schema paths\nSCHEMA_DIR = Path(__file__).parent.parent / \"schemas\"\n\n# Ensure schema directory exists\nSCHEMA_DIR.mkdir(exist_ok=True, parents=True)\n\nclass ConfigValidator:\n    \"\"\"\n    Validates configuration files against JSON schemas\n    \n    This class ensures that configuration files contain all required fields\n    and that values have the correct types and meet any specified constraints.\n    \"\"\"\n    \n    def __init__(self, schema_dir: Optional[Path] = None):\n        \"\"\"\n        Initialize the configuration validator\n        \n        Args:\n            schema_dir: Optional custom directory for JSON schemas\n        \"\"\"\n        self.schema_dir = schema_dir or SCHEMA_DIR\n        self.schemas: Dict[str, Dict[str, Any]] = {}\n        self.load_schemas()\n    \n    def load_schemas(self) -> None:\n        \"\"\"Load all JSON schemas from the schema directory\"\"\"\n        if not self.schema_dir.exists():\n            logger.warning(f\"Schema directory not found: {self.schema_dir}\")\n            return\n            \n        for schema_file in self.schema_dir.glob(\"*.json\"):\n            try:\n                with open(schema_file, 'r') as f:\n                    schema = json.load(f)\n                    \n                schema_name = schema_file.stem\n                self.schemas[schema_name] = schema\n                logger.info(f\"Loaded schema: {schema_name}\")\n            except Exception as e:\n                logger.error(f\"Error loading schema {schema_file}: {e}\")\n    \n    def validate_config(self, config: Dict[str, Any], schema_name: str) -> Tuple[bool, List[str]]:\n        \"\"\"\n        Validate a configuration against a schema\n        \n        Args:\n            config: Configuration dictionary to validate\n            schema_name: Name of the schema to validate against\n            \n        Returns:\n            Tuple of (is_valid, error_messages)\n        \"\"\"\n        if schema_name not in self.schemas:\n            logger.error(f\"Schema not found: {schema_name}\")\n            return False, [f\"Schema not found: {schema_name}\"]\n            \n        schema = self.schemas[schema_name]\n        validator = jsonschema.Draft7Validator(schema)\n        \n        # Collect all validation errors\n        errors = list(validator.iter_errors(config))\n        \n        if not errors:\n            return True, []\n            \n        # Format error messages\n        error_messages = []\n        for error in errors:\n            path = \".\".join([str(p) for p in error.path]) if error.path else \"root\"\n            message = f\"{path}: {error.message}\"\n            error_messages.append(message)\n            \n        return False, error_messages\n    \n    def validate_yaml_config(self, yaml_path: Path, schema_name: str) -> Tuple[bool, List[str]]:\n        \"\"\"\n        Validate a YAML configuration file against a schema\n        \n        Args:\n            yaml_path: Path to YAML configuration file\n            schema_name: Name of the schema to validate against\n            \n        Returns:\n            Tuple of (is_valid, error_messages)\n        \"\"\"\n        try:\n            with open(yaml_path, 'r') as f:\n                config = yaml.safe_load(f)\n                \n            return self.validate_config(config, schema_name)\n        except Exception as e:\n            logger.error(f\"Error loading YAML file {yaml_path}: {e}\")\n            return False, [f\"Error loading YAML file: {e}\"]\n    \n    def validate_all_configs(self, config_dir: Path) -> Dict[str, Tuple[bool, List[str]]]:\n        \"\"\"\n        Validate all configuration files in a directory\n        \n        Args:\n            config_dir: Directory containing configuration files\n            \n        Returns:\n            Dictionary mapping file names to validation results\n        \"\"\"\n        results = {}\n        \n        if not config_dir.exists():\n            logger.error(f\"Configuration directory not found: {config_dir}\")\n            return results\n            \n        for config_file in config_dir.glob(\"*.yaml\"):\n            schema_name = config_file.stem\n            \n            if schema_name in self.schemas:\n                is_valid, errors = self.validate_yaml_config(config_file, schema_name)\n                results[config_file.name] = (is_valid, errors)\n            else:\n                results[config_file.name] = (False, [f\"No schema defined for {schema_name}\"])\n                \n        return results\n\n\ndef create_default_schemas() -> None:\n    \"\"\"Create default JSON schemas for configuration validation\"\"\"\n    schemas = {\n        \"extraction\": {\n            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n            \"type\": \"object\",\n            \"required\": [\"embedding\", \"sentence_extraction\", \"entity_resolution\"],\n            \"properties\": {\n                \"embedding\": {\n                    \"type\": \"object\",\n                    \"required\": [\"model\", \"dimension\", \"batch_size\", \"device\"],\n                    \"properties\": {\n                        \"model\": {\"type\": \"string\"},\n                        \"dimension\": {\"type\": \"integer\", \"minimum\": 1},\n                        \"batch_size\": {\"type\": \"integer\", \"minimum\": 1},\n                        \"device\": {\"type\": \"string\", \"enum\": [\"auto\", \"cpu\", \"cuda\"]},\n                        \"cache_dir\": {\"type\": \"string\"}\n                    }\n                },\n                \"sentence_extraction\": {\n                    \"type\": \"object\",\n                    \"required\": [\"confidence_threshold\", \"batch_size\"],\n                    \"properties\": {\n                        \"confidence_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                        \"batch_size\": {\"type\": \"integer\", \"minimum\": 1},\n                        \"extract_patterns\": {\"type\": \"boolean\"},\n                        \"normalize_entities\": {\"type\": \"boolean\"},\n                        \"max_sentence_length\": {\"type\": \"integer\", \"minimum\": 10}\n                    }\n                },\n                \"entity_resolution\": {\n                    \"type\": \"object\",\n                    \"required\": [\"similarity_threshold\"],\n                    \"properties\": {\n                        \"similarity_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                        \"high_confidence_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                        \"use_parliament\": {\"type\": \"boolean\"},\n                        \"parliament_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n                    }\n                }\n            }\n        },\n        \"parliament\": {\n            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n            \"type\": \"object\",\n            \"required\": [\"core\", \"members\", \"decision_types\"],\n            \"properties\": {\n                \"core\": {\n                    \"type\": \"object\",\n                    \"required\": [\"enabled\", \"default_mode\", \"num_perspectives\", \"consensus_threshold\"],\n                    \"properties\": {\n                        \"enabled\": {\"type\": \"boolean\"},\n                        \"default_mode\": {\"type\": \"string\", \"enum\": [\"cooperative\", \"adversarial\", \"divergent\"]},\n                        \"num_perspectives\": {\"type\": \"integer\", \"minimum\": 2},\n                        \"num_rounds\": {\"type\": \"integer\", \"minimum\": 1},\n                        \"time_limit\": {\"type\": \"integer\", \"minimum\": 1},\n                        \"consensus_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 100},\n                        \"fallback_strategy\": {\"type\": \"string\", \"enum\": [\"majority\", \"weighted\", \"chairperson\"]},\n                        \"summarize\": {\"type\": \"boolean\"},\n                        \"cache_decisions\": {\"type\": \"boolean\"},\n                        \"cache_lifetime\": {\"type\": \"integer\", \"minimum\": 1}\n                    }\n                },\n                \"members\": {\n                    \"type\": \"object\",\n                    \"required\": [\"default_roles\"],\n                    \"properties\": {\n                        \"default_roles\": {\n                            \"type\": \"array\",\n                            \"items\": {\n                                \"type\": \"object\",\n                                \"required\": [\"role\", \"weight\", \"focus\", \"perspective\"],\n                                \"properties\": {\n                                    \"role\": {\"type\": \"string\"},\n                                    \"weight\": {\"type\": \"number\", \"minimum\": 0},\n                                    \"focus\": {\"type\": \"string\"},\n                                    \"perspective\": {\"type\": \"string\"}\n                                }\n                            }\n                        },\n                        \"chairperson\": {\"type\": \"string\"},\n                        \"custom_roles\": {\n                            \"type\": \"array\",\n                            \"items\": {\n                                \"type\": \"object\",\n                                \"required\": [\"role\", \"weight\", \"focus\", \"perspective\"],\n                                \"properties\": {\n                                    \"role\": {\"type\": \"string\"},\n                                    \"weight\": {\"type\": \"number\", \"minimum\": 0},\n                                    \"focus\": {\"type\": \"string\"},\n                                    \"perspective\": {\"type\": \"string\"},\n                                    \"domains\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                                }\n                            }\n                        }\n                    }\n                },\n                \"decision_types\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"entity_merge\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"enabled\": {\"type\": \"boolean\"},\n                                \"required_roles\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                                \"confidence_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                                \"max_decision_time\": {\"type\": \"integer\", \"minimum\": 1},\n                                \"prompt_template\": {\"type\": \"string\"}\n                            }\n                        },\n                        \"relation_validation\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"enabled\": {\"type\": \"boolean\"},\n                                \"required_roles\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                                \"confidence_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                                \"max_decision_time\": {\"type\": \"integer\", \"minimum\": 1},\n                                \"prompt_template\": {\"type\": \"string\"}\n                            }\n                        },\n                        \"tier_classification\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"enabled\": {\"type\": \"boolean\"},\n                                \"required_roles\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                                \"confidence_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                                \"max_decision_time\": {\"type\": \"integer\", \"minimum\": 1},\n                                \"prompt_template\": {\"type\": \"string\"}\n                            }\n                        },\n                        \"conflict_resolution\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"enabled\": {\"type\": \"boolean\"},\n                                \"required_roles\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                                \"confidence_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                                \"max_decision_time\": {\"type\": \"integer\", \"minimum\": 1},\n                                \"prompt_template\": {\"type\": \"string\"}\n                            }\n                        },\n                        \"pattern_validation\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"enabled\": {\"type\": \"boolean\"},\n                                \"required_roles\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                                \"confidence_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                                \"max_decision_time\": {\"type\": \"integer\", \"minimum\": 1},\n                                \"prompt_template\": {\"type\": \"string\"}\n                            }\n                        }\n                    }\n                }\n            }\n        },\n        \"knowledge_tiers\": {\n            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n            \"type\": \"object\",\n            \"required\": [\"core\", \"tiers\", \"promotion\", \"demotion\"],\n            \"properties\": {\n                \"core\": {\n                    \"type\": \"object\",\n                    \"required\": [\"enabled\", \"default_tier\"],\n                    \"properties\": {\n                        \"enabled\": {\"type\": \"boolean\"},\n                        \"default_tier\": {\"type\": \"string\", \"enum\": [\"personal\", \"digested\", \"peripheral\"]},\n                        \"auto_promote\": {\"type\": \"boolean\"},\n                        \"min_evidence\": {\"type\": \"integer\", \"minimum\": 1},\n                        \"tier_aging\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"enabled\": {\"type\": \"boolean\"},\n                                \"personal_to_peripheral_days\": {\"type\": \"integer\", \"minimum\": 1},\n                                \"digested_to_personal_days\": {\"type\": \"integer\", \"minimum\": 1}\n                            }\n                        },\n                        \"notify_tier_changes\": {\"type\": \"boolean\"}\n                    }\n                },\n                \"tiers\": {\n                    \"type\": \"object\",\n                    \"required\": [\"personal\", \"digested\", \"peripheral\"],\n                    \"properties\": {\n                        \"personal\": {\n                            \"type\": \"object\",\n                            \"required\": [\"name\", \"description\"],\n                            \"properties\": {\n                                \"name\": {\"type\": \"string\"},\n                                \"description\": {\"type\": \"string\"},\n                                \"confidence_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                                \"evidence_required\": {\"type\": \"integer\", \"minimum\": 1},\n                                \"visibility\": {\"type\": \"string\"},\n                                \"color\": {\"type\": \"string\"},\n                                \"settings\": {\"type\": \"object\"}\n                            }\n                        },\n                        \"digested\": {\n                            \"type\": \"object\",\n                            \"required\": [\"name\", \"description\"],\n                            \"properties\": {\n                                \"name\": {\"type\": \"string\"},\n                                \"description\": {\"type\": \"string\"},\n                                \"confidence_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                                \"evidence_required\": {\"type\": \"integer\", \"minimum\": 1},\n                                \"visibility\": {\"type\": \"string\"},\n                                \"color\": {\"type\": \"string\"},\n                                \"settings\": {\"type\": \"object\"}\n                            }\n                        },\n                        \"peripheral\": {\n                            \"type\": \"object\",\n                            \"required\": [\"name\", \"description\"],\n                            \"properties\": {\n                                \"name\": {\"type\": \"string\"},\n                                \"description\": {\"type\": \"string\"},\n                                \"confidence_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                                \"evidence_required\": {\"type\": \"integer\", \"minimum\": 1},\n                                \"visibility\": {\"type\": \"string\"},\n                                \"color\": {\"type\": \"string\"},\n                                \"settings\": {\"type\": \"object\"}\n                            }\n                        }\n                    }\n                },\n                \"promotion\": {\n                    \"type\": \"object\",\n                    \"required\": [\"personal_to_digested\", \"peripheral_to_personal\"],\n                    \"properties\": {\n                        \"personal_to_digested\": {\n                            \"type\": \"object\",\n                            \"required\": [\"enabled\"],\n                            \"properties\": {\n                                \"enabled\": {\"type\": \"boolean\"},\n                                \"criteria\": {\"type\": \"object\"},\n                                \"check_frequency_days\": {\"type\": \"integer\", \"minimum\": 1},\n                                \"evidence_weights\": {\"type\": \"object\"}\n                            }\n                        },\n                        \"peripheral_to_personal\": {\n                            \"type\": \"object\",\n                            \"required\": [\"enabled\"],\n                            \"properties\": {\n                                \"enabled\": {\"type\": \"boolean\"},\n                                \"criteria\": {\"type\": \"object\"},\n                                \"check_frequency_days\": {\"type\": \"integer\", \"minimum\": 1},\n                                \"evidence_weights\": {\"type\": \"object\"}\n                            }\n                        }\n                    }\n                },\n                \"demotion\": {\n                    \"type\": \"object\",\n                    \"required\": [\"digested_to_personal\", \"personal_to_peripheral\"],\n                    \"properties\": {\n                        \"digested_to_personal\": {\n                            \"type\": \"object\",\n                            \"required\": [\"enabled\"],\n                            \"properties\": {\n                                \"enabled\": {\"type\": \"boolean\"},\n                                \"criteria\": {\"type\": \"object\"},\n                                \"check_frequency_days\": {\"type\": \"integer\", \"minimum\": 1}\n                            }\n                        },\n                        \"personal_to_peripheral\": {\n                            \"type\": \"object\",\n                            \"required\": [\"enabled\"],\n                            \"properties\": {\n                                \"enabled\": {\"type\": \"boolean\"},\n                                \"criteria\": {\"type\": \"object\"},\n                                \"check_frequency_days\": {\"type\": \"integer\", \"minimum\": 1}\n                            }\n                        }\n                    }\n                }\n            }\n        },\n        \"artifacts\": {\n            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n            \"type\": \"object\",\n            \"required\": [\"dependency_graph\", \"version_manager\", \"meta_artifacts\", \"storage\", \"staleness\"],\n            \"properties\": {\n                \"dependency_graph\": {\n                    \"type\": \"object\",\n                    \"required\": [\"persistent\", \"auto_update\"],\n                    \"properties\": {\n                        \"persistent\": {\"type\": \"boolean\"},\n                        \"auto_update\": {\"type\": \"boolean\"},\n                        \"max_depth\": {\"type\": \"integer\", \"minimum\": 1},\n                        \"include_dependency_types\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"visualization\": {\"type\": \"object\"}\n                    }\n                },\n                \"version_manager\": {\n                    \"type\": \"object\",\n                    \"required\": [\"enabled\", \"keep_versions\"],\n                    \"properties\": {\n                        \"enabled\": {\"type\": \"boolean\"},\n                        \"keep_versions\": {\"type\": \"integer\", \"minimum\": 0},\n                        \"auto_version\": {\"type\": \"boolean\"},\n                        \"min_change_threshold\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100},\n                        \"compress_versions\": {\"type\": \"boolean\"},\n                        \"diff_algorithm\": {\"type\": \"string\", \"enum\": [\"unified\", \"context\", \"ndiff\", \"html\"]},\n                        \"include_metadata\": {\"type\": \"boolean\"}\n                    }\n                },\n                \"meta_artifacts\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"max_artifacts\": {\"type\": \"integer\", \"minimum\": 1},\n                        \"infer_relationships\": {\"type\": \"boolean\"},\n                        \"relationship_confidence\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100},\n                        \"default_types\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"auto_organize\": {\"type\": \"object\"}\n                    }\n                },\n                \"storage\": {\n                    \"type\": \"object\",\n                    \"required\": [\"base_path\"],\n                    \"properties\": {\n                        \"base_path\": {\"type\": \"string\"},\n                        \"directories\": {\"type\": \"object\"},\n                        \"file_naming\": {\"type\": \"string\"},\n                        \"formats\": {\"type\": \"object\"}\n                    }\n                },\n                \"staleness\": {\n                    \"type\": \"object\",\n                    \"required\": [\"enabled\", \"check_frequency\"],\n                    \"properties\": {\n                        \"enabled\": {\"type\": \"boolean\"},\n                        \"check_frequency\": {\"type\": \"integer\", \"minimum\": 1},\n                        \"auto_refresh\": {\"type\": \"boolean\"},\n                        \"notifications\": {\"type\": \"object\"},\n                        \"prioritization\": {\"type\": \"object\"}\n                    }\n                }\n            }\n        }\n    }\n    \n    # Ensure schema directory exists\n    os.makedirs(SCHEMA_DIR, exist_ok=True)\n    \n    # Write schemas to files\n    for schema_name, schema in schemas.items():\n        schema_path = SCHEMA_DIR / f\"{schema_name}.json\"\n        \n        with open(schema_path, 'w') as f:\n            json.dump(schema, f, indent=2)\n            \n        logger.info(f\"Created schema: {schema_path}\")\n\n\ndef validate_config_on_startup() -> bool:\n    \"\"\"\n    Validate all configuration files on startup\n    \n    Returns:\n        True if all configurations are valid, False otherwise\n    \"\"\"\n    # Create default schemas if they don't exist\n    if not any(SCHEMA_DIR.glob(\"*.json\")):\n        create_default_schemas()\n    \n    # Initialize validator\n    validator = ConfigValidator()\n    \n    # Validate all configuration files\n    config_dir = Path(__file__).parent.parent / \"config\"\n    results = validator.validate_all_configs(config_dir)\n    \n    # Log results\n    all_valid = True\n    for config_file, (is_valid, errors) in results.items():\n        if is_valid:\n            logger.info(f\"Configuration valid: {config_file}\")\n        else:\n            all_valid = False\n            logger.error(f\"Configuration invalid: {config_file}\")\n            for error in errors:\n                logger.error(f\"  - {error}\")\n    \n    return all_valid\n\n\nif __name__ == \"__main__\":\n    # Set up logging\n    logging.basicConfig(level=logging.INFO)\n    \n    # Create default schemas\n    create_default_schemas()\n    \n    # Validate configurations\n    validate_config_on_startup()\n\n\n\n#================================================================================\n# IMPROVEMENT AREA: Extraction Pipeline\n#================================================================================\n\n\n#=============================================================================\n# File: extraction/embedding_generator.py\n# Description: Embedding generation and similarity search for knowledge garden\n# Improvement Area: Extraction Pipeline\n#=============================================================================\n\"\"\"Embedding generation and similarity search for knowledge garden\"\"\"\nimport logging\nimport os\nimport numpy as np\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom sentence_transformers import SentenceTransformer\nimport torch\n\nlogger = logging.getLogger(__name__)\n\nclass EmbeddingGenerator:\n    \"\"\"Generate and manage embeddings using Jina or other models\n    \n    This service handles embedding generation, caching, and similarity search\n    for knowledge entities and sentences.\n    \"\"\"\n    \n    DEFAULT_MODEL = 'jinaai/jina-embeddings-v2-base-en'\n    \n    def __init__(self, model_name: Optional[str] = None, device: Optional[str] = None,\n                cache_dir: Optional[str] = None):\n        \"\"\"Initialize the embedding generator\n        \n        Args:\n            model_name: Name of the embedding model to use\n            device: Device to run model on ('cpu', 'cuda', etc.)\n            cache_dir: Directory to cache embeddings\n        \"\"\"\n        self.model_name = model_name or self.DEFAULT_MODEL\n        \n        # Initialize device\n        if device:\n            self.device = device\n        else:\n            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        \n        # Cache directory for storing embeddings\n        self.cache_dir = cache_dir\n        \n        # Load model\n        self._load_model()\n        \n        logger.info(f\"Initialized embedding generator with model {self.model_name} on {self.device}\")\n    \n    def _load_model(self):\n        \"\"\"Load the embedding model\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name, device=self.device)\n            self.embedding_dim = self.model.get_sentence_embedding_dimension()\n            logger.info(f\"Loaded embedding model with dimension {self.embedding_dim}\")\n        except Exception as e:\n            logger.error(f\"Error loading embedding model: {e}\")\n            raise\n    \n    async def encode(self, text: str) -> np.ndarray:\n        \"\"\"Generate embedding for text\n        \n        Args:\n            text: Text to encode\n            \n        Returns:\n            Embedding vector\n        \"\"\"\n        if not text.strip():\n            # Return zero vector for empty text\n            return np.zeros(self.embedding_dim)\n        \n        try:\n            # Normalize text before encoding\n            normalized_text = self._normalize_text(text)\n            \n            # Generate embedding\n            embedding = self.model.encode(normalized_text, convert_to_numpy=True)\n            return embedding\n        except Exception as e:\n            logger.error(f\"Error generating embedding: {e}\")\n            # Return zero vector on error\n            return np.zeros(self.embedding_dim)\n    \n    async def encode_batch(self, texts: List[str]) -> np.ndarray:\n        \"\"\"Generate embeddings for multiple texts\n        \n        Args:\n            texts: List of texts to encode\n            \n        Returns:\n            Array of embedding vectors\n        \"\"\"\n        # Normalize all texts\n        normalized_texts = [self._normalize_text(text) for text in texts]\n        \n        try:\n            # Generate embeddings in batch\n            embeddings = self.model.encode(normalized_texts, convert_to_numpy=True)\n            return embeddings\n        except Exception as e:\n            logger.error(f\"Error generating batch embeddings: {e}\")\n            # Return zero vectors on error\n            return np.zeros((len(texts), self.embedding_dim))\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normalize text for consistent embeddings\n        \n        Args:\n            text: Text to normalize\n            \n        Returns:\n            Normalized text\n        \"\"\"\n        # Simple normalization - trim whitespace and ensure text ends with punctuation\n        text = text.strip()\n        \n        # Ensure text has ending punctuation if missing\n        if text and not text[-1] in '.!?':\n            text = text + '.'\n            \n        return text\n    \n    def cosine_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n        \"\"\"Calculate cosine similarity between two embeddings\n        \n        Args:\n            embedding1: First embedding vector\n            embedding2: Second embedding vector\n            \n        Returns:\n            Similarity score (0-1)\n        \"\"\"\n        # Check for zero vectors\n        if np.all(embedding1 == 0) or np.all(embedding2 == 0):\n            return 0.0\n        \n        # Calculate cosine similarity: dot(a, b) / (norm(a) * norm(b))\n        similarity = np.dot(embedding1, embedding2) / (\n            np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n        )\n        \n        # Ensure result is in range [0, 1]\n        similarity = max(0.0, min(1.0, float(similarity)))\n        return similarity\n    \n    def find_similar(self, embedding: np.ndarray, \n                    stored_embeddings: Dict[str, np.ndarray], \n                    threshold: float = 0.85,\n                    top_k: int = 5) -> List[Tuple[str, float]]:\n        \"\"\"Find similar entities based on embedding similarity\n        \n        Args:\n            embedding: Query embedding\n            stored_embeddings: Dictionary of {entity_id: embedding}\n            threshold: Minimum similarity threshold\n            top_k: Maximum number of results to return\n            \n        Returns:\n            List of (entity_id, similarity) tuples sorted by similarity\n        \"\"\"\n        similarities = {}\n        \n        for entity_id, stored_emb in stored_embeddings.items():\n            similarity = self.cosine_similarity(embedding, stored_emb)\n            if similarity > threshold:\n                similarities[entity_id] = similarity\n        \n        # Sort by similarity (descending) and limit to top_k\n        results = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_k]\n        return results\n    \n    async def find_similar_texts(self, query_text: str, \n                               candidate_texts: List[str],\n                               threshold: float = 0.85,\n                               top_k: int = 5) -> List[Tuple[int, float]]:\n        \"\"\"Find similar texts directly\n        \n        Args:\n            query_text: Text to find similar matches for\n            candidate_texts: List of texts to compare against\n            threshold: Minimum similarity threshold\n            top_k: Maximum number of results to return\n            \n        Returns:\n            List of (text_index, similarity) tuples sorted by similarity\n        \"\"\"\n        # Generate embeddings\n        query_embedding = await self.encode(query_text)\n        candidate_embeddings = await self.encode_batch(candidate_texts)\n        \n        # Calculate similarities\n        similarities = []\n        for i, emb in enumerate(candidate_embeddings):\n            similarity = self.cosine_similarity(query_embedding, emb)\n            if similarity > threshold:\n                similarities.append((i, similarity))\n        \n        # Sort by similarity (descending) and limit to top_k\n        results = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]\n        return results\n\n\n#=============================================================================\n# File: extraction/entity_resolver.py\n# Description: Entity resolution using embeddings and LLM validation\n# Improvement Area: Extraction Pipeline\n#=============================================================================\n\"\"\"Entity resolution using embeddings and LLM validation\"\"\"\nimport logging\nimport json\nfrom typing import Dict, List, Any, Optional, Tuple, NamedTuple\nimport numpy as np\nfrom datetime import datetime\n\nfrom knowledge_garden.models.evidence import Evidence\n\nlogger = logging.getLogger(__name__)\n\nclass Entity:\n    \"\"\"Entity in the knowledge graph\"\"\"\n    \n    def __init__(\n        self,\n        entity_id: str,\n        label: str,\n        embedding: Optional[np.ndarray] = None,\n        entity_type: Optional[str] = None,\n        attributes: Optional[Dict[str, Any]] = None,\n        source_uri: Optional[str] = None,\n        evidence_ids: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"Initialize an entity\n        \n        Args:\n            entity_id: Unique identifier for the entity\n            label: Human-readable label/name\n            embedding: Vector representation of the entity\n            entity_type: Type of entity (Person, Organization, Concept, etc.)\n            attributes: Key-value attributes of the entity\n            source_uri: Source document/conversation URI\n            evidence_ids: IDs of evidence records\n            metadata: Additional metadata\n        \"\"\"\n        self.entity_id = entity_id\n        self.label = label\n        self.embedding = embedding\n        self.entity_type = entity_type\n        self.attributes = attributes or {}\n        self.source_uri = source_uri\n        self.evidence_ids = evidence_ids or []\n        self.metadata = metadata or {}\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary representation\"\"\"\n        from knowledge_garden.models.extracted_sentence import ExtractedSentence\n        \n        return {\n            \"entity_id\": self.entity_id,\n            \"label\": self.label,\n            \"embedding\": ExtractedSentence.encode_embedding(self.embedding) if self.embedding is not None else None,\n            \"entity_type\": self.entity_type,\n            \"attributes\": self.attributes,\n            \"source_uri\": self.source_uri,\n            \"evidence_ids\": self.evidence_ids,\n            \"metadata\": self.metadata\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Entity':\n        \"\"\"Create from dictionary representation\"\"\"\n        from knowledge_garden.models.extracted_sentence import ExtractedSentence\n        \n        # Handle embedding conversion\n        embedding = None\n        if data.get(\"embedding\"):\n            embedding = ExtractedSentence.decode_embedding(data[\"embedding\"])\n        \n        return cls(\n            entity_id=data[\"entity_id\"],\n            label=data[\"label\"],\n            embedding=embedding,\n            entity_type=data.get(\"entity_type\"),\n            attributes=data.get(\"attributes\", {}),\n            source_uri=data.get(\"source_uri\"),\n            evidence_ids=data.get(\"evidence_ids\", []),\n            metadata=data.get(\"metadata\", {})\n        )\n\nclass ResolutionResult(NamedTuple):\n    \"\"\"Result of entity resolution\"\"\"\n    action: str  # 'create_new', 'merge', 'keep_separate'\n    confidence: float\n    entity: Optional[Entity] = None\n    target_id: Optional[str] = None\n    reasoning: Optional[str] = None\n    requires_parliament: bool = False\n\nclass EntityResolver:\n    \"\"\"Resolve entities using embeddings and LLM validation\"\"\"\n    \n    # LLM prompt template for entity resolution\n    ENTITY_RESOLUTION_PROMPT = \"\"\"\n    Determine if these entities should be merged or kept separate in a knowledge graph.\n    \n    New entity:\n    - Label: \"{new_entity_label}\"\n    - Type: \"{new_entity_type}\"\n    - Attributes: {new_entity_attributes}\n    \n    Candidate entities:\n    {candidates}\n    \n    Analyze:\n    1. Are these referring to the same real-world entity?\n    2. Consider name/label variations, spelling differences, and potential context\n    3. Calculate a confidence score (0.0-1.0) for your decision\n    \n    Return a JSON object:\n    {{\n      \"action\": \"merge\"|\"keep_separate\",\n      \"target_id\": \"ID of entity to merge with (if merge)\",\n      \"confidence\": 0.95,\n      \"reasoning\": \"Explanation of your decision\"\n    }}\n    \n    ONLY return the JSON object, no other text.\n    \"\"\"\n    \n    def __init__(self, embedding_generator, llm_service, parliament_engine=None):\n        \"\"\"Initialize entity resolver\n        \n        Args:\n            embedding_generator: Service for generating and comparing embeddings\n            llm_service: Service for LLM validation\n            parliament_engine: Optional parliament decision engine\n        \"\"\"\n        self.embedder = embedding_generator\n        self.llm = llm_service\n        self.parliament = parliament_engine\n    \n    def _format_entity_for_llm(self, entity: Entity) -> str:\n        \"\"\"Format entity for LLM prompt\n        \n        Args:\n            entity: Entity to format\n            \n        Returns:\n            Formatted entity string\n        \"\"\"\n        return f\"\"\"\n        - ID: {entity.entity_id}\n        - Label: {entity.label}\n        - Type: {entity.entity_type or \"Unknown\"}\n        - Attributes: {json.dumps(entity.attributes, ensure_ascii=False, indent=2)}\n        \"\"\"\n    \n    def _format_candidates_for_llm(self, candidates: List[Tuple[str, float, Entity]]) -> str:\n        \"\"\"Format candidate entities for LLM prompt\n        \n        Args:\n            candidates: List of (entity_id, similarity, entity) tuples\n            \n        Returns:\n            Formatted candidates string\n        \"\"\"\n        formatted = []\n        \n        for i, (entity_id, similarity, entity) in enumerate(candidates):\n            candidate = f\"\"\"\n            Candidate {i+1} (similarity: {similarity:.2f}):\n            - ID: {entity_id}\n            - Label: {entity.label}\n            - Type: {entity.entity_type or \"Unknown\"}\n            - Attributes: {json.dumps(entity.attributes, ensure_ascii=False, indent=2)}\n            \"\"\"\n            formatted.append(candidate)\n        \n        return \"\\n\".join(formatted)\n    \n    async def _validate_with_llm(self, new_entity: Entity, \n                              candidates: List[Tuple[str, float, Entity]]) -> ResolutionResult:\n        \"\"\"Validate entity resolution with LLM\n        \n        Args:\n            new_entity: New entity to resolve\n            candidates: List of (entity_id, similarity, entity) tuples\n            \n        Returns:\n            Resolution result\n        \"\"\"\n        # Format entities for prompt\n        candidates_str = self._format_candidates_for_llm(candidates)\n        \n        # Create prompt\n        prompt = self.ENTITY_RESOLUTION_PROMPT.format(\n            new_entity_label=new_entity.label,\n            new_entity_type=new_entity.entity_type or \"Unknown\",\n            new_entity_attributes=json.dumps(new_entity.attributes, ensure_ascii=False, indent=2),\n            candidates=candidates_str\n        )\n        \n        # Get LLM response\n        response = await self.llm.generate(prompt)\n        \n        try:\n            # Parse response\n            decision = self._parse_llm_response(response)\n            \n            if decision.get('action') == 'merge':\n                return ResolutionResult(\n                    action='merge',\n                    confidence=float(decision.get('confidence', 0.0)),\n                    target_id=decision.get('target_id'),\n                    reasoning=decision.get('reasoning'),\n                    requires_parliament=False\n                )\n            else:\n                return ResolutionResult(\n                    action='keep_separate',\n                    confidence=float(decision.get('confidence', 0.0)),\n                    entity=new_entity,\n                    reasoning=decision.get('reasoning'),\n                    requires_parliament=False\n                )\n                \n        except Exception as e:\n            logger.error(f\"Error parsing LLM response: {e}\")\n            # Default to keeping separate on error\n            return ResolutionResult(\n                action='keep_separate',\n                confidence=0.5,\n                entity=new_entity,\n                reasoning=\"Error in LLM validation\",\n                requires_parliament=False\n            )\n    \n    def _parse_llm_response(self, response: str) -> Dict[str, Any]:\n        \"\"\"Parse LLM response for entity resolution\n        \n        Args:\n            response: LLM response text\n            \n        Returns:\n            Decision dictionary\n        \"\"\"\n        try:\n            # Find JSON in response\n            import re\n            json_match = re.search(r'{.*}', response, re.DOTALL)\n            \n            if json_match:\n                decision = json.loads(json_match.group(0))\n                return decision\n            else:\n                logger.warning(\"Could not find JSON in LLM response\")\n                return {\"action\": \"keep_separate\", \"confidence\": 0.5}\n                \n        except json.JSONDecodeError as e:\n            logger.error(f\"Error parsing JSON from LLM response: {e}\")\n            return {\"action\": \"keep_separate\", \"confidence\": 0.5}\n    \n    async def resolve_entity(self, new_entity: Entity, \n                           existing_entities: Dict[str, Entity],\n                           similarity_threshold: float = 0.85,\n                           high_confidence_threshold: float = 0.95) -> ResolutionResult:\n        \"\"\"Resolve entity against existing entities\n        \n        Args:\n            new_entity: New entity to resolve\n            existing_entities: Dictionary of {entity_id: entity}\n            similarity_threshold: Minimum similarity for candidates\n            high_confidence_threshold: Threshold for automatic decisions\n            \n        Returns:\n            Resolution result\n        \"\"\"\n        # If no embedding, generate one\n        if new_entity.embedding is None and new_entity.label:\n            new_entity.embedding = await self.embedder.encode(new_entity.label)\n        \n        # If still no embedding, create new entity\n        if new_entity.embedding is None:\n            return ResolutionResult(\n                action='create_new',\n                confidence=1.0,\n                entity=new_entity\n            )\n        \n        # Prepare existing embeddings dictionary\n        entity_embeddings = {}\n        for entity_id, entity in existing_entities.items():\n            if entity.embedding is not None:\n                entity_embeddings[entity_id] = entity.embedding\n        \n        # Find similar entities\n        similar = self.embedder.find_similar(\n            new_entity.embedding,\n            entity_embeddings,\n            threshold=similarity_threshold\n        )\n        \n        # If no similar entities, create new\n        if not similar:\n            return ResolutionResult(\n                action='create_new',\n                confidence=1.0,\n                entity=new_entity\n            )\n        \n        # Get top candidate\n        top_id, top_similarity = similar[0]\n        \n        # If very high similarity, automatically merge\n        if top_similarity >= high_confidence_threshold:\n            return ResolutionResult(\n                action='merge',\n                confidence=top_similarity,\n                target_id=top_id\n            )\n        \n        # Prepare candidates for validation\n        candidates = []\n        for entity_id, similarity in similar:\n            candidates.append((entity_id, similarity, existing_entities[entity_id]))\n        \n        # Determine if we need parliament or simple LLM validation\n        complex_case = self._is_complex_resolution_case(new_entity, candidates)\n        \n        if complex_case and self.parliament:\n            # Use parliament for complex cases\n            return await self._resolve_with_parliament(new_entity, candidates)\n        else:\n            # Use LLM validation for simple cases\n            return await self._validate_with_llm(new_entity, candidates)\n    \n    def _is_complex_resolution_case(self, new_entity: Entity, \n                                  candidates: List[Tuple[str, float, Entity]]) -> bool:\n        \"\"\"Determine if this is a complex resolution case\n        \n        Args:\n            new_entity: New entity to resolve\n            candidates: List of (entity_id, similarity, entity) tuples\n            \n        Returns:\n            True if complex case, False otherwise\n        \"\"\"\n        if len(candidates) > 3:\n            return True\n            \n        # Check for conflicting entity types\n        types = set(e.entity_type for _, _, e in candidates if e.entity_type)\n        if new_entity.entity_type and new_entity.entity_type not in types and len(types) > 0:\n            return True\n            \n        # Check for multiple similar candidates with similar scores\n        if len(candidates) > 1:\n            top_similarity = candidates[0][1]\n            second_similarity = candidates[1][1]\n            if (top_similarity - second_similarity) < 0.05:\n                return True\n                \n        return False\n    \n    async def _resolve_with_parliament(self, new_entity: Entity,\n                                     candidates: List[Tuple[str, float, Entity]]) -> ResolutionResult:\n        \"\"\"Resolve entity using parliament decision engine\n        \n        Args:\n            new_entity: New entity to resolve\n            candidates: List of (entity_id, similarity, entity) tuples\n            \n        Returns:\n            Resolution result\n        \"\"\"\n        if not self.parliament:\n            logger.warning(\"Parliament engine requested but not available\")\n            return await self._validate_with_llm(new_entity, candidates)\n        \n        # Prepare context for parliament\n        context = {\n            \"decision_type\": \"entity_merge\",\n            \"new_entity\": new_entity.to_dict(),\n            \"candidates\": [\n                {\n                    \"entity_id\": entity_id,\n                    \"similarity\": similarity,\n                    \"entity\": entity.to_dict()\n                }\n                for entity_id, similarity, entity in candidates\n            ]\n        }\n        \n        # Get parliament decision\n        decision = await self.parliament.decide(context)\n        \n        # Convert to resolution result\n        if decision.get('action') == 'merge':\n            return ResolutionResult(\n                action='merge',\n                confidence=float(decision.get('confidence', 0.0)),\n                target_id=decision.get('target_id'),\n                reasoning=decision.get('reasoning'),\n                requires_parliament=True\n            )\n        else:\n            return ResolutionResult(\n                action='keep_separate',\n                confidence=float(decision.get('confidence', 0.0)),\n                entity=new_entity,\n                reasoning=decision.get('reasoning'),\n                requires_parliament=True\n            )\n    \n    async def merge_entities(self, target_entity: Entity, source_entity: Entity) -> Entity:\n        \"\"\"Merge source entity into target entity\n        \n        Args:\n            target_entity: Entity to merge into\n            source_entity: Entity to merge from\n            \n        Returns:\n            Updated target entity\n        \"\"\"\n        # Merge attributes\n        for key, value in source_entity.attributes.items():\n            if key not in target_entity.attributes:\n                target_entity.attributes[key] = value\n        \n        # Merge evidence IDs\n        for evidence_id in source_entity.evidence_ids:\n            if evidence_id not in target_entity.evidence_ids:\n                target_entity.evidence_ids.append(evidence_id)\n        \n        # Update embedding if needed\n        if target_entity.embedding is None and source_entity.embedding is not None:\n            target_entity.embedding = source_entity.embedding\n        \n        # Track merge in metadata\n        if 'merged_from' not in target_entity.metadata:\n            target_entity.metadata['merged_from'] = []\n            \n        target_entity.metadata['merged_from'].append({\n            'entity_id': source_entity.entity_id,\n            'timestamp': datetime.now().isoformat()\n        })\n        \n        return target_entity\n\n\n#=============================================================================\n# File: extraction/evidence_tracker.py\n# Description: Evidence tracking for knowledge provenance\n# Improvement Area: Extraction Pipeline\n#=============================================================================\n\"\"\"Evidence tracking for knowledge provenance\"\"\"\nimport logging\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom datetime import datetime\n\nfrom knowledge_garden.models.evidence import Evidence\nfrom knowledge_garden.core.fuseki_client import FusekiClient\n\nlogger = logging.getLogger(__name__)\n\nclass EvidenceTracker:\n    \"\"\"Track and manage evidence for extracted knowledge\n    \n    Maintains provenance for all extracted triples, entities, and relationships\n    by storing evidence records and linking them to knowledge graph elements.\n    \"\"\"\n    \n    def __init__(self, fuseki_client: FusekiClient, base_uri: str = \"urn:knowledgegarden:instance\"):\n        \"\"\"Initialize the evidence tracker\n        \n        Args:\n            fuseki_client: Client for Fuseki triple store\n            base_uri: Base URI for knowledge graph entities\n        \"\"\"\n        self.fuseki = fuseki_client\n        self.base_uri = base_uri\n    \n    async def store_evidence(self, evidence: Evidence, target_uri: str) -> str:\n        \"\"\"Store evidence in knowledge graph\n        \n        Args:\n            evidence: Evidence record\n            target_uri: URI of entity/triple this evidence supports\n            \n        Returns:\n            Evidence URI\n        \"\"\"\n        # Convert evidence to RDF triples\n        triples = evidence.to_rdf_triples(target_uri, self.base_uri)\n        \n        # Insert into Fuseki\n        await self.fuseki.insert_triples(triples, f\"{self.base_uri}/graph/evidence\")\n        \n        # Return evidence URI\n        return f\"{self.base_uri}/evidence/{evidence.evidence_id}\"\n    \n    async def store_batch_evidence(self, evidence_batch: List[Tuple[Evidence, str]]) -> List[str]:\n        \"\"\"Store multiple evidence records in batch\n        \n        Args:\n            evidence_batch: List of (evidence, target_uri) tuples\n            \n        Returns:\n            List of evidence URIs\n        \"\"\"\n        all_triples = []\n        evidence_uris = []\n        \n        # Collect all triples from evidence records\n        for evidence, target_uri in evidence_batch:\n            triples = evidence.to_rdf_triples(target_uri, self.base_uri)\n            all_triples.extend(triples)\n            evidence_uris.append(f\"{self.base_uri}/evidence/{evidence.evidence_id}\")\n        \n        # Insert all triples in one batch\n        if all_triples:\n            await self.fuseki.insert_triples(all_triples, f\"{self.base_uri}/graph/evidence\")\n        \n        return evidence_uris\n    \n    async def get_evidence(self, evidence_id: str) -> Optional[Evidence]:\n        \"\"\"Retrieve evidence by ID\n        \n        Args:\n            evidence_id: ID of evidence to retrieve\n            \n        Returns:\n            Evidence record if found, None otherwise\n        \"\"\"\n        # Query for evidence properties\n        query = f\"\"\"\n        PREFIX kg: <urn:knowledgegarden:ontology#>\n        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n        \n        SELECT ?quote ?source ?extractor ?confidence ?offset ?time\n        WHERE {{\n            <{self.base_uri}/evidence/{evidence_id}> rdf:type kg:Evidence ;\n                kg:quote ?quote ;\n                kg:sourceDocument ?source ;\n                kg:extractorId ?extractor ;\n                kg:confidence ?confidence .\n            OPTIONAL {{ <{self.base_uri}/evidence/{evidence_id}> kg:characterOffset ?offset }}\n            OPTIONAL {{ <{self.base_uri}/evidence/{evidence_id}> kg:extractionTime ?time }}\n        }}\n        \"\"\"\n        \n        results = await self.fuseki.query(query)\n        \n        if not results:\n            return None\n        \n        # Extract evidence data\n        result = results[0]\n        \n        # Parse extraction time\n        extraction_time = None\n        if 'time' in result:\n            try:\n                extraction_time = datetime.fromisoformat(result['time'])\n            except (ValueError, TypeError):\n                extraction_time = datetime.now()\n        \n        # Parse character offset\n        char_offset = None\n        if 'offset' in result:\n            try:\n                char_offset = int(result['offset'])\n            except (ValueError, TypeError):\n                pass\n        \n        # Get additional context and metadata\n        context, metadata = await self._get_evidence_context_metadata(evidence_id)\n        \n        # Create evidence object\n        evidence = Evidence(\n            quote=result['quote'],\n            source_uri=result['source'],\n            extractor_id=result['extractor'],\n            confidence=float(result['confidence']),\n            char_offset=char_offset,\n            extraction_time=extraction_time,\n            evidence_id=evidence_id,\n            context=context,\n            metadata=metadata\n        )\n        \n        return evidence\n    \n    async def _get_evidence_context_metadata(self, evidence_id: str) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n        \"\"\"Get context and metadata for evidence\n        \n        Args:\n            evidence_id: ID of evidence\n            \n        Returns:\n            Tuple of (context, metadata) dictionaries\n        \"\"\"\n        # Query for context and metadata\n        query = f\"\"\"\n        PREFIX kg: <urn:knowledgegarden:ontology#>\n        \n        SELECT ?p ?o\n        WHERE {{\n            <{self.base_uri}/evidence/{evidence_id}> ?p ?o .\n            FILTER(STRSTARTS(STR(?p), STR(kg:context_)) || STRSTARTS(STR(?p), STR(kg:meta_)))\n        }}\n        \"\"\"\n        \n        results = await self.fuseki.query(query)\n        \n        context = {}\n        metadata = {}\n        \n        for result in results:\n            predicate = result['p']\n            object_value = result['o']\n            \n            # Extract key from predicate URI\n            if \"context_\" in predicate:\n                key = predicate.split(\"context_\")[1]\n                context[key] = object_value\n            elif \"meta_\" in predicate:\n                key = predicate.split(\"meta_\")[1]\n                metadata[key] = object_value\n        \n        return context, metadata\n    \n    async def get_entity_evidence(self, entity_uri: str) -> List[Evidence]:\n        \"\"\"Get all evidence for an entity\n        \n        Args:\n            entity_uri: URI of entity\n            \n        Returns:\n            List of evidence records\n        \"\"\"\n        # Query for evidence IDs\n        query = f\"\"\"\n        PREFIX kg: <urn:knowledgegarden:ontology#>\n        \n        SELECT ?evidence\n        WHERE {{\n            <{entity_uri}> kg:hasEvidence ?evidence .\n        }}\n        \"\"\"\n        \n        results = await self.fuseki.query(query)\n        \n        evidence_list = []\n        \n        for result in results:\n            evidence_uri = result['evidence']\n            \n            # Extract evidence ID from URI\n            evidence_id = evidence_uri.split('/')[-1]\n            \n            # Get evidence record\n            evidence = await self.get_evidence(evidence_id)\n            if evidence:\n                evidence_list.append(evidence)\n        \n        return evidence_list\n    \n    async def get_highest_confidence_evidence(self, entity_uri: str) -> Optional[Evidence]:\n        \"\"\"Get highest confidence evidence for an entity\n        \n        Args:\n            entity_uri: URI of entity\n            \n        Returns:\n            Highest confidence evidence, or None if no evidence\n        \"\"\"\n        # Query for evidence with highest confidence\n        query = f\"\"\"\n        PREFIX kg: <urn:knowledgegarden:ontology#>\n        \n        SELECT ?evidence ?confidence\n        WHERE {{\n            <{entity_uri}> kg:hasEvidence ?evidence .\n            ?evidence kg:confidence ?confidence .\n        }}\n        ORDER BY DESC(?confidence)\n        LIMIT 1\n        \"\"\"\n        \n        results = await self.fuseki.query(query)\n        \n        if not results:\n            return None\n        \n        # Extract evidence ID from URI\n        evidence_uri = results[0]['evidence']\n        evidence_id = evidence_uri.split('/')[-1]\n        \n        # Get evidence record\n        return await self.get_evidence(evidence_id)\n    \n    async def update_evidence(self, evidence: Evidence) -> bool:\n        \"\"\"Update existing evidence record\n        \n        Args:\n            evidence: Updated evidence record\n            \n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n        # Delete existing evidence\n        delete_query = f\"\"\"\n        PREFIX kg: <urn:knowledgegarden:ontology#>\n        \n        DELETE {{\n            <{self.base_uri}/evidence/{evidence.evidence_id}> ?p ?o .\n        }}\n        WHERE {{\n            <{self.base_uri}/evidence/{evidence.evidence_id}> ?p ?o .\n        }}\n        \"\"\"\n        \n        await self.fuseki.execute_update(delete_query)\n        \n        # Get linked entities\n        query = f\"\"\"\n        PREFIX kg: <urn:knowledgegarden:ontology#>\n        \n        SELECT ?entity\n        WHERE {{\n            ?entity kg:hasEvidence <{self.base_uri}/evidence/{evidence.evidence_id}> .\n        }}\n        \"\"\"\n        \n        results = await self.fuseki.query(query)\n        \n        # Store updated evidence for each entity\n        for result in results:\n            entity_uri = result['entity']\n            await self.store_evidence(evidence, entity_uri)\n        \n        return True\n\n\n#=============================================================================\n# File: extraction/pattern_extractor.py\n# Description: Pattern Extractor for extracting CAPs, questions, insights, and tensions\n# Improvement Area: Extraction Pipeline\n#=============================================================================\n\"\"\"Pattern Extractor for extracting CAPs, questions, insights, and tensions\"\"\"\nimport logging\nimport re\nfrom typing import List, Dict, Any\n\nlogger = logging.getLogger(__name__)\n\nclass PatternExtractor:\n    \"\"\"Extract various patterns from text\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the pattern extractor with regex patterns\"\"\"\n        # CAP patterns (Context, Action, Pattern)\n        self.cap_patterns = [\n            # Moments when...\n            r\"(?:MOMENTS|Moments|moments)(?:\\s+when\\s+)([^.!?]+[.!?])\",\n            # Opportunities for...\n            r\"(?:OPPORTUNITIES|Opportunities|opportunities)(?:\\s+for\\s+)([^.!?]+[.!?])\",\n            # Circumstances where...\n            r\"(?:CIRCUMSTANCES|Circumstances|circumstances)(?:\\s+where\\s+)([^.!?]+[.!?])\",\n            # Contexts that...\n            r\"(?:CONTEXTS|Contexts|contexts)(?:\\s+that\\s+)([^.!?]+[.!?])\",\n            # When X, we can...\n            r\"When\\s+([^,]+),\\s+we\\s+can\\s+([^.!?]+[.!?])\",\n            # X is a pattern in...\n            r\"([A-Z][^.!?]+)\\s+is\\s+a\\s+pattern\\s+in\\s+([^.!?]+[.!?])\"\n        ]\n        \n        # Question patterns\n        self.question_patterns = [\n            # Direct questions\n            r\"([A-Z][^.!?]+\\?)\",\n            # How might we...\n            r\"How\\s+might\\s+we\\s+([^.!?]+[?])\",\n            # Key question is...\n            r\"(?:The\\s+)?[Kk]ey\\s+question\\s+is\\s+([^.!?]+[?])\",\n            # We should ask...\n            r\"[Ww]e\\s+should\\s+ask\\s+([^.!?]+[?])\"\n        ]\n        \n        # Insight patterns\n        self.insight_patterns = [\n            # I realized that...\n            r\"I\\s+realized\\s+that\\s+([^.!?]+[.!?])\",\n            # The insight is...\n            r\"[Tt]he\\s+insight\\s+is\\s+that\\s+([^.!?]+[.!?])\",\n            # A key insight...\n            r\"[Aa]\\s+key\\s+insight\\s+is\\s+that\\s+([^.!?]+[.!?])\",\n            # This made me understand...\n            r\"[Tt]his\\s+made\\s+me\\s+understand\\s+that\\s+([^.!?]+[.!?])\",\n            # I discovered...\n            r\"I\\s+discovered\\s+that\\s+([^.!?]+[.!?])\"\n        ]\n        \n        # Tension patterns\n        self.tension_patterns = [\n            # Tension between X and Y\n            r\"[Tt]ension\\s+between\\s+([^.!?]+)\\s+and\\s+([^.!?]+[.!?])\",\n            # X versus Y\n            r\"([A-Z][a-z]+(?:\\s+[A-Za-z]+){0,3})\\s+versus\\s+([A-Za-z][a-z]+(?:\\s+[A-Za-z]+){0,3})\",\n            # The trade-off between X and Y\n            r\"[Tt]he\\s+trade-off\\s+between\\s+([^.!?]+)\\s+and\\s+([^.!?]+[.!?])\",\n            # Balancing X and Y\n            r\"[Bb]alancing\\s+([^.!?]+)\\s+and\\s+([^.!?]+[.!?])\",\n            # The struggle between X and Y\n            r\"[Tt]he\\s+struggle\\s+between\\s+([^.!?]+)\\s+and\\s+([^.!?]+[.!?])\"\n        ]\n    \n    def extract(self, text: str, elements):\n        \"\"\"Extract patterns from text and add them to the elements container\n        \n        Args:\n            text: The text to extract patterns from\n            elements: ExtractedElements instance to add results to\n        \"\"\"\n        self._extract_caps(text, elements)\n        self._extract_questions(text, elements)\n        self._extract_insights(text, elements)\n        self._extract_tensions(text, elements)\n    \n    def _extract_caps(self, text: str, elements):\n        \"\"\"Extract CAPs (Context, Action, Pattern) from text\"\"\"\n        for pattern in self.cap_patterns:\n            matches = re.finditer(pattern, text)\n            for match in matches:\n                if len(match.groups()) == 1:\n                    # Simple pattern with one capture group\n                    cap_text = match.group(1).strip()\n                    evidence = match.group(0)\n                    elements.add_cap(cap_text, evidence)\n                elif len(match.groups()) == 2:\n                    # Pattern with two capture groups (e.g., \"When X, we can Y\")\n                    context = match.group(1).strip()\n                    action = match.group(2).strip()\n                    cap_text = f\"When {context}, we can {action}\"\n                    evidence = match.group(0)\n                    elements.add_cap(cap_text, evidence)\n    \n    def _extract_questions(self, text: str, elements):\n        \"\"\"Extract questions from text\"\"\"\n        for pattern in self.question_patterns:\n            matches = re.finditer(pattern, text)\n            for match in matches:\n                if len(match.groups()) >= 1:\n                    question_text = match.group(1).strip()\n                    source_text = match.group(0)\n                    elements.add_question(question_text, source_text)\n    \n    def _extract_insights(self, text: str, elements):\n        \"\"\"Extract insights from text\"\"\"\n        for pattern in self.insight_patterns:\n            matches = re.finditer(pattern, text)\n            for match in matches:\n                if len(match.groups()) >= 1:\n                    insight_text = match.group(1).strip()\n                    source_text = match.group(0)\n                    elements.add_insight(insight_text, source_text)\n    \n    def _extract_tensions(self, text: str, elements):\n        \"\"\"Extract tensions from text\"\"\"\n        for pattern in self.tension_patterns:\n            matches = re.finditer(pattern, text)\n            for match in matches:\n                if len(match.groups()) >= 2:\n                    pole1 = match.group(1).strip()\n                    pole2 = match.group(2).strip().rstrip('.!?')\n                    source_text = match.group(0)\n                    elements.add_tension(pole1, pole2, source_text)\n\n\n#=============================================================================\n# File: extraction/pipeline.py\n# Description: Knowledge extraction pipeline for processing text data\n# Improvement Area: Extraction Pipeline\n#=============================================================================\n\"\"\"Knowledge extraction pipeline for processing text data\"\"\"\nimport logging\nimport asyncio\nfrom typing import Dict, List, Any, Optional, Union, Tuple\nfrom datetime import datetime\n\nfrom knowledge_garden.extraction.embedding_generator import EmbeddingGenerator\nfrom knowledge_garden.extraction.sentence_extractor import SentenceExtractor\nfrom knowledge_garden.extraction.pattern_extractor import PatternExtractor\nfrom knowledge_garden.extraction.entity_resolver import EntityResolver, Entity, ResolutionResult\nfrom knowledge_garden.extraction.evidence_tracker import EvidenceTracker\nfrom knowledge_garden.extraction.llm_extractor import LLMExtractor\nfrom knowledge_garden.models.evidence import Evidence\nfrom knowledge_garden.models.extracted_sentence import ExtractedSentence\nfrom knowledge_garden.core.uri_generator import URIGenerator\n\nlogger = logging.getLogger(__name__)\n\nclass ExtractionPipeline:\n    \"\"\"Orchestrates the knowledge extraction process\n    \n    The pipeline coordinates the extraction of knowledge from text data:\n    1. Preprocesses text and generates embeddings\n    2. Extracts knowledge using sentence and pattern extractors\n    3. Resolves entities to avoid duplication\n    4. Tracks evidence and provenance\n    5. Outputs structured knowledge in RDF format\n    \"\"\"\n    \n    def __init__(\n        self, \n        embedding_service: Optional[EmbeddingGenerator] = None,\n        llm_service: Optional[Any] = None,\n        fuseki_client: Optional[Any] = None,\n        parliament_engine: Optional[Any] = None,\n        config: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"Initialize the extraction pipeline\n        \n        Args:\n            embedding_service: Service for generating embeddings\n            llm_service: Service for LLM-based extraction\n            fuseki_client: Client for Fuseki triple store\n            parliament_engine: Optional parliament decision engine\n            config: Configuration parameters\n        \"\"\"\n        self.config = config or {}\n        \n        # Initialize services\n        self.embedding_service = embedding_service\n        self.llm_service = llm_service\n        self.fuseki_client = fuseki_client\n        \n        # Initialize extractors\n        self.pattern_extractor = PatternExtractor()\n        self.sentence_extractor = None\n        self.llm_extractor = None\n        \n        if self.llm_service:\n            self.sentence_extractor = SentenceExtractor(\n                llm_service=self.llm_service,\n                embedding_service=self.embedding_service\n            )\n            self.llm_extractor = LLMExtractor(\n                api_key=self.config.get(\"anthropic_api_key\")\n            )\n        \n        # Initialize entity resolution and evidence tracking\n        self.entity_resolver = None\n        self.evidence_tracker = None\n        \n        if self.embedding_service and self.llm_service:\n            self.entity_resolver = EntityResolver(\n                embedding_generator=self.embedding_service,\n                llm_service=self.llm_service,\n                parliament_engine=parliament_engine\n            )\n        \n        if self.fuseki_client:\n            self.evidence_tracker = EvidenceTracker(\n                fuseki_client=self.fuseki_client,\n                base_uri=self.config.get(\"base_uri\", \"urn:knowledgegarden:instance\")\n            )\n        \n        # URI generation\n        self.uri_generator = URIGenerator(\n            base_uri=self.config.get(\"base_uri\", \"urn:knowledgegarden:instance\")\n        )\n        \n        # Entity cache\n        self.entity_cache: Dict[str, Entity] = {}\n        self.extraction_stats: Dict[str, int] = {\n            \"sentences_processed\": 0,\n            \"triples_extracted\": 0,\n            \"entities_created\": 0,\n            \"entities_merged\": 0,\n            \"evidence_records\": 0,\n            \"caps_extracted\": 0,\n            \"questions_extracted\": 0,\n            \"insights_extracted\": 0,\n            \"tensions_extracted\": 0\n        }\n    \n    async def extract_from_text(\n        self, \n        text: str, \n        source_uri: str,\n        context: Optional[Dict[str, Any]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Extract knowledge from text\n        \n        Args:\n            text: Text to extract from\n            source_uri: URI of the source document/conversation\n            context: Additional context information\n            \n        Returns:\n            Extraction results including entities and triples\n        \"\"\"\n        if not text.strip():\n            logger.warning(\"Empty text provided for extraction, skipping\")\n            return {\n                \"entities\": [],\n                \"triples\": [],\n                \"evidence\": [],\n                \"stats\": self.extraction_stats\n            }\n        \n        # Initialize context if not provided\n        context = context or {}\n        \n        # Step 1: Preprocess text and extract sentences\n        extracted_sentences = await self._extract_sentences(text, source_uri)\n        self.extraction_stats[\"sentences_processed\"] += len(extracted_sentences)\n        \n        # Step 2: Extract entities and triples from sentences\n        entities, triples, evidence = await self._process_sentences(\n            extracted_sentences, source_uri, context\n        )\n        \n        # Step 3: Extract additional patterns using regex and LLM\n        pattern_entities, pattern_triples, pattern_evidence = await self._extract_patterns(\n            text, source_uri, context\n        )\n        \n        # Combine results\n        all_entities = entities + pattern_entities\n        all_triples = triples + pattern_triples\n        all_evidence = evidence + pattern_evidence\n        \n        # Update stats\n        self.extraction_stats[\"triples_extracted\"] += len(all_triples)\n        self.extraction_stats[\"evidence_records\"] += len(all_evidence)\n        \n        return {\n            \"entities\": all_entities,\n            \"triples\": all_triples,\n            \"evidence\": all_evidence,\n            \"stats\": self.extraction_stats\n        }\n    \n    async def _extract_sentences(\n        self, \n        text: str, \n        source_uri: str\n    ) -> List[ExtractedSentence]:\n        \"\"\"Extract and process sentences from text\n        \n        Args:\n            text: Text to process\n            source_uri: URI of the source\n            \n        Returns:\n            List of extracted sentences\n        \"\"\"\n        if not self.sentence_extractor:\n            # If no sentence extractor, just split text into sentences\n            sentences = []\n            if self.embedding_service:\n                for i, sent_text in enumerate(text.split('.')):\n                    if sent_text.strip():\n                        embedding = await self.embedding_service.encode(sent_text)\n                        sentences.append(ExtractedSentence(\n                            text=sent_text,\n                            embedding=embedding,\n                            context={\"position\": i, \"total_sentences\": len(text.split('.'))}\n                        ))\n            return sentences\n        \n        # Use sentence extractor for full processing\n        extracted_sentences, _ = await self.sentence_extractor.process_text(\n            text, source_uri, window_size=2\n        )\n        return extracted_sentences\n    \n    async def _process_sentences(\n        self,\n        sentences: List[ExtractedSentence],\n        source_uri: str,\n        context: Dict[str, Any]\n    ) -> Tuple[List[Entity], List[Dict[str, Any]], List[Evidence]]:\n        \"\"\"Process extracted sentences to get entities and triples\n        \n        Args:\n            sentences: List of extracted sentences\n            source_uri: URI of the source\n            context: Additional context\n            \n        Returns:\n            Tuple of (entities, triples, evidence records)\n        \"\"\"\n        entities = []\n        triples = []\n        evidence_records = []\n        \n        # Process each sentence\n        for sentence in sentences:\n            # Skip sentences without triples\n            if not sentence.triples:\n                continue\n                \n            # Process each triple\n            for triple_data in sentence.triples:\n                # Extract entities from triple\n                subject = triple_data.get(\"subject\", \"\").strip()\n                predicate = triple_data.get(\"predicate\", \"\").strip()\n                object_val = triple_data.get(\"object\", \"\").strip()\n                confidence = float(triple_data.get(\"confidence\", 0.7))\n                \n                # Skip incomplete triples\n                if not (subject and predicate and object_val):\n                    continue\n                \n                # Create subject entity\n                subject_entity = await self._create_or_resolve_entity(\n                    label=subject,\n                    entity_type=triple_data.get(\"subject_type\"),\n                    source_uri=source_uri,\n                    confidence=confidence,\n                    context=context\n                )\n                \n                # Create object entity if not a literal\n                object_type = triple_data.get(\"object_type\", \"\")\n                is_literal = object_type.lower() in [\"value\", \"literal\", \"string\", \"number\"]\n                \n                object_entity = None\n                if not is_literal:\n                    object_entity = await self._create_or_resolve_entity(\n                        label=object_val,\n                        entity_type=object_type,\n                        source_uri=source_uri,\n                        confidence=confidence,\n                        context=context\n                    )\n                \n                # Create triple\n                triple = {\n                    \"subject_uri\": subject_entity.entity_id if subject_entity else None,\n                    \"predicate\": predicate,\n                    \"object_uri\": object_entity.entity_id if object_entity else None,\n                    \"object_literal\": object_val if is_literal else None,\n                    \"is_literal\": is_literal,\n                    \"confidence\": confidence,\n                    \"source_uri\": source_uri,\n                    \"triple_id\": self.uri_generator.generate_id(\"triple\")\n                }\n                \n                # Add entities and triple to results\n                if subject_entity and subject_entity not in entities:\n                    entities.append(subject_entity)\n                if object_entity and object_entity not in entities:\n                    entities.append(object_entity)\n                triples.append(triple)\n                \n                # Create evidence\n                evidence = Evidence(\n                    quote=sentence.text,\n                    source_uri=source_uri,\n                    extractor_id=\"sentence_extractor\",\n                    confidence=confidence,\n                    context={\"sentence_id\": sentence.sentence_id},\n                    metadata=context\n                )\n                evidence_records.append(evidence)\n        \n        return entities, triples, evidence_records\n    \n    async def _create_or_resolve_entity(\n        self,\n        label: str,\n        entity_type: Optional[str] = None,\n        source_uri: Optional[str] = None,\n        confidence: float = 0.7,\n        context: Optional[Dict[str, Any]] = None\n    ) -> Optional[Entity]:\n        \"\"\"Create a new entity or resolve against existing entities\n        \n        Args:\n            label: Entity label\n            entity_type: Type of entity\n            source_uri: Source document URI\n            confidence: Confidence score\n            context: Additional context\n            \n        Returns:\n            Entity object (new or merged)\n        \"\"\"\n        if not label:\n            return None\n            \n        # Normalize entity_type\n        if entity_type:\n            entity_type = entity_type.lower().strip()\n            # Map common variations to standard types\n            type_mapping = {\n                \"person\": \"Person\",\n                \"people\": \"Person\",\n                \"individual\": \"Person\",\n                \"organization\": \"Organization\",\n                \"org\": \"Organization\",\n                \"company\": \"Organization\",\n                \"concept\": \"Concept\",\n                \"idea\": \"Concept\",\n                \"location\": \"Location\",\n                \"place\": \"Location\",\n                \"event\": \"Event\",\n                \"time\": \"Time\",\n                \"date\": \"Time\"\n            }\n            entity_type = type_mapping.get(entity_type, entity_type.capitalize())\n        \n        # Generate entity ID\n        entity_id = self.uri_generator.generate_entity_uri(\n            label, entity_type or \"Entity\"\n        )\n        \n        # Create new entity\n        entity = Entity(\n            entity_id=entity_id,\n            label=label,\n            entity_type=entity_type,\n            source_uri=source_uri,\n            attributes=context or {}\n        )\n        \n        # Generate embedding\n        if self.embedding_service:\n            entity.embedding = await self.embedding_service.encode(label)\n        \n        # If no entity resolver, just return the new entity\n        if not self.entity_resolver:\n            self.entity_cache[entity_id] = entity\n            self.extraction_stats[\"entities_created\"] += 1\n            return entity\n        \n        # Resolve against existing entities\n        resolution = await self.entity_resolver.resolve_entity(\n            entity, self.entity_cache\n        )\n        \n        # Handle resolution result\n        if resolution.action == 'create_new':\n            # Add new entity to cache\n            self.entity_cache[entity_id] = entity\n            self.extraction_stats[\"entities_created\"] += 1\n            return entity\n            \n        elif resolution.action == 'merge':\n            # Merge with existing entity\n            target_id = resolution.target_id\n            if target_id in self.entity_cache:\n                target_entity = self.entity_cache[target_id]\n                updated_entity = await self.entity_resolver.merge_entities(\n                    target_entity, entity\n                )\n                self.entity_cache[target_id] = updated_entity\n                self.extraction_stats[\"entities_merged\"] += 1\n                return updated_entity\n            else:\n                # Target not found, create new entity\n                self.entity_cache[entity_id] = entity\n                self.extraction_stats[\"entities_created\"] += 1\n                return entity\n                \n        else:  # 'keep_separate'\n            # Add as separate entity\n            self.entity_cache[entity_id] = entity\n            self.extraction_stats[\"entities_created\"] += 1\n            return entity\n    \n    async def _extract_patterns(\n        self,\n        text: str,\n        source_uri: str,\n        context: Dict[str, Any]\n    ) -> Tuple[List[Entity], List[Dict[str, Any]], List[Evidence]]:\n        \"\"\"Extract patterns using regex and LLM approaches\n        \n        Args:\n            text: Text to analyze\n            source_uri: URI of the source\n            context: Additional context\n            \n        Returns:\n            Tuple of (entities, triples, evidence records)\n        \"\"\"\n        entities = []\n        triples = []\n        evidence_records = []\n        \n        # Create container for extracted patterns\n        class ExtractedElements:\n            def __init__(self):\n                self.caps = []\n                self.questions = []\n                self.insights = []\n                self.tensions = []\n                \n            def add_cap(self, text, evidence, confidence=0.8):\n                self.caps.append({\n                    \"text\": text,\n                    \"evidence\": evidence,\n                    \"confidence\": confidence\n                })\n                \n            def add_question(self, text, evidence, confidence=0.8):\n                self.questions.append({\n                    \"text\": text,\n                    \"evidence\": evidence,\n                    \"confidence\": confidence\n                })\n                \n            def add_insight(self, text, evidence, confidence=0.8):\n                self.insights.append({\n                    \"text\": text,\n                    \"evidence\": evidence,\n                    \"confidence\": confidence\n                })\n                \n            def add_tension(self, pole1, pole2, evidence, confidence=0.8):\n                self.tensions.append({\n                    \"pole1\": pole1,\n                    \"pole2\": pole2,\n                    \"evidence\": evidence,\n                    \"confidence\": confidence\n                })\n        \n        # Extract patterns\n        elements = ExtractedElements()\n        \n        # Use pattern extractor\n        self.pattern_extractor.extract(text, elements)\n        \n        # Use LLM extractor if available\n        if self.llm_extractor:\n            self.llm_extractor.extract(text, elements)\n        \n        # Update stats\n        self.extraction_stats[\"caps_extracted\"] += len(elements.caps)\n        self.extraction_stats[\"questions_extracted\"] += len(elements.questions)\n        self.extraction_stats[\"insights_extracted\"] += len(elements.insights)\n        self.extraction_stats[\"tensions_extracted\"] += len(elements.tensions)\n        \n        # Process CAPs\n        for cap in elements.caps:\n            # Create CAP entity\n            cap_entity = await self._create_or_resolve_entity(\n                label=cap[\"text\"],\n                entity_type=\"CAP\",\n                source_uri=source_uri,\n                confidence=cap[\"confidence\"],\n                context=context\n            )\n            if cap_entity:\n                entities.append(cap_entity)\n                \n                # Create evidence\n                evidence = Evidence(\n                    quote=cap[\"evidence\"],\n                    source_uri=source_uri,\n                    extractor_id=\"pattern_extractor\",\n                    confidence=cap[\"confidence\"],\n                    metadata={\"pattern_type\": \"CAP\"}\n                )\n                evidence_records.append(evidence)\n        \n        # Process questions\n        for question in elements.questions:\n            # Create question entity\n            question_entity = await self._create_or_resolve_entity(\n                label=question[\"text\"],\n                entity_type=\"Question\",\n                source_uri=source_uri,\n                confidence=question[\"confidence\"],\n                context=context\n            )\n            if question_entity:\n                entities.append(question_entity)\n                \n                # Create evidence\n                evidence = Evidence(\n                    quote=question[\"evidence\"],\n                    source_uri=source_uri,\n                    extractor_id=\"pattern_extractor\",\n                    confidence=question[\"confidence\"],\n                    metadata={\"pattern_type\": \"Question\"}\n                )\n                evidence_records.append(evidence)\n        \n        # Process insights\n        for insight in elements.insights:\n            # Create insight entity\n            insight_entity = await self._create_or_resolve_entity(\n                label=insight[\"text\"],\n                entity_type=\"Insight\",\n                source_uri=source_uri,\n                confidence=insight[\"confidence\"],\n                context=context\n            )\n            if insight_entity:\n                entities.append(insight_entity)\n                \n                # Create evidence\n                evidence = Evidence(\n                    quote=insight[\"evidence\"],\n                    source_uri=source_uri,\n                    extractor_id=\"pattern_extractor\",\n                    confidence=insight[\"confidence\"],\n                    metadata={\"pattern_type\": \"Insight\"}\n                )\n                evidence_records.append(evidence)\n        \n        # Process tensions\n        for tension in elements.tensions:\n            # Create tension entity\n            tension_label = f\"Tension between {tension['pole1']} and {tension['pole2']}\"\n            tension_entity = await self._create_or_resolve_entity(\n                label=tension_label,\n                entity_type=\"Tension\",\n                source_uri=source_uri,\n                confidence=tension[\"confidence\"],\n                context=context\n            )\n            \n            if tension_entity:\n                entities.append(tension_entity)\n                \n                # Create pole entities\n                pole1_entity = await self._create_or_resolve_entity(\n                    label=tension[\"pole1\"],\n                    entity_type=\"Concept\",\n                    source_uri=source_uri,\n                    confidence=tension[\"confidence\"],\n                    context=context\n                )\n                \n                pole2_entity = await self._create_or_resolve_entity(\n                    label=tension[\"pole2\"],\n                    entity_type=\"Concept\",\n                    source_uri=source_uri,\n                    confidence=tension[\"confidence\"],\n                    context=context\n                )\n                \n                if pole1_entity and pole2_entity:\n                    entities.append(pole1_entity)\n                    entities.append(pole2_entity)\n                    \n                    # Create triples\n                    tension_triple1 = {\n                        \"subject_uri\": tension_entity.entity_id,\n                        \"predicate\": \"hasPole\",\n                        \"object_uri\": pole1_entity.entity_id,\n                        \"object_literal\": None,\n                        \"is_literal\": False,\n                        \"confidence\": tension[\"confidence\"],\n                        \"source_uri\": source_uri,\n                        \"triple_id\": self.uri_generator.generate_id(\"triple\")\n                    }\n                    \n                    tension_triple2 = {\n                        \"subject_uri\": tension_entity.entity_id,\n                        \"predicate\": \"hasPole\",\n                        \"object_uri\": pole2_entity.entity_id,\n                        \"object_literal\": None,\n                        \"is_literal\": False,\n                        \"confidence\": tension[\"confidence\"],\n                        \"source_uri\": source_uri,\n                        \"triple_id\": self.uri_generator.generate_id(\"triple\")\n                    }\n                    \n                    triples.append(tension_triple1)\n                    triples.append(tension_triple2)\n                \n                # Create evidence\n                evidence = Evidence(\n                    quote=tension[\"evidence\"],\n                    source_uri=source_uri,\n                    extractor_id=\"pattern_extractor\",\n                    confidence=tension[\"confidence\"],\n                    metadata={\"pattern_type\": \"Tension\"}\n                )\n                evidence_records.append(evidence)\n        \n        return entities, triples, evidence_records\n    \n    async def store_extraction_results(\n        self, \n        results: Dict[str, Any]\n    ) -> bool:\n        \"\"\"Store extraction results in Fuseki\n        \n        Args:\n            results: Extraction results from extract_from_text\n            \n        Returns:\n            Success status\n        \"\"\"\n        if not self.fuseki_client:\n            logger.warning(\"No Fuseki client available, skipping storage\")\n            return False\n            \n        try:\n            # Extract results\n            entities = results.get(\"entities\", [])\n            triples = results.get(\"triples\", [])\n            evidence_records = results.get(\"evidence\", [])\n            \n            # Store entities\n            for entity in entities:\n                entity_triples = self._entity_to_triples(entity)\n                await self.fuseki_client.insert_triples(\n                    entity_triples, \n                    f\"{self.config.get('base_uri', 'urn:knowledgegarden:instance')}/graph/entities\"\n                )\n                \n                # Store evidence for entity\n                if self.evidence_tracker:\n                    for evidence in evidence_records:\n                        if evidence.metadata.get(\"entity_id\") == entity.entity_id:\n                            await self.evidence_tracker.store_evidence(\n                                evidence, \n                                entity.entity_id\n                            )\n            \n            # Store triples\n            for triple in triples:\n                triple_triples = self._triple_to_triples(triple)\n                await self.fuseki_client.insert_triples(\n                    triple_triples,\n                    f\"{self.config.get('base_uri', 'urn:knowledgegarden:instance')}/graph/triples\"\n                )\n                \n                # Store evidence for triple\n                if self.evidence_tracker:\n                    for evidence in evidence_records:\n                        if evidence.metadata.get(\"triple_id\") == triple[\"triple_id\"]:\n                            await self.evidence_tracker.store_evidence(\n                                evidence, \n                                f\"{self.config.get('base_uri', 'urn:knowledgegarden:instance')}/triple/{triple['triple_id']}\"\n                            )\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Error storing extraction results: {e}\")\n            return False\n    \n    def _entity_to_triples(self, entity: Entity) -> List[Dict[str, Any]]:\n        \"\"\"Convert entity to RDF triples\n        \n        Args:\n            entity: Entity object\n            \n        Returns:\n            List of RDF triple dictionaries\n        \"\"\"\n        entity_uri = entity.entity_id\n        \n        triples = [\n            {\n                \"subject\": entity_uri,\n                \"predicate\": \"rdf:type\",\n                \"object\": f\"kg:{entity.entity_type or 'Entity'}\",\n                \"is_literal\": False\n            },\n            {\n                \"subject\": entity_uri,\n                \"predicate\": \"rdfs:label\",\n                \"object\": entity.label,\n                \"is_literal\": True\n            }\n        ]\n        \n        # Add source URI if available\n        if entity.source_uri:\n            triples.append({\n                \"subject\": entity_uri,\n                \"predicate\": \"kg:sourceUri\",\n                \"object\": entity.source_uri,\n                \"is_literal\": False\n            })\n        \n        # Add attributes\n        for key, value in entity.attributes.items():\n            if isinstance(value, str):\n                triples.append({\n                    \"subject\": entity_uri,\n                    \"predicate\": f\"kg:{key}\",\n                    \"object\": value,\n                    \"is_literal\": True\n                })\n            elif isinstance(value, (int, float)):\n                triples.append({\n                    \"subject\": entity_uri,\n                    \"predicate\": f\"kg:{key}\",\n                    \"object\": str(value),\n                    \"is_literal\": True,\n                    \"datatype\": \"xsd:decimal\"\n                })\n        \n        return triples\n    \n    def _triple_to_triples(self, triple: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Convert knowledge triple to RDF triples\n        \n        Args:\n            triple: Knowledge triple dictionary\n            \n        Returns:\n            List of RDF triple dictionaries\n        \"\"\"\n        triple_uri = f\"{self.config.get('base_uri', 'urn:knowledgegarden:instance')}/triple/{triple['triple_id']}\"\n        \n        triples = [\n            {\n                \"subject\": triple_uri,\n                \"predicate\": \"rdf:type\",\n                \"object\": \"kg:Triple\",\n                \"is_literal\": False\n            },\n            {\n                \"subject\": triple_uri,\n                \"predicate\": \"kg:confidence\",\n                \"object\": str(triple[\"confidence\"]),\n                \"is_literal\": True,\n                \"datatype\": \"xsd:decimal\"\n            }\n        ]\n        \n        # Add subject\n        if triple.get(\"subject_uri\"):\n            triples.append({\n                \"subject\": triple_uri,\n                \"predicate\": \"kg:subject\",\n                \"object\": triple[\"subject_uri\"],\n                \"is_literal\": False\n            })\n        \n        # Add predicate\n        triples.append({\n            \"subject\": triple_uri,\n            \"predicate\": \"kg:predicate\",\n            \"object\": triple[\"predicate\"],\n            \"is_literal\": True\n        })\n        \n        # Add object (URI or literal)\n        if triple.get(\"is_literal\"):\n            triples.append({\n                \"subject\": triple_uri,\n                \"predicate\": \"kg:objectLiteral\",\n                \"object\": triple[\"object_literal\"] or \"\",\n                \"is_literal\": True\n            })\n        elif triple.get(\"object_uri\"):\n            triples.append({\n                \"subject\": triple_uri,\n                \"predicate\": \"kg:objectUri\",\n                \"object\": triple[\"object_uri\"],\n                \"is_literal\": False\n            })\n        \n        # Create direct relationship triple if both subject and object are URIs\n        if triple.get(\"subject_uri\") and triple.get(\"object_uri\") and not triple.get(\"is_literal\"):\n            triples.append({\n                \"subject\": triple[\"subject_uri\"],\n                \"predicate\": f\"kg:{triple['predicate']}\",\n                \"object\": triple[\"object_uri\"],\n                \"is_literal\": False\n            })\n        \n        return triples\n\n\n#=============================================================================\n# File: extraction/sentence_extractor.py\n# Description: Sentence-level extraction with LLM for knowledge triples\n# Improvement Area: Extraction Pipeline\n#=============================================================================\n\"\"\"Sentence-level extraction with LLM for knowledge triples\"\"\"\nimport logging\nimport re\nimport json\nfrom typing import List, Dict, Any, Optional, Tuple\nimport nltk\n\n# Ensure the necessary NLTK data is available\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\n\nfrom nltk.tokenize import sent_tokenize\n\nfrom knowledge_garden.models.extracted_sentence import ExtractedSentence\nfrom knowledge_garden.models.evidence import Evidence\n\nlogger = logging.getLogger(__name__)\n\nclass SentenceExtractor:\n    \"\"\"Extract knowledge from individual sentences using LLM\"\"\"\n    \n    # LLM prompt template for sentence extraction\n    SENTENCE_EXTRACTION_PROMPT = \"\"\"\n    Extract structured knowledge from this sentence. If no relevant knowledge is present, return an empty list.\n    \n    Sentence: \"{sentence}\"\n    \n    Context: {context}\n    \n    Extract the following:\n    - Subject-Predicate-Object triples (entities and relationships)\n    - Entity types and attributes\n    - Confidence score for each extraction (0.0-1.0)\n    \n    Return in JSON format:\n    [\n      {{\n        \"subject\": \"The entity or concept that is the subject\",\n        \"predicate\": \"The relationship or property\",\n        \"object\": \"The target entity, concept, or value\",\n        \"subject_type\": \"Person|Organization|Concept|Other\",\n        \"object_type\": \"Person|Organization|Concept|Value|Other\",\n        \"confidence\": 0.95,\n        \"notes\": \"Optional explanation\"\n      }}\n    ]\n    \n    ONLY return the JSON array, no other text.\n    \"\"\"\n    \n    def __init__(self, llm_service, embedding_service):\n        \"\"\"Initialize the sentence extractor\n        \n        Args:\n            llm_service: Service for LLM interactions\n            embedding_service: Service for generating embeddings\n        \"\"\"\n        self.llm = llm_service\n        self.embedder = embedding_service\n    \n    def split_sentences(self, text: str) -> List[str]:\n        \"\"\"Split text into sentences\n        \n        Args:\n            text: Text to split\n            \n        Returns:\n            List of sentences\n        \"\"\"\n        # Clean text before splitting\n        text = text.replace('\\n', ' ').strip()\n        text = re.sub(r'\\s+', ' ', text)\n        \n        # Split into sentences\n        sentences = sent_tokenize(text)\n        return sentences\n    \n    def build_context(self, sentences: List[str], index: int, \n                     window_size: int = 1) -> Dict[str, Any]:\n        \"\"\"Build context information for a sentence\n        \n        Args:\n            sentences: List of all sentences\n            index: Index of current sentence\n            window_size: Number of sentences before/after to include\n            \n        Returns:\n            Context dictionary\n        \"\"\"\n        total_sentences = len(sentences)\n        \n        # Get previous and next sentences within window\n        prev_start = max(0, index - window_size)\n        prev_sentences = sentences[prev_start:index]\n        \n        next_end = min(total_sentences, index + window_size + 1)\n        next_sentences = sentences[index+1:next_end]\n        \n        # Create context dictionary\n        context = {\n            \"position\": index,\n            \"total_sentences\": total_sentences,\n            \"previous\": \" \".join(prev_sentences),\n            \"next\": \" \".join(next_sentences)\n        }\n        \n        return context\n    \n    def format_context_for_llm(self, context: Dict[str, Any]) -> str:\n        \"\"\"Format context for LLM prompt\n        \n        Args:\n            context: Context dictionary\n            \n        Returns:\n            Formatted context string\n        \"\"\"\n        context_parts = []\n        \n        # Add sentence position information\n        position_info = f\"Sentence {context['position']+1} of {context['total_sentences']}\"\n        context_parts.append(position_info)\n        \n        # Add previous sentences if available\n        if context.get('previous'):\n            prev_text = f\"Previous context: {context['previous']}\"\n            context_parts.append(prev_text)\n        \n        # Add following sentences if available\n        if context.get('next'):\n            next_text = f\"Following context: {context['next']}\"\n            context_parts.append(next_text)\n        \n        return \"\\n\".join(context_parts)\n    \n    async def extract_sentence(self, sentence: str, context: Dict[str, Any]) -> ExtractedSentence:\n        \"\"\"Extract knowledge from a single sentence\n        \n        Args:\n            sentence: The sentence to extract from\n            context: Context information (surrounding sentences, etc.)\n            \n        Returns:\n            Extracted sentence with triples and embedding\n        \"\"\"\n        # Generate embedding\n        embedding = await self.embedder.encode(sentence)\n        \n        # Format context for LLM\n        formatted_context = self.format_context_for_llm(context)\n        \n        # Extract triples using LLM\n        prompt = self.SENTENCE_EXTRACTION_PROMPT.format(\n            sentence=sentence,\n            context=formatted_context\n        )\n        \n        response = await self.llm.generate(prompt)\n        triples, confidence = self._parse_response(response)\n        \n        # Create extracted sentence\n        return ExtractedSentence(\n            text=sentence,\n            embedding=embedding,\n            triples=triples,\n            confidence=confidence,\n            context=context\n        )\n    \n    def _parse_response(self, response: str) -> Tuple[List[Dict[str, Any]], float]:\n        \"\"\"Parse LLM response to extract triples and confidence\n        \n        Args:\n            response: LLM response text\n            \n        Returns:\n            Tuple of (extracted triples, overall confidence)\n        \"\"\"\n        triples = []\n        overall_confidence = 0.0\n        \n        try:\n            # Try to find JSON in the response\n            json_match = re.search(r'\\[\\s*{.*}\\s*\\]', response, re.DOTALL)\n            \n            if json_match:\n                # Extract and parse JSON\n                json_str = json_match.group(0)\n                triples = json.loads(json_str)\n                \n                # Calculate overall confidence\n                if triples:\n                    confidences = [t.get('confidence', 0.0) for t in triples]\n                    overall_confidence = sum(confidences) / len(confidences)\n            else:\n                logger.warning(\"Could not find JSON in LLM response\")\n                \n        except json.JSONDecodeError as e:\n            logger.error(f\"Error parsing JSON from LLM response: {e}\")\n        except Exception as e:\n            logger.error(f\"Error processing LLM response: {e}\")\n        \n        return triples, overall_confidence\n    \n    async def process_text(self, text: str, source_uri: str, \n                         window_size: int = 1) -> Tuple[List[ExtractedSentence], List[Evidence]]:\n        \"\"\"Process full text to extract sentences and evidence\n        \n        Args:\n            text: Full text to process\n            source_uri: URI of the source document/conversation\n            window_size: Context window size (sentences before/after)\n            \n        Returns:\n            Tuple of (extracted sentences, evidence records)\n        \"\"\"\n        # Split text into sentences\n        sentences = self.split_sentences(text)\n        extracted_sentences = []\n        evidence_records = []\n        \n        # Process each sentence\n        for i, sentence in enumerate(sentences):\n            # Skip empty sentences\n            if not sentence.strip():\n                continue\n            \n            # Build context for this sentence\n            context = self.build_context(sentences, i, window_size)\n            \n            # Extract knowledge from sentence\n            extracted = await self.extract_sentence(sentence, context)\n            extracted_sentences.append(extracted)\n            \n            # Create evidence record if triples were extracted\n            if extracted.triples and extracted.confidence > 0:\n                evidence = Evidence(\n                    quote=sentence,\n                    source_uri=source_uri,\n                    extractor_id=\"sentence_llm\",\n                    confidence=extracted.confidence,\n                    context={\"sentence_id\": extracted.sentence_id}\n                )\n                evidence_records.append(evidence)\n        \n        return extracted_sentences, evidence_records\n\n\n#=============================================================================\n# File: process_conversations.py\n# Description: Process conversations file for knowledge extraction\n# Improvement Area: Extraction Pipeline\n#=============================================================================\n#!/usr/bin/env python3\n\"\"\"Process conversations file for knowledge extraction\"\"\"\nimport os\nimport sys\nimport json\nimport asyncio\nimport logging\nimport argparse\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Tuple\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, \n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(\"conversation_extractor\")\n\n# Sample extraction components (for demonstration)\nclass SimpleExtractor:\n    \"\"\"Simple extraction without dependencies\"\"\"\n    \n    def __init__(self, api_key: Optional[str] = None):\n        \"\"\"Initialize with API key if available\"\"\"\n        self.api_key = api_key\n        self.has_api = api_key is not None\n        \n    async def extract_from_text(self, text: str, source_uri: str) -> Dict[str, Any]:\n        \"\"\"Extract knowledge from text\n        \n        Args:\n            text: Text to extract from\n            source_uri: URI of the source\n            \n        Returns:\n            Extraction results\n        \"\"\"\n        # Simplified extraction process\n        sentences = text.split('.')\n        entities = []\n        triples = []\n        \n        # Extract entities and relationships using basic NLP techniques\n        entity_candidates = set()\n        for sentence in sentences:\n            words = sentence.strip().split()\n            for i, word in enumerate(words):\n                if word and word[0].isupper() and len(word) > 3:\n                    entity_candidates.add(word.strip(',.;:!?()[]{}'))\n        \n        # Convert entity candidates to entity objects\n        for i, entity in enumerate(entity_candidates):\n            entities.append({\n                \"entity_id\": f\"entity_{i}\",\n                \"label\": entity,\n                \"entity_type\": \"Concept\"\n            })\n        \n        # Create triples based on co-occurrence in sentences\n        entity_list = list(entity_candidates)\n        for i, sentence in enumerate(sentences):\n            sentence = sentence.strip()\n            if not sentence:\n                continue\n                \n            found_entities = []\n            for entity in entity_list:\n                if entity in sentence:\n                    found_entities.append(entity)\n            \n            # Create triples between co-occurring entities\n            for j in range(len(found_entities)):\n                for k in range(j+1, len(found_entities)):\n                    triples.append({\n                        \"subject_uri\": f\"urn:entity:{found_entities[j]}\",\n                        \"predicate\": \"related_to\",\n                        \"object_uri\": f\"urn:entity:{found_entities[k]}\",\n                        \"confidence\": 0.7,\n                        \"source_uri\": source_uri\n                    })\n        \n        # Return extraction results\n        return {\n            \"entities\": entities,\n            \"triples\": triples,\n            \"evidence\": [],\n            \"stats\": {\n                \"sentences_processed\": len(sentences),\n                \"triples_extracted\": len(triples),\n                \"entities_created\": len(entities),\n                \"entities_merged\": 0,\n                \"evidence_records\": 0,\n                \"caps_extracted\": 0,\n                \"questions_extracted\": 0,\n                \"insights_extracted\": 0,\n                \"tensions_extracted\": 0\n            }\n        }\n\nclass LLMExtractor:\n    \"\"\"LLM-based extraction using Anthropic API\"\"\"\n    \n    def __init__(self, api_key: str):\n        \"\"\"Initialize with API key\"\"\"\n        self.api_key = api_key\n        try:\n            import anthropic\n            self.client = anthropic.Anthropic(api_key=api_key)\n            logger.info(\"Initialized Anthropic client\")\n        except ImportError:\n            logger.error(\"Anthropic package not installed\")\n            self.client = None\n    \n    async def extract_from_text(self, text: str, source_uri: str) -> Dict[str, Any]:\n        \"\"\"Extract knowledge using Claude API\n        \n        Args:\n            text: Text to extract from\n            source_uri: URI of the source\n            \n        Returns:\n            Extraction results\n        \"\"\"\n        if not self.client:\n            logger.error(\"No Anthropic client available\")\n            return {\n                \"entities\": [],\n                \"triples\": [],\n                \"evidence\": [],\n                \"stats\": {}\n            }\n        \n        # Truncate text if too long\n        max_length = 5000\n        if len(text) > max_length:\n            logger.info(f\"Truncating text from {len(text)} to {max_length} characters\")\n            text = text[:max_length]\n        \n        # Create prompt for extraction\n        prompt = f\"\"\"\n        Extract structured knowledge from the following text. Identify entities, relationships, and key insights.\n        \n        Text to analyze: {text}\n        \n        Return a JSON object with the following structure:\n        {{\n            \"entities\": [\n                {{\n                    \"label\": \"Entity name\",\n                    \"entity_type\": \"Person|Organization|Concept|Other\",\n                    \"attributes\": {{\"key\": \"value\"}}\n                }}\n            ],\n            \"triples\": [\n                {{\n                    \"subject\": \"Subject entity\",\n                    \"predicate\": \"relationship type\",\n                    \"object\": \"Object entity or value\",\n                    \"confidence\": 0.9\n                }}\n            ],\n            \"patterns\": {{\n                \"caps\": [\"Context-Action-Pattern extracted from text\"],\n                \"questions\": [\"Question extracted from text\"],\n                \"insights\": [\"Insight extracted from text\"],\n                \"tensions\": [\"Tension between X and Y extracted from text\"]\n            }}\n        }}\n        \n        Only return the JSON, no other text.\n        \"\"\"\n        \n        try:\n            # Call Claude API\n            response = self.client.messages.create(\n                model=\"claude-3-sonnet-20240229\",\n                max_tokens=4000,\n                temperature=0.0,\n                system=\"You are a knowledge extraction AI that extracts structured information from text.\",\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt}\n                ]\n            )\n            \n            # Extract JSON from response\n            response_text = response.content[0].text\n            \n            # Try to parse JSON\n            try:\n                import re\n                import json\n                \n                # Find JSON in response\n                json_match = re.search(r'({.*})', response_text, re.DOTALL)\n                if json_match:\n                    extracted_data = json.loads(json_match.group(1))\n                else:\n                    logger.error(\"No JSON found in response\")\n                    return {\n                        \"entities\": [],\n                        \"triples\": [],\n                        \"evidence\": [],\n                        \"stats\": {}\n                    }\n                \n                # Convert to standard format\n                entities = []\n                for i, entity in enumerate(extracted_data.get(\"entities\", [])):\n                    entities.append({\n                        \"entity_id\": f\"entity_{i}\",\n                        \"label\": entity[\"label\"],\n                        \"entity_type\": entity.get(\"entity_type\", \"Concept\"),\n                        \"attributes\": entity.get(\"attributes\", {})\n                    })\n                \n                triples = []\n                for i, triple in enumerate(extracted_data.get(\"triples\", [])):\n                    triples.append({\n                        \"subject_uri\": f\"urn:entity:{triple['subject']}\",\n                        \"predicate\": triple[\"predicate\"],\n                        \"object_uri\": f\"urn:entity:{triple['object']}\",\n                        \"confidence\": triple.get(\"confidence\", 0.8),\n                        \"source_uri\": source_uri\n                    })\n                \n                # Calculate statistics\n                patterns = extracted_data.get(\"patterns\", {})\n                \n                return {\n                    \"entities\": entities,\n                    \"triples\": triples,\n                    \"evidence\": [],\n                    \"stats\": {\n                        \"sentences_processed\": len(text.split('.')),\n                        \"triples_extracted\": len(triples),\n                        \"entities_created\": len(entities),\n                        \"entities_merged\": 0,\n                        \"evidence_records\": 0,\n                        \"caps_extracted\": len(patterns.get(\"caps\", [])),\n                        \"questions_extracted\": len(patterns.get(\"questions\", [])),\n                        \"insights_extracted\": len(patterns.get(\"insights\", [])),\n                        \"tensions_extracted\": len(patterns.get(\"tensions\", []))\n                    }\n                }\n                \n            except json.JSONDecodeError:\n                logger.error(\"Failed to parse JSON from response\")\n                return {\n                    \"entities\": [],\n                    \"triples\": [],\n                    \"evidence\": [],\n                    \"stats\": {}\n                }\n                \n        except Exception as e:\n            logger.error(f\"Error calling Claude API: {e}\")\n            return {\n                \"entities\": [],\n                \"triples\": [],\n                \"evidence\": [],\n                \"stats\": {}\n            }\n\ndef extract_text_from_conversation(conversation: Dict[str, Any]) -> str:\n    \"\"\"Extract text from a conversation\n    \n    Args:\n        conversation: Conversation data from JSON file\n        \n    Returns:\n        Extracted text from messages\n    \"\"\"\n    text = []\n    \n    # Add conversation name\n    if \"name\" in conversation:\n        text.append(f\"Conversation: {conversation['name']}\")\n    \n    # Extract messages\n    for message in conversation.get(\"chat_messages\", []):\n        sender = message.get(\"sender\", \"unknown\")\n        \n        # Get message content\n        message_text = \"\"\n        \n        # Try direct text field\n        if \"text\" in message:\n            message_text = message[\"text\"]\n        \n        # Try content list\n        elif \"content\" in message:\n            for content_item in message[\"content\"]:\n                if content_item.get(\"type\") == \"text\":\n                    message_text += content_item.get(\"text\", \"\")\n        \n        if message_text:\n            text.append(f\"{sender}: {message_text}\")\n    \n    return \"\\n\\n\".join(text)\n\nasync def process_conversation(conversation: Dict[str, Any], extractor, output_dir: str) -> Dict[str, Any]:\n    \"\"\"Process a single conversation\n    \n    Args:\n        conversation: Conversation data from JSON file\n        extractor: Extractor to use\n        output_dir: Directory to save results\n        \n    Returns:\n        Extraction results\n    \"\"\"\n    # Extract conversation ID and name\n    conversation_id = conversation.get(\"uuid\", \"unknown\")\n    conversation_name = conversation.get(\"name\", \"Unnamed Conversation\")\n    \n    logger.info(f\"Processing conversation: {conversation_name}\")\n    \n    # Extract text from conversation\n    text = extract_text_from_conversation(conversation)\n    \n    # Create source URI\n    source_uri = f\"urn:conversation:{conversation_id}\"\n    \n    # Extract knowledge\n    extraction_results = await extractor.extract_from_text(text, source_uri)\n    \n    # Save results to file\n    output_file = os.path.join(output_dir, f\"extraction_{conversation_id}.json\")\n    with open(output_file, 'w') as f:\n        json.dump(extraction_results, f, indent=2)\n    \n    logger.info(f\"Saved extraction results to {output_file}\")\n    \n    return extraction_results\n\nasync def process_conversations_file(file_path: str, api_key: Optional[str], limit: int, output_dir: str) -> List[Dict[str, Any]]:\n    \"\"\"Process conversations from JSON file\n    \n    Args:\n        file_path: Path to conversations JSON file\n        api_key: API key for LLM service\n        limit: Maximum number of conversations to process\n        output_dir: Directory to save results\n        \n    Returns:\n        List of extraction results\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Load conversations from file\n    logger.info(f\"Loading conversations from {file_path}\")\n    with open(file_path, 'r') as f:\n        conversations = json.load(f)\n    \n    logger.info(f\"Loaded {len(conversations)} conversations\")\n    \n    # Limit the number of conversations if specified\n    if limit > 0 and limit < len(conversations):\n        logger.info(f\"Limiting to {limit} conversations\")\n        conversations = conversations[:limit]\n    \n    # Choose extractor based on API key\n    if api_key:\n        extractor = LLMExtractor(api_key)\n    else:\n        extractor = SimpleExtractor()\n    \n    # Process each conversation\n    results = []\n    for i, conversation in enumerate(conversations):\n        logger.info(f\"Processing conversation {i+1}/{len(conversations)}\")\n        result = await process_conversation(conversation, extractor, output_dir)\n        results.append(result)\n    \n    # Generate summary of results\n    total_triples = sum(r.get(\"stats\", {}).get(\"triples_extracted\", 0) for r in results)\n    total_entities = sum(r.get(\"stats\", {}).get(\"entities_created\", 0) for r in results)\n    \n    logger.info(f\"Processed {len(results)} conversations\")\n    logger.info(f\"Total triples extracted: {total_triples}\")\n    logger.info(f\"Total entities created: {total_entities}\")\n    \n    # Save summary to file\n    summary = {\n        \"conversations_processed\": len(results),\n        \"total_triples\": total_triples,\n        \"total_entities\": total_entities,\n        \"conversation_stats\": [r.get(\"stats\", {}) for r in results]\n    }\n    \n    summary_file = os.path.join(output_dir, \"extraction_summary.json\")\n    with open(summary_file, 'w') as f:\n        json.dump(summary, f, indent=2)\n    \n    logger.info(f\"Saved summary to {summary_file}\")\n    \n    return results\n\nasync def main_async():\n    \"\"\"Async main entry point\"\"\"\n    parser = argparse.ArgumentParser(description=\"Process conversations for knowledge extraction\")\n    parser.add_argument(\"--file\", type=str, required=True, help=\"Path to conversations JSON file\")\n    parser.add_argument(\"--api-key\", type=str, help=\"API key for LLM service\")\n    parser.add_argument(\"--limit\", type=int, default=5, help=\"Maximum number of conversations to process\")\n    parser.add_argument(\"--output-dir\", type=str, default=\"extraction_results\", help=\"Directory to save results\")\n    \n    args = parser.parse_args()\n    \n    # Process conversations\n    await process_conversations_file(args.file, args.api_key, args.limit, args.output_dir)\n    \n    logger.info(\"Processing complete\")\n    return 0\n\ndef main():\n    \"\"\"Command-line entry point\"\"\"\n    try:\n        return asyncio.run(main_async())\n    except Exception as e:\n        logger.error(f\"Error processing conversations: {e}\")\n        import traceback\n        traceback.print_exc()\n        return 1\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n\n\n\n#================================================================================\n# IMPROVEMENT AREA: Models\n#================================================================================\n\n\n#=============================================================================\n# File: models/evidence.py\n# Description: Evidence model for tracking provenance of extracted knowledge\n# Improvement Area: Models\n#=============================================================================\n\"\"\"Evidence model for tracking provenance of extracted knowledge\"\"\"\nfrom typing import Dict, Any, Optional, List\nfrom datetime import datetime\nfrom uuid import uuid4\n\nclass Evidence:\n    \"\"\"Evidence for an extracted knowledge item\n    \n    Tracks the source of an extracted triple, entity, or relationship,\n    including the exact quote, confidence, and extraction method.\n    \"\"\"\n    \n    def __init__(\n        self,\n        quote: str,\n        source_uri: str,\n        extractor_id: str,\n        confidence: float,\n        char_offset: Optional[int] = None,\n        extraction_time: Optional[datetime] = None,\n        evidence_id: Optional[str] = None,\n        context: Optional[Dict[str, Any]] = None,\n        metadata: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"Initialize evidence record\n        \n        Args:\n            quote: The exact text that supports this extraction (1-3 sentences)\n            source_uri: URI of the source document/conversation\n            extractor_id: ID of the extraction method (e.g., \"llm\", \"pattern\", \"ner\")\n            confidence: Confidence score (0-1)\n            char_offset: Character offset in source document (if available)\n            extraction_time: When this evidence was created\n            evidence_id: Unique ID for this evidence\n            context: Additional context information (e.g., surrounding sentences)\n            metadata: Additional metadata about extraction\n        \"\"\"\n        self.quote = quote\n        self.source_uri = source_uri\n        self.extractor_id = extractor_id\n        self.confidence = confidence\n        self.char_offset = char_offset\n        self.extraction_time = extraction_time or datetime.now()\n        self.evidence_id = evidence_id or f\"evidence_{uuid4().hex[:8]}\"\n        self.context = context or {}\n        self.metadata = metadata or {}\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary representation\"\"\"\n        return {\n            \"evidence_id\": self.evidence_id,\n            \"quote\": self.quote,\n            \"source_uri\": self.source_uri,\n            \"extractor_id\": self.extractor_id,\n            \"confidence\": self.confidence,\n            \"char_offset\": self.char_offset,\n            \"extraction_time\": self.extraction_time.isoformat(),\n            \"context\": self.context,\n            \"metadata\": self.metadata\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Evidence':\n        \"\"\"Create from dictionary representation\"\"\"\n        # Parse extraction time\n        extraction_time = None\n        if data.get(\"extraction_time\"):\n            extraction_time = datetime.fromisoformat(data[\"extraction_time\"])\n        \n        return cls(\n            quote=data[\"quote\"],\n            source_uri=data[\"source_uri\"],\n            extractor_id=data[\"extractor_id\"],\n            confidence=data[\"confidence\"],\n            char_offset=data.get(\"char_offset\"),\n            extraction_time=extraction_time,\n            evidence_id=data.get(\"evidence_id\"),\n            context=data.get(\"context\", {}),\n            metadata=data.get(\"metadata\", {})\n        )\n    \n    def to_rdf_triples(self, target_uri: str, base_uri: str) -> List[Dict[str, Any]]:\n        \"\"\"Convert to RDF triple dictionaries for storage\n        \n        Args:\n            target_uri: URI of the entity/triple this evidence supports\n            base_uri: Base URI for evidence entities\n        \"\"\"\n        evidence_uri = f\"{base_uri}/evidence/{self.evidence_id}\"\n        \n        triples = [\n            {\n                \"subject\": evidence_uri,\n                \"predicate\": \"rdf:type\",\n                \"object\": \"kg:Evidence\",\n                \"is_literal\": False\n            },\n            {\n                \"subject\": target_uri,\n                \"predicate\": \"kg:hasEvidence\",\n                \"object\": evidence_uri,\n                \"is_literal\": False\n            },\n            {\n                \"subject\": evidence_uri,\n                \"predicate\": \"kg:quote\",\n                \"object\": self.quote,\n                \"is_literal\": True\n            },\n            {\n                \"subject\": evidence_uri,\n                \"predicate\": \"kg:sourceDocument\",\n                \"object\": self.source_uri,\n                \"is_literal\": False\n            },\n            {\n                \"subject\": evidence_uri,\n                \"predicate\": \"kg:extractorId\",\n                \"object\": self.extractor_id,\n                \"is_literal\": True\n            },\n            {\n                \"subject\": evidence_uri,\n                \"predicate\": \"kg:confidence\",\n                \"object\": str(self.confidence),\n                \"is_literal\": True,\n                \"datatype\": \"xsd:decimal\"\n            },\n            {\n                \"subject\": evidence_uri,\n                \"predicate\": \"kg:extractionTime\",\n                \"object\": self.extraction_time.isoformat(),\n                \"is_literal\": True,\n                \"datatype\": \"xsd:dateTime\"\n            }\n        ]\n        \n        # Add character offset if available\n        if self.char_offset is not None:\n            triples.append({\n                \"subject\": evidence_uri,\n                \"predicate\": \"kg:characterOffset\",\n                \"object\": str(self.char_offset),\n                \"is_literal\": True,\n                \"datatype\": \"xsd:integer\"\n            })\n        \n        # Add context information\n        for key, value in self.context.items():\n            if isinstance(value, str):\n                triples.append({\n                    \"subject\": evidence_uri,\n                    \"predicate\": f\"kg:context_{key}\",\n                    \"object\": value,\n                    \"is_literal\": True\n                })\n        \n        # Add metadata\n        for key, value in self.metadata.items():\n            if isinstance(value, str):\n                triples.append({\n                    \"subject\": evidence_uri,\n                    \"predicate\": f\"kg:meta_{key}\",\n                    \"object\": value,\n                    \"is_literal\": True\n                })\n            elif isinstance(value, (int, float)):\n                triples.append({\n                    \"subject\": evidence_uri,\n                    \"predicate\": f\"kg:meta_{key}\",\n                    \"object\": str(value),\n                    \"is_literal\": True,\n                    \"datatype\": \"xsd:decimal\"\n                })\n        \n        return triples\n\n\n#=============================================================================\n# File: models/extracted_sentence.py\n# Description: Model for extracted sentences with embeddings\n# Improvement Area: Models\n#=============================================================================\n\"\"\"Model for extracted sentences with embeddings\"\"\"\nfrom typing import List, Dict, Any, Optional\nimport base64\nimport numpy as np\nfrom datetime import datetime\nfrom uuid import uuid4\n\nclass ExtractedSentence:\n    \"\"\"Container for extracted sentence with embedding and triples\"\"\"\n    \n    def __init__(\n        self,\n        text: str,\n        embedding: Optional[np.ndarray] = None,\n        triples: Optional[List[Dict[str, Any]]] = None,\n        confidence: float = 0.0,\n        context: Optional[Dict[str, Any]] = None,\n        extraction_time: Optional[datetime] = None,\n        sentence_id: Optional[str] = None\n    ):\n        \"\"\"Initialize an extracted sentence\n        \n        Args:\n            text: The sentence text\n            embedding: Vector representation of the sentence\n            triples: Extracted triple data from the sentence\n            confidence: Confidence score for extraction (0-1)\n            context: Additional context info (surrounding sentences, etc.)\n            extraction_time: When this sentence was processed\n            sentence_id: Unique ID for this sentence\n        \"\"\"\n        self.text = text\n        self.embedding = embedding\n        self.triples = triples or []\n        self.confidence = confidence\n        self.context = context or {}\n        self.extraction_time = extraction_time or datetime.now()\n        self.sentence_id = sentence_id or f\"sentence_{uuid4().hex[:8]}\"\n    \n    def encode_embedding(self) -> str:\n        \"\"\"Encode embedding as Base64 string for RDF storage\"\"\"\n        if self.embedding is None:\n            return \"\"\n        \n        # Convert numpy array to bytes and encode as base64\n        embedding_bytes = self.embedding.tobytes()\n        encoded = base64.b64encode(embedding_bytes).decode('ascii')\n        return encoded\n    \n    @classmethod\n    def decode_embedding(cls, encoded_str: str, shape: tuple = (768,)) -> np.ndarray:\n        \"\"\"Decode Base64 string back to numpy array\"\"\"\n        if not encoded_str:\n            return None\n        \n        # Decode base64 to bytes and convert to numpy array\n        embedding_bytes = base64.b64decode(encoded_str)\n        embedding = np.frombuffer(embedding_bytes, dtype=np.float32).reshape(shape)\n        return embedding\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary representation\"\"\"\n        return {\n            \"sentence_id\": self.sentence_id,\n            \"text\": self.text,\n            \"embedding\": self.encode_embedding() if self.embedding is not None else None,\n            \"triples\": self.triples,\n            \"confidence\": self.confidence,\n            \"context\": self.context,\n            \"extraction_time\": self.extraction_time.isoformat()\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'ExtractedSentence':\n        \"\"\"Create from dictionary representation\"\"\"\n        # Handle embedding conversion\n        embedding = None\n        if data.get(\"embedding\"):\n            embedding = cls.decode_embedding(data[\"embedding\"])\n        \n        # Parse extraction time\n        extraction_time = None\n        if data.get(\"extraction_time\"):\n            extraction_time = datetime.fromisoformat(data[\"extraction_time\"])\n        \n        return cls(\n            text=data[\"text\"],\n            embedding=embedding,\n            triples=data.get(\"triples\", []),\n            confidence=data.get(\"confidence\", 0.0),\n            context=data.get(\"context\", {}),\n            extraction_time=extraction_time,\n            sentence_id=data.get(\"sentence_id\")\n        )\n    \n    def to_rdf_triples(self, base_uri: str) -> List[Dict[str, Any]]:\n        \"\"\"Convert to RDF triple dictionaries for storage\"\"\"\n        sentence_uri = f\"{base_uri}/sentence/{self.sentence_id}\"\n        \n        triples = [\n            {\n                \"subject\": sentence_uri,\n                \"predicate\": \"rdf:type\",\n                \"object\": \"kg:Sentence\",\n                \"is_literal\": False\n            },\n            {\n                \"subject\": sentence_uri,\n                \"predicate\": \"kg:text\",\n                \"object\": self.text,\n                \"is_literal\": True\n            },\n            {\n                \"subject\": sentence_uri,\n                \"predicate\": \"kg:extractionConfidence\",\n                \"object\": str(self.confidence),\n                \"is_literal\": True,\n                \"datatype\": \"xsd:decimal\"\n            },\n            {\n                \"subject\": sentence_uri,\n                \"predicate\": \"kg:extractionTime\",\n                \"object\": self.extraction_time.isoformat(),\n                \"is_literal\": True,\n                \"datatype\": \"xsd:dateTime\"\n            }\n        ]\n        \n        # Add embedding if available\n        if self.embedding is not None:\n            triples.append({\n                \"subject\": sentence_uri,\n                \"predicate\": \"kg:embedding\",\n                \"object\": self.encode_embedding(),\n                \"is_literal\": True,\n                \"datatype\": \"xsd:base64Binary\"\n            })\n        \n        # Add context information\n        for key, value in self.context.items():\n            if isinstance(value, str):\n                triples.append({\n                    \"subject\": sentence_uri,\n                    \"predicate\": f\"kg:context_{key}\",\n                    \"object\": value,\n                    \"is_literal\": True\n                })\n        \n        # Add extracted triple references\n        for i, triple_data in enumerate(self.triples):\n            triple_uri = f\"{sentence_uri}/triple/{i}\"\n            \n            # Add triple entity\n            triples.append({\n                \"subject\": triple_uri,\n                \"predicate\": \"rdf:type\", \n                \"object\": \"kg:ExtractedTriple\",\n                \"is_literal\": False\n            })\n            \n            # Link to sentence\n            triples.append({\n                \"subject\": sentence_uri,\n                \"predicate\": \"kg:hasTriple\",\n                \"object\": triple_uri,\n                \"is_literal\": False\n            })\n            \n            # Add triple properties\n            for key, value in triple_data.items():\n                if isinstance(value, str):\n                    triples.append({\n                        \"subject\": triple_uri,\n                        \"predicate\": f\"kg:{key}\",\n                        \"object\": value,\n                        \"is_literal\": True\n                    })\n                elif isinstance(value, (int, float)):\n                    triples.append({\n                        \"subject\": triple_uri,\n                        \"predicate\": f\"kg:{key}\",\n                        \"object\": str(value),\n                        \"is_literal\": True,\n                        \"datatype\": \"xsd:decimal\"\n                    })\n        \n        return triples\n\n\n#=============================================================================\n# File: models/triple.py\n# Description: Triple data model for RDF\n# Improvement Area: Models\n#=============================================================================\n\"\"\"Triple data model for RDF\"\"\"\nimport re\nfrom typing import Optional\n\n\nclass Triple:\n    \"\"\"RDF Triple for knowledge graph\"\"\"\n    \n    def __init__(self, subject: str, predicate: str, object_: str, \n                is_literal: bool = False, datatype: Optional[str] = None,\n                language: Optional[str] = None):\n        \"\"\"Initialize a triple\n        \n        Args:\n            subject: The subject URI\n            predicate: The predicate URI\n            object_: The object URI or literal value\n            is_literal: Whether the object is a literal value (not a URI)\n            datatype: Optional datatype URI for literals\n            language: Optional language tag for literals\n        \"\"\"\n        # Remove any wrapping angle brackets\n        self.subject = subject.strip('<>')\n        self.predicate = predicate.strip('<>')\n        \n        if not is_literal:\n            self.object = object_.strip('<>')\n        else:\n            self.object = object_\n            \n        self.is_literal = is_literal\n        \n        # Clean up datatype if provided\n        if datatype:\n            self.datatype = datatype.strip('<>')\n        else:\n            self.datatype = None\n            \n        self.language = language\n        \n    def to_sparql(self) -> str:\n        \"\"\"Convert to SPARQL representation\"\"\"\n        obj = self._format_object()\n        return f\"<{self.subject}> <{self.predicate}> {obj}\"\n    \n    def _format_object(self) -> str:\n        \"\"\"Format the object for SPARQL\"\"\"\n        if not self.is_literal:\n            return f\"<{self.object}>\"\n        \n        # Escape special characters in literals\n        escaped_obj = self._escape_string(self.object)\n        \n        # Handle literal with datatype\n        if self.datatype:\n            return f'\"{escaped_obj}\"^^<{self.datatype}>'\n        \n        # Handle literal with language tag\n        if self.language:\n            return f'\"{escaped_obj}\"@{self.language}'\n        \n        # Simple literal\n        return f'\"{escaped_obj}\"'\n    \n    def _escape_string(self, s: str) -> str:\n        \"\"\"Escape a string for SPARQL\"\"\"\n        if s is None:\n            return \"\"\n            \n        # Replace backslashes first to avoid double escaping\n        s = s.replace('\\\\', '\\\\\\\\')\n        \n        # Replace quotes and control characters\n        s = s.replace('\"', '\\\\\"')\n        s = s.replace('\\n', '\\\\n')\n        s = s.replace('\\r', '\\\\r')\n        s = s.replace('\\t', '\\\\t')\n        \n        return s\n    \n    def __str__(self) -> str:\n        \"\"\"String representation\"\"\"\n        return self.to_sparql()\n        \n    def __repr__(self) -> str:\n        \"\"\"Representation\"\"\"\n        return f\"Triple({self.subject}, {self.predicate}, {self.object}, {self.is_literal})\"\n\n\n\n#================================================================================\n# IMPROVEMENT AREA: Prompt Design\n#================================================================================\n\n\n#=============================================================================\n# File: extraction/llm_extractor.py\n# Description: LLM-based Extractor for enhanced extraction\n# Improvement Area: Prompt Design\n#=============================================================================\n\"\"\"LLM-based Extractor for enhanced extraction\"\"\"\nimport logging\nimport json\nfrom typing import List, Dict, Any, Optional\n\nlogger = logging.getLogger(__name__)\n\nclass LLMExtractor:\n    \"\"\"Extract elements using Large Language Models\"\"\"\n    \n    def __init__(self, api_key: Optional[str] = None):\n        \"\"\"Initialize the LLM extractor\n        \n        Args:\n            api_key: Anthropic API key\n        \"\"\"\n        self.api_key = api_key\n        self.client = None\n        \n        if api_key:\n            try:\n                import anthropic\n                self.client = anthropic.Anthropic(api_key=api_key)\n                logger.info(\"Initialized Anthropic client\")\n            except ImportError:\n                logger.warning(\"Anthropic package not installed, LLM extraction disabled\")\n                self.api_key = None\n    \n    def extract(self, text: str, elements):\n        \"\"\"Extract elements from text using LLM\n        \n        Args:\n            text: The text to analyze\n            elements: ExtractedElements instance to add results to\n        \"\"\"\n        if not self.client:\n            logger.warning(\"No LLM client available, skipping LLM extraction\")\n            return\n            \n        # Truncate text if too long\n        if len(text) > 8000:\n            logger.info(f\"Truncating text from {len(text)} to 8000 characters\")\n            text = text[:8000]\n            \n        # Extract insights\n        self._extract_insights(text, elements)\n        \n        # Extract questions\n        self._extract_questions(text, elements)\n        \n        # Extract tensions\n        self._extract_tensions(text, elements)\n            \n    def _extract_insights(self, text: str, elements):\n        \"\"\"Extract insights using LLM\"\"\"\n        try:\n            prompt = f\"\"\"\n            Please analyze the following text and identify key insights or realizations.\n            For each insight, extract:\n            1. The realization or insight\n            2. The evidence or source text\n            \n            Format your response as a JSON array with the following structure:\n            [\n                {{\n                    \"realization\": \"The insight or realization\",\n                    \"evidence\": \"The source text that supports this insight\",\n                    \"confidence\": 0.9 // Your confidence in this extraction (0-1)\n                }}\n            ]\n            \n            Text to analyze:\n            {text}\n            \n            Return ONLY the JSON array, with no additional explanation.\n            \"\"\"\n            \n            response = self._generate(prompt, max_tokens=1000)\n            if not response:\n                return\n                \n            insights = self._parse_json_response(response)\n            if not insights:\n                return\n                \n            for insight in insights:\n                if 'realization' in insight:\n                    elements.add_insight(\n                        insight.get('realization', ''),\n                        insight.get('evidence', ''),\n                        insight.get('confidence', 0.7)\n                    )\n                    \n        except Exception as e:\n            logger.error(f\"Error extracting insights with LLM: {e}\")\n    \n    def _extract_questions(self, text: str, elements):\n        \"\"\"Extract questions using LLM\"\"\"\n        try:\n            prompt = f\"\"\"\n            Please analyze the following text and identify important questions that are raised.\n            For each question, extract:\n            1. The question\n            2. The surrounding context\n            \n            Format your response as a JSON array with the following structure:\n            [\n                {{\n                    \"text\": \"The question\",\n                    \"context\": \"The surrounding context\",\n                    \"confidence\": 0.9 // Your confidence in this extraction (0-1)\n                }}\n            ]\n            \n            Text to analyze:\n            {text}\n            \n            Return ONLY the JSON array, with no additional explanation.\n            \"\"\"\n            \n            response = self._generate(prompt, max_tokens=1000)\n            if not response:\n                return\n                \n            questions = self._parse_json_response(response)\n            if not questions:\n                return\n                \n            for question in questions:\n                if 'text' in question:\n                    elements.add_question(\n                        question.get('text', ''),\n                        question.get('context', ''),\n                        question.get('confidence', 0.7)\n                    )\n                    \n        except Exception as e:\n            logger.error(f\"Error extracting questions with LLM: {e}\")\n    \n    def _extract_tensions(self, text: str, elements):\n        \"\"\"Extract tensions using LLM\"\"\"\n        try:\n            prompt = f\"\"\"\n            Please analyze the following text and identify tensions or opposing forces.\n            For each tension, extract:\n            1. The first pole or concept\n            2. The second pole or opposing concept\n            3. The context or evidence\n            \n            Format your response as a JSON array with the following structure:\n            [\n                {{\n                    \"pole1\": \"First concept\",\n                    \"pole2\": \"Second concept\",\n                    \"evidence\": \"The context or evidence\",\n                    \"confidence\": 0.9 // Your confidence in this extraction (0-1)\n                }}\n            ]\n            \n            Text to analyze:\n            {text}\n            \n            Return ONLY the JSON array, with no additional explanation.\n            \"\"\"\n            \n            response = self._generate(prompt, max_tokens=1000)\n            if not response:\n                return\n                \n            tensions = self._parse_json_response(response)\n            if not tensions:\n                return\n                \n            for tension in tensions:\n                if 'pole1' in tension and 'pole2' in tension:\n                    elements.add_tension(\n                        tension.get('pole1', ''),\n                        tension.get('pole2', ''),\n                        tension.get('evidence', ''),\n                        tension.get('confidence', 0.7)\n                    )\n                    \n        except Exception as e:\n            logger.error(f\"Error extracting tensions with LLM: {e}\")\n    \n    def _generate(self, prompt: str, max_tokens: int = 1000) -> Optional[str]:\n        \"\"\"Generate text using LLM\"\"\"\n        if not self.client:\n            return None\n            \n        try:\n            response = self.client.messages.create(\n                model=\"claude-3-haiku-20240307\",\n                max_tokens=max_tokens,\n                temperature=0.1,\n                system=\"You are an expert at extracting structured information from text.\",\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            return response.content[0].text\n        except Exception as e:\n            logger.error(f\"Error generating with LLM: {e}\")\n            return None\n    \n    def _parse_json_response(self, response: str) -> List[Dict[str, Any]]:\n        \"\"\"Parse JSON from LLM response\"\"\"\n        try:\n            # Find JSON array in response\n            start_idx = response.find('[')\n            end_idx = response.rfind(']') + 1\n            \n            if start_idx == -1 or end_idx == 0:\n                # Try to find JSON object instead\n                start_idx = response.find('{')\n                end_idx = response.rfind('}') + 1\n                \n                if start_idx == -1 or end_idx == 0:\n                    logger.warning(\"No JSON found in LLM response\")\n                    return []\n                    \n                # Parse single JSON object and wrap in list\n                json_str = response[start_idx:end_idx]\n                obj = json.loads(json_str)\n                return [obj]\n                \n            # Parse JSON array\n            json_str = response[start_idx:end_idx]\n            return json.loads(json_str)\n            \n        except json.JSONDecodeError as e:\n            logger.error(f\"Error parsing JSON from LLM response: {e}\")\n            return []\n\n\n#=============================================================================\n# File: extraction/prompt_manager.py\n# Description: Prompt management for LLM-based extraction\n# Improvement Area: Prompt Design\n#=============================================================================\n\"\"\"Prompt management for LLM-based extraction\"\"\"\nimport os\nimport logging\nfrom typing import Dict, Any, Optional, List, Callable\nimport json\nimport yaml\n\nfrom knowledge_garden.extraction.prompts import (\n    load_prompt, \n    format_prompt, \n    get_prompt_parameters, \n    get_output_schema\n)\n\nlogger = logging.getLogger(__name__)\n\nclass PromptManager:\n    \"\"\"Manages prompts for LLM-based extraction\n    \n    Handles prompt loading, formatting, validation, and caching for\n    efficient and consistent LLM interactions during extraction.\n    \"\"\"\n    \n    def __init__(\n        self, \n        llm_service: Any, \n        cache_enabled: bool = True,\n        custom_prompt_dir: Optional[str] = None\n    ):\n        \"\"\"Initialize the prompt manager\n        \n        Args:\n            llm_service: Service for LLM interactions\n            cache_enabled: Whether to cache prompt results\n            custom_prompt_dir: Optional path to custom prompts\n        \"\"\"\n        self.llm = llm_service\n        self.cache_enabled = cache_enabled\n        self.custom_prompt_dir = custom_prompt_dir\n        \n        # Cache for prompt results\n        self.result_cache: Dict[str, Any] = {}\n        \n        # Register available prompt types\n        self.prompt_types = {\n            \"entity_resolution\": self._handle_entity_resolution,\n            \"sentence_extraction\": self._handle_sentence_extraction\n        }\n    \n    async def run_prompt(\n        self, \n        prompt_type: str, \n        variables: Dict[str, Any],\n        cache_key: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Run a prompt with variables\n        \n        Args:\n            prompt_type: Type of prompt to run\n            variables: Variables to substitute in the prompt\n            cache_key: Optional key for caching results\n            \n        Returns:\n            Structured result from prompt\n        \"\"\"\n        # Check cache if enabled\n        if self.cache_enabled and cache_key and cache_key in self.result_cache:\n            logger.debug(f\"Using cached result for {prompt_type} with key {cache_key}\")\n            return self.result_cache[cache_key]\n            \n        # Check if prompt type is supported\n        if prompt_type not in self.prompt_types:\n            logger.error(f\"Unsupported prompt type: {prompt_type}\")\n            return {\"error\": f\"Unsupported prompt type: {prompt_type}\"}\n            \n        # Run prompt handler\n        handler = self.prompt_types[prompt_type]\n        result = await handler(variables)\n        \n        # Cache result if enabled\n        if self.cache_enabled and cache_key:\n            self.result_cache[cache_key] = result\n            \n        return result\n    \n    async def _handle_entity_resolution(self, variables: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Handle entity resolution prompt\n        \n        Args:\n            variables: Variables for prompt\n            \n        Returns:\n            Resolution result\n        \"\"\"\n        # Get and format prompt\n        prompt_text = format_prompt(\"entity_resolution\", **variables)\n        if not prompt_text:\n            return {\"error\": \"Failed to format entity resolution prompt\"}\n        \n        # Run LLM\n        response = await self.llm.generate(prompt_text)\n        \n        # Parse result\n        try:\n            # Find JSON in response\n            import re\n            json_match = re.search(r'{.*}', response, re.DOTALL)\n            \n            if json_match:\n                result = json.loads(json_match.group(0))\n                \n                # Validate against schema\n                schema = get_output_schema(\"entity_resolution\")\n                if not self._validate_against_schema(result, schema):\n                    logger.warning(\"Entity resolution result doesn't match schema\")\n                \n                # Map resolution to standard format\n                if \"resolution\" in result:\n                    resolution_mapping = {\n                        \"MATCH\": \"merge\",\n                        \"NO_MATCH\": \"keep_separate\",\n                        \"UNCERTAIN\": \"keep_separate\"\n                    }\n                    result[\"action\"] = resolution_mapping.get(\n                        result[\"resolution\"], \"keep_separate\"\n                    )\n                    \n                # Map target_id if needed\n                if \"recommended_label\" in result and variables.get(\"existing_entity_id\"):\n                    result[\"target_id\"] = variables[\"existing_entity_id\"]\n                \n                return result\n            else:\n                logger.warning(\"Could not find JSON in LLM response for entity resolution\")\n                return {\n                    \"action\": \"keep_separate\", \n                    \"confidence\": 0.5, \n                    \"reasoning\": \"Error parsing LLM response\"\n                }\n                \n        except json.JSONDecodeError as e:\n            logger.error(f\"Error parsing JSON from LLM response: {e}\")\n            return {\n                \"action\": \"keep_separate\", \n                \"confidence\": 0.5, \n                \"reasoning\": f\"JSON parse error: {str(e)}\"\n            }\n    \n    async def _handle_sentence_extraction(self, variables: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Handle sentence extraction prompt\n        \n        Args:\n            variables: Variables for prompt\n            \n        Returns:\n            Extraction result\n        \"\"\"\n        # Get and format prompt\n        prompt_text = format_prompt(\"sentence_extraction\", **variables)\n        if not prompt_text:\n            return {\"triples\": [], \"confidence\": 0.0, \"reasoning\": \"Failed to format prompt\"}\n        \n        # Run LLM\n        response = await self.llm.generate(prompt_text)\n        \n        # Parse result\n        try:\n            # Find JSON in response\n            import re\n            json_match = re.search(r'{.*}', response, re.DOTALL)\n            \n            if json_match:\n                result = json.loads(json_match.group(0))\n                \n                # Validate against schema\n                schema = get_output_schema(\"sentence_extraction\")\n                if not self._validate_against_schema(result, schema):\n                    logger.warning(\"Sentence extraction result doesn't match schema\")\n                \n                return result\n            else:\n                logger.warning(\"Could not find JSON in LLM response for sentence extraction\")\n                return {\"triples\": [], \"confidence\": 0.0, \"reasoning\": \"No valid JSON found in response\"}\n                \n        except json.JSONDecodeError as e:\n            logger.error(f\"Error parsing JSON from LLM response: {e}\")\n            return {\"triples\": [], \"confidence\": 0.0, \"reasoning\": f\"JSON parse error: {str(e)}\"}\n    \n    def _validate_against_schema(self, data: Dict[str, Any], schema: Dict[str, Any]) -> bool:\n        \"\"\"Validate data against JSON schema\n        \n        Args:\n            data: Data to validate\n            schema: JSON schema\n            \n        Returns:\n            True if valid, False otherwise\n        \"\"\"\n        # Simple validation for required fields\n        if \"required\" in schema:\n            for field in schema[\"required\"]:\n                if field not in data:\n                    logger.warning(f\"Missing required field in result: {field}\")\n                    return False\n        \n        # Validate field types\n        if \"properties\" in schema:\n            for field, field_schema in schema[\"properties\"].items():\n                if field in data:\n                    # Check type\n                    if \"type\" in field_schema:\n                        if field_schema[\"type\"] == \"number\" and not isinstance(data[field], (int, float)):\n                            logger.warning(f\"Field {field} should be a number\")\n                            return False\n                        elif field_schema[\"type\"] == \"string\" and not isinstance(data[field], str):\n                            logger.warning(f\"Field {field} should be a string\")\n                            return False\n                        elif field_schema[\"type\"] == \"array\" and not isinstance(data[field], list):\n                            logger.warning(f\"Field {field} should be an array\")\n                            return False\n                        elif field_schema[\"type\"] == \"object\" and not isinstance(data[field], dict):\n                            logger.warning(f\"Field {field} should be an object\")\n                            return False\n                    \n                    # Check enum values\n                    if \"enum\" in field_schema and data[field] not in field_schema[\"enum\"]:\n                        logger.warning(f\"Field {field} value not in allowed values: {field_schema['enum']}\")\n                        return False\n        \n        return True\n\n\n#=============================================================================\n# File: extraction/prompts/__init__.py\n# Description: Prompt templates for LLM-based extraction\n# Improvement Area: Prompt Design\n#=============================================================================\n\"\"\"Prompt templates for LLM-based extraction\n\nThis module provides YAML-based prompt templates for various extraction tasks:\n\n- entity_resolution.yaml: Prompts for resolving entity references\n- sentence_extraction.yaml: Prompts for extracting triples from sentences\n\nThe prompts are designed for use with the Claude API and include:\n- Detailed task descriptions\n- Examples and guidelines\n- Output schemas for validation\n\"\"\"\n\nimport os\nimport yaml\nimport logging\nfrom typing import Dict, Any, Optional\n\nlogger = logging.getLogger(__name__)\n\n# Directory containing prompt templates\n_PROMPT_DIR = os.path.dirname(os.path.abspath(__file__))\n\ndef load_prompt(prompt_name: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Load a prompt template from YAML file\n    \n    Args:\n        prompt_name: Name of the prompt (without .yaml extension)\n        \n    Returns:\n        Prompt template dictionary, or None if not found\n    \"\"\"\n    prompt_path = os.path.join(_PROMPT_DIR, f\"{prompt_name}.yaml\")\n    \n    if not os.path.exists(prompt_path):\n        logger.error(f\"Prompt file not found: {prompt_path}\")\n        return None\n        \n    try:\n        with open(prompt_path, 'r') as file:\n            prompt_data = yaml.safe_load(file)\n        return prompt_data\n    except Exception as e:\n        logger.error(f\"Error loading prompt {prompt_name}: {e}\")\n        return None\n\ndef get_template_text(prompt_name: str) -> Optional[str]:\n    \"\"\"Get the template text from a prompt\n    \n    Args:\n        prompt_name: Name of the prompt (without .yaml extension)\n        \n    Returns:\n        Template text, or None if not found\n    \"\"\"\n    prompt_data = load_prompt(prompt_name)\n    if not prompt_data or 'template' not in prompt_data:\n        return None\n        \n    return prompt_data['template']\n\ndef format_prompt(prompt_name: str, **kwargs) -> Optional[str]:\n    \"\"\"Format a prompt template with variables\n    \n    Args:\n        prompt_name: Name of the prompt (without .yaml extension)\n        **kwargs: Variables to substitute in the template\n        \n    Returns:\n        Formatted prompt text, or None on error\n    \"\"\"\n    template = get_template_text(prompt_name)\n    if not template:\n        return None\n        \n    try:\n        return template.format(**kwargs)\n    except KeyError as e:\n        logger.error(f\"Missing required variable for prompt {prompt_name}: {e}\")\n        return None\n    except Exception as e:\n        logger.error(f\"Error formatting prompt {prompt_name}: {e}\")\n        return None\n\ndef get_prompt_parameters(prompt_name: str) -> Dict[str, Any]:\n    \"\"\"Get the parameters from a prompt\n    \n    Args:\n        prompt_name: Name of the prompt (without .yaml extension)\n        \n    Returns:\n        Parameters dictionary, or empty dict if not found\n    \"\"\"\n    prompt_data = load_prompt(prompt_name)\n    if not prompt_data or 'parameters' not in prompt_data:\n        return {}\n        \n    return prompt_data['parameters']\n\ndef get_output_schema(prompt_name: str) -> Dict[str, Any]:\n    \"\"\"Get the output schema from a prompt\n    \n    Args:\n        prompt_name: Name of the prompt (without .yaml extension)\n        \n    Returns:\n        Output schema dictionary, or empty dict if not found\n    \"\"\"\n    prompt_data = load_prompt(prompt_name)\n    if not prompt_data or 'output_schema' not in prompt_data:\n        return {}\n        \n    return prompt_data['output_schema']\n\n\n#=============================================================================\n# File: extraction/prompts/entity_resolution.yaml\n# Description: YAML configuration file\n# Improvement Area: Prompt Design\n#=============================================================================\nprompt_type: entity_resolution\ntitle: Entity Resolution for Knowledge Garden\ndescription: Resolves potential entity matches using semantic understanding and similarity scores\n\n# Resolution parameters\nparameters:\n  similarity_threshold: 0.85\n  high_confidence_threshold: 0.95\n  normalize_entities: true\n  consider_context: true\n\n# Full prompt template with improved structure\ntemplate: |\n  # Entity Resolution for Knowledge Garden v2.0\n\n  ## SYSTEM CONTEXT\n  You are the Entity Resolver component in Knowledge Garden v2.0, a system that builds a structured knowledge graph from conversations. Your task is to determine if a newly extracted entity matches an existing entity in the knowledge graph. This resolution is critical for maintaining a clean, coherent knowledge graph without redundant entities.\n\n  ## ENTITIES TO COMPARE\n\n  ### New Entity\n  ID: {new_entity_id}\n  Label: \"{new_entity_label}\"\n  Description: \"{new_entity_description}\"\n  Type: \"{new_entity_type}\"\n  Attributes: {new_entity_attributes}\n  Extracted from: {source}\n\n  ### Potential Match\n  ID: {existing_entity_id}\n  Label: \"{existing_entity_label}\"\n  Description: \"{existing_entity_description}\"\n  Type: \"{existing_entity_type}\"\n  Attributes: {existing_entity_attributes}\n  Knowledge tier: {existing_entity_tier}\n  Creation date: {existing_entity_created}\n\n  Computed similarity score: {similarity_score}\n\n  ## RESOLUTION TASK\n  Determine whether these two entities refer to the same real-world concept and should be merged, or if they represent distinct concepts and should remain separate. Consider linguistic variations, contextual clues, entity attributes, and domain knowledge.\n\n  ## ENTITY RESOLUTION GUIDELINES\n\n  - Evaluate semantic meaning beyond surface text similarity\n  - Consider synonym and abbreviation variations (e.g., \"ML\" and \"Machine Learning\")\n  - Check for hypernym/hyponym relationships (e.g., \"Dog\" and \"Animal\")\n  - Assess contextual usage to disambiguate similar terms\n  - Consider domain-specific terminology and jargon\n  - Look for complementary or contradictory attributes\n  - Balance precision (avoiding false merges) with recall (avoiding duplicates)\n\n  ## EXAMPLE RESOLUTIONS\n\n  ### Example 1: Merge (Same Entity)\n  New Entity: Label = \"Natural Language Processing\"\n  Existing Entity: Label = \"NLP\"\n  Resolution: MATCH (Confidence: 0.95)\n  Reasoning: \"NLP\" is a standard abbreviation for \"Natural Language Processing.\" The descriptions align conceptually, and the attributes are complementary. This is clearly the same concept with different naming conventions.\n\n  ### Example 2: No Merge (Different Entities)\n  New Entity: Label = \"Java (Programming Language)\"\n  Existing Entity: Label = \"Java (Indonesian Island)\"\n  Resolution: NO MATCH (Confidence: 0.98)\n  Reasoning: Despite identical base terms, these entities refer to completely different concepts. The entity types (ProgrammingLanguage vs. Location) and descriptions confirm they are distinct concepts that should not be merged.\n\n  ### Example 3: Uncertain Case\n  New Entity: Label = \"Deep Learning\"\n  Existing Entity: Label = \"Neural Networks\"\n  Resolution: UNCERTAIN (Confidence: 0.60)\n  Reasoning: While deeply related concepts with significant overlap, deep learning is generally considered a subset of neural network approaches. They could be merged for some knowledge graph purposes, but in technical contexts, the distinction may be important. Additional context would help determine if merging is appropriate.\n\n  ## EDGE CASES AND ERROR HANDLING\n\n  - If entity descriptions are vague or missing, rely more on attributes and context\n  - If one entity is significantly more detailed than the other, consider if one is a more specific version\n  - Watch for polysemy (same term with multiple meanings) and homonymy (different concepts with same spelling)\n  - Consider domain context - terms may have different meanings in different fields\n  - If uncertain, lean toward NOT merging to prevent information loss\n  - Note when additional information would help resolve ambiguous cases\n\n  ## VALIDATION CRITERIA\n\n  Before finalizing your resolution, verify that:\n  1. You've considered semantic meaning beyond simple text matching\n  2. You've evaluated all available attributes and contextual information\n  3. You've checked for naming variations, abbreviations, and alternative forms\n  4. Your confidence level accurately reflects the certainty of the match\n  5. Your reasoning explicitly addresses key similarities or differences\n  6. Your decision follows domain-specific conventions when applicable\n\n  ## OUTPUT FORMAT\n  Provide your resolution as valid JSON with the following structure:\n\n  ```json\n  {\n    \"resolution\": \"MATCH\", \"NO_MATCH\", or \"UNCERTAIN\",\n    \"confidence\": 0.0-1.0,\n    \"reasoning\": \"Detailed explanation of your resolution reasoning...\",\n    \"recommended_label\": \"If matching, the preferred label to use\",\n    \"additional_info_needed\": [\"Optional list of information that would help resolve uncertain cases\"]\n  }\n  ```\n\n  If you determine the entities should be merged, include a recommended label that best represents the entity.\n  If you're uncertain, explain what additional information would help make a more confident decision.\n\n# Output validation schema\noutput_schema:\n  type: object\n  required:\n    - resolution\n    - confidence\n    - reasoning\n  properties:\n    resolution:\n      type: string\n      enum: [\"MATCH\", \"NO_MATCH\", \"UNCERTAIN\"]\n    confidence:\n      type: number\n      minimum: 0.0\n      maximum: 1.0\n    reasoning:\n      type: string\n      minLength: 50\n    recommended_label:\n      type: string\n    additional_info_needed:\n      type: array\n      items:\n        type: string\n\n\n#=============================================================================\n# File: extraction/prompts/sentence_extraction.yaml\n# Description: YAML configuration file\n# Improvement Area: Prompt Design\n#=============================================================================\nprompt_type: sentence_extraction\ntitle: Knowledge Extraction from Sentences\ndescription: Extracts structured knowledge triples from individual sentences with context\n\n# Extraction parameters\nparameters:\n  embedding_enabled: true\n  extract_patterns: true\n  confidence_threshold: 0.6\n  normalize_entities: true\n\n# Full prompt template with improved structure\ntemplate: |\n  # Knowledge Extraction from Sentence for Knowledge Garden v2.0\n\n  ## SYSTEM CONTEXT\n  You are the Sentence Extractor component in Knowledge Garden v2.0, a system that builds a structured knowledge graph from conversations. Your task is to extract semantic triples (subject-predicate-object) from individual sentences while considering conversational context. These triples will be stored in a knowledge graph and used to generate artifacts.\n\n  ## INPUT\n  Sentence: \"{sentence}\"\n\n  Conversation context:\n  {context}\n\n  ## EXTRACTION TASK\n  Extract all meaningful knowledge triples from the sentence by following these steps:\n\n  1. Identify entities (people, concepts, ideas, objects) in the sentence\n  2. Detect relationships between these entities\n  3. Capture attributes or properties of entities\n  4. Normalize entities and relationships to canonical forms\n  5. Assign a confidence score to each extraction\n\n  ## KNOWLEDGE REPRESENTATION GUIDELINES\n\n  - Use clear, concise labels for entities and relationships\n  - Prefer general concepts over overly specific ones unless specificity is important\n  - Normalize synonymous terms to canonical forms\n  - Capture implied knowledge when it has high confidence\n  - Format relationships in present tense, active voice\n  - Preserve time-bound information by adding temporal qualifiers\n  - Assign URIs to entities that appear to match existing knowledge\n\n  ## EXAMPLE EXTRACTIONS\n\n  ### Example 1:\n  Input: \"I've noticed that moments of uncertainty often lead to creative breakthroughs.\"\n  ```json\n  {\n    \"triples\": [\n      {\"subject\": \"uncertainty\", \"predicate\": \"leads_to\", \"object\": \"creative_breakthrough\", \"confidence\": 0.92},\n      {\"subject\": \"uncertainty\", \"predicate\": \"has_quality\", \"object\": \"momentary\", \"confidence\": 0.85}\n    ],\n    \"extracted_patterns\": {\n      \"CAP\": {\"context\": \"uncertainty\", \"action\": \"creative thinking\", \"pattern\": \"moments of uncertainty lead to creative breakthroughs\"}\n    },\n    \"confidence\": 0.92,\n    \"reasoning\": \"The sentence directly states a causal relationship between uncertainty and creative breakthroughs, while implying that uncertainty occurs in moments (is momentary).\"\n  }\n  ```\n\n  ### Example 2:\n  Input: \"Agile methodology emphasizes iterative development and customer feedback.\"\n  ```json\n  {\n    \"triples\": [\n      {\"subject\": \"agile_methodology\", \"predicate\": \"emphasizes\", \"object\": \"iterative_development\", \"confidence\": 0.98},\n      {\"subject\": \"agile_methodology\", \"predicate\": \"emphasizes\", \"object\": \"customer_feedback\", \"confidence\": 0.98},\n      {\"subject\": \"iterative_development\", \"predicate\": \"related_to\", \"object\": \"customer_feedback\", \"confidence\": 0.75}\n    ],\n    \"confidence\": 0.98,\n    \"reasoning\": \"The sentence explicitly states that Agile methodology emphasizes both iterative development and customer feedback. The relationship between iterative development and customer feedback is implied but less certain.\"\n  }\n  ```\n\n  ### Example 3:\n  Input: \"I realized that prioritizing tasks based on impact rather than urgency often leads to better outcomes.\"\n  ```json\n  {\n    \"triples\": [\n      {\"subject\": \"task_prioritization_by_impact\", \"predicate\": \"leads_to\", \"object\": \"better_outcomes\", \"confidence\": 0.90},\n      {\"subject\": \"task_prioritization_by_urgency\", \"predicate\": \"contrasts_with\", \"object\": \"task_prioritization_by_impact\", \"confidence\": 0.85}\n    ],\n    \"extracted_patterns\": {\n      \"insight\": \"prioritizing tasks based on impact rather than urgency leads to better outcomes\"\n    },\n    \"confidence\": 0.90,\n    \"reasoning\": \"The sentence expresses an insight about task prioritization methods and their outcomes. It contrasts impact-based prioritization with urgency-based prioritization.\"\n  }\n  ```\n\n  ## SPECIAL PATTERN DETECTION\n\n  In addition to general knowledge extraction, identify if the sentence contains any of these patterns:\n\n  1. **CAPs (Contexts, Actions, Patterns)**:\n     - Format: \"MOMENTS/CONTEXTS when X are OPPORTUNITIES for Y\"\n     - Example: \"Moments when I feel overwhelmed are opportunities for practicing mindfulness\"\n     - Return as: `\"CAP\": {\"context\": \"X\", \"action\": \"Y\", \"pattern\": \"full pattern\"}`\n\n  2. **Insights**:\n     - Format: \"I realized/noticed/discovered that X\"\n     - Example: \"I realized that regular breaks improve my productivity\"\n     - Return as: `\"insight\": \"X\"`\n\n  3. **Questions**:\n     - Format: Any sentence ending with a question mark that seeks information\n     - Example: \"How might we balance productivity with mental well-being?\"\n     - Return as: `\"question\": \"full question\"`\n\n  4. **Tensions**:\n     - Format: Sentences describing opposing concepts or polarities\n     - Example: \"There's a tension between wanting to do everything at once and focusing on one thing at a time\"\n     - Return as: `\"tension\": {\"pole1\": \"X\", \"pole2\": \"Y\"}`\n\n  ## HANDLING EDGE CASES\n\n  1. **Ambiguity**: When a sentence has multiple interpretations, extract the most likely interpretation and note alternatives if confidence is close\n  2. **Hypotheticals**: For hypothetical statements, add a \"hypothetical\" attribute to affected triples\n  3. **Opinions/beliefs**: Attribute opinions to their holders (e.g., \"user believes X\" rather than just \"X\")\n  4. **Negations**: Capture negations accurately (e.g., \"not_related_to\" rather than \"related_to\")\n  5. **Uncertainty**: If the sentence expresses uncertainty, reflect this in triple confidence\n  6. **Complex sentences**: Break down into multiple simple triples\n  7. **Missing context**: If crucial context is missing, reduce confidence and note the missing information\n\n  ## VALIDATION CRITERIA\n\n  Before finalizing your output, verify that:\n  1. All important entities and relationships are captured\n  2. No incorrect or unsupported triples are included\n  3. Entities and relationships use normalized terminology\n  4. Confidence scores accurately reflect certainty\n  5. Complex ideas are broken down appropriately\n  6. Context information is properly incorporated\n  7. Output strictly adheres to the required JSON format\n\n  ## OUTPUT FORMAT\n  Provide your extraction as valid JSON with the following structure:\n\n  ```json\n  {\n    \"triples\": [\n      {\n        \"subject\": \"entity1\",\n        \"predicate\": \"relationship\",\n        \"object\": \"entity2\",\n        \"confidence\": 0.0-1.0\n      }\n    ],\n    \"extracted_patterns\": {\n      \"CAP\": {\"context\": \"...\", \"action\": \"...\", \"pattern\": \"...\"},\n      \"question\": \"...\",\n      \"insight\": \"...\",\n      \"tension\": {\"pole1\": \"...\", \"pole2\": \"...\"}\n    },\n    \"confidence\": 0.0-1.0,\n    \"reasoning\": \"Brief explanation of your extraction reasoning\"\n  }\n  ```\n\n  If no meaningful triples can be extracted, return:\n  ```json\n  {\n    \"triples\": [],\n    \"confidence\": 0.0,\n    \"reasoning\": \"Explanation of why no triples could be extracted\"\n  }\n  ```\n\n# Output validation schema\noutput_schema:\n  type: object\n  required:\n    - triples\n    - confidence\n    - reasoning\n  properties:\n    triples:\n      type: array\n      items:\n        type: object\n        required:\n          - subject\n          - predicate\n          - object\n          - confidence\n        properties:\n          subject:\n            type: string\n            minLength: 1\n          predicate:\n            type: string\n            minLength: 1\n          object:\n            type: string\n            minLength: 1\n          confidence:\n            type: number\n            minimum: 0.0\n            maximum: 1.0\n    extracted_patterns:\n      type: object\n      properties:\n        CAP:\n          type: object\n          properties:\n            context:\n              type: string\n            action:\n              type: string\n            pattern:\n              type: string\n        question:\n          type: string\n        insight:\n          type: string\n        tension:\n          type: object\n          properties:\n            pole1:\n              type: string\n            pole2:\n              type: string\n    confidence:\n      type: number\n      minimum: 0.0\n      maximum: 1.0\n    reasoning:\n      type: string\n      minLength: 10\n\n\n#=============================================================================\n# File: parliament/prompts/entity_merge.yaml\n# Description: YAML configuration file\n# Improvement Area: Prompt Design\n#=============================================================================\ndecision_type: entity_merge\ntitle: Entity Merge Decision\ndescription: Determines whether two entities should be merged or kept separate in the knowledge graph\n\n# Parliament members and their perspectives\nmembers:\n  - name: \"Precision Expert\"\n    focus: \"Exact naming and disambiguation\"\n    perspective: \"Focuses on precise language and avoids over-merging distinct concepts\"\n    \n  - name: \"Pattern Recognizer\"\n    focus: \"Identifying relationships and similarities\"\n    perspective: \"Identifies connections between concepts and recognizes similarities\"\n    \n  - name: \"Domain Expert\"\n    focus: \"Deep domain understanding\"\n    perspective: \"Evaluates decisions based on domain-specific knowledge and norms\"\n    \n  - name: \"Skeptic\"\n    focus: \"Critical evaluation and evidence\"\n    perspective: \"Questions assumptions and requires evidence for claims\"\n    \n  - name: \"Pragmatist\"\n    focus: \"Practical outcomes and usability\"\n    perspective: \"Evaluates decisions based on practical utility and user experience\"\n\n# Full prompt template with improved structure\ntemplate: |\n  # Parliament Decision: Entity Merge in Knowledge Garden\n\n  ## CONTEXT\n  You are the Parliament Decision Engine in the Knowledge Garden v2.0 system. Your task is to determine whether two entities should be merged in our knowledge graph. This decision is critical because:\n  - Merging similar entities reduces redundancy and improves knowledge coherence\n  - Incorrect merges can lose important distinctions and create confusion\n  - The decision affects how knowledge is organized and retrieved\n\n  ## ENTITIES TO EVALUATE\n\n  Entity 1: {entity1}\n  - Label: \"{label1}\"\n  - Description: \"{description1}\"\n  - Attributes: {attributes1}\n  - Knowledge tier: {tier1}\n  - Evidence count: {evidence_count1}\n\n  Entity 2: {entity2}\n  - Label: \"{label2}\"\n  - Description: \"{description2}\"\n  - Attributes: {attributes2}\n  - Knowledge tier: {tier2}\n  - Evidence count: {evidence_count2}\n\n  Computed similarity score: {similarity}\n\n  ## EXPERT PANEL ANALYSIS\n\n  Each expert should analyze the entities from their unique perspective, considering the evidence provided:\n\n  1. Precision Expert (focus on exact naming and disambiguation):\n     - Analyze the linguistic nuances in the labels and descriptions\n     - Consider whether differences are substantive or superficial\n     - Identify potential disambiguation issues if merged\n\n  2. Pattern Recognizer (focus on identifying relationships and similarities):\n     - Examine structural patterns in both entities\n     - Identify common relationships they share with other entities\n     - Assess if their attribute patterns suggest they represent the same concept\n\n  3. Domain Expert (focus on domain-specific knowledge):\n     - Evaluate whether these entities have domain-specific distinctions\n     - Consider knowledge tier implications (merging personal with digested knowledge)\n     - Assess domain-specific terminology that might explain differences\n\n  4. Skeptic (focus on critical evaluation and evidence):\n     - Challenge the similarity score - is it misleading?\n     - Identify potential evidence conflicts between entities\n     - Consider worst-case scenarios if merged incorrectly\n\n  5. Pragmatist (focus on practical outcomes and usability):\n     - Evaluate the practical benefits of merging\n     - Consider user experience implications\n     - Assess maintenance burden of keeping separate vs. merging\n\n  ## EXAMPLE DECISIONS\n\n  ### Example 1: Merge Approved\n  Entity 1: \"machine_learning\" | Label: \"Machine Learning\"\n  Entity 2: \"ML\" | Label: \"ML\"\n  Decision: MERGE (Confidence: 0.95)\n  Reasoning: While the labels differ (full term vs. acronym), these clearly refer to the same concept. The descriptions and attributes align perfectly, and the acronym \"ML\" is a standard abbreviation for \"Machine Learning.\" No information would be lost by merging.\n\n  ### Example 2: Merge Rejected\n  Entity 1: \"python_language\" | Label: \"Python (Programming Language)\"\n  Entity 2: \"python_snake\" | Label: \"Python (Snake Species)\"\n  Decision: DO NOT MERGE (Confidence: 0.98)\n  Reasoning: Despite identical terms \"Python,\" these entities refer to completely different concepts (programming language vs. snake species). The high text similarity is misleading, and the domain contexts are entirely separate.\n\n  ## EDGE CASES AND ERROR HANDLING\n\n  - If either entity has incomplete information (missing description or attributes), note this uncertainty in your reasoning and reduce confidence accordingly\n  - If conflicting information exists, prioritize more recent and higher-tier knowledge\n  - If entities appear identical but come from different knowledge tiers, proceed cautiously and consider tier-specific implications\n  - If similarity score contradicts semantic analysis, explain why the score might be misleading\n\n  ## DELIBERATION AND FINAL DECISION\n\n  After each expert has provided their perspective, synthesize a final decision:\n\n  ```json\n  {\n    \"decision\": \"MERGE\" or \"DO NOT MERGE\",\n    \"confidence\": 0.0-1.0,\n    \"reasoning\": \"Detailed explanation of decision rationale...\",\n    \"recommended_label\": \"If merging, the recommended label to use\",\n    \"dissenting_opinions\": [\"Note any strong dissenting views from experts\"]\n  }\n  ```\n\n  Your decision should clearly state whether to merge the entities, with confidence level and detailed reasoning that references specific attributes and evidence from the entities.\n\n# Output validation schema\noutput_schema:\n  type: object\n  required:\n    - decision\n    - confidence\n    - reasoning\n    - recommended_label\n  properties:\n    decision:\n      type: string\n      enum: [\"MERGE\", \"DO NOT MERGE\"]\n    confidence:\n      type: number\n      minimum: 0.0\n      maximum: 1.0\n    reasoning:\n      type: string\n      minLength: 50\n    recommended_label:\n      type: string\n    dissenting_opinions:\n      type: array\n      items:\n        type: string\n\n\n#=============================================================================\n# File: parliament/prompts/knowledge_tier.yaml\n# Description: YAML configuration file\n# Improvement Area: Prompt Design\n#=============================================================================\ndecision_type: knowledge_tier\ntitle: Knowledge Tier Assignment\ndescription: Determines the appropriate knowledge tier for a piece of information\n\nmembers:\n  - name: \"Source Validator\"\n    focus: \"Information provenance and reliability\"\n    perspective: \"Evaluates the reliability and authority of information sources\"\n  \n  - name: \"User Context Specialist\"\n    focus: \"User-generated content analysis\"\n    perspective: \"Identifies patterns in how users express personal knowledge\"\n  \n  - name: \"Knowledge Integrator\"\n    focus: \"Connection to existing knowledge\"\n    perspective: \"Considers how new information connects to established knowledge\"\n  \n  - name: \"Metadata Analyst\"\n    focus: \"Content signal analysis\"\n    perspective: \"Examines metadata and linguistic signals of information type\"\n\nprompt: |\n  You are a parliament of knowledge management experts deciding the appropriate knowledge tier for information.\n  \n  Entity to classify:\n  ```json\n  {entity}\n  ```\n  \n  Source information:\n  ```json\n  {source_info}\n  ```\n  \n  TASK: Assign this entity to one of the three knowledge tiers:\n  \n  1. PERSONAL: Knowledge directly expressed by the user in conversations\n     - User beliefs, experiences, preferences, and perspectives\n     - Marked by first-person statements and personal assertions\n  \n  2. DIGESTED: External knowledge the user has validated or accepted\n     - Information from sources the user has endorsed\n     - External facts the user has cited or referenced with approval\n     - Knowledge that has been explicitly verified\n  \n  3. PERIPHERAL: Referenced external knowledge without clear validation\n     - Citations and references without explicit user endorsement\n     - Background information mentioned but not deeply engaged with\n     - \"See also\" type connections to external concepts\n  \n  Each parliament member should analyze from their unique perspective, then collaborate to reach a decision.\n  \n  Provide a confidence score and clear reasoning for your tier assignment.\n\noutput_schema:\n  action: \"assign_tier\"\n  tier: \"personal|digested|peripheral\"\n  confidence: 0.95\n  reasoning: \"Explanation of the tier assignment with key factors\"\n  evidence: [\"List of specific evidence points that supported the decision\"]\n\n\n#=============================================================================\n# File: prompt_examples.md\n# Description: Knowledge Garden v2.0 Prompt Assessment\n# Improvement Area: Prompt Design\n#=============================================================================\n# Knowledge Garden v2.0 Prompt Assessment\n\n## Current Prompts vs. Improved Prompts\n\n### 1. Entity Merge Prompt\n\n#### Current Version (Score: 3/5)\n```\nParliament Decision: Entity Merge\n\nYou are a parliament of expert perspectives considering whether two entities should be merged.\n\nEntity 1: {entity1}\n- Label: \"{label1}\"\n- Description: \"{description1}\"\n- Attributes: {attributes1}\n\nEntity 2: {entity2}\n- Label: \"{label2}\"\n- Description: \"{description2}\"\n- Attributes: {attributes2}\n\nSimilarity score: {similarity}\n\nEach expert should provide their perspective:\n\n1. Precision Expert (focus on exact naming and disambiguation):\n\n2. Pattern Recognizer (focus on identifying relationships and similarities):\n\n3. Domain Expert (focus on domain-specific knowledge):\n\n4. Skeptic (focus on critical evaluation and evidence):\n\n5. Pragmatist (focus on practical outcomes and usability):\n\nFinal Decision:\nBased on the discussion above, provide a final decision on whether to merge these entities.\nInclude reasoning and confidence level.\n```\n\n#### Weaknesses:\n- Lacks context about the knowledge system\n- No examples of successful/unsuccessful merges\n- Limited error handling guidance\n- No explicit reasoning steps\n- No format specification for the output\n\n#### Improved Version (Estimated Score: 4.5/5)\n```\n# Parliament Decision: Entity Merge in Knowledge Garden\n\n## CONTEXT\nYou are the Parliament Decision Engine in the Knowledge Garden v2.0 system. Your task is to determine whether two entities should be merged in our knowledge graph. This decision is critical because:\n- Merging similar entities reduces redundancy and improves knowledge coherence\n- Incorrect merges can lose important distinctions and create confusion\n- The decision affects how knowledge is organized and retrieved\n\n## ENTITIES TO EVALUATE\n\nEntity 1: {entity1}\n- Label: \"{label1}\"\n- Description: \"{description1}\"\n- Attributes: {attributes1}\n- Knowledge tier: {tier1}\n- Evidence count: {evidence_count1}\n\nEntity 2: {entity2}\n- Label: \"{label2}\"\n- Description: \"{description2}\"\n- Attributes: {attributes2}\n- Knowledge tier: {tier2}\n- Evidence count: {evidence_count2}\n\nComputed similarity score: {similarity}\n\n## EXPERT PANEL ANALYSIS\n\nEach expert should analyze the entities from their unique perspective, considering the evidence provided:\n\n1. Precision Expert (focus on exact naming and disambiguation):\n   - Analyze the linguistic nuances in the labels and descriptions\n   - Consider whether differences are substantive or superficial\n   - Identify potential disambiguation issues if merged\n\n2. Pattern Recognizer (focus on identifying relationships and similarities):\n   - Examine structural patterns in both entities\n   - Identify common relationships they share with other entities\n   - Assess if their attribute patterns suggest they represent the same concept\n\n3. Domain Expert (focus on domain-specific knowledge):\n   - Evaluate whether these entities have domain-specific distinctions\n   - Consider knowledge tier implications (merging personal with digested knowledge)\n   - Assess domain-specific terminology that might explain differences\n\n4. Skeptic (focus on critical evaluation and evidence):\n   - Challenge the similarity score - is it misleading?\n   - Identify potential evidence conflicts between entities\n   - Consider worst-case scenarios if merged incorrectly\n\n5. Pragmatist (focus on practical outcomes and usability):\n   - Evaluate the practical benefits of merging\n   - Consider user experience implications\n   - Assess maintenance burden of keeping separate vs. merging\n\n## EXAMPLE DECISIONS\n\n### Example 1: Merge Approved\nEntity 1: \"machine_learning\" | Label: \"Machine Learning\"\nEntity 2: \"ML\" | Label: \"ML\"\nDecision: MERGE (Confidence: 0.95)\nReasoning: While the labels differ (full term vs. acronym), these clearly refer to the same concept. The descriptions and attributes align perfectly, and the acronym \"ML\" is a standard abbreviation for \"Machine Learning.\" No information would be lost by merging.\n\n### Example 2: Merge Rejected\nEntity 1: \"python_language\" | Label: \"Python (Programming Language)\"\nEntity 2: \"python_snake\" | Label: \"Python (Snake Species)\"\nDecision: DO NOT MERGE (Confidence: 0.98)\nReasoning: Despite identical terms \"Python,\" these entities refer to completely different concepts (programming language vs. snake species). The high text similarity is misleading, and the domain contexts are entirely separate.\n\n## EDGE CASES AND ERROR HANDLING\n\n- If either entity has incomplete information (missing description or attributes), note this uncertainty in your reasoning and reduce confidence accordingly\n- If conflicting information exists, prioritize more recent and higher-tier knowledge\n- If entities appear identical but come from different knowledge tiers, proceed cautiously and consider tier-specific implications\n- If similarity score contradicts semantic analysis, explain why the score might be misleading\n\n## DELIBERATION AND FINAL DECISION\n\nAfter each expert has provided their perspective, synthesize a final decision:\n\n```json\n{\n  \"decision\": \"MERGE\" or \"DO NOT MERGE\",\n  \"confidence\": 0.0-1.0,\n  \"reasoning\": \"Detailed explanation of decision rationale...\",\n  \"recommended_label\": \"If merging, the recommended label to use\",\n  \"dissenting_opinions\": [\"Note any strong dissenting views from experts\"]\n}\n```\n\nYour decision should clearly state whether to merge the entities, with confidence level and detailed reasoning that references specific attributes and evidence from the entities.\n```\n\n### 2. Sentence Extraction Prompt\n\n#### Current Version (Score: 3/5)\n```\nExtract knowledge from the following sentence. Consider the context provided.\n\nSentence: \"{sentence}\"\n\nContext:\n{context}\n\nInstructions:\n1. Identify the main entities mentioned\n2. Identify relationships between entities\n3. Extract any attributes or properties\n4. Determine the confidence level (0-1)\n\nFormat your response as JSON:\n```json\n{\n    \"triples\": [\n        {\"subject\": \"entity1\", \"predicate\": \"relation\", \"object\": \"entity2\"},\n        ...\n    ],\n    \"confidence\": 0.85\n}\n```\n```\n\n#### Weaknesses:\n- Limited context about the knowledge garden system\n- No examples of good/bad extractions\n- Minimal guidance on handling ambiguous cases\n- No explanation of how to handle context\n- Limited validation criteria\n\n#### Improved Version (Estimated Score: 4.8/5)\n```\n# Knowledge Extraction from Sentence for Knowledge Garden v2.0\n\n## SYSTEM CONTEXT\nYou are the Sentence Extractor component in Knowledge Garden v2.0, a system that builds a structured knowledge graph from conversations. Your task is to extract semantic triples (subject-predicate-object) from individual sentences while considering conversational context. These triples will be stored in a knowledge graph and used to generate artifacts.\n\n## INPUT\nSentence: \"{sentence}\"\n\nConversation context:\n{context}\n\n## EXTRACTION TASK\nExtract all meaningful knowledge triples from the sentence by following these steps:\n\n1. Identify entities (people, concepts, ideas, objects) in the sentence\n2. Detect relationships between these entities\n3. Capture attributes or properties of entities\n4. Normalize entities and relationships to canonical forms\n5. Assign a confidence score to each extraction\n\n## KNOWLEDGE REPRESENTATION GUIDELINES\n\n- Use clear, concise labels for entities and relationships\n- Prefer general concepts over overly specific ones unless specificity is important\n- Normalize synonymous terms to canonical forms\n- Capture implied knowledge when it has high confidence\n- Format relationships in present tense, active voice\n- Preserve time-bound information by adding temporal qualifiers\n- Assign URIs to entities that appear to match existing knowledge\n\n## EXAMPLE EXTRACTIONS\n\n### Example 1:\nInput: \"I've noticed that moments of uncertainty often lead to creative breakthroughs.\"\n```json\n{\n  \"triples\": [\n    {\"subject\": \"uncertainty\", \"predicate\": \"leads_to\", \"object\": \"creative_breakthrough\", \"confidence\": 0.92},\n    {\"subject\": \"uncertainty\", \"predicate\": \"has_quality\", \"object\": \"momentary\", \"confidence\": 0.85}\n  ],\n  \"embedding_vector\": \"[base64-encoded-vector]\",\n  \"confidence\": 0.92,\n  \"extracted_CAP\": {\"context\": \"uncertainty\", \"action\": \"creative thinking\", \"pattern\": \"moments of uncertainty lead to creative breakthroughs\"}\n}\n```\n\n### Example 2:\nInput: \"Agile methodology emphasizes iterative development and customer feedback.\"\n```json\n{\n  \"triples\": [\n    {\"subject\": \"agile_methodology\", \"predicate\": \"emphasizes\", \"object\": \"iterative_development\", \"confidence\": 0.98},\n    {\"subject\": \"agile_methodology\", \"predicate\": \"emphasizes\", \"object\": \"customer_feedback\", \"confidence\": 0.98},\n    {\"subject\": \"iterative_development\", \"predicate\": \"related_to\", \"object\": \"customer_feedback\", \"confidence\": 0.75}\n  ],\n  \"embedding_vector\": \"[base64-encoded-vector]\",\n  \"confidence\": 0.98\n}\n```\n\n## HANDLING EDGE CASES\n\n1. **Ambiguity**: When a sentence has multiple interpretations, extract the most likely interpretation and note alternatives if confidence is close\n2. **Hypotheticals**: For hypothetical statements, add a \"hypothetical\" attribute to affected triples\n3. **Opinions/beliefs**: Attribute opinions to their holders (e.g., \"user believes X\" rather than just \"X\")\n4. **Negations**: Capture negations accurately (e.g., \"not_related_to\" rather than \"related_to\")\n5. **Uncertainty**: If the sentence expresses uncertainty, reflect this in triple confidence\n6. **Complex sentences**: Break down into multiple simple triples\n7. **Missing context**: If crucial context is missing, reduce confidence and note the missing information\n\n## VALIDATION CRITERIA\n\nBefore finalizing your output, verify that:\n1. All important entities and relationships are captured\n2. No incorrect or unsupported triples are included\n3. Entities and relationships use normalized terminology\n4. Confidence scores accurately reflect certainty\n5. Complex ideas are broken down appropriately\n6. Context information is properly incorporated\n7. Output strictly adheres to the required JSON format\n\n## OUTPUT FORMAT\nProvide your extraction as valid JSON with the following structure:\n\n```json\n{\n  \"triples\": [\n    {\n      \"subject\": \"entity1\",\n      \"predicate\": \"relationship\",\n      \"object\": \"entity2\",\n      \"confidence\": 0.0-1.0\n    },\n    ...\n  ],\n  \"extracted_patterns\": {\n    \"CAP\": {\"context\": \"...\", \"action\": \"...\", \"pattern\": \"...\"},\n    \"question\": \"...\",\n    \"insight\": \"...\",\n    \"tension\": {\"pole1\": \"...\", \"pole2\": \"...\"}\n  },\n  \"confidence\": 0.0-1.0,\n  \"reasoning\": \"Brief explanation of your extraction reasoning\"\n}\n```\n\nIf no meaningful triples can be extracted, return:\n```json\n{\n  \"triples\": [],\n  \"confidence\": 0.0,\n  \"reasoning\": \"Explanation of why no triples could be extracted\"\n}\n```\n```\n\n## Recommendations for Improving All Prompts\n\nBased on our assessment, here are key recommendations for improving all prompts in Knowledge Garden v2.0:\n\n1. **Add System Context**: Begin prompts with a clear explanation of the Knowledge Garden system and how the specific component fits within it.\n\n2. **Include Few-Shot Examples**: Provide 2-3 concrete examples showing both standard cases and edge cases.\n\n3. **Add Error Handling Guidance**: Include specific instructions for handling ambiguous cases, missing information, and contradictions.\n\n4. **Provide Reasoning Steps**: Break down the expected reasoning process into explicit steps that guide the LLM's thought process.\n\n5. **Specify Output Format Strictly**: Use JSON schema with detailed field descriptions and validation requirements.\n\n6. **Include Validation Criteria**: Add a section for self-checking that helps the LLM verify its output meets quality standards.\n\n7. **Enhance Adaptability**: Structure prompts to handle varying input complexity and domain-specific content.\n\n8. **Add Guard Rails**: Include explicit instructions about what not to do and how to handle problematic inputs.\n\n9. **Standardize Structure**: Use consistent sections across all prompts (Context, Input, Task, Guidelines, Examples, Edge Cases, Validation, Output Format).\n\n10. **Balance Conciseness and Detail**: Keep prompts focused while providing sufficient guidance for complex tasks.\n\nBy implementing these improvements, we can significantly enhance the quality and consistency of LLM-generated outputs throughout the Knowledge Garden system.\n\n\n\n#================================================================================\n# IMPROVEMENT AREA: Testing Infrastructure\n#================================================================================\n\n\n#=============================================================================\n# File: assessment.py\n# Description: Assessment script for Knowledge Garden v2.0\n# Improvement Area: Testing Infrastructure\n#=============================================================================\n#!/usr/bin/env python3\n\"\"\"\nAssessment script for Knowledge Garden v2.0\n\nThis script evaluates different components of Knowledge Garden v2.0\nand generates a CSV report with assessment results.\n\"\"\"\n\nimport sys\nimport os\nimport json\nimport csv\nimport asyncio\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Tuple\nimport argparse\n\n# Add the parent directory to the Python path\nsys.path.append(str(Path(__file__).parent.parent))\n\n# Import common utilities\nfrom knowledge_garden.core.config import DEFAULT_FUSEKI_URL, DEFAULT_USER_ID\n\n# Define assessment criteria\nCRITERIA = [\n    \"architecture\",\n    \"code_quality\",\n    \"extensibility\",\n    \"maintainability\",\n    \"performance\",\n    \"readability\",\n    \"testability\",\n    \"documentation\",\n    \"error_handling\",\n    \"security\",\n    \"scalability\",\n    \"usability\"\n]\n\n# Define components to assess\nCOMPONENTS = [\n    \"extraction_core\",\n    \"embedding_generation\",\n    \"entity_resolution\",\n    \"parliament_engine\",\n    \"knowledge_tiers\",\n    \"artifact_dependencies\",\n    \"version_management\",\n    \"meta_artifacts\",\n    \"configuration_system\",\n    \"main_integration\"\n]\n\n# Sample data for assessment\nSAMPLE_TEXT = \"\"\"\nI've noticed that MOMENTS when I feel overwhelmed with tasks are OPPORTUNITIES for practicing mindfulness.\nThere's a tension between wanting to do everything at once and focusing on one thing at a time.\nI realized that prioritizing tasks based on impact rather than urgency often leads to better outcomes.\nHow might we balance productivity with mental well-being in our daily work?\n\"\"\"\n\ndef check_directory_structure() -> Tuple[bool, List[str]]:\n    \"\"\"Check if the directory structure is correct for Knowledge Garden v2.0\"\"\"\n    required_dirs = [\n        \"core\",\n        \"models\",\n        \"extraction\",\n        \"knowledge\",\n        \"parliament\",\n        \"artifacts\",\n        \"config\"\n    ]\n    \n    missing_dirs = []\n    for dir_name in required_dirs:\n        if not Path(dir_name).exists():\n            missing_dirs.append(dir_name)\n    \n    return len(missing_dirs) == 0, missing_dirs\n\ndef check_configuration_files() -> Tuple[bool, List[str]]:\n    \"\"\"Check if the configuration files exist\"\"\"\n    required_configs = [\n        \"config/extraction.yaml\",\n        \"config/parliament.yaml\",\n        \"config/knowledge_tiers.yaml\",\n        \"config/artifacts.yaml\"\n    ]\n    \n    missing_configs = []\n    for config_file in required_configs:\n        if not Path(config_file).exists():\n            missing_configs.append(config_file)\n    \n    return len(missing_configs) == 0, missing_configs\n\ndef check_core_files() -> Tuple[bool, List[str]]:\n    \"\"\"Check if core files exist\"\"\"\n    required_files = [\n        \"core/living_artifact_system.py\",\n        \"core/uri_generator.py\",\n        \"core/fuseki_client.py\",\n        \"core/config.py\",\n        \"core/parliament_engine.py\",\n        \"main.py\"\n    ]\n    \n    missing_files = []\n    for file_path in required_files:\n        if not Path(file_path).exists():\n            missing_files.append(file_path)\n    \n    return len(missing_files) == 0, missing_files\n\ndef check_file_quality(file_path: str) -> Dict[str, Any]:\n    \"\"\"Assess the quality of a specific file\"\"\"\n    try:\n        with open(file_path, 'r') as f:\n            content = f.read()\n            \n        # Simple metrics\n        line_count = len(content.splitlines())\n        char_count = len(content)\n        has_docstring = '\"\"\"' in content[:500]  # Check if file has a docstring near the top\n        class_count = content.count('class ')\n        function_count = content.count('def ')\n        comment_lines = content.count('#')\n        \n        # Type hint check (very basic)\n        has_type_hints = ' -> ' in content or ': ' in content\n        \n        # Error handling check (very basic)\n        has_error_handling = 'try:' in content and 'except' in content\n        \n        return {\n            \"file_path\": file_path,\n            \"line_count\": line_count,\n            \"char_count\": char_count,\n            \"has_docstring\": has_docstring,\n            \"class_count\": class_count,\n            \"function_count\": function_count,\n            \"comment_lines\": comment_lines,\n            \"has_type_hints\": has_type_hints,\n            \"has_error_handling\": has_error_handling\n        }\n    except Exception as e:\n        return {\n            \"file_path\": file_path,\n            \"error\": str(e)\n        }\n\ndef evaluate_component(component: str) -> Dict[str, Any]:\n    \"\"\"Evaluate a specific component\"\"\"\n    component_files = {\n        \"extraction_core\": [\"extraction/base_extractor.py\", \"extraction/pattern_extractor.py\", \"extraction/llm_extractor.py\"],\n        \"embedding_generation\": [\"extraction/embedding_generator.py\", \"models/extracted_sentence.py\"],\n        \"entity_resolution\": [\"extraction/entity_resolver.py\"],\n        \"parliament_engine\": [\"parliament/decision_engine.py\", \"core/parliament_engine.py\"],\n        \"knowledge_tiers\": [\"knowledge/three_tier_model.py\"],\n        \"artifact_dependencies\": [\"artifacts/dependency_graph.py\"],\n        \"version_management\": [\"artifacts/version_manager.py\"],\n        \"meta_artifacts\": [\"artifacts/meta_artifacts.py\"],\n        \"configuration_system\": [\"config/extraction.yaml\", \"config/parliament.yaml\", \"config/knowledge_tiers.yaml\", \"config/artifacts.yaml\"],\n        \"main_integration\": [\"main.py\"]\n    }\n    \n    files = component_files.get(component, [])\n    file_metrics = []\n    \n    for file_path in files:\n        if Path(file_path).exists():\n            file_metrics.append(check_file_quality(file_path))\n    \n    # Calculate overall metrics\n    total_lines = sum(m.get(\"line_count\", 0) for m in file_metrics)\n    avg_lines_per_file = total_lines / len(file_metrics) if file_metrics else 0\n    \n    has_docstrings = all(m.get(\"has_docstring\", False) for m in file_metrics) if file_metrics else False\n    has_type_hints = all(m.get(\"has_type_hints\", False) for m in file_metrics) if file_metrics else False\n    has_error_handling = any(m.get(\"has_error_handling\", False) for m in file_metrics) if file_metrics else False\n    \n    # Evaluate against criteria\n    scores = {}\n    for criterion in CRITERIA:\n        # Calculate a score based on metrics\n        score = 0\n        if criterion == \"code_quality\":\n            score = 5 if has_docstrings and has_type_hints else 3\n        elif criterion == \"documentation\":\n            score = 5 if has_docstrings else 2\n        elif criterion == \"error_handling\":\n            score = 5 if has_error_handling else 2\n        elif criterion == \"readability\":\n            score = 4 if avg_lines_per_file < 100 else 3\n        else:\n            # Default score for criteria we can't measure automatically\n            score = 3\n            \n        scores[criterion] = score\n    \n    return {\n        \"component\": component,\n        \"files\": [m.get(\"file_path\") for m in file_metrics],\n        \"file_count\": len(file_metrics),\n        \"total_lines\": total_lines,\n        \"avg_lines_per_file\": avg_lines_per_file,\n        \"scores\": scores\n    }\n\ndef generate_assessment_report() -> str:\n    \"\"\"Generate a full assessment report and return path to CSV file\"\"\"\n    results = []\n    \n    # Check directory structure\n    dir_structure_ok, missing_dirs = check_directory_structure()\n    \n    # Check configuration files\n    config_files_ok, missing_configs = check_configuration_files()\n    \n    # Check core files\n    core_files_ok, missing_files = check_core_files()\n    \n    # Overall structure assessment\n    structure_score = 5 if dir_structure_ok else 3 if len(missing_dirs) <= 2 else 1\n    config_score = 5 if config_files_ok else 3 if len(missing_configs) <= 1 else 1\n    core_score = 5 if core_files_ok else 3 if len(missing_files) <= 2 else 1\n    \n    results.append({\n        \"component\": \"overall_structure\",\n        \"architecture\": structure_score,\n        \"code_quality\": (structure_score + config_score + core_score) / 3,\n        \"extensibility\": structure_score,\n        \"maintainability\": structure_score,\n        \"performance\": \"N/A\",\n        \"readability\": config_score,\n        \"testability\": 3,  # Default\n        \"documentation\": config_score,\n        \"error_handling\": \"N/A\",\n        \"security\": \"N/A\",\n        \"scalability\": structure_score,\n        \"usability\": \"N/A\",\n        \"notes\": f\"Missing dirs: {missing_dirs}, Missing configs: {missing_configs}, Missing core files: {missing_files}\"\n    })\n    \n    # Evaluate each component\n    for component in COMPONENTS:\n        evaluation = evaluate_component(component)\n        \n        result = {\n            \"component\": component,\n            \"notes\": f\"Files: {evaluation.get('file_count', 0)}, Lines: {evaluation.get('total_lines', 0)}\"\n        }\n        \n        # Add scores for each criterion\n        for criterion in CRITERIA:\n            result[criterion] = evaluation.get(\"scores\", {}).get(criterion, \"N/A\")\n            \n        results.append(result)\n    \n    # Create CSV file\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    csv_path = f\"assessment_report_{timestamp}.csv\"\n    \n    with open(csv_path, 'w', newline='') as csvfile:\n        fieldnames = [\"component\"] + CRITERIA + [\"notes\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for result in results:\n            writer.writerow(result)\n    \n    return csv_path\n\ndef main():\n    \"\"\"Main entry point\"\"\"\n    parser = argparse.ArgumentParser(description=\"Knowledge Garden v2.0 Assessment\")\n    parser.add_argument('--output', type=str, help=\"Output CSV file path\")\n    \n    args = parser.parse_args()\n    \n    print(\"Assessing Knowledge Garden v2.0...\")\n    \n    # Generate assessment report\n    csv_path = args.output if args.output else generate_assessment_report()\n    \n    print(f\"Assessment complete. Report saved to: {csv_path}\")\n    \n    # Print summary\n    with open(csv_path, 'r') as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n        \n    print(\"\\nSummary:\")\n    print(f\"Components assessed: {len(rows)}\")\n    \n    # Calculate average scores\n    avg_scores = {}\n    for criterion in CRITERIA:\n        scores = [float(row[criterion]) for row in rows if row[criterion] != \"N/A\"]\n        avg_scores[criterion] = sum(scores) / len(scores) if scores else \"N/A\"\n    \n    print(\"\\nAverage scores:\")\n    for criterion, score in avg_scores.items():\n        if score != \"N/A\":\n            print(f\"  {criterion}: {score:.2f}/5.0\")\n    \n    print(\"\\nStrengths:\")\n    strengths = [criterion for criterion, score in avg_scores.items() \n                if score != \"N/A\" and score >= 4.0]\n    for strength in strengths:\n        print(f\"  - {strength}\")\n    \n    print(\"\\nAreas for improvement:\")\n    weaknesses = [criterion for criterion, score in avg_scores.items() \n                 if score != \"N/A\" and score < 3.0]\n    for weakness in weaknesses:\n        print(f\"  - {weakness}\")\n        \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n\n\n#=============================================================================\n# File: prompt_assessment.py\n# Description: Prompt Quality Assessment for Knowledge Garden v2.0\n# Improvement Area: Testing Infrastructure\n#=============================================================================\n#!/usr/bin/env python3\n\"\"\"\nPrompt Quality Assessment for Knowledge Garden v2.0\n\nThis script evaluates the quality of prompts used in Knowledge Garden v2.0\nand generates a CSV report with assessment results.\n\"\"\"\n\nimport sys\nimport os\nimport json\nimport csv\nimport yaml\nimport asyncio\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Tuple\nimport argparse\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, \n                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(\"prompt_assessment\")\n\n# Add the parent directory to the Python path\nsys.path.append(str(Path(__file__).parent.parent))\n\n# Import components if available\ntry:\n    from knowledge_garden.extraction.prompts import load_prompt, get_template_text, format_prompt\nexcept ImportError:\n    logger.warning(\"Could not import prompts module, will use built-in samples\")\n\n# Define prompt assessment criteria\nCRITERIA = [\n    \"clarity\",\n    \"specificity\",\n    \"context\",\n    \"error_handling\",\n    \"output_format\",\n    \"few_shot_examples\",\n    \"guard_rails\",\n    \"adaptability\",\n    \"conciseness\",\n    \"precision\",\n    \"reasoning_guidance\",\n    \"validation\"\n]\n\n# Define prompt types to assess\nPROMPT_TYPES = [\n    \"sentence_extraction\",\n    \"entity_resolution\",\n    \"entity_merge\",\n    \"relation_validation\",\n    \"tier_classification\",\n    \"conflict_resolution\",\n    \"pattern_validation\"\n]\n\n# Sample prompt data for testing\nSAMPLE_PROMPTS = {\n    \"sentence_extraction\": \"\"\"\n    Extract knowledge from the following sentence. Consider the context provided.\n    \n    Sentence: \"{sentence}\"\n    \n    Context:\n    {context}\n    \n    Instructions:\n    1. Identify the main entities mentioned\n    2. Identify relationships between entities\n    3. Extract any attributes or properties\n    4. Determine the confidence level (0-1)\n    \n    Format your response as JSON:\n    ```json\n    {\n        \"triples\": [\n            {\"subject\": \"entity1\", \"predicate\": \"relation\", \"object\": \"entity2\"},\n            ...\n        ],\n        \"confidence\": 0.85\n    }\n    ```\n    \"\"\",\n    \n    \"entity_resolution\": \"\"\"\n    Determine if these two entities refer to the same real-world concept.\n    \n    Entity 1: {entity1}\n    - Label: \"{label1}\"\n    - Description: \"{description1}\"\n    - Attributes: {attributes1}\n    \n    Entity 2: {entity2}\n    - Label: \"{label2}\"\n    - Description: \"{description2}\"\n    - Attributes: {attributes2}\n    \n    Similarity score: {similarity}\n    \n    Instructions:\n    1. Compare the entities semantically\n    2. Consider string similarity but prioritize meaning\n    3. Assess whether differences are substantive or superficial\n    4. Provide your reasoning\n    \n    Format your response as JSON:\n    ```json\n    {\n        \"should_merge\": true/false,\n        \"confidence\": 0.9,\n        \"reasoning\": \"Detailed explanation...\",\n        \"recommended_label\": \"The best label to use if merged\"\n    }\n    ```\n    \"\"\",\n    \n    \"entity_merge\": \"\"\"\n    Parliament Decision: Entity Merge\n\n    You are a parliament of expert perspectives considering whether two entities should be merged.\n    \n    Entity 1: {entity1}\n    - Label: \"{label1}\"\n    - Description: \"{description1}\"\n    - Attributes: {attributes1}\n    \n    Entity 2: {entity2}\n    - Label: \"{label2}\"\n    - Description: \"{description2}\"\n    - Attributes: {attributes2}\n    \n    Similarity score: {similarity}\n    \n    Each expert should provide their perspective:\n    \n    1. Precision Expert (focus on exact naming and disambiguation):\n    \n    2. Pattern Recognizer (focus on identifying relationships and similarities):\n    \n    3. Domain Expert (focus on domain-specific knowledge):\n    \n    4. Skeptic (focus on critical evaluation and evidence):\n    \n    5. Pragmatist (focus on practical outcomes and usability):\n    \n    Final Decision:\n    Based on the discussion above, provide a final decision on whether to merge these entities.\n    Include reasoning and confidence level.\n    \"\"\"\n}\n\n# Sample test cases\nSAMPLE_TEST_CASES = [\n    {\n        \"prompt_type\": \"entity_resolution\",\n        \"variables\": {\n            \"new_entity_id\": \"urn:knowledgegarden:instance:entity_1\",\n            \"new_entity_label\": \"Knowledge Graph\",\n            \"new_entity_description\": \"A structured knowledge representation\",\n            \"new_entity_type\": \"Concept\",\n            \"new_entity_attributes\": json.dumps({\"domain\": \"information science\"}),\n            \"source\": \"Conversation 123\",\n            \"existing_entity_id\": \"urn:knowledgegarden:instance:entity_2\",\n            \"existing_entity_label\": \"Knowledge Base\",\n            \"existing_entity_description\": \"A database for knowledge management\",\n            \"existing_entity_type\": \"Concept\",\n            \"existing_entity_attributes\": json.dumps({\"domain\": \"database systems\"}),\n            \"existing_entity_tier\": \"verified\",\n            \"existing_entity_created\": \"2023-01-01\",\n            \"similarity_score\": 0.82\n        },\n        \"expected_fields\": [\"resolution\", \"confidence\", \"reasoning\", \"recommended_label\"]\n    },\n    {\n        \"prompt_type\": \"sentence_extraction\",\n        \"variables\": {\n            \"sentence\": \"Knowledge graphs consist of entities connected by relationships.\",\n            \"context\": \"Position 2 of 10\\nPrevious context: Knowledge graphs are powerful tools.\\nFollowing context: They help organize information.\"\n        },\n        \"expected_fields\": [\"triples\", \"confidence\", \"reasoning\"]\n    }\n]\n\nclass MockLLMService:\n    \"\"\"Mock LLM service for testing prompts\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize with responses for test cases\"\"\"\n        self.responses = {\n            \"entity_resolution\": {\n                \"resolution\": \"NO_MATCH\",\n                \"confidence\": 0.85,\n                \"reasoning\": \"While both entities are related to knowledge representation, they refer to different concepts. Knowledge Graphs emphasize relationships between entities, while Knowledge Bases focus on storing factual information. The different descriptions and domains (information science vs. database systems) support keeping them separate.\",\n                \"recommended_label\": None,\n                \"additional_info_needed\": [\"Usage examples\", \"Formal definitions\"]\n            },\n            \"sentence_extraction\": [\n                {\n                    \"triples\": [\n                        {\n                            \"subject\": \"knowledge_graph\",\n                            \"predicate\": \"consists_of\",\n                            \"object\": \"entity\",\n                            \"subject_type\": \"Concept\",\n                            \"object_type\": \"Concept\",\n                            \"confidence\": 0.95\n                        },\n                        {\n                            \"subject\": \"entity\",\n                            \"predicate\": \"connected_by\",\n                            \"object\": \"relationship\",\n                            \"subject_type\": \"Concept\",\n                            \"object_type\": \"Concept\",\n                            \"confidence\": 0.92\n                        }\n                    ],\n                    \"confidence\": 0.93,\n                    \"reasoning\": \"The sentence clearly states the composition of knowledge graphs.\"\n                }\n            ]\n        }\n    \n    async def generate(self, prompt: str) -> str:\n        \"\"\"Generate mock response based on prompt content\"\"\"\n        if \"New entity\" in prompt and \"Existing entity\" in prompt:\n            return json.dumps(self.responses[\"entity_resolution\"], indent=2)\n        elif \"Extract structured knowledge\" in prompt:\n            return json.dumps(self.responses[\"sentence_extraction\"][0], indent=2)\n        else:\n            return json.dumps({\"error\": \"No matching test case\"})\n\ndef load_prompts_from_config() -> Dict[str, str]:\n    \"\"\"Load prompts from configuration files\"\"\"\n    prompts = {}\n    \n    # First try to use the prompts module\n    try:\n        for prompt_type in PROMPT_TYPES:\n            template = get_template_text(prompt_type)\n            if template:\n                prompts[prompt_type] = template\n    except (NameError, TypeError):\n        logger.warning(\"Could not use prompts module, falling back to file search\")\n    \n    # Check extraction prompt templates\n    if not prompts:\n        extraction_dir = Path(\"knowledge_garden/extraction/prompts\")\n        if extraction_dir.exists():\n            for prompt_file in extraction_dir.glob(\"*.yaml\"):\n                try:\n                    with open(prompt_file, 'r') as f:\n                        prompt_data = yaml.safe_load(f)\n                        prompt_type = prompt_file.stem\n                        if isinstance(prompt_data, dict) and \"template\" in prompt_data:\n                            prompts[prompt_type] = prompt_data.get(\"template\", \"\")\n                except Exception as e:\n                    logger.error(f\"Error loading {prompt_file}: {e}\")\n    \n    # Check parliament prompt templates\n    if not prompts:\n        parliament_dir = Path(\"parliament/prompts\")\n        if parliament_dir.exists():\n            for prompt_file in parliament_dir.glob(\"*.yaml\"):\n                try:\n                    with open(prompt_file, 'r') as f:\n                        prompt_data = yaml.safe_load(f)\n                        prompt_type = prompt_file.stem\n                        if isinstance(prompt_data, dict) and \"description\" in prompt_data:\n                            prompts[prompt_type] = prompt_data.get(\"template\", \"\")\n                except Exception as e:\n                    logger.error(f\"Error loading {prompt_file}: {e}\")\n    \n    # If no prompts found in files, use the sample prompts\n    if not prompts:\n        prompts = SAMPLE_PROMPTS\n    \n    return prompts\n\ndef assess_prompt(prompt_type: str, prompt_text: str) -> Dict[str, Any]:\n    \"\"\"Assess the quality of a prompt based on best practices\"\"\"\n    # Check for various prompt quality indicators\n    has_clear_instructions = \"Instructions:\" in prompt_text or \"instructions:\" in prompt_text.lower()\n    has_examples = \"Example\" in prompt_text or \"example\" in prompt_text.lower()\n    has_format_guidance = \"Format\" in prompt_text or \"format\" in prompt_text.lower() or \"JSON\" in prompt_text\n    has_reasoning = \"reasoning\" in prompt_text.lower() or \"Reasoning\" in prompt_text\n    has_confidence = \"confidence\" in prompt_text.lower()\n    \n    # Count sections and structure indicators\n    section_count = prompt_text.count(':') + prompt_text.count('\\n\\n')\n    word_count = len(prompt_text.split())\n    \n    # Check for JSON structure\n    has_json_structure = \"```json\" in prompt_text or \"{\" in prompt_text and \"}\" in prompt_text\n    \n    # Evaluate against criteria\n    scores = {}\n    for criterion in CRITERIA:\n        # Calculate a score based on prompt characteristics\n        score = 0\n        \n        if criterion == \"clarity\":\n            score = 5 if has_clear_instructions else 3\n        elif criterion == \"specificity\":\n            score = 5 if section_count > 5 else 3\n        elif criterion == \"context\":\n            score = 5 if \"{context}\" in prompt_text or \"Context:\" in prompt_text else 2\n        elif criterion == \"error_handling\":\n            score = 4 if \"error\" in prompt_text.lower() else 2\n        elif criterion == \"output_format\":\n            score = 5 if has_format_guidance and has_json_structure else 3\n        elif criterion == \"few_shot_examples\":\n            score = 5 if has_examples else 2\n        elif criterion == \"guard_rails\":\n            score = 4 if \"avoid\" in prompt_text.lower() or \"don't\" in prompt_text.lower() else 3\n        elif criterion == \"adaptability\":\n            score = 4 if prompt_text.count(\"{\") > 2 else 3  # More variables = more adaptable\n        elif criterion == \"conciseness\":\n            score = 5 if word_count < 300 else 3 if word_count < 500 else 2\n        elif criterion == \"precision\":\n            score = 4 if prompt_text.count(\"specific\") > 0 else 3\n        elif criterion == \"reasoning_guidance\":\n            score = 5 if has_reasoning else 2\n        elif criterion == \"validation\":\n            score = 5 if has_confidence else 3\n            \n        scores[criterion] = score\n    \n    return {\n        \"prompt_type\": prompt_type,\n        \"word_count\": word_count,\n        \"section_count\": section_count,\n        \"has_instructions\": has_clear_instructions,\n        \"has_examples\": has_examples,\n        \"has_format_guidance\": has_format_guidance,\n        \"scores\": scores\n    }\n\nasync def assess_prompt_with_llm(\n    prompt_type: str, \n    variables: Dict[str, Any],\n    expected_fields: List[str],\n    llm_service: Any\n) -> Dict[str, Any]:\n    \"\"\"Assess a prompt with test variables using an LLM\n    \n    Args:\n        prompt_type: Type of prompt to assess\n        variables: Variables to substitute in the prompt\n        expected_fields: Expected fields in the response\n        llm_service: LLM service to use\n        \n    Returns:\n        Assessment results\n    \"\"\"\n    # Get prompt template\n    try:\n        template = get_template_text(prompt_type)\n    except (NameError, TypeError):\n        # Fall back to sample prompts\n        template = SAMPLE_PROMPTS.get(prompt_type, \"\")\n        \n    if not template:\n        return {\n            \"prompt_type\": prompt_type,\n            \"success\": False,\n            \"error\": f\"Template not found for prompt type: {prompt_type}\",\n            \"metrics\": {}\n        }\n    \n    # Format prompt\n    try:\n        formatted_prompt = format_prompt(prompt_type, **variables)\n    except (NameError, TypeError):\n        # Simple format with string formatting\n        try:\n            formatted_prompt = template.format(**variables)\n        except KeyError as e:\n            return {\n                \"prompt_type\": prompt_type,\n                \"success\": False,\n                \"error\": f\"Missing variable in template: {e}\",\n                \"metrics\": {}\n            }\n    \n    if not formatted_prompt:\n        return {\n            \"prompt_type\": prompt_type,\n            \"success\": False,\n            \"error\": f\"Failed to format prompt with variables\",\n            \"metrics\": {}\n        }\n    \n    try:\n        # Get LLM response\n        start_time = datetime.now()\n        response_text = await llm_service.generate(formatted_prompt)\n        end_time = datetime.now()\n        \n        # Parse response\n        try:\n            response = json.loads(response_text)\n        except json.JSONDecodeError:\n            # Try to extract JSON from text\n            import re\n            json_match = re.search(r'({.*}|\\[.*\\])', response_text, re.DOTALL)\n            if json_match:\n                response = json.loads(json_match.group(0))\n            else:\n                return {\n                    \"prompt_type\": prompt_type,\n                    \"success\": False,\n                    \"error\": \"Failed to parse JSON response\",\n                    \"raw_response\": response_text,\n                    \"metrics\": {\n                        \"response_time_ms\": (end_time - start_time).total_seconds() * 1000\n                    }\n                }\n        \n        # Calculate metrics\n        duration_ms = (end_time - start_time).total_seconds() * 1000\n        token_count = len(formatted_prompt.split()) + len(response_text.split())\n        missing_fields = [field for field in expected_fields if field not in response]\n        \n        return {\n            \"prompt_type\": prompt_type,\n            \"success\": len(missing_fields) == 0,\n            \"response\": response,\n            \"missing_fields\": missing_fields,\n            \"metrics\": {\n                \"response_time_ms\": duration_ms,\n                \"prompt_tokens\": len(formatted_prompt.split()),\n                \"response_tokens\": len(response_text.split()),\n                \"total_tokens\": token_count,\n                \"field_coverage\": (len(expected_fields) - len(missing_fields)) / len(expected_fields) if expected_fields else 1.0\n            }\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error assessing prompt: {e}\")\n        return {\n            \"prompt_type\": prompt_type,\n            \"success\": False,\n            \"error\": str(e),\n            \"metrics\": {}\n        }\n\nasync def run_functional_assessment(test_cases: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Run functional assessment on test cases\n    \n    Args:\n        test_cases: List of test cases\n        \n    Returns:\n        Assessment results\n    \"\"\"\n    llm_service = MockLLMService()\n    results = []\n    \n    for case in test_cases:\n        result = await assess_prompt_with_llm(\n            case[\"prompt_type\"],\n            case[\"variables\"],\n            case[\"expected_fields\"],\n            llm_service\n        )\n        results.append(result)\n    \n    return results\n\ndef generate_prompt_assessment_report() -> str:\n    \"\"\"Generate a full prompt assessment report and return path to CSV file\"\"\"\n    results = []\n    \n    # Load prompts from config\n    prompts = load_prompts_from_config()\n    \n    # Assess each prompt\n    for prompt_type, prompt_text in prompts.items():\n        assessment = assess_prompt(prompt_type, prompt_text)\n        \n        result = {\n            \"prompt_type\": prompt_type,\n            \"notes\": f\"Words: {assessment.get('word_count', 0)}, Sections: {assessment.get('section_count', 0)}\"\n        }\n        \n        # Add scores for each criterion\n        for criterion in CRITERIA:\n            result[criterion] = assessment.get(\"scores\", {}).get(criterion, \"N/A\")\n            \n        results.append(result)\n    \n    # Create CSV file\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    csv_path = f\"prompt_assessment_{timestamp}.csv\"\n    \n    with open(csv_path, 'w', newline='') as csvfile:\n        fieldnames = [\"prompt_type\"] + CRITERIA + [\"notes\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for result in results:\n            writer.writerow(result)\n    \n    return csv_path\n\nasync def run_functional_assessment_report(output_path: Optional[str] = None) -> str:\n    \"\"\"Run functional assessment and generate report\n    \n    Args:\n        output_path: Output directory for report\n        \n    Returns:\n        Path to the generated report\n    \"\"\"\n    # Create timestamp for filename\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"prompt_assessment_{timestamp}.csv\"\n    \n    if output_path:\n        filepath = os.path.join(output_path, filename)\n    else:\n        # Use current directory\n        filepath = filename\n    \n    # Run assessment\n    results = await run_functional_assessment(SAMPLE_TEST_CASES)\n    \n    # Flatten metrics for CSV\n    rows = []\n    for result in results:\n        row = {\n            \"prompt_type\": result[\"prompt_type\"],\n            \"success\": result[\"success\"]\n        }\n        \n        # Add metrics\n        for key, value in result.get(\"metrics\", {}).items():\n            row[f\"metric_{key}\"] = value\n        \n        # Add missing fields\n        row[\"missing_fields\"] = \", \".join(result.get(\"missing_fields\", []))\n        \n        # Add to rows\n        rows.append(row)\n    \n    # Get all field names\n    fieldnames = set()\n    for row in rows:\n        fieldnames.update(row.keys())\n    \n    # Write CSV\n    with open(filepath, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=sorted(fieldnames))\n        writer.writeheader()\n        for row in rows:\n            writer.writerow(row)\n    \n    logger.info(f\"Assessment report saved to {filepath}\")\n    return filepath\n\nasync def main_async():\n    \"\"\"Async main entry point\"\"\"\n    parser = argparse.ArgumentParser(description=\"Knowledge Garden v2.0 Prompt Assessment\")\n    parser.add_argument('--output', type=str, help=\"Output CSV file path\")\n    parser.add_argument('--mode', type=str, choices=['static', 'functional', 'both'], \n                      default='both', help=\"Assessment mode (static, functional, or both)\")\n    \n    args = parser.parse_args()\n    \n    logger.info(\"Starting prompt assessment...\")\n    \n    # Generate assessment reports based on mode\n    if args.mode in ['static', 'both']:\n        logger.info(\"Running static prompt assessment...\")\n        static_csv_path = args.output if args.output else generate_prompt_assessment_report()\n        logger.info(f\"Static assessment complete. Report saved to: {static_csv_path}\")\n    \n    if args.mode in ['functional', 'both']:\n        logger.info(\"Running functional prompt assessment...\")\n        functional_csv_path = await run_functional_assessment_report(\n            os.path.dirname(args.output) if args.output else None\n        )\n        logger.info(f\"Functional assessment complete. Report saved to: {functional_csv_path}\")\n    \n    logger.info(\"Assessment complete.\")\n    return 0\n\ndef main():\n    \"\"\"Command-line entry point\"\"\"\n    try:\n        return asyncio.run(main_async())\n    except Exception as e:\n        logger.error(f\"Assessment failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return 1\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n\n\n#=============================================================================\n# File: run_assessment.py\n# Description: Simple assessment script for Knowledge Garden extraction without dependencies\n# Improvement Area: Testing Infrastructure\n#=============================================================================\n#!/usr/bin/env python3\n\"\"\"\nSimple assessment script for Knowledge Garden extraction without dependencies\n\nThis script generates a quality assessment report without requiring external dependencies.\n\"\"\"\n\nimport os\nimport sys\nimport csv\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Create timestamp for the report\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n# Sample extraction metrics\nSAMPLE_METRICS = {\n    \"sentences_processed\": 10,\n    \"triples_extracted\": 25,\n    \"entities_created\": 18,\n    \"entities_merged\": 3,\n    \"evidence_records\": 28,\n    \"caps_extracted\": 2,\n    \"questions_extracted\": 1,\n    \"insights_extracted\": 3,\n    \"tensions_extracted\": 1,\n    \"avg_confidence\": 0.87,\n    \"entity_types\": {\n        \"Concept\": 10,\n        \"Organization\": 3,\n        \"Person\": 2,\n        \"Question\": 1,\n        \"Insight\": 3\n    },\n    \"triple_types\": {\n        \"consists_of\": 4,\n        \"has_property\": 6,\n        \"related_to\": 5,\n        \"creates\": 3,\n        \"part_of\": 2,\n        \"other\": 5\n    }\n}\n\n# Sample triple data\nSAMPLE_TRIPLES = [\n    {\n        \"subject_uri\": \"urn:knowledgegarden:instance:entity_knowledge_graph\",\n        \"predicate\": \"consists_of\",\n        \"object_uri\": \"urn:knowledgegarden:instance:entity_entity\",\n        \"confidence\": 0.92,\n        \"source_uri\": \"urn:test:conversation:1\"\n    },\n    {\n        \"subject_uri\": \"urn:knowledgegarden:instance:entity_entity\",\n        \"predicate\": \"connected_by\",\n        \"object_uri\": \"urn:knowledgegarden:instance:entity_relationship\",\n        \"confidence\": 0.90,\n        \"source_uri\": \"urn:test:conversation:1\"\n    },\n    {\n        \"subject_uri\": \"urn:knowledgegarden:instance:entity_knowledge_graph\",\n        \"predicate\": \"combined_with\",\n        \"object_uri\": \"urn:knowledgegarden:instance:entity_llm\",\n        \"confidence\": 0.85,\n        \"source_uri\": \"urn:test:conversation:1\"\n    },\n    {\n        \"subject_uri\": \"urn:knowledgegarden:instance:entity_combination\",\n        \"predicate\": \"creates\",\n        \"object_uri\": \"urn:knowledgegarden:instance:entity_information_retrieval\",\n        \"confidence\": 0.82,\n        \"source_uri\": \"urn:test:conversation:1\"\n    }\n]\n\ndef generate_csv_report():\n    \"\"\"Generate assessment report as CSV\"\"\"\n    # Prepare the data for the CSV\n    rows = []\n    \n    # Add statistical metrics\n    for key, value in SAMPLE_METRICS.items():\n        if key not in [\"entity_types\", \"triple_types\"]:\n            rows.append({\n                \"category\": \"Statistics\",\n                \"metric\": key,\n                \"value\": value\n            })\n    \n    # Add entity type distribution\n    for entity_type, count in SAMPLE_METRICS[\"entity_types\"].items():\n        rows.append({\n            \"category\": \"Entity Types\",\n            \"metric\": entity_type,\n            \"value\": count\n        })\n    \n    # Add triple type distribution\n    for triple_type, count in SAMPLE_METRICS[\"triple_types\"].items():\n        rows.append({\n            \"category\": \"Triple Types\",\n            \"metric\": triple_type,\n            \"value\": count\n        })\n    \n    # Add quality metrics\n    rows.append({\n        \"category\": \"Quality\",\n        \"metric\": \"Triples per Sentence\",\n        \"value\": SAMPLE_METRICS[\"triples_extracted\"] / SAMPLE_METRICS[\"sentences_processed\"]\n    })\n    \n    rows.append({\n        \"category\": \"Quality\",\n        \"metric\": \"Entity Type Coverage\",\n        \"value\": len(SAMPLE_METRICS[\"entity_types\"]) / SAMPLE_METRICS[\"entities_created\"]\n    })\n    \n    # Write the CSV file\n    csv_filename = f\"assessment_report_{timestamp}.csv\"\n    with open(csv_filename, 'w', newline='') as csvfile:\n        fieldnames = [\"category\", \"metric\", \"value\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for row in rows:\n            writer.writerow(row)\n    \n    print(f\"CSV report generated: {csv_filename}\")\n    return csv_filename\n\ndef generate_markdown_summary():\n    \"\"\"Generate assessment summary in Markdown\"\"\"\n    md_filename = \"assessment_summary.md\"\n    \n    with open(md_filename, 'w') as f:\n        f.write(\"# Knowledge Extraction Quality Assessment\\n\\n\")\n        f.write(f\"*Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\\n\")\n        \n        f.write(\"## Extraction Statistics\\n\\n\")\n        f.write(f\"- **Sentences Processed**: {SAMPLE_METRICS['sentences_processed']}\\n\")\n        f.write(f\"- **Triples Extracted**: {SAMPLE_METRICS['triples_extracted']}\\n\")\n        f.write(f\"- **Entities Created**: {SAMPLE_METRICS['entities_created']}\\n\")\n        f.write(f\"- **Entities Merged**: {SAMPLE_METRICS['entities_merged']}\\n\")\n        f.write(f\"- **Evidence Records**: {SAMPLE_METRICS['evidence_records']}\\n\\n\")\n        \n        f.write(\"## Pattern Extraction\\n\\n\")\n        f.write(f\"- **CAPs Extracted**: {SAMPLE_METRICS['caps_extracted']}\\n\")\n        f.write(f\"- **Questions Extracted**: {SAMPLE_METRICS['questions_extracted']}\\n\")\n        f.write(f\"- **Insights Extracted**: {SAMPLE_METRICS['insights_extracted']}\\n\")\n        f.write(f\"- **Tensions Extracted**: {SAMPLE_METRICS['tensions_extracted']}\\n\\n\")\n        \n        f.write(\"## Quality Metrics\\n\\n\")\n        f.write(f\"- **Average Triple Confidence**: {SAMPLE_METRICS['avg_confidence']:.2f}\\n\")\n        f.write(f\"- **Triples per Sentence**: {SAMPLE_METRICS['triples_extracted'] / SAMPLE_METRICS['sentences_processed']:.2f}\\n\")\n        f.write(f\"- **Entity Type Distribution**:\\n\")\n        for entity_type, count in SAMPLE_METRICS[\"entity_types\"].items():\n            f.write(f\"  - {entity_type}: {count}\\n\")\n        f.write(\"\\n\")\n        \n        f.write(\"## Sample Triples\\n\\n\")\n        f.write(\"| Subject | Predicate | Object | Confidence |\\n\")\n        f.write(\"|---------|-----------|--------|------------|\\n\")\n        for triple in SAMPLE_TRIPLES:\n            subject = triple.get(\"subject_uri\", \"\").split(\"/\")[-1].split(\"_\")[-1] if triple.get(\"subject_uri\") else \"\"\n            predicate = triple.get(\"predicate\", \"\")\n            object_val = triple.get(\"object_uri\", \"\").split(\"/\")[-1].split(\"_\")[-1] if triple.get(\"object_uri\") else \"\"\n            confidence = f\"{triple.get('confidence', 0):.2f}\"\n            f.write(f\"| {subject} | {predicate} | {object_val} | {confidence} |\\n\")\n    \n    print(f\"Markdown summary generated: {md_filename}\")\n    return md_filename\n\ndef main():\n    \"\"\"Main function\"\"\"\n    print(\"Generating Knowledge Garden extraction assessment...\")\n    \n    # Generate reports\n    csv_file = generate_csv_report()\n    md_file = generate_markdown_summary()\n    \n    print(\"\\nAssessment Report Summary:\")\n    print(f\"- CSV Report: {csv_file}\")\n    print(f\"- Markdown Summary: {md_file}\")\n    \n    # Print a summary of the assessment\n    print(\"\\nQuick Summary:\")\n    print(f\"- Processed {SAMPLE_METRICS['sentences_processed']} sentences\")\n    print(f\"- Extracted {SAMPLE_METRICS['triples_extracted']} triples\")\n    print(f\"- Created {SAMPLE_METRICS['entities_created']} entities\")\n    print(f\"- Average confidence: {SAMPLE_METRICS['avg_confidence']:.2f}\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n\n\n#=============================================================================\n# File: test_extraction.py\n# Description: Test script for Knowledge Garden extraction\n# Improvement Area: Testing Infrastructure\n#=============================================================================\n#!/usr/bin/env python3\n\"\"\"Test script for Knowledge Garden extraction\n\nThis script processes a sample conversation file to verify that the\nextraction pipeline works correctly and assess quality.\n\"\"\"\nimport os\nimport sys\nimport json\nimport tempfile\nimport argparse\nimport asyncio\nimport logging\nimport csv\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional, Tuple\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, \n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(\"extraction_test\")\n\n# Add the parent directory to the Python path if needed\nsys.path.append(str(Path(__file__).parent.parent))\n\n# Import extraction components\nfrom knowledge_garden.extraction.embedding_generator import EmbeddingGenerator\nfrom knowledge_garden.extraction.pipeline import ExtractionPipeline\nfrom knowledge_garden.extraction.prompt_manager import PromptManager\nfrom knowledge_garden.core.fuseki_client import FusekiClient\nfrom knowledge_garden.extraction.config import ExtractionConfig\n\n# Sample text for testing\nSAMPLE_TEXT = \"\"\"\nKnowledge graphs are powerful tools for organizing information in a structured way. \nThey consist of entities connected by relationships, forming a network of knowledge.\nLarge Language Models like Claude can help extract knowledge from unstructured text and convert it into structured data.\nI've realized that combining knowledge graphs with LLMs creates opportunities for more robust information retrieval.\n\nThere's a tension between automated extraction and human curation of knowledge. While automation is efficient,\nhuman expertise ensures quality and relevance. Moments when automation fails are opportunities for improving extraction algorithms.\n\nHow might we balance precision and recall in knowledge extraction systems? This is a key question for researchers.\nOrganizations like Anthropic are working on making AI systems more helpful, harmless, and honest.\n\nThe insight is that knowledge graphs provide context that enhances LLM performance, while LLMs can help populate and query knowledge graphs.\n\"\"\"\n\nclass LLMService:\n    \"\"\"Simple LLM service for testing\"\"\"\n    \n    def __init__(self, api_key: Optional[str] = None):\n        \"\"\"Initialize with API key if available\"\"\"\n        self.api_key = api_key\n        self.has_api = api_key is not None\n        self.client = None\n        \n        if api_key:\n            try:\n                import anthropic\n                self.client = anthropic.Anthropic(api_key=api_key)\n                logger.info(\"Initialized Anthropic client\")\n            except ImportError:\n                logger.warning(\"Anthropic package not installed\")\n                self.has_api = False\n    \n    async def generate(self, prompt: str, max_tokens: int = 1000) -> str:\n        \"\"\"Generate text using LLM or mock response for testing\"\"\"\n        if self.has_api and self.client:\n            try:\n                response = self.client.messages.create(\n                    model=\"claude-3-haiku-20240307\",\n                    max_tokens=max_tokens,\n                    temperature=0.1,\n                    messages=[{\"role\": \"user\", \"content\": prompt}]\n                )\n                return response.content[0].text\n            except Exception as e:\n                logger.error(f\"Error generating with LLM: {e}\")\n                return self._mock_response(prompt)\n        else:\n            return self._mock_response(prompt)\n    \n    def _mock_response(self, prompt: str) -> str:\n        \"\"\"Generate mock response for testing without API\"\"\"\n        if \"entity resolution\" in prompt.lower():\n            return \"\"\"\n            {\n              \"action\": \"keep_separate\",\n              \"confidence\": 0.85,\n              \"reasoning\": \"These appear to be different entities based on context.\"\n            }\n            \"\"\"\n        elif \"extract structured knowledge\" in prompt.lower():\n            return \"\"\"\n            {\n              \"triples\": [\n                {\n                  \"subject\": \"knowledge_graph\",\n                  \"predicate\": \"organizes\",\n                  \"object\": \"information\",\n                  \"subject_type\": \"Concept\",\n                  \"object_type\": \"Concept\",\n                  \"confidence\": 0.92\n                },\n                {\n                  \"subject\": \"knowledge_graph\",\n                  \"predicate\": \"consists_of\",\n                  \"object\": \"entities\",\n                  \"subject_type\": \"Concept\",\n                  \"object_type\": \"Concept\",\n                  \"confidence\": 0.90\n                }\n              ],\n              \"extracted_patterns\": {\n                \"insight\": \"knowledge graphs provide context that enhances LLM performance\"\n              },\n              \"confidence\": 0.88,\n              \"reasoning\": \"The sentence clearly defines knowledge graphs and their components.\"\n            }\n            \"\"\"\n        else:\n            return \"\"\"\n            {\n              \"result\": \"Mock response for testing\",\n              \"confidence\": 0.75\n            }\n            \"\"\"\n\nclass MockFusekiClient:\n    \"\"\"Mock Fuseki client for testing\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize with empty triple store\"\"\"\n        self.triples = []\n        self.query_results = {}\n    \n    async def insert_triples(self, triples: List[Dict[str, Any]], graph_uri: Optional[str] = None) -> bool:\n        \"\"\"Mock triple insertion\"\"\"\n        self.triples.extend(triples)\n        logger.info(f\"Inserted {len(triples)} triples into graph {graph_uri}\")\n        return True\n    \n    async def query(self, sparql_query: str) -> List[Dict[str, Any]]:\n        \"\"\"Mock SPARQL query execution\"\"\"\n        # Return different mock results based on query content\n        if \"COUNT\" in sparql_query:\n            return [{\"count\": len(self.triples)}]\n        elif \"entityType\" in sparql_query:\n            return [\n                {\"type\": \"Concept\", \"count\": \"10\"},\n                {\"type\": \"Person\", \"count\": \"5\"},\n                {\"type\": \"Organization\", \"count\": \"3\"}\n            ]\n        elif \"extractorId\" in sparql_query:\n            return [\n                {\"extractor\": \"sentence_extractor\", \"count\": \"15\"},\n                {\"extractor\": \"pattern_extractor\", \"count\": \"8\"}\n            ]\n        return []\n    \n    async def execute_update(self, sparql_update: str) -> bool:\n        \"\"\"Mock SPARQL update execution\"\"\"\n        logger.info(f\"Executed update: {sparql_update[:50]}...\")\n        return True\n\ndef create_sample_conversation():\n    \"\"\"Create a sample conversation for testing\"\"\"\n    sample = {\n        \"id\": f\"sample_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n        \"title\": \"Test Conversation\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"\"\"\n                I noticed that MOMENTS when people feel stuck are OPPORTUNITIES for creative thinking.\n                There's a tension between structured planning and spontaneous creativity.\n                I realized that combining both approaches leads to better outcomes.\n                How might we balance these opposing forces?\n                \"\"\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"That's an interesting observation about creative processes.\"\n            }\n        ]\n    }\n    \n    # Write to temporary file\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n        json.dump([sample], f)\n        return Path(f.name)\n\nasync def run_extraction_test(config_path: Optional[str] = None) -> Dict[str, Any]:\n    \"\"\"Run extraction pipeline test\n    \n    Args:\n        config_path: Path to configuration file\n    \n    Returns:\n        Test results\n    \"\"\"\n    # Create config\n    config = ExtractionConfig(config_file=config_path).as_dict()\n    \n    # Get API key from environment or config\n    api_key = os.environ.get(\"ANTHROPIC_API_KEY\") or config.get(\"api_keys\", {}).get(\"anthropic\")\n    \n    # Initialize services\n    embedding_service = EmbeddingGenerator()\n    llm_service = LLMService(api_key=api_key)\n    fuseki_client = MockFusekiClient()\n    \n    # Initialize prompt manager\n    prompt_manager = PromptManager(llm_service=llm_service)\n    \n    # Initialize extraction pipeline\n    pipeline = ExtractionPipeline(\n        embedding_service=embedding_service,\n        llm_service=llm_service,\n        fuseki_client=fuseki_client,\n        config=config\n    )\n    \n    # Run extraction on sample text\n    logger.info(\"Running extraction pipeline on sample text\")\n    source_uri = \"urn:test:conversation:1\"\n    \n    extraction_results = await pipeline.extract_from_text(\n        text=SAMPLE_TEXT,\n        source_uri=source_uri,\n        context={\"test_mode\": True}\n    )\n    \n    # Store results\n    logger.info(\"Storing extraction results\")\n    await pipeline.store_extraction_results(extraction_results)\n    \n    # Run SPARQL queries to assess results\n    logger.info(\"Running assessment queries\")\n    assessment_results = await run_assessment_queries(fuseki_client)\n    \n    # Combine results\n    results = {\n        \"extraction\": extraction_results,\n        \"assessment\": assessment_results,\n        \"timestamp\": datetime.now().isoformat()\n    }\n    \n    return results\n\nasync def run_assessment_queries(fuseki_client: Any) -> Dict[str, Any]:\n    \"\"\"Run assessment queries on the knowledge graph\n    \n    Args:\n        fuseki_client: Fuseki client instance\n    \n    Returns:\n        Assessment results\n    \"\"\"\n    # Define assessment queries\n    queries = {\n        \"total_triples\": \"\"\"\n            SELECT (COUNT(?s) as ?count)\n            WHERE {\n                ?s ?p ?o .\n            }\n        \"\"\",\n        \n        \"entity_types\": \"\"\"\n            PREFIX kg: <urn:knowledgegarden:ontology#>\n            SELECT ?type (COUNT(?e) as ?count)\n            WHERE {\n                ?e kg:entityType ?type .\n            }\n            GROUP BY ?type\n            ORDER BY DESC(?count)\n        \"\"\",\n        \n        \"extractors\": \"\"\"\n            PREFIX kg: <urn:knowledgegarden:ontology#>\n            SELECT ?extractor (COUNT(?ev) as ?count)\n            WHERE {\n                ?ev kg:extractorId ?extractor .\n            }\n            GROUP BY ?extractor\n            ORDER BY DESC(?count)\n        \"\"\"\n    }\n    \n    # Run queries\n    results = {}\n    for name, query in queries.items():\n        results[name] = await fuseki_client.query(query)\n    \n    return results\n\ndef generate_assessment_report(results: Dict[str, Any], output_path: str) -> str:\n    \"\"\"Generate assessment report in CSV format\n    \n    Args:\n        results: Test results\n        output_path: Output directory for report\n        \n    Returns:\n        Path to the generated report\n    \"\"\"\n    # Create timestamp for filename\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"assessment_report_{timestamp}.csv\"\n    filepath = os.path.join(output_path, filename)\n    \n    # Extract metrics\n    extraction_stats = results[\"extraction\"][\"stats\"]\n    entities = results[\"extraction\"][\"entities\"]\n    triples = results[\"extraction\"][\"triples\"]\n    \n    # Quality metrics\n    metrics = [\n        # General statistics\n        {\"category\": \"Statistics\", \"metric\": \"Sentences Processed\", \"value\": extraction_stats[\"sentences_processed\"]},\n        {\"category\": \"Statistics\", \"metric\": \"Triples Extracted\", \"value\": extraction_stats[\"triples_extracted\"]},\n        {\"category\": \"Statistics\", \"metric\": \"Entities Created\", \"value\": extraction_stats[\"entities_created\"]},\n        {\"category\": \"Statistics\", \"metric\": \"Entities Merged\", \"value\": extraction_stats[\"entities_merged\"]},\n        {\"category\": \"Statistics\", \"metric\": \"Evidence Records\", \"value\": extraction_stats[\"evidence_records\"]},\n        \n        # Pattern extraction\n        {\"category\": \"Patterns\", \"metric\": \"CAPs Extracted\", \"value\": extraction_stats[\"caps_extracted\"]},\n        {\"category\": \"Patterns\", \"metric\": \"Questions Extracted\", \"value\": extraction_stats[\"questions_extracted\"]},\n        {\"category\": \"Patterns\", \"metric\": \"Insights Extracted\", \"value\": extraction_stats[\"insights_extracted\"]},\n        {\"category\": \"Patterns\", \"metric\": \"Tensions Extracted\", \"value\": extraction_stats[\"tensions_extracted\"]},\n        \n        # Quality metrics\n        {\"category\": \"Quality\", \"metric\": \"Avg Triple Confidence\", \n         \"value\": np.mean([t[\"confidence\"] for t in triples]) if triples else 0},\n        {\"category\": \"Quality\", \"metric\": \"Triples per Sentence\", \n         \"value\": extraction_stats[\"triples_extracted\"] / extraction_stats[\"sentences_processed\"] \n                 if extraction_stats[\"sentences_processed\"] > 0 else 0},\n        {\"category\": \"Quality\", \"metric\": \"Entity Type Coverage\", \n         \"value\": len(set(e.entity_type for e in entities if e.entity_type)) / len(entities) if entities else 0}\n    ]\n    \n    # Write CSV\n    with open(filepath, 'w', newline='') as csvfile:\n        fieldnames = ['category', 'metric', 'value']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for metric in metrics:\n            writer.writerow(metric)\n    \n    logger.info(f\"Assessment report saved to {filepath}\")\n    return filepath\n\ndef generate_quality_summary(results: Dict[str, Any], output_path: str) -> str:\n    \"\"\"Generate quality assessment summary in Markdown\n    \n    Args:\n        results: Test results\n        output_path: Output directory for summary\n        \n    Returns:\n        Path to the generated summary\n    \"\"\"\n    filepath = os.path.join(output_path, \"assessment_summary.md\")\n    \n    # Extract data for summary\n    extraction_stats = results[\"extraction\"][\"stats\"]\n    entities = results[\"extraction\"][\"entities\"]\n    triples = results[\"extraction\"][\"triples\"]\n    \n    # Calculate metrics\n    avg_confidence = np.mean([t[\"confidence\"] for t in triples]) if triples else 0\n    entity_types = {}\n    for e in entities:\n        entity_type = e.entity_type or \"Unknown\"\n        entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n    \n    # Generate summary\n    with open(filepath, 'w') as f:\n        f.write(\"# Knowledge Extraction Quality Assessment\\n\\n\")\n        f.write(f\"*Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\\n\")\n        \n        f.write(\"## Extraction Statistics\\n\\n\")\n        f.write(f\"- **Sentences Processed**: {extraction_stats['sentences_processed']}\\n\")\n        f.write(f\"- **Triples Extracted**: {extraction_stats['triples_extracted']}\\n\")\n        f.write(f\"- **Entities Created**: {extraction_stats['entities_created']}\\n\")\n        f.write(f\"- **Entities Merged**: {extraction_stats['entities_merged']}\\n\")\n        f.write(f\"- **Evidence Records**: {extraction_stats['evidence_records']}\\n\\n\")\n        \n        f.write(\"## Pattern Extraction\\n\\n\")\n        f.write(f\"- **CAPs Extracted**: {extraction_stats['caps_extracted']}\\n\")\n        f.write(f\"- **Questions Extracted**: {extraction_stats['questions_extracted']}\\n\")\n        f.write(f\"- **Insights Extracted**: {extraction_stats['insights_extracted']}\\n\")\n        f.write(f\"- **Tensions Extracted**: {extraction_stats['tensions_extracted']}\\n\\n\")\n        \n        f.write(\"## Quality Metrics\\n\\n\")\n        f.write(f\"- **Average Triple Confidence**: {avg_confidence:.2f}\\n\")\n        f.write(f\"- **Triples per Sentence**: {extraction_stats['triples_extracted'] / extraction_stats['sentences_processed']:.2f}\\n\")\n        f.write(f\"- **Entity Type Distribution**:\\n\")\n        for entity_type, count in entity_types.items():\n            f.write(f\"  - {entity_type}: {count}\\n\")\n        f.write(\"\\n\")\n        \n        f.write(\"## Sample Triples\\n\\n\")\n        f.write(\"| Subject | Predicate | Object | Confidence |\\n\")\n        f.write(\"|---------|-----------|--------|------------|\\n\")\n        for i, triple in enumerate(triples):\n            if i >= 10:  # Limit to 10 samples\n                break\n            subject = triple.get(\"subject_uri\", \"\").split(\"/\")[-1] if triple.get(\"subject_uri\") else \"\"\n            predicate = triple.get(\"predicate\", \"\")\n            object_val = triple.get(\"object_uri\", \"\").split(\"/\")[-1] if triple.get(\"object_uri\") and not triple.get(\"is_literal\") else triple.get(\"object_literal\", \"\")\n            confidence = f\"{triple.get('confidence', 0):.2f}\"\n            f.write(f\"| {subject} | {predicate} | {object_val} | {confidence} |\\n\")\n    \n    logger.info(f\"Quality summary saved to {filepath}\")\n    return filepath\n\nasync def main_async():\n    \"\"\"Async main entry point\"\"\"\n    logger.info(\"Starting extraction test\")\n    \n    # Run extraction test\n    results = await run_extraction_test()\n    \n    # Generate reports\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    generate_assessment_report(results, current_dir)\n    generate_quality_summary(results, current_dir)\n    \n    logger.info(\"Extraction test complete\")\n    return 0\n\ndef main():\n    \"\"\"Command-line entry point\"\"\"\n    parser = argparse.ArgumentParser(description=\"Test Knowledge Garden extraction\")\n    parser.add_argument('--config', type=str, help=\"Path to configuration file\")\n    parser.add_argument('--api-key', help=\"Anthropic API key\")\n    parser.add_argument('--output-dir', type=str, help=\"Output directory for reports\")\n    \n    args = parser.parse_args()\n    \n    # Set API key from args or environment\n    if args.api_key:\n        os.environ[\"ANTHROPIC_API_KEY\"] = args.api_key\n    \n    # Run async main\n    try:\n        return asyncio.run(main_async())\n    except Exception as e:\n        logger.error(f\"Test failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return 1\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n\n"
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          },
          {
            "file_name": "paste-2.txt"
          },
          {
            "file_name": "knowledge_garden_improvement_code.py"
          }
        ]
      }
    },
    {
      "id": "a311ba85-2626-4954-8e33-2ce8a85261e8",
      "role": "assistant",
      "content": "Looking at the Knowledge Garden v2.0 implementation plan and the improvement requests, I can see several key areas that need attention:\n\n1. **Prompt Design Issues** (Current Score: 2.8/5) - The prompts lack proper structure, examples, and error handling\n2. **Testing Infrastructure** (Current Score: 1.3/5) - Missing unit tests, integration tests, and test data\n3. **Configuration System** (Current Score: 3.3/5) - Limited validation and documentation\n4. **Core Implementation Issues** - Several implementation files have issues that need fixing\n\nLet me identify the core issues and provide fixed versions of the key files:\n\n### Core Issues Identified:\n\n1. **Prompt Templates** - The YAML prompt files in `extraction/prompts/` and `parliament/prompts/` need to follow the improved structure suggested in the improvement document\n2. **Testing Infrastructure** - The test files need proper structure and comprehensive test coverage\n3. **Entity Resolution** - The entity resolver has some implementation issues with entity comparison\n4. **Pipeline Integration** - The extraction pipeline needs better error handling and integration\n5. **Configuration Validation** - The config validator needs to actually validate against schemas\n\nLet me fix the most critical files:Looking at the Knowledge Garden v2.0 implementation and improvement requests, I've identified several core issues that need to be addressed. Here are the fixed versions of the critical files:\n\n## 1. Fixed Entity Resolution Prompt (extraction/prompts/entity_resolution.yaml)\n\n```yaml\nprompt_type: entity_resolution\ntitle: Entity Resolution for Knowledge Garden\ndescription: Resolves potential entity matches using semantic understanding and similarity scores\nversion: 2.0\n\n# Resolution parameters\nparameters:\n  similarity_threshold: 0.85\n  high_confidence_threshold: 0.95\n  normalize_entities: true\n  consider_context: true\n\n# System context\nsystem_context: |\n  You are the Entity Resolver component in Knowledge Garden v2.0, a system that builds a structured knowledge graph from conversations. \n  Your task is to determine if a newly extracted entity matches an existing entity in the knowledge graph. \n  This resolution is critical for maintaining a clean, coherent knowledge graph without redundant entities.\n  \n  The Knowledge Garden system extracts entities from conversations and documents, organizing them into a three-tier knowledge model:\n  - Personal: Knowledge from user conversations\n  - Digested: Validated external knowledge\n  - Peripheral: References and citations\n\n# Input schema  \ninput_schema:\n  new_entity:\n    type: object\n    required: [id, label, type]\n    properties:\n      id: {type: string}\n      label: {type: string}\n      description: {type: string}\n      type: {type: string}\n      attributes: {type: object}\n      source: {type: string}\n  existing_entity:\n    type: object\n    required: [id, label, type]\n    properties:\n      id: {type: string}\n      label: {type: string}\n      description: {type: string}\n      type: {type: string}\n      attributes: {type: object}\n      tier: {type: string}\n      created: {type: string}\n  similarity_score:\n    type: number\n    minimum: 0.0\n    maximum: 1.0\n\n# Examples section with diverse cases\nexamples:\n  - name: \"Abbreviation Match\"\n    input:\n      new_entity:\n        label: \"ML\"\n        type: \"Concept\"\n        description: \"Machine learning techniques\"\n      existing_entity:\n        label: \"Machine Learning\"\n        type: \"Concept\"\n        description: \"Field of artificial intelligence focused on learning from data\"\n      similarity_score: 0.87\n    output:\n      resolution: \"MATCH\"\n      confidence: 0.95\n      reasoning: |\n        \"ML\" is a standard abbreviation for \"Machine Learning\". Both entities refer to the same concept\n        with consistent descriptions. The type matches (Concept) and the descriptions are complementary.\n        High confidence merge is appropriate.\n      recommended_label: \"Machine Learning\"\n      \n  - name: \"Homonym Non-Match\"\n    input:\n      new_entity:\n        label: \"Python\"\n        type: \"Programming_Language\"\n        description: \"High-level programming language\"\n      existing_entity:\n        label: \"Python\"\n        type: \"Animal\"\n        description: \"Large snake species\"\n      similarity_score: 0.92\n    output:\n      resolution: \"NO_MATCH\"\n      confidence: 0.98\n      reasoning: |\n        Despite identical labels and high text similarity, these are completely different concepts.\n        The entity types (Programming_Language vs Animal) and descriptions clearly indicate distinct\n        entities. This is a classic homonym case where the same word has different meanings.\n        \n  - name: \"Related but Distinct\"\n    input:\n      new_entity:\n        label: \"Deep Learning\"\n        type: \"Concept\"\n        description: \"Neural networks with multiple layers\"\n      existing_entity:\n        label: \"Neural Networks\"\n        type: \"Concept\"\n        description: \"Computing systems inspired by biological neural networks\"\n      similarity_score: 0.76\n    output:\n      resolution: \"UNCERTAIN\"\n      confidence: 0.60\n      reasoning: |\n        These concepts are closely related - deep learning is a subset of neural networks.\n        However, they represent different levels of specificity. In a technical knowledge graph,\n        maintaining the distinction may be valuable. The decision depends on the desired\n        granularity of the knowledge representation.\n      additional_info_needed: [\"Domain context\", \"User preference for granularity\"]\n\n# Reasoning guidance\nreasoning_steps:\n  - step: \"Label Analysis\"\n    description: \"Compare entity labels for exact matches, abbreviations, synonyms, or variations\"\n    checks:\n      - \"Are the labels identical?\"\n      - \"Is one label an abbreviation of the other?\"\n      - \"Are they known synonyms in the domain?\"\n      - \"Do they share significant substring overlap?\"\n      \n  - step: \"Type Compatibility\"\n    description: \"Verify if entity types are compatible for merging\"\n    checks:\n      - \"Are the types identical?\"\n      - \"Is one type a supertype/subtype of the other?\"\n      - \"Are the types semantically incompatible?\"\n      \n  - step: \"Description Analysis\"\n    description: \"Compare descriptions for semantic similarity\"\n    checks:\n      - \"Do descriptions refer to the same concept?\"\n      - \"Are there contradictory elements?\"\n      - \"Is one description more specific than the other?\"\n      \n  - step: \"Attribute Comparison\"\n    description: \"Examine attributes for consistency\"\n    checks:\n      - \"Are attributes complementary or conflicting?\"\n      - \"Do shared attributes have consistent values?\"\n      - \"Would merging lose important distinctions?\"\n      \n  - step: \"Context Evaluation\"\n    description: \"Consider source and usage context\"\n    checks:\n      - \"Do entities come from the same knowledge tier?\"\n      - \"Is the source context compatible?\"\n      - \"Would merging cross important boundaries?\"\n      \n  - step: \"Confidence Assessment\"\n    description: \"Determine confidence based on evidence strength\"\n    checks:\n      - \"How many positive indicators support merging?\"\n      - \"Are there any strong negative indicators?\"\n      - \"Is additional information needed for certainty?\"\n\n# Error handling\nerror_handling:\n  missing_data:\n    - condition: \"Missing entity label\"\n      action: \"Return resolution='INSUFFICIENT_DATA', confidence=0.0\"\n    - condition: \"Missing entity type\"\n      action: \"Proceed with reduced confidence (-0.2)\"\n    - condition: \"Missing description\"\n      action: \"Rely more on label and attribute comparison\"\n    - condition: \"Invalid similarity score\"\n      action: \"Use 0.5 as default, note in reasoning\"\n      \n  edge_cases:\n    - condition: \"Entities from different knowledge tiers\"\n      action: \"Consider tier compatibility, may reduce merge confidence\"\n    - condition: \"Temporal entities (different time references)\"\n      action: \"Generally keep separate unless explicitly the same\"\n    - condition: \"Partial string matches\"\n      action: \"Require additional evidence beyond string similarity\"\n\n# Full prompt template\ntemplate: |\n  # Entity Resolution for Knowledge Garden v2.0\n\n  ## SYSTEM CONTEXT\n  {system_context}\n\n  ## ENTITIES TO COMPARE\n\n  ### New Entity\n  ID: {new_entity_id}\n  Label: \"{new_entity_label}\"\n  Description: \"{new_entity_description}\"\n  Type: \"{new_entity_type}\"\n  Attributes: {new_entity_attributes}\n  Extracted from: {source}\n\n  ### Existing Entity  \n  ID: {existing_entity_id}\n  Label: \"{existing_entity_label}\"\n  Description: \"{existing_entity_description}\"\n  Type: \"{existing_entity_type}\"\n  Attributes: {existing_entity_attributes}\n  Knowledge tier: {existing_entity_tier}\n  Creation date: {existing_entity_created}\n\n  Computed similarity score: {similarity_score}\n\n  ## RESOLUTION TASK\n  Determine whether these two entities refer to the same real-world concept and should be merged, \n  or if they represent distinct concepts and should remain separate.\n\n  ## RESOLUTION GUIDELINES\n\n  1. **Label Analysis**: {reasoning_steps[0].description}\n  2. **Type Compatibility**: {reasoning_steps[1].description}\n  3. **Description Analysis**: {reasoning_steps[2].description}\n  4. **Attribute Comparison**: {reasoning_steps[3].description}\n  5. **Context Evaluation**: {reasoning_steps[4].description}\n  6. **Confidence Assessment**: {reasoning_steps[5].description}\n\n  ## EXAMPLES\n\n  ### Example 1: Abbreviation Match\n  New: \"ML\" (Concept) | Existing: \"Machine Learning\" (Concept)\n  Resolution: MATCH (0.95) - Standard abbreviation for the same concept\n\n  ### Example 2: Homonym Non-Match  \n  New: \"Python\" (Programming_Language) | Existing: \"Python\" (Animal)\n  Resolution: NO_MATCH (0.98) - Same word, completely different concepts\n\n  ### Example 3: Related but Distinct\n  New: \"Deep Learning\" | Existing: \"Neural Networks\"\n  Resolution: UNCERTAIN (0.60) - Related concepts, granularity question\n\n  ## ERROR HANDLING\n\n  - If critical information is missing, return INSUFFICIENT_DATA\n  - For ambiguous cases, prefer NO_MATCH to prevent information loss\n  - Note when additional context would improve confidence\n\n  ## OUTPUT FORMAT\n  \n  Analyze the entities step-by-step following the reasoning guidelines, then provide your resolution as valid JSON:\n\n  ```json\n  {\n    \"resolution\": \"MATCH\" | \"NO_MATCH\" | \"UNCERTAIN\" | \"INSUFFICIENT_DATA\",\n    \"confidence\": 0.0-1.0,\n    \"reasoning\": \"Detailed step-by-step explanation of your resolution process and key factors...\",\n    \"recommended_label\": \"If MATCH, the best label to use for the merged entity\",\n    \"additional_info_needed\": [\"Optional list of information that would improve confidence\"]\n  }\n  ```\n\n# Output validation schema\noutput_schema:\n  type: object\n  required: [\"resolution\", \"confidence\", \"reasoning\"]\n  properties:\n    resolution:\n      type: string\n      enum: [\"MATCH\", \"NO_MATCH\", \"UNCERTAIN\", \"INSUFFICIENT_DATA\"]\n      description: \"The resolution decision\"\n    confidence:\n      type: number\n      minimum: 0.0\n      maximum: 1.0\n      description: \"Confidence in the resolution (0-1)\"\n    reasoning:\n      type: string\n      minLength: 50\n      description: \"Detailed explanation of the resolution logic\"\n    recommended_label:\n      type: string\n      description: \"Recommended label if entities are merged\"\n    additional_info_needed:\n      type: array\n      items:\n        type: string\n      description: \"Information that would improve resolution confidence\"\n```\n\n## 2. Fixed Entity Resolver Implementation (extraction/entity_resolver.py)\n\n```python\n\"\"\"Entity resolution using embeddings and LLM validation\"\"\"\nimport logging\nimport json\nfrom typing import Dict, List, Any, Optional, Tuple, NamedTuple\nimport numpy as np\nfrom datetime import datetime\n\nfrom knowledge_garden.models.evidence import Evidence\n\nlogger = logging.getLogger(__name__)\n\nclass Entity:\n    \"\"\"Entity in the knowledge graph\"\"\"\n    \n    def __init__(\n        self,\n        entity_id: str,\n        label: str,\n        embedding: Optional[np.ndarray] = None,\n        entity_type: Optional[str] = None,\n        attributes: Optional[Dict[str, Any]] = None,\n        source_uri: Optional[str] = None,\n        evidence_ids: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        tier: Optional[str] = None\n    ):\n        \"\"\"Initialize an entity\n        \n        Args:\n            entity_id: Unique identifier for the entity\n            label: Human-readable label/name\n            embedding: Vector representation of the entity\n            entity_type: Type of entity (Person, Organization, Concept, etc.)\n            attributes: Key-value attributes of the entity\n            source_uri: Source document/conversation URI\n            evidence_ids: IDs of evidence records\n            metadata: Additional metadata\n            tier: Knowledge tier (personal, digested, peripheral)\n        \"\"\"\n        self.entity_id = entity_id\n        self.label = label\n        self.embedding = embedding\n        self.entity_type = entity_type\n        self.attributes = attributes or {}\n        self.source_uri = source_uri\n        self.evidence_ids = evidence_ids or []\n        self.metadata = metadata or {}\n        self.tier = tier or \"personal\"\n        self.created_at = datetime.now()\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary representation\"\"\"\n        from knowledge_garden.models.extracted_sentence import ExtractedSentence\n        \n        return {\n            \"entity_id\": self.entity_id,\n            \"label\": self.label,\n            \"embedding\": ExtractedSentence.encode_embedding(self.embedding) if self.embedding is not None else None,\n            \"entity_type\": self.entity_type,\n            \"attributes\": self.attributes,\n            \"source_uri\": self.source_uri,\n            \"evidence_ids\": self.evidence_ids,\n            \"metadata\": self.metadata,\n            \"tier\": self.tier,\n            \"created_at\": self.created_at.isoformat()\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Entity':\n        \"\"\"Create from dictionary representation\"\"\"\n        from knowledge_garden.models.extracted_sentence import ExtractedSentence\n        \n        # Handle embedding conversion\n        embedding = None\n        if data.get(\"embedding\"):\n            embedding = ExtractedSentence.decode_embedding(data[\"embedding\"])\n        \n        entity = cls(\n            entity_id=data[\"entity_id\"],\n            label=data[\"label\"],\n            embedding=embedding,\n            entity_type=data.get(\"entity_type\"),\n            attributes=data.get(\"attributes\", {}),\n            source_uri=data.get(\"source_uri\"),\n            evidence_ids=data.get(\"evidence_ids\", []),\n            metadata=data.get(\"metadata\", {}),\n            tier=data.get(\"tier\", \"personal\")\n        )\n        \n        # Set created_at if available\n        if data.get(\"created_at\"):\n            entity.created_at = datetime.fromisoformat(data[\"created_at\"])\n            \n        return entity\n\nclass ResolutionResult(NamedTuple):\n    \"\"\"Result of entity resolution\"\"\"\n    action: str  # 'create_new', 'merge', 'keep_separate'\n    confidence: float\n    entity: Optional[Entity] = None\n    target_id: Optional[str] = None\n    reasoning: Optional[str] = None\n    requires_parliament: bool = False\n\nclass EntityResolver:\n    \"\"\"Resolve entities using embeddings and LLM validation\"\"\"\n    \n    def __init__(self, embedding_generator, llm_service, parliament_engine=None, config=None):\n        \"\"\"Initialize entity resolver\n        \n        Args:\n            embedding_generator: Service for generating and comparing embeddings\n            llm_service: Service for LLM validation\n            parliament_engine: Optional parliament decision engine\n            config: Configuration parameters\n        \"\"\"\n        self.embedder = embedding_generator\n        self.llm = llm_service\n        self.parliament = parliament_engine\n        self.config = config or {}\n        \n        # Load prompt template\n        self.prompt_template = self._load_prompt_template()\n    \n    def _load_prompt_template(self) -> str:\n        \"\"\"Load the entity resolution prompt template\"\"\"\n        try:\n            from knowledge_garden.extraction.prompts import get_template_text\n            template = get_template_text(\"entity_resolution\")\n            if template:\n                return template\n        except Exception as e:\n            logger.warning(f\"Could not load prompt template: {e}\")\n        \n        # Fallback to inline template\n        return \"\"\"\n        # Entity Resolution for Knowledge Garden v2.0\n        \n        Determine if these entities should be merged or kept separate.\n        \n        New entity:\n        - Label: \"{new_entity_label}\"\n        - Type: \"{new_entity_type}\"\n        - Description: \"{new_entity_description}\"\n        - Attributes: {new_entity_attributes}\n        \n        Existing entity:\n        - Label: \"{existing_entity_label}\"\n        - Type: \"{existing_entity_type}\"\n        - Description: \"{existing_entity_description}\"\n        - Attributes: {existing_entity_attributes}\n        - Tier: {existing_entity_tier}\n        \n        Similarity score: {similarity_score}\n        \n        Consider:\n        1. Semantic similarity beyond surface text\n        2. Entity type compatibility\n        3. Attribute consistency\n        4. Knowledge tier implications\n        \n        Return JSON:\n        {{\n          \"resolution\": \"MATCH\" or \"NO_MATCH\" or \"UNCERTAIN\",\n          \"confidence\": 0.0-1.0,\n          \"reasoning\": \"Explanation\",\n          \"recommended_label\": \"Best label if merged\"\n        }}\n        \"\"\"\n    \n    def _format_entity_attributes(self, entity: Entity) -> str:\n        \"\"\"Format entity attributes for prompt\"\"\"\n        if not entity.attributes:\n            return \"{}\"\n        \n        # Limit attributes to prevent prompt overflow\n        limited_attrs = {}\n        for key, value in list(entity.attributes.items())[:5]:\n            if isinstance(value, str) and len(value) > 100:\n                limited_attrs[key] = value[:100] + \"...\"\n            else:\n                limited_attrs[key] = value\n                \n        return json.dumps(limited_attrs, ensure_ascii=False, indent=2)\n    \n    def _get_entity_description(self, entity: Entity) -> str:\n        \"\"\"Get entity description from attributes or generate one\"\"\"\n        # Check for description in attributes\n        for key in [\"description\", \"desc\", \"summary\", \"definition\"]:\n            if key in entity.attributes:\n                return str(entity.attributes[key])\n        \n        # Generate description based on type and label\n        if entity.entity_type:\n            return f\"A {entity.entity_type.lower()} entity: {entity.label}\"\n        else:\n            return f\"Entity: {entity.label}\"\n    \n    async def resolve_entity(self, new_entity: Entity, \n                           existing_entities: Dict[str, Entity],\n                           similarity_threshold: float = None,\n                           high_confidence_threshold: float = None) -> ResolutionResult:\n        \"\"\"Resolve entity against existing entities\n        \n        Args:\n            new_entity: New entity to resolve\n            existing_entities: Dictionary of {entity_id: entity}\n            similarity_threshold: Minimum similarity for candidates\n            high_confidence_threshold: Threshold for automatic decisions\n            \n        Returns:\n            Resolution result\n        \"\"\"\n        # Use config values if not provided\n        if similarity_threshold is None:\n            similarity_threshold = self.config.get(\"similarity_threshold\", 0.85)\n        if high_confidence_threshold is None:\n            high_confidence_threshold = self.config.get(\"high_confidence_threshold\", 0.95)\n        \n        # If no embedding, generate one\n        if new_entity.embedding is None and new_entity.label:\n            new_entity.embedding = await self.embedder.encode(new_entity.label)\n        \n        # If still no embedding, create new entity\n        if new_entity.embedding is None:\n            return ResolutionResult(\n                action='create_new',\n                confidence=1.0,\n                entity=new_entity,\n                reasoning=\"No embedding available for comparison\"\n            )\n        \n        # Prepare existing embeddings dictionary\n        entity_embeddings = {}\n        for entity_id, entity in existing_entities.items():\n            if entity.embedding is not None:\n                entity_embeddings[entity_id] = entity.embedding\n        \n        # Find similar entities\n        similar = self.embedder.find_similar(\n            new_entity.embedding,\n            entity_embeddings,\n            threshold=similarity_threshold,\n            top_k=5\n        )\n        \n        # If no similar entities, create new\n        if not similar:\n            return ResolutionResult(\n                action='create_new',\n                confidence=1.0,\n                entity=new_entity,\n                reasoning=\"No similar entities found above threshold\"\n            )\n        \n        # Get top candidate\n        top_id, top_similarity = similar[0]\n        top_entity = existing_entities[top_id]\n        \n        # Check for exact match conditions\n        if self._is_exact_match(new_entity, top_entity, top_similarity):\n            return ResolutionResult(\n                action='merge',\n                confidence=top_similarity,\n                target_id=top_id,\n                reasoning=\"Exact match based on label and type\"\n            )\n        \n        # If very high similarity, automatically merge\n        if top_similarity >= high_confidence_threshold:\n            return ResolutionResult(\n                action='merge',\n                confidence=top_similarity,\n                target_id=top_id,\n                reasoning=f\"High similarity score ({top_similarity:.2f}) indicates same entity\"\n            )\n        \n        # Prepare candidates for validation\n        candidates = []\n        for entity_id, similarity in similar[:3]:  # Top 3 candidates\n            candidates.append((entity_id, similarity, existing_entities[entity_id]))\n        \n        # Determine if we need parliament or simple LLM validation\n        complex_case = self._is_complex_resolution_case(new_entity, candidates)\n        \n        if complex_case and self.parliament and self.config.get(\"use_parliament_for_complex\", True):\n            # Use parliament for complex cases\n            return await self._resolve_with_parliament(new_entity, candidates)\n        else:\n            # Use LLM validation for simple cases\n            return await self._validate_with_llm(new_entity, candidates)\n    \n    def _is_exact_match(self, entity1: Entity, entity2: Entity, similarity: float) -> bool:\n        \"\"\"Check if entities are exact matches\"\"\"\n        # Same label and type\n        if entity1.label.lower() == entity2.label.lower() and entity1.entity_type == entity2.entity_type:\n            return True\n        \n        # Check for known abbreviations\n        if self._is_abbreviation(entity1.label, entity2.label) and entity1.entity_type == entity2.entity_type:\n            return True\n            \n        return False\n    \n    def _is_abbreviation(self, label1: str, label2: str) -> bool:\n        \"\"\"Check if one label is an abbreviation of the other\"\"\"\n        # Simple abbreviation check\n        shorter, longer = (label1, label2) if len(label1) < len(label2) else (label2, label1)\n        \n        # Check if shorter is an acronym of longer\n        if shorter.isupper() and len(shorter) >= 2:\n            words = longer.split()\n            acronym = ''.join(w[0].upper() for w in words if w)\n            if shorter == acronym:\n                return True\n        \n        # Check common abbreviations\n        abbreviations = {\n            \"ML\": \"Machine Learning\",\n            \"AI\": \"Artificial Intelligence\",\n            \"NLP\": \"Natural Language Processing\",\n            \"KB\": \"Knowledge Base\",\n            \"KG\": \"Knowledge Graph\"\n        }\n        \n        return (shorter in abbreviations and abbreviations[shorter].lower() == longer.lower()) or \\\n               (longer in abbreviations and abbreviations[longer].lower() == shorter.lower())\n    \n    def _is_complex_resolution_case(self, new_entity: Entity, \n                                  candidates: List[Tuple[str, float, Entity]]) -> bool:\n        \"\"\"Determine if this is a complex resolution case requiring parliament\"\"\"\n        # Multiple similar candidates\n        if len(candidates) > 2:\n            return True\n        \n        # Close similarity scores\n        if len(candidates) >= 2:\n            top_similarity = candidates[0][1]\n            second_similarity = candidates[1][1]\n            if abs(top_similarity - second_similarity) < 0.05:\n                return True\n        \n        # Cross-tier merge\n        for _, _, candidate in candidates:\n            if new_entity.tier != candidate.tier:\n                return True\n        \n        # Conflicting entity types\n        types = set(e.entity_type for _, _, e in candidates if e.entity_type)\n        if new_entity.entity_type and new_entity.entity_type not in types and len(types) > 0:\n            return True\n        \n        return False\n    \n    async def _validate_with_llm(self, new_entity: Entity, \n                              candidates: List[Tuple[str, float, Entity]]) -> ResolutionResult:\n        \"\"\"Validate entity resolution with LLM\"\"\"\n        # Get the top candidate for detailed comparison\n        if not candidates:\n            return ResolutionResult(\n                action='create_new',\n                confidence=1.0,\n                entity=new_entity,\n                reasoning=\"No candidates for comparison\"\n            )\n        \n        top_id, similarity, top_entity = candidates[0]\n        \n        # Format prompt\n        prompt_vars = {\n            \"new_entity_id\": new_entity.entity_id,\n            \"new_entity_label\": new_entity.label,\n            \"new_entity_type\": new_entity.entity_type or \"Unknown\",\n            \"new_entity_description\": self._get_entity_description(new_entity),\n            \"new_entity_attributes\": self._format_entity_attributes(new_entity),\n            \"source\": new_entity.source_uri or \"Unknown\",\n            \"existing_entity_id\": top_entity.entity_id,\n            \"existing_entity_label\": top_entity.label,\n            \"existing_entity_type\": top_entity.entity_type or \"Unknown\",\n            \"existing_entity_description\": self._get_entity_description(top_entity),\n            \"existing_entity_attributes\": self._format_entity_attributes(top_entity),\n            \"existing_entity_tier\": top_entity.tier,\n            \"existing_entity_created\": top_entity.created_at.strftime(\"%Y-%m-%d\"),\n            \"similarity_score\": f\"{similarity:.2f}\"\n        }\n        \n        try:\n            prompt = self.prompt_template.format(**prompt_vars)\n        except KeyError as e:\n            logger.error(f\"Missing prompt variable: {e}\")\n            # Use simplified prompt\n            prompt = f\"\"\"\n            Should these entities be merged?\n            \n            New: {new_entity.label} ({new_entity.entity_type})\n            Existing: {top_entity.label} ({top_entity.entity_type})\n            Similarity: {similarity:.2f}\n            \n            Return JSON with: resolution (MATCH/NO_MATCH), confidence, reasoning\n            \"\"\"\n        \n        # Get LLM response\n        try:\n            response = await self.llm.generate(prompt)\n            decision = self._parse_llm_response(response)\n            \n            # Map resolution to action\n            if decision.get('resolution') == 'MATCH':\n                return ResolutionResult(\n                    action='merge',\n                    confidence=float(decision.get('confidence', similarity)),\n                    target_id=top_id,\n                    reasoning=decision.get('reasoning', ''),\n                    requires_parliament=False\n                )\n            else:\n                return ResolutionResult(\n                    action='keep_separate',\n                    confidence=float(decision.get('confidence', 0.7)),\n                    entity=new_entity,\n                    reasoning=decision.get('reasoning', ''),\n                    requires_parliament=False\n                )\n                \n        except Exception as e:\n            logger.error(f\"Error in LLM validation: {e}\")\n            # Default to keeping separate on error\n            return ResolutionResult(\n                action='keep_separate',\n                confidence=0.5,\n                entity=new_entity,\n                reasoning=f\"Error in LLM validation: {str(e)}\",\n                requires_parliament=False\n            )\n    \n    def _parse_llm_response(self, response: str) -> Dict[str, Any]:\n        \"\"\"Parse LLM response for entity resolution\"\"\"\n        try:\n            # Find JSON in response\n            import re\n            json_match = re.search(r'{.*}', response, re.DOTALL)\n            \n            if json_match:\n                decision = json.loads(json_match.group(0))\n                \n                # Normalize resolution value\n                if 'resolution' in decision:\n                    resolution = decision['resolution'].upper()\n                    if resolution in ['MATCH', 'MERGE', 'YES']:\n                        decision['resolution'] = 'MATCH'\n                    elif resolution in ['NO_MATCH', 'NO_MERGE', 'NO', 'KEEP_SEPARATE']:\n                        decision['resolution'] = 'NO_MATCH'\n                    elif resolution in ['UNCERTAIN', 'MAYBE']:\n                        decision['resolution'] = 'UNCERTAIN'\n                \n                return decision\n            else:\n                logger.warning(\"Could not find JSON in LLM response\")\n                return {\"resolution\": \"NO_MATCH\", \"confidence\": 0.5, \"reasoning\": \"Could not parse response\"}\n                \n        except json.JSONDecodeError as e:\n            logger.error(f\"Error parsing JSON from LLM response: {e}\")\n            return {\"resolution\": \"NO_MATCH\", \"confidence\": 0.5, \"reasoning\": f\"JSON parse error: {str(e)}\"}\n    \n    async def _resolve_with_parliament(self, new_entity: Entity,\n                                     candidates: List[Tuple[str, float, Entity]]) -> ResolutionResult:\n        \"\"\"Resolve entity using parliament decision engine\"\"\"\n        if not self.parliament:\n            logger.warning(\"Parliament engine requested but not available\")\n            return await self._validate_with_llm(new_entity, candidates)\n        \n        # Prepare context for parliament\n        context = {\n            \"decision_type\": \"entity_merge\",\n            \"new_entity\": new_entity.to_dict(),\n            \"candidates\": [\n                {\n                    \"entity_id\": entity_id,\n                    \"similarity\": similarity,\n                    \"entity\": entity.to_dict()\n                }\n                for entity_id, similarity, entity in candidates\n            ]\n        }\n        \n        # Get parliament decision\n        try:\n            decision = await self.parliament.decide(context)\n            \n            # Convert to resolution result\n            if decision.get('action') == 'merge' or decision.get('decision') == 'MERGE':\n                return ResolutionResult(\n                    action='merge',\n                    confidence=float(decision.get('confidence', 0.0)),\n                    target_id=decision.get('target_id') or candidates[0][0],\n                    reasoning=decision.get('reasoning'),\n                    requires_parliament=True\n                )\n            else:\n                return ResolutionResult(\n                    action='keep_separate',\n                    confidence=float(decision.get('confidence', 0.0)),\n                    entity=new_entity,\n                    reasoning=decision.get('reasoning'),\n                    requires_parliament=True\n                )\n        except Exception as e:\n            logger.error(f\"Parliament decision failed: {e}\")\n            return await self._validate_with_llm(new_entity, candidates)\n    \n    async def merge_entities(self, target_entity: Entity, source_entity: Entity) -> Entity:\n        \"\"\"Merge source entity into target entity\n        \n        Args:\n            target_entity: Entity to merge into\n            source_entity: Entity to merge from\n            \n        Returns:\n            Updated target entity\n        \"\"\"\n        # Merge attributes (source overwrites target for conflicts)\n        for key, value in source_entity.attributes.items():\n            if key not in target_entity.attributes:\n                target_entity.attributes[key] = value\n            elif isinstance(value, list) and isinstance(target_entity.attributes[key], list):\n                # Merge lists\n                target_entity.attributes[key] = list(set(target_entity.attributes[key] + value))\n        \n        # Merge evidence IDs\n        target_entity.evidence_ids = list(set(target_entity.evidence_ids + source_entity.evidence_ids))\n        \n        # Update embedding if target doesn't have one\n        if target_entity.embedding is None and source_entity.embedding is not None:\n            target_entity.embedding = source_entity.embedding\n        elif target_entity.embedding is not None and source_entity.embedding is not None:\n            # Average embeddings\n            target_entity.embedding = (target_entity.embedding + source_entity.embedding) / 2\n            # Normalize\n            norm = np.linalg.norm(target_entity.embedding)\n            if norm > 0:\n                target_entity.embedding = target_entity.embedding / norm\n        \n        # Update tier if source is higher confidence\n        tier_priority = {\"peripheral\": 1, \"personal\": 2, \"digested\": 3}\n        if tier_priority.get(source_entity.tier, 0) > tier_priority.get(target_entity.tier, 0):\n            target_entity.tier = source_entity.tier\n        \n        # Track merge in metadata\n        if 'merged_from' not in target_entity.metadata:\n            target_entity.metadata['merged_from'] = []\n            \n        target_entity.metadata['merged_from'].append({\n            'entity_id': source_entity.entity_id,\n            'label': source_entity.label,\n            'timestamp': datetime.now().isoformat()\n        })\n        \n        return target_entity\n```\n\n## 3. Fixed Test Infrastructure (tests/test_entity_resolver.py)\n\n```python\n\"\"\"Test suite for Entity Resolver component\"\"\"\nimport pytest\nimport numpy as np\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Tuple\nimport json\n\nfrom knowledge_garden.extraction.entity_resolver import Entity, EntityResolver, ResolutionResult\nfrom knowledge_garden.extraction.embedding_generator import EmbeddingGenerator\n\n\nclass MockEmbeddingGenerator:\n    \"\"\"Mock embedding generator for testing\"\"\"\n    \n    def __init__(self):\n        self.embedding_dim = 768\n        \n    async def encode(self, text: str) -> np.ndarray:\n        \"\"\"Generate mock embedding based on text hash for consistency\"\"\"\n        # Use hash to generate consistent embeddings for same text\n        hash_val = hash(text)\n        np.random.seed(abs(hash_val) % 2**32)\n        embedding = np.random.randn(self.embedding_dim)\n        # Normalize\n        embedding = embedding / np.linalg.norm(embedding)\n        return embedding\n    \n    def cosine_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:\n        \"\"\"Calculate cosine similarity\"\"\"\n        return float(np.dot(emb1, emb2))\n    \n    def find_similar(self, embedding: np.ndarray, \n                    stored_embeddings: Dict[str, np.ndarray], \n                    threshold: float = 0.85,\n                    top_k: int = 5) -> List[Tuple[str, float]]:\n        \"\"\"Find similar embeddings\"\"\"\n        similarities = []\n        for entity_id, stored_emb in stored_embeddings.items():\n            sim = self.cosine_similarity(embedding, stored_emb)\n            if sim > threshold:\n                similarities.append((entity_id, sim))\n        \n        # Sort by similarity\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return similarities[:top_k]\n\n\nclass MockLLMService:\n    \"\"\"Mock LLM service for testing\"\"\"\n    \n    def __init__(self, responses: Dict[str, Any] = None):\n        self.responses = responses or {}\n        self.call_count = 0\n        \n    async def generate(self, prompt: str) -> str:\n        \"\"\"Generate mock response based on prompt content\"\"\"\n        self.call_count += 1\n        \n        # Check for specific test cases in prompt\n        if \"Machine Learning\" in prompt and \"ML\" in prompt:\n            return json.dumps({\n                \"resolution\": \"MATCH\",\n                \"confidence\": 0.95,\n                \"reasoning\": \"ML is a standard abbreviation for Machine Learning\",\n                \"recommended_label\": \"Machine Learning\"\n            })\n        elif \"Python\" in prompt and \"Programming_Language\" in prompt and \"Animal\" in prompt:\n            return json.dumps({\n                \"resolution\": \"NO_MATCH\",\n                \"confidence\": 0.98,\n                \"reasoning\": \"Different entity types indicate completely different concepts\"\n            })\n        elif \"Deep Learning\" in prompt and \"Neural Networks\" in prompt:\n            return json.dumps({\n                \"resolution\": \"UNCERTAIN\",\n                \"confidence\": 0.60,\n                \"reasoning\": \"Related concepts but distinct - Deep Learning is a subset of Neural Networks\"\n            })\n        \n        # Default response\n        return json.dumps({\n            \"resolution\": \"NO_MATCH\",\n            \"confidence\": 0.70,\n            \"reasoning\": \"Entities appear to be different based on analysis\"\n        })\n\n\n@pytest.fixture\ndef embedding_generator():\n    \"\"\"Create mock embedding generator\"\"\"\n    return MockEmbeddingGenerator()\n\n\n@pytest.fixture\ndef llm_service():\n    \"\"\"Create mock LLM service\"\"\"\n    return MockLLMService()\n\n\n@pytest.fixture\ndef entity_resolver(embedding_generator, llm_service):\n    \"\"\"Create entity resolver with mocks\"\"\"\n    config = {\n        \"similarity_threshold\": 0.85,\n        \"high_confidence_threshold\": 0.95,\n        \"use_parliament_for_complex\": False\n    }\n    return EntityResolver(\n        embedding_generator=embedding_generator,\n        llm_service=llm_service,\n        parliament_engine=None,\n        config=config\n    )\n\n\n@pytest.fixture\nasync def sample_entities(embedding_generator):\n    \"\"\"Create sample entities for testing\"\"\"\n    entities = {}\n    \n    # Machine Learning entity\n    ml_entity = Entity(\n        entity_id=\"entity_ml\",\n        label=\"Machine Learning\",\n        entity_type=\"Concept\",\n        attributes={\"domain\": \"AI\", \"description\": \"Field of AI focused on learning from data\"},\n        tier=\"digested\"\n    )\n    ml_entity.embedding = await embedding_generator.encode(\"Machine Learning\")\n    entities[ml_entity.entity_id] = ml_entity\n    \n    # Python programming entity\n    python_prog = Entity(\n        entity_id=\"entity_python_prog\",\n        label=\"Python\",\n        entity_type=\"Programming_Language\",\n        attributes={\"paradigm\": \"multi-paradigm\", \"year\": \"1991\"},\n        tier=\"digested\"\n    )\n    python_prog.embedding = await embedding_generator.encode(\"Python Programming Language\")\n    entities[python_prog.entity_id] = python_prog\n    \n    # Python snake entity\n    python_snake = Entity(\n        entity_id=\"entity_python_snake\",\n        label=\"Python\",\n        entity_type=\"Animal\",\n        attributes={\"class\": \"Reptilia\", \"family\": \"Pythonidae\"},\n        tier=\"peripheral\"\n    )\n    python_snake.embedding = await embedding_generator.encode(\"Python Snake\")\n    entities[python_snake.entity_id] = python_snake\n    \n    # Neural Networks entity\n    nn_entity = Entity(\n        entity_id=\"entity_nn\",\n        label=\"Neural Networks\",\n        entity_type=\"Concept\",\n        attributes={\"domain\": \"AI\", \"description\": \"Computing systems inspired by biological neural networks\"},\n        tier=\"digested\"\n    )\n    nn_entity.embedding = await embedding_generator.encode(\"Neural Networks\")\n    entities[nn_entity.entity_id] = nn_entity\n    \n    return entities\n\n\nclass TestEntityResolver:\n    \"\"\"Test cases for Entity Resolver\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_abbreviation_match(self, entity_resolver, sample_entities, embedding_generator):\n        \"\"\"Test that abbreviations are correctly matched\"\"\"\n        # Create ML entity\n        new_entity = Entity(\n            entity_id=\"entity_ml_new\",\n            label=\"ML\",\n            entity_type=\"Concept\",\n            attributes={\"description\": \"Machine learning techniques\"}\n        )\n        new_entity.embedding = await embedding_generator.encode(\"ML Machine Learning\")\n        \n        # Resolve\n        result = await entity_resolver.resolve_entity(new_entity, sample_entities)\n        \n        # Should merge with Machine Learning\n        assert result.action == \"merge\"\n        assert result.confidence >= 0.9\n        assert result.target_id == \"entity_ml\"\n        assert \"abbreviation\" in result.reasoning.lower() or \"ML\" in result.reasoning\n    \n    @pytest.mark.asyncio\n    async def test_homonym_no_match(self, entity_resolver, sample_entities, embedding_generator):\n        \"\"\"Test that homonyms with different types are not matched\"\"\"\n        # Create new Python programming entity\n        new_entity = Entity(\n            entity_id=\"entity_python_new\",\n            label=\"Python\",\n            entity_type=\"Programming_Language\",\n            attributes={\"description\": \"High-level programming language\"}\n        )\n        new_entity.embedding = await embedding_generator.encode(\"Python Programming\")\n        \n        # Modify sample to only have snake entity\n        snake_only = {\"entity_python_snake\": sample_entities[\"entity_python_snake\"]}\n        \n        # Resolve\n        result = await entity_resolver.resolve_entity(new_entity, snake_only)\n        \n        # Should not merge\n        assert result.action == \"keep_separate\"\n        assert result.confidence >= 0.9\n        assert \"different\" in result.reasoning.lower()\n    \n    @pytest.mark.asyncio\n    async def test_related_concepts_uncertain(self, entity_resolver, sample_entities, embedding_generator):\n        \"\"\"Test that related but distinct concepts get uncertain resolution\"\"\"\n        # Create Deep Learning entity\n        new_entity = Entity(\n            entity_id=\"entity_dl\",\n            label=\"Deep Learning\",\n            entity_type=\"Concept\",\n            attributes={\"description\": \"Neural networks with multiple layers\"}\n        )\n        new_entity.embedding = await embedding_generator.encode(\"Deep Learning Neural Networks\")\n        \n        # Resolve\n        result = await entity_resolver.resolve_entity(new_entity, sample_entities)\n        \n        # Should get LLM validation with uncertain result\n        assert result.action == \"keep_separate\"  # Based on mock response\n        assert 0.5 <= result.confidence <= 0.7\n        assert \"related\" in result.reasoning.lower() or \"subset\" in result.reasoning.lower()\n    \n    @pytest.mark.asyncio\n    async def test_no_similar_entities(self, entity_resolver, sample_entities, embedding_generator):\n        \"\"\"Test creation of new entity when no similar ones exist\"\"\"\n        # Create completely different entity\n        new_entity = Entity(\n            entity_id=\"entity_quantum\",\n            label=\"Quantum Computing\",\n            entity_type=\"Concept\",\n            attributes={\"description\": \"Computing using quantum phenomena\"}\n        )\n        new_entity.embedding = await embedding_generator.encode(\"Quantum Computing Physics\")\n        \n        # Use high threshold to ensure no matches\n        result = await entity_resolver.resolve_entity(\n            new_entity, sample_entities, similarity_threshold=0.99\n        )\n        \n        # Should create new\n        assert result.action == \"create_new\"\n        assert result.confidence == 1.0\n        assert result.entity == new_entity\n    \n    @pytest.mark.asyncio\n    async def test_exact_match(self, entity_resolver, sample_entities, embedding_generator):\n        \"\"\"Test exact match by label and type\"\"\"\n        # Create entity with same label and type\n        new_entity = Entity(\n            entity_id=\"entity_ml_duplicate\",\n            label=\"Machine Learning\",\n            entity_type=\"Concept\",\n            attributes={\"source\": \"different\"}\n        )\n        new_entity.embedding = await embedding_generator.encode(\"Machine Learning\")\n        \n        # Resolve\n        result = await entity_resolver.resolve_entity(new_entity, sample_entities)\n        \n        # Should merge due to exact match\n        assert result.action == \"merge\"\n        assert result.target_id == \"entity_ml\"\n        assert \"exact match\" in result.reasoning.lower() or \"same\" in result.reasoning.lower()\n    \n    @pytest.mark.asyncio\n    async def test_merge_entities(self, entity_resolver):\n        \"\"\"Test entity merging functionality\"\"\"\n        # Create two entities to merge\n        target = Entity(\n            entity_id=\"entity_target\",\n            label=\"Artificial Intelligence\",\n            entity_type=\"Concept\",\n            attributes={\"field\": \"Computer Science\", \"year\": \"1956\"},\n            evidence_ids=[\"ev1\", \"ev2\"],\n            tier=\"digested\"\n        )\n        target.embedding = np.array([1.0, 0.0, 0.0])\n        \n        source = Entity(\n            entity_id=\"entity_source\",\n            label=\"AI\",\n            entity_type=\"Concept\",\n            attributes={\"applications\": [\"ML\", \"NLP\"], \"year\": \"1956\"},\n            evidence_ids=[\"ev3\", \"ev4\"],\n            tier=\"personal\"\n        )\n        source.embedding = np.array([0.0, 1.0, 0.0])\n        \n        # Merge\n        merged = await entity_resolver.merge_entities(target, source)\n        \n        # Check merge results\n        assert merged.entity_id == \"entity_target\"\n        assert merged.label == \"Artificial Intelligence\"  # Keep target label\n        assert \"field\" in merged.attributes  # Keep target attributes\n        assert \"applications\" in merged.attributes  # Add source attributes\n        assert len(merged.evidence_ids) == 4  # Merge evidence\n        assert merged.tier == \"digested\"  # Keep higher tier\n        \n        # Check merge tracking\n        assert \"merged_from\" in merged.metadata\n        assert len(merged.metadata[\"merged_from\"]) == 1\n        assert merged.metadata[\"merged_from\"][0][\"entity_id\"] == \"entity_source\"\n        \n        # Check embedding averaging\n        expected_embedding = np.array([0.5, 0.5, 0.0])\n        expected_embedding = expected_embedding / np.linalg.norm(expected_embedding)\n        np.testing.assert_allclose(merged.embedding, expected_embedding, rtol=1e-5)\n    \n    @pytest.mark.asyncio  \n    async def test_complex_case_detection(self, entity_resolver, sample_entities):\n        \"\"\"Test detection of complex resolution cases\"\"\"\n        # Create new entity\n        new_entity = Entity(\n            entity_id=\"entity_new\",\n            label=\"Test Entity\",\n            entity_type=\"Concept\",\n            tier=\"personal\"\n        )\n        \n        # Test multiple candidates\n        candidates = [\n            (\"e1\", 0.88, Entity(\"e1\", \"Entity 1\", entity_type=\"Concept\")),\n            (\"e2\", 0.87, Entity(\"e2\", \"Entity 2\", entity_type=\"Concept\")),\n            (\"e3\", 0.86, Entity(\"e3\", \"Entity 3\", entity_type=\"Concept\"))\n        ]\n        assert entity_resolver._is_complex_resolution_case(new_entity, candidates) == True\n        \n        # Test close similarity scores\n        candidates = [\n            (\"e1\", 0.88, Entity(\"e1\", \"Entity 1\", entity_type=\"Concept\")),\n            (\"e2\", 0.85, Entity(\"e2\", \"Entity 2\", entity_type=\"Concept\"))\n        ]\n        assert entity_resolver._is_complex_resolution_case(new_entity, candidates) == True\n        \n        # Test cross-tier merge\n        candidates = [\n            (\"e1\", 0.90, Entity(\"e1\", \"Entity 1\", entity_type=\"Concept\", tier=\"digested\"))\n        ]\n        assert entity_resolver._is_complex_resolution_case(new_entity, candidates) == True\n        \n        # Test conflicting types\n        new_entity.entity_type = \"Person\"\n        candidates = [\n            (\"e1\", 0.88, Entity(\"e1\", \"Entity 1\", entity_type=\"Organization\"))\n        ]\n        assert entity_resolver._is_complex_resolution_case(new_entity, candidates) == True\n    \n    @pytest.mark.asyncio\n    async def test_no_embedding_handling(self, entity_resolver, sample_entities):\n        \"\"\"Test handling of entities without embeddings\"\"\"\n        # Create entity without embedding\n        new_entity = Entity(\n            entity_id=\"entity_no_emb\",\n            label=\"No Embedding Entity\",\n            entity_type=\"Concept\"\n        )\n        \n        # Resolve - should generate embedding\n        result = await entity_resolver.resolve_entity(new_entity, sample_entities)\n        \n        # Should have generated embedding\n        assert new_entity.embedding is not None\n        assert result.action in [\"create_new\", \"keep_separate\", \"merge\"]\n    \n    def test_abbreviation_detection(self, entity_resolver):\n        \"\"\"Test abbreviation detection logic\"\"\"\n        # Test acronym detection\n        assert entity_resolver._is_abbreviation(\"ML\", \"Machine Learning\") == True\n        assert entity_resolver._is_abbreviation(\"AI\", \"Artificial Intelligence\") == True\n        assert entity_resolver._is_abbreviation(\"NLP\", \"Natural Language Processing\") == True\n        \n        # Test known abbreviations\n        assert entity_resolver._is_abbreviation(\"KB\", \"Knowledge Base\") == True\n        assert entity_resolver._is_abbreviation(\"KG\", \"Knowledge Graph\") == True\n        \n        # Test non-abbreviations\n        assert entity_resolver._is_abbreviation(\"Python\", \"Java\") == False\n        assert entity_resolver._is_abbreviation(\"ML\", \"Deep Learning\") == False\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n```\n\n## 4. Fixed Configuration Validator (core/config_validator.py)\n\n```python\n\"\"\"\nConfiguration validation module for Knowledge Garden v2.0\n\nThis module provides functionality to validate configuration files against JSON schemas,\nensuring that all required fields are present and have correct types and values.\n\"\"\"\n\nimport os\nimport json\nimport logging\nimport jsonschema\nimport yaml\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\n# Define schema paths\nSCHEMA_DIR = Path(__file__).parent.parent / \"schemas\"\n\n# Ensure schema directory exists\nSCHEMA_DIR.mkdir(exist_ok=True, parents=True)\n\nclass ConfigValidator:\n    \"\"\"\n    Validates configuration files against JSON schemas\n    \n    This class ensures that configuration files contain all required fields\n    and that values have the correct types and meet any specified constraints.\n    \"\"\"\n    \n    def __init__(self, schema_dir: Optional[Path] = None):\n        \"\"\"\n        Initialize the configuration validator\n        \n        Args:\n            schema_dir: Optional custom directory for JSON schemas\n        \"\"\"\n        self.schema_dir = schema_dir or SCHEMA_DIR\n        self.schemas: Dict[str, Dict[str, Any]] = {}\n        self._create_default_schemas()\n        self.load_schemas()\n    \n    def _create_default_schemas(self) -> None:\n        \"\"\"Create default schemas if they don't exist\"\"\"\n        default_schemas = {\n            \"extraction\": {\n                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n                \"type\": \"object\",\n                \"required\": [\"embedding\", \"similarity\", \"evidence\", \"sentence_extraction\", \"entity_resolution\", \"llm\"],\n                \"properties\": {\n                    \"embedding\": {\n                        \"type\": \"object\",\n                        \"required\": [\"model\", \"dimension\", \"batch_size\", \"device\"],\n                        \"properties\": {\n                            \"model\": {\"type\": \"string\", \"minLength\": 1},\n                            \"dimension\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"batch_size\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"device\": {\"type\": \"string\", \"enum\": [\"auto\", \"cpu\", \"cuda\"]},\n                            \"cache_dir\": {\"type\": \"string\"}\n                        }\n                    },\n                    \"similarity\": {\n                        \"type\": \"object\",\n                        \"required\": [\"exact_match\", \"merge_candidate\", \"potentially_related\"],\n                        \"properties\": {\n                            \"exact_match\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                            \"merge_candidate\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                            \"potentially_related\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n                        }\n                    },\n                    \"evidence\": {\n                        \"type\": \"object\",\n                        \"required\": [\"max_quote_length\", \"context_window\"],\n                        \"properties\": {\n                            \"max_quote_length\": {\"type\": \"integer\", \"minimum\": 10},\n                            \"context_window\": {\"type\": \"integer\", \"minimum\": 0},\n                            \"max_batch_size\": {\"type\": \"integer\", \"minimum\": 1}\n                        }\n                    },\n                    \"sentence_extraction\": {\n                        \"type\": \"object\",\n                        \"required\": [\"min_sentence_length\", \"max_sentence_length\", \"batch_size\", \"mode\"],\n                        \"properties\": {\n                            \"min_sentence_length\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"max_sentence_length\": {\"type\": \"integer\", \"minimum\": 10},\n                            \"context_window\": {\"type\": \"integer\", \"minimum\": 0},\n                            \"batch_size\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"mode\": {\"type\": \"string\", \"enum\": [\"all\", \"filtered\", \"sampled\"]},\n                            \"relevance_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                            \"entity_density_threshold\": {\"type\": \"integer\", \"minimum\": 0},\n                            \"sample_rate\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                            \"min_samples\": {\"type\": \"integer\", \"minimum\": 1}\n                        }\n                    },\n                    \"entity_resolution\": {\n                        \"type\": \"object\",\n                        \"required\": [\"auto_merge_threshold\", \"parliament_threshold\", \"use_parliament_for_complex\"],\n                        \"properties\": {\n                            \"auto_merge_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                            \"parliament_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                            \"use_parliament_for_complex\": {\"type\": \"boolean\"},\n                            \"complexity_indicators\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                    \"multiple_candidates\": {\"type\": \"boolean\"},\n                                    \"conflicting_types\": {\"type\": \"boolean\"},\n                                    \"ambiguous_attributes\": {\"type\": \"boolean\"},\n                                    \"cross_tier_merge\": {\"type\": \"boolean\"}\n                                }\n                            }\n                        }\n                    },\n                    \"llm\": {\n                        \"type\": \"object\",\n                        \"required\": [\"default_model\", \"timeout\", \"max_retries\"],\n                        \"properties\": {\n                            \"default_model\": {\"type\": \"string\", \"minLength\": 1},\n                            \"timeout\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"max_retries\": {\"type\": \"integer\", \"minimum\": 0},\n                            \"sentence_extraction_model\": {\"type\": \"string\"},\n                            \"entity_resolution_model\": {\"type\": \"string\"},\n                            \"complex_resolution_model\": {\"type\": \"string\"}\n                        }\n                    }\n                }\n            },\n            \n            \"parliament\": {\n                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n                \"type\": \"object\",\n                \"required\": [\"core\", \"members\", \"decision_types\", \"llm\"],\n                \"properties\": {\n                    \"core\": {\n                        \"type\": \"object\",\n                        \"required\": [\"enabled\", \"default_mode\", \"num_perspectives\", \"consensus_threshold\"],\n                        \"properties\": {\n                            \"enabled\": {\"type\": \"boolean\"},\n                            \"default_mode\": {\"type\": \"string\", \"enum\": [\"cooperative\", \"adversarial\", \"divergent\"]},\n                            \"num_perspectives\": {\"type\": \"integer\", \"minimum\": 2, \"maximum\": 10},\n                            \"num_rounds\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n                            \"time_limit\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"consensus_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 100},\n                            \"fallback_strategy\": {\"type\": \"string\", \"enum\": [\"majority\", \"weighted\", \"chairperson\"]},\n                            \"summarize\": {\"type\": \"boolean\"},\n                            \"cache_decisions\": {\"type\": \"boolean\"},\n                            \"cache_lifetime\": {\"type\": \"integer\", \"minimum\": 1}\n                        }\n                    }\n                }\n            },\n            \n            \"artifacts\": {\n                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n                \"type\": \"object\",\n                \"required\": [\"dependency_graph\", \"version_manager\", \"storage\", \"staleness\"],\n                \"properties\": {\n                    \"dependency_graph\": {\n                        \"type\": \"object\",\n                        \"required\": [\"persistent\", \"auto_update\"],\n                        \"properties\": {\n                            \"persistent\": {\"type\": \"boolean\"},\n                            \"auto_update\": {\"type\": \"boolean\"},\n                            \"max_depth\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"include_dependency_types\": {\n                                \"type\": \"array\",\n                                \"items\": {\"type\": \"string\"}\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        \n        # Save schemas to files if they don't exist\n        for schema_name, schema in default_schemas.items():\n            schema_path = self.schema_dir / f\"{schema_name}.json\"\n            if not schema_path.exists():\n                with open(schema_path, 'w') as f:\n                    json.dump(schema, f, indent=2)\n                logger.info(f\"Created default schema: {schema_path}\")\n    \n    def load_schemas(self) -> None:\n        \"\"\"Load all JSON schemas from the schema directory\"\"\"\n        if not self.schema_dir.exists():\n            logger.warning(f\"Schema directory not found: {self.schema_dir}\")\n            return\n            \n        for schema_file in self.schema_dir.glob(\"*.json\"):\n            try:\n                with open(schema_file, 'r') as f:\n                    schema = json.load(f)\n                    \n                schema_name = schema_file.stem\n                self.schemas[schema_name] = schema\n                logger.info(f\"Loaded schema: {schema_name}\")\n            except Exception as e:\n                logger.error(f\"Error loading schema {schema_file}: {e}\")\n    \n    def validate_config(self, config: Dict[str, Any], schema_name: str) -> Tuple[bool, List[str]]:\n        \"\"\"\n        Validate a configuration against a schema\n        \n        Args:\n            config: Configuration dictionary to validate\n            schema_name: Name of the schema to validate against\n            \n        Returns:\n            Tuple of (is_valid, error_messages)\n        \"\"\"\n        if schema_name not in self.schemas:\n            logger.error(f\"Schema not found: {schema_name}\")\n            return False, [f\"Schema not found: {schema_name}\"]\n            \n        schema = self.schemas[schema_name]\n        \n        try:\n            # Create validator with custom error messages\n            validator = jsonschema.Draft7Validator(schema)\n            \n            # Collect all validation errors\n            errors = sorted(validator.iter_errors(config), key=lambda e: e.path)\n            \n            if not errors:\n                return True, []\n                \n            # Format error messages\n            error_messages = []\n            for error in errors:\n                # Build path string\n                path_parts = list(error.path)\n                if path_parts:\n                    path = \".\".join(str(p) for p in path_parts)\n                else:\n                    path = \"root\"\n                \n                # Create user-friendly error message\n                if error.validator == \"required\":\n                    missing_fields = error.validator_value\n                    message = f\"{path}: Missing required field(s): {', '.join(missing_fields)}\"\n                elif error.validator == \"type\":\n                    expected = error.validator_value\n                    actual = type(error.instance).__name__\n                    message = f\"{path}: Expected type '{expected}' but got '{actual}'\"\n                elif error.validator == \"enum\":\n                    allowed = error.validator_value\n                    message = f\"{path}: Value must be one of {allowed}, got '{error.instance}'\"\n                elif error.validator == \"minimum\":\n                    message = f\"{path}: Value {error.instance} is below minimum {error.validator_value}\"\n                elif error.validator == \"maximum\":\n                    message = f\"{path}: Value {error.instance} is above maximum {error.validator_value}\"\n                else:\n                    message = f\"{path}: {error.message}\"\n                    \n                error_messages.append(message)\n                \n            return False, error_messages\n            \n        except jsonschema.SchemaError as e:\n            logger.error(f\"Invalid schema: {e}\")\n            return False, [f\"Invalid schema: {str(e)}\"]\n        except Exception as e:\n            logger.error(f\"Validation error: {e}\")\n            return False, [f\"Validation error: {str(e)}\"]\n    \n    def validate_yaml_config(self, yaml_path: Path, schema_name: str) -> Tuple[bool, List[str]]:\n        \"\"\"\n        Validate a YAML configuration file against a schema\n        \n        Args:\n            yaml_path: Path to YAML configuration file\n            schema_name: Name of the schema to validate against\n            \n        Returns:\n            Tuple of (is_valid, error_messages)\n        \"\"\"\n        try:\n            with open(yaml_path, 'r') as f:\n                config = yaml.safe_load(f)\n                \n            if config is None:\n                return False, [\"Empty configuration file\"]\n                \n            return self.validate_config(config, schema_name)\n        except yaml.YAMLError as e:\n            logger.error(f\"Error parsing YAML file {yaml_path}: {e}\")\n            return False, [f\"YAML parsing error: {str(e)}\"]\n        except FileNotFoundError:\n            return False, [f\"Configuration file not found: {yaml_path}\"]\n        except Exception as e:\n            logger.error(f\"Error loading YAML file {yaml_path}: {e}\")\n            return False, [f\"Error loading file: {str(e)}\"]\n    \n    def validate_all_configs(self, config_dir: Path) -> Dict[str, Tuple[bool, List[str]]]:\n        \"\"\"\n        Validate all configuration files in a directory\n        \n        Args:\n            config_dir: Directory containing configuration files\n            \n        Returns:\n            Dictionary mapping file names to validation results\n        \"\"\"\n        results = {}\n        \n        if not config_dir.exists():\n            logger.error(f\"Configuration directory not found: {config_dir}\")\n            return results\n            \n        # Process YAML files\n        for config_file in config_dir.glob(\"*.yaml\"):\n            schema_name = config_file.stem\n            \n            if schema_name in self.schemas:\n                is_valid, errors = self.validate_yaml_config(config_file, schema_name)\n                results[config_file.name] = (is_valid, errors)\n                \n                if is_valid:\n                    logger.info(f\"✓ Valid configuration: {config_file.name}\")\n                else:\n                    logger.error(f\"✗ Invalid configuration: {config_file.name}\")\n                    for error in errors:\n                        logger.error(f\"  - {error}\")\n            else:\n                results[config_file.name] = (False, [f\"No schema defined for {schema_name}\"])\n                logger.warning(f\"No schema for configuration: {config_file.name}\")\n                \n        return results\n    \n    def generate_example_config(self, schema_name: str) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Generate an example configuration that satisfies the schema\n        \n        Args:\n            schema_name: Name of the schema\n            \n        Returns:\n            Example configuration dictionary\n        \"\"\"\n        if schema_name not in self.schemas:\n            return None\n            \n        schema = self.schemas[schema_name]\n        return self._generate_from_schema(schema)\n    \n    def _generate_from_schema(self, schema: Dict[str, Any]) -> Any:\n        \"\"\"Generate example data from a JSON schema\"\"\"\n        schema_type = schema.get(\"type\", \"object\")\n        \n        if schema_type == \"object\":\n            obj = {}\n            properties = schema.get(\"properties\", {})\n            required = schema.get(\"required\", [])\n            \n            for prop, prop_schema in properties.items():\n                if prop in required or len(properties) <= 5:  # Include all if few properties\n                    obj[prop] = self._generate_from_schema(prop_schema)\n                    \n            return obj\n            \n        elif schema_type == \"array\":\n            item_schema = schema.get(\"items\", {\"type\": \"string\"})\n            return [self._generate_from_schema(item_schema)]\n            \n        elif schema_type == \"string\":\n            if \"enum\" in schema:\n                return schema[\"enum\"][0]\n            elif \"minLength\" in schema:\n                return \"example_string\"\n            else:\n                return \"string\"\n                \n        elif schema_type == \"number\":\n            minimum = schema.get(\"minimum\", 0)\n            maximum = schema.get(\"maximum\", 1)\n            return (minimum + maximum) / 2\n            \n        elif schema_type == \"integer\":\n            minimum = schema.get(\"minimum\", 1)\n            return minimum\n            \n        elif schema_type == \"boolean\":\n            return True\n            \n        else:\n            return None\n\n\ndef validate_config_on_startup(config_dir: Optional[Path] = None) -> bool:\n    \"\"\"\n    Validate all configuration files on startup\n    \n    Args:\n        config_dir: Configuration directory path\n        \n    Returns:\n        True if all configurations are valid, False otherwise\n    \"\"\"\n    if config_dir is None:\n        config_dir = Path(__file__).parent.parent / \"config\"\n    \n    # Initialize validator\n    validator = ConfigValidator()\n    \n    # Validate all configuration files\n    results = validator.validate_all_configs(config_dir)\n    \n    # Check if all are valid\n    all_valid = all(is_valid for is_valid, _ in results.values())\n    \n    if all_valid:\n        logger.info(\"✓ All configurations are valid\")\n    else:\n        logger.error(\"✗ Some configurations have validation errors\")\n        \n    # Generate example configs for missing files\n    for schema_name in validator.schemas:\n        config_file = config_dir / f\"{schema_name}.yaml\"\n        if not config_file.exists():\n            logger.warning(f\"Missing configuration file: {config_file.name}\")\n            example = validator.generate_example_config(schema_name)\n            if example:\n                logger.info(f\"Example configuration for {schema_name}:\")\n                print(yaml.dump(example, default_flow_style=False))\n    \n    return all_valid\n\n\nif __name__ == \"__main__\":\n    # Set up logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(levelname)s - %(message)s'\n    )\n    \n    # Validate configurations\n    config_valid = validate_config_on_startup()\n    \n    # Exit with appropriate code\n    sys.exit(0 if config_valid else 1)\n```\n\nThese fixes address the core issues identified:\n\n1. **Improved Prompt Structure** - The entity resolution prompt now includes proper context, examples, reasoning steps, and error handling\n2. **Better Entity Resolution** - Fixed implementation with proper tier handling, abbreviation detection, and improved merging logic\n3. **Comprehensive Testing** - Added proper test suite with various test cases for entity resolution\n4. **Working Config Validation** - The validator now properly creates schemas, validates configs, and provides helpful error messages\n\nThe improvements follow the recommendations in the improvement document and should significantly enhance the system's reliability and performance.",
      "created_at": "2025-06-11T14:09:25.995540Z",
      "sequence_order": 1,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2025-06-11T14:04:14.963772Z",
            "stop_timestamp": "2025-06-11T14:04:25.403155Z",
            "flags": null,
            "type": "thinking",
            "thinking": "Looking at the Knowledge Garden v2.0 implementation plan and the improvement requests, I can see several key areas that need attention:\n\n1. **Prompt Design Issues** (Current Score: 2.8/5) - The prompts lack proper structure, examples, and error handling\n2. **Testing Infrastructure** (Current Score: 1.3/5) - Missing unit tests, integration tests, and test data\n3. **Configuration System** (Current Score: 3.3/5) - Limited validation and documentation\n4. **Core Implementation Issues** - Several implementation files have issues that need fixing\n\nLet me identify the core issues and provide fixed versions of the key files:\n\n### Core Issues Identified:\n\n1. **Prompt Templates** - The YAML prompt files in `extraction/prompts/` and `parliament/prompts/` need to follow the improved structure suggested in the improvement document\n2. **Testing Infrastructure** - The test files need proper structure and comprehensive test coverage\n3. **Entity Resolution** - The entity resolver has some implementation issues with entity comparison\n4. **Pipeline Integration** - The extraction pipeline needs better error handling and integration\n5. **Configuration Validation** - The config validator needs to actually validate against schemas\n\nLet me fix the most critical files:",
            "summaries": [
              {
                "summary": "Thinking about reviewing, fixing, and resolving strategic implementation issues"
              },
              {
                "summary": "Investigating prompt design issues and testing strategies"
              },
              {
                "summary": "Diagnosed implementation gaps and strategized comprehensive system improvements"
              }
            ],
            "cut_off": false
          },
          {
            "start_timestamp": "2025-06-11T14:04:25.564362Z",
            "stop_timestamp": "2025-06-11T14:09:25.827135Z",
            "flags": null,
            "type": "text",
            "text": "Looking at the Knowledge Garden v2.0 implementation and improvement requests, I've identified several core issues that need to be addressed. Here are the fixed versions of the critical files:\n\n## 1. Fixed Entity Resolution Prompt (extraction/prompts/entity_resolution.yaml)\n\n```yaml\nprompt_type: entity_resolution\ntitle: Entity Resolution for Knowledge Garden\ndescription: Resolves potential entity matches using semantic understanding and similarity scores\nversion: 2.0\n\n# Resolution parameters\nparameters:\n  similarity_threshold: 0.85\n  high_confidence_threshold: 0.95\n  normalize_entities: true\n  consider_context: true\n\n# System context\nsystem_context: |\n  You are the Entity Resolver component in Knowledge Garden v2.0, a system that builds a structured knowledge graph from conversations. \n  Your task is to determine if a newly extracted entity matches an existing entity in the knowledge graph. \n  This resolution is critical for maintaining a clean, coherent knowledge graph without redundant entities.\n  \n  The Knowledge Garden system extracts entities from conversations and documents, organizing them into a three-tier knowledge model:\n  - Personal: Knowledge from user conversations\n  - Digested: Validated external knowledge\n  - Peripheral: References and citations\n\n# Input schema  \ninput_schema:\n  new_entity:\n    type: object\n    required: [id, label, type]\n    properties:\n      id: {type: string}\n      label: {type: string}\n      description: {type: string}\n      type: {type: string}\n      attributes: {type: object}\n      source: {type: string}\n  existing_entity:\n    type: object\n    required: [id, label, type]\n    properties:\n      id: {type: string}\n      label: {type: string}\n      description: {type: string}\n      type: {type: string}\n      attributes: {type: object}\n      tier: {type: string}\n      created: {type: string}\n  similarity_score:\n    type: number\n    minimum: 0.0\n    maximum: 1.0\n\n# Examples section with diverse cases\nexamples:\n  - name: \"Abbreviation Match\"\n    input:\n      new_entity:\n        label: \"ML\"\n        type: \"Concept\"\n        description: \"Machine learning techniques\"\n      existing_entity:\n        label: \"Machine Learning\"\n        type: \"Concept\"\n        description: \"Field of artificial intelligence focused on learning from data\"\n      similarity_score: 0.87\n    output:\n      resolution: \"MATCH\"\n      confidence: 0.95\n      reasoning: |\n        \"ML\" is a standard abbreviation for \"Machine Learning\". Both entities refer to the same concept\n        with consistent descriptions. The type matches (Concept) and the descriptions are complementary.\n        High confidence merge is appropriate.\n      recommended_label: \"Machine Learning\"\n      \n  - name: \"Homonym Non-Match\"\n    input:\n      new_entity:\n        label: \"Python\"\n        type: \"Programming_Language\"\n        description: \"High-level programming language\"\n      existing_entity:\n        label: \"Python\"\n        type: \"Animal\"\n        description: \"Large snake species\"\n      similarity_score: 0.92\n    output:\n      resolution: \"NO_MATCH\"\n      confidence: 0.98\n      reasoning: |\n        Despite identical labels and high text similarity, these are completely different concepts.\n        The entity types (Programming_Language vs Animal) and descriptions clearly indicate distinct\n        entities. This is a classic homonym case where the same word has different meanings.\n        \n  - name: \"Related but Distinct\"\n    input:\n      new_entity:\n        label: \"Deep Learning\"\n        type: \"Concept\"\n        description: \"Neural networks with multiple layers\"\n      existing_entity:\n        label: \"Neural Networks\"\n        type: \"Concept\"\n        description: \"Computing systems inspired by biological neural networks\"\n      similarity_score: 0.76\n    output:\n      resolution: \"UNCERTAIN\"\n      confidence: 0.60\n      reasoning: |\n        These concepts are closely related - deep learning is a subset of neural networks.\n        However, they represent different levels of specificity. In a technical knowledge graph,\n        maintaining the distinction may be valuable. The decision depends on the desired\n        granularity of the knowledge representation.\n      additional_info_needed: [\"Domain context\", \"User preference for granularity\"]\n\n# Reasoning guidance\nreasoning_steps:\n  - step: \"Label Analysis\"\n    description: \"Compare entity labels for exact matches, abbreviations, synonyms, or variations\"\n    checks:\n      - \"Are the labels identical?\"\n      - \"Is one label an abbreviation of the other?\"\n      - \"Are they known synonyms in the domain?\"\n      - \"Do they share significant substring overlap?\"\n      \n  - step: \"Type Compatibility\"\n    description: \"Verify if entity types are compatible for merging\"\n    checks:\n      - \"Are the types identical?\"\n      - \"Is one type a supertype/subtype of the other?\"\n      - \"Are the types semantically incompatible?\"\n      \n  - step: \"Description Analysis\"\n    description: \"Compare descriptions for semantic similarity\"\n    checks:\n      - \"Do descriptions refer to the same concept?\"\n      - \"Are there contradictory elements?\"\n      - \"Is one description more specific than the other?\"\n      \n  - step: \"Attribute Comparison\"\n    description: \"Examine attributes for consistency\"\n    checks:\n      - \"Are attributes complementary or conflicting?\"\n      - \"Do shared attributes have consistent values?\"\n      - \"Would merging lose important distinctions?\"\n      \n  - step: \"Context Evaluation\"\n    description: \"Consider source and usage context\"\n    checks:\n      - \"Do entities come from the same knowledge tier?\"\n      - \"Is the source context compatible?\"\n      - \"Would merging cross important boundaries?\"\n      \n  - step: \"Confidence Assessment\"\n    description: \"Determine confidence based on evidence strength\"\n    checks:\n      - \"How many positive indicators support merging?\"\n      - \"Are there any strong negative indicators?\"\n      - \"Is additional information needed for certainty?\"\n\n# Error handling\nerror_handling:\n  missing_data:\n    - condition: \"Missing entity label\"\n      action: \"Return resolution='INSUFFICIENT_DATA', confidence=0.0\"\n    - condition: \"Missing entity type\"\n      action: \"Proceed with reduced confidence (-0.2)\"\n    - condition: \"Missing description\"\n      action: \"Rely more on label and attribute comparison\"\n    - condition: \"Invalid similarity score\"\n      action: \"Use 0.5 as default, note in reasoning\"\n      \n  edge_cases:\n    - condition: \"Entities from different knowledge tiers\"\n      action: \"Consider tier compatibility, may reduce merge confidence\"\n    - condition: \"Temporal entities (different time references)\"\n      action: \"Generally keep separate unless explicitly the same\"\n    - condition: \"Partial string matches\"\n      action: \"Require additional evidence beyond string similarity\"\n\n# Full prompt template\ntemplate: |\n  # Entity Resolution for Knowledge Garden v2.0\n\n  ## SYSTEM CONTEXT\n  {system_context}\n\n  ## ENTITIES TO COMPARE\n\n  ### New Entity\n  ID: {new_entity_id}\n  Label: \"{new_entity_label}\"\n  Description: \"{new_entity_description}\"\n  Type: \"{new_entity_type}\"\n  Attributes: {new_entity_attributes}\n  Extracted from: {source}\n\n  ### Existing Entity  \n  ID: {existing_entity_id}\n  Label: \"{existing_entity_label}\"\n  Description: \"{existing_entity_description}\"\n  Type: \"{existing_entity_type}\"\n  Attributes: {existing_entity_attributes}\n  Knowledge tier: {existing_entity_tier}\n  Creation date: {existing_entity_created}\n\n  Computed similarity score: {similarity_score}\n\n  ## RESOLUTION TASK\n  Determine whether these two entities refer to the same real-world concept and should be merged, \n  or if they represent distinct concepts and should remain separate.\n\n  ## RESOLUTION GUIDELINES\n\n  1. **Label Analysis**: {reasoning_steps[0].description}\n  2. **Type Compatibility**: {reasoning_steps[1].description}\n  3. **Description Analysis**: {reasoning_steps[2].description}\n  4. **Attribute Comparison**: {reasoning_steps[3].description}\n  5. **Context Evaluation**: {reasoning_steps[4].description}\n  6. **Confidence Assessment**: {reasoning_steps[5].description}\n\n  ## EXAMPLES\n\n  ### Example 1: Abbreviation Match\n  New: \"ML\" (Concept) | Existing: \"Machine Learning\" (Concept)\n  Resolution: MATCH (0.95) - Standard abbreviation for the same concept\n\n  ### Example 2: Homonym Non-Match  \n  New: \"Python\" (Programming_Language) | Existing: \"Python\" (Animal)\n  Resolution: NO_MATCH (0.98) - Same word, completely different concepts\n\n  ### Example 3: Related but Distinct\n  New: \"Deep Learning\" | Existing: \"Neural Networks\"\n  Resolution: UNCERTAIN (0.60) - Related concepts, granularity question\n\n  ## ERROR HANDLING\n\n  - If critical information is missing, return INSUFFICIENT_DATA\n  - For ambiguous cases, prefer NO_MATCH to prevent information loss\n  - Note when additional context would improve confidence\n\n  ## OUTPUT FORMAT\n  \n  Analyze the entities step-by-step following the reasoning guidelines, then provide your resolution as valid JSON:\n\n  ```json\n  {\n    \"resolution\": \"MATCH\" | \"NO_MATCH\" | \"UNCERTAIN\" | \"INSUFFICIENT_DATA\",\n    \"confidence\": 0.0-1.0,\n    \"reasoning\": \"Detailed step-by-step explanation of your resolution process and key factors...\",\n    \"recommended_label\": \"If MATCH, the best label to use for the merged entity\",\n    \"additional_info_needed\": [\"Optional list of information that would improve confidence\"]\n  }\n  ```\n\n# Output validation schema\noutput_schema:\n  type: object\n  required: [\"resolution\", \"confidence\", \"reasoning\"]\n  properties:\n    resolution:\n      type: string\n      enum: [\"MATCH\", \"NO_MATCH\", \"UNCERTAIN\", \"INSUFFICIENT_DATA\"]\n      description: \"The resolution decision\"\n    confidence:\n      type: number\n      minimum: 0.0\n      maximum: 1.0\n      description: \"Confidence in the resolution (0-1)\"\n    reasoning:\n      type: string\n      minLength: 50\n      description: \"Detailed explanation of the resolution logic\"\n    recommended_label:\n      type: string\n      description: \"Recommended label if entities are merged\"\n    additional_info_needed:\n      type: array\n      items:\n        type: string\n      description: \"Information that would improve resolution confidence\"\n```\n\n## 2. Fixed Entity Resolver Implementation (extraction/entity_resolver.py)\n\n```python\n\"\"\"Entity resolution using embeddings and LLM validation\"\"\"\nimport logging\nimport json\nfrom typing import Dict, List, Any, Optional, Tuple, NamedTuple\nimport numpy as np\nfrom datetime import datetime\n\nfrom knowledge_garden.models.evidence import Evidence\n\nlogger = logging.getLogger(__name__)\n\nclass Entity:\n    \"\"\"Entity in the knowledge graph\"\"\"\n    \n    def __init__(\n        self,\n        entity_id: str,\n        label: str,\n        embedding: Optional[np.ndarray] = None,\n        entity_type: Optional[str] = None,\n        attributes: Optional[Dict[str, Any]] = None,\n        source_uri: Optional[str] = None,\n        evidence_ids: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        tier: Optional[str] = None\n    ):\n        \"\"\"Initialize an entity\n        \n        Args:\n            entity_id: Unique identifier for the entity\n            label: Human-readable label/name\n            embedding: Vector representation of the entity\n            entity_type: Type of entity (Person, Organization, Concept, etc.)\n            attributes: Key-value attributes of the entity\n            source_uri: Source document/conversation URI\n            evidence_ids: IDs of evidence records\n            metadata: Additional metadata\n            tier: Knowledge tier (personal, digested, peripheral)\n        \"\"\"\n        self.entity_id = entity_id\n        self.label = label\n        self.embedding = embedding\n        self.entity_type = entity_type\n        self.attributes = attributes or {}\n        self.source_uri = source_uri\n        self.evidence_ids = evidence_ids or []\n        self.metadata = metadata or {}\n        self.tier = tier or \"personal\"\n        self.created_at = datetime.now()\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary representation\"\"\"\n        from knowledge_garden.models.extracted_sentence import ExtractedSentence\n        \n        return {\n            \"entity_id\": self.entity_id,\n            \"label\": self.label,\n            \"embedding\": ExtractedSentence.encode_embedding(self.embedding) if self.embedding is not None else None,\n            \"entity_type\": self.entity_type,\n            \"attributes\": self.attributes,\n            \"source_uri\": self.source_uri,\n            \"evidence_ids\": self.evidence_ids,\n            \"metadata\": self.metadata,\n            \"tier\": self.tier,\n            \"created_at\": self.created_at.isoformat()\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Entity':\n        \"\"\"Create from dictionary representation\"\"\"\n        from knowledge_garden.models.extracted_sentence import ExtractedSentence\n        \n        # Handle embedding conversion\n        embedding = None\n        if data.get(\"embedding\"):\n            embedding = ExtractedSentence.decode_embedding(data[\"embedding\"])\n        \n        entity = cls(\n            entity_id=data[\"entity_id\"],\n            label=data[\"label\"],\n            embedding=embedding,\n            entity_type=data.get(\"entity_type\"),\n            attributes=data.get(\"attributes\", {}),\n            source_uri=data.get(\"source_uri\"),\n            evidence_ids=data.get(\"evidence_ids\", []),\n            metadata=data.get(\"metadata\", {}),\n            tier=data.get(\"tier\", \"personal\")\n        )\n        \n        # Set created_at if available\n        if data.get(\"created_at\"):\n            entity.created_at = datetime.fromisoformat(data[\"created_at\"])\n            \n        return entity\n\nclass ResolutionResult(NamedTuple):\n    \"\"\"Result of entity resolution\"\"\"\n    action: str  # 'create_new', 'merge', 'keep_separate'\n    confidence: float\n    entity: Optional[Entity] = None\n    target_id: Optional[str] = None\n    reasoning: Optional[str] = None\n    requires_parliament: bool = False\n\nclass EntityResolver:\n    \"\"\"Resolve entities using embeddings and LLM validation\"\"\"\n    \n    def __init__(self, embedding_generator, llm_service, parliament_engine=None, config=None):\n        \"\"\"Initialize entity resolver\n        \n        Args:\n            embedding_generator: Service for generating and comparing embeddings\n            llm_service: Service for LLM validation\n            parliament_engine: Optional parliament decision engine\n            config: Configuration parameters\n        \"\"\"\n        self.embedder = embedding_generator\n        self.llm = llm_service\n        self.parliament = parliament_engine\n        self.config = config or {}\n        \n        # Load prompt template\n        self.prompt_template = self._load_prompt_template()\n    \n    def _load_prompt_template(self) -> str:\n        \"\"\"Load the entity resolution prompt template\"\"\"\n        try:\n            from knowledge_garden.extraction.prompts import get_template_text\n            template = get_template_text(\"entity_resolution\")\n            if template:\n                return template\n        except Exception as e:\n            logger.warning(f\"Could not load prompt template: {e}\")\n        \n        # Fallback to inline template\n        return \"\"\"\n        # Entity Resolution for Knowledge Garden v2.0\n        \n        Determine if these entities should be merged or kept separate.\n        \n        New entity:\n        - Label: \"{new_entity_label}\"\n        - Type: \"{new_entity_type}\"\n        - Description: \"{new_entity_description}\"\n        - Attributes: {new_entity_attributes}\n        \n        Existing entity:\n        - Label: \"{existing_entity_label}\"\n        - Type: \"{existing_entity_type}\"\n        - Description: \"{existing_entity_description}\"\n        - Attributes: {existing_entity_attributes}\n        - Tier: {existing_entity_tier}\n        \n        Similarity score: {similarity_score}\n        \n        Consider:\n        1. Semantic similarity beyond surface text\n        2. Entity type compatibility\n        3. Attribute consistency\n        4. Knowledge tier implications\n        \n        Return JSON:\n        {{\n          \"resolution\": \"MATCH\" or \"NO_MATCH\" or \"UNCERTAIN\",\n          \"confidence\": 0.0-1.0,\n          \"reasoning\": \"Explanation\",\n          \"recommended_label\": \"Best label if merged\"\n        }}\n        \"\"\"\n    \n    def _format_entity_attributes(self, entity: Entity) -> str:\n        \"\"\"Format entity attributes for prompt\"\"\"\n        if not entity.attributes:\n            return \"{}\"\n        \n        # Limit attributes to prevent prompt overflow\n        limited_attrs = {}\n        for key, value in list(entity.attributes.items())[:5]:\n            if isinstance(value, str) and len(value) > 100:\n                limited_attrs[key] = value[:100] + \"...\"\n            else:\n                limited_attrs[key] = value\n                \n        return json.dumps(limited_attrs, ensure_ascii=False, indent=2)\n    \n    def _get_entity_description(self, entity: Entity) -> str:\n        \"\"\"Get entity description from attributes or generate one\"\"\"\n        # Check for description in attributes\n        for key in [\"description\", \"desc\", \"summary\", \"definition\"]:\n            if key in entity.attributes:\n                return str(entity.attributes[key])\n        \n        # Generate description based on type and label\n        if entity.entity_type:\n            return f\"A {entity.entity_type.lower()} entity: {entity.label}\"\n        else:\n            return f\"Entity: {entity.label}\"\n    \n    async def resolve_entity(self, new_entity: Entity, \n                           existing_entities: Dict[str, Entity],\n                           similarity_threshold: float = None,\n                           high_confidence_threshold: float = None) -> ResolutionResult:\n        \"\"\"Resolve entity against existing entities\n        \n        Args:\n            new_entity: New entity to resolve\n            existing_entities: Dictionary of {entity_id: entity}\n            similarity_threshold: Minimum similarity for candidates\n            high_confidence_threshold: Threshold for automatic decisions\n            \n        Returns:\n            Resolution result\n        \"\"\"\n        # Use config values if not provided\n        if similarity_threshold is None:\n            similarity_threshold = self.config.get(\"similarity_threshold\", 0.85)\n        if high_confidence_threshold is None:\n            high_confidence_threshold = self.config.get(\"high_confidence_threshold\", 0.95)\n        \n        # If no embedding, generate one\n        if new_entity.embedding is None and new_entity.label:\n            new_entity.embedding = await self.embedder.encode(new_entity.label)\n        \n        # If still no embedding, create new entity\n        if new_entity.embedding is None:\n            return ResolutionResult(\n                action='create_new',\n                confidence=1.0,\n                entity=new_entity,\n                reasoning=\"No embedding available for comparison\"\n            )\n        \n        # Prepare existing embeddings dictionary\n        entity_embeddings = {}\n        for entity_id, entity in existing_entities.items():\n            if entity.embedding is not None:\n                entity_embeddings[entity_id] = entity.embedding\n        \n        # Find similar entities\n        similar = self.embedder.find_similar(\n            new_entity.embedding,\n            entity_embeddings,\n            threshold=similarity_threshold,\n            top_k=5\n        )\n        \n        # If no similar entities, create new\n        if not similar:\n            return ResolutionResult(\n                action='create_new',\n                confidence=1.0,\n                entity=new_entity,\n                reasoning=\"No similar entities found above threshold\"\n            )\n        \n        # Get top candidate\n        top_id, top_similarity = similar[0]\n        top_entity = existing_entities[top_id]\n        \n        # Check for exact match conditions\n        if self._is_exact_match(new_entity, top_entity, top_similarity):\n            return ResolutionResult(\n                action='merge',\n                confidence=top_similarity,\n                target_id=top_id,\n                reasoning=\"Exact match based on label and type\"\n            )\n        \n        # If very high similarity, automatically merge\n        if top_similarity >= high_confidence_threshold:\n            return ResolutionResult(\n                action='merge',\n                confidence=top_similarity,\n                target_id=top_id,\n                reasoning=f\"High similarity score ({top_similarity:.2f}) indicates same entity\"\n            )\n        \n        # Prepare candidates for validation\n        candidates = []\n        for entity_id, similarity in similar[:3]:  # Top 3 candidates\n            candidates.append((entity_id, similarity, existing_entities[entity_id]))\n        \n        # Determine if we need parliament or simple LLM validation\n        complex_case = self._is_complex_resolution_case(new_entity, candidates)\n        \n        if complex_case and self.parliament and self.config.get(\"use_parliament_for_complex\", True):\n            # Use parliament for complex cases\n            return await self._resolve_with_parliament(new_entity, candidates)\n        else:\n            # Use LLM validation for simple cases\n            return await self._validate_with_llm(new_entity, candidates)\n    \n    def _is_exact_match(self, entity1: Entity, entity2: Entity, similarity: float) -> bool:\n        \"\"\"Check if entities are exact matches\"\"\"\n        # Same label and type\n        if entity1.label.lower() == entity2.label.lower() and entity1.entity_type == entity2.entity_type:\n            return True\n        \n        # Check for known abbreviations\n        if self._is_abbreviation(entity1.label, entity2.label) and entity1.entity_type == entity2.entity_type:\n            return True\n            \n        return False\n    \n    def _is_abbreviation(self, label1: str, label2: str) -> bool:\n        \"\"\"Check if one label is an abbreviation of the other\"\"\"\n        # Simple abbreviation check\n        shorter, longer = (label1, label2) if len(label1) < len(label2) else (label2, label1)\n        \n        # Check if shorter is an acronym of longer\n        if shorter.isupper() and len(shorter) >= 2:\n            words = longer.split()\n            acronym = ''.join(w[0].upper() for w in words if w)\n            if shorter == acronym:\n                return True\n        \n        # Check common abbreviations\n        abbreviations = {\n            \"ML\": \"Machine Learning\",\n            \"AI\": \"Artificial Intelligence\",\n            \"NLP\": \"Natural Language Processing\",\n            \"KB\": \"Knowledge Base\",\n            \"KG\": \"Knowledge Graph\"\n        }\n        \n        return (shorter in abbreviations and abbreviations[shorter].lower() == longer.lower()) or \\\n               (longer in abbreviations and abbreviations[longer].lower() == shorter.lower())\n    \n    def _is_complex_resolution_case(self, new_entity: Entity, \n                                  candidates: List[Tuple[str, float, Entity]]) -> bool:\n        \"\"\"Determine if this is a complex resolution case requiring parliament\"\"\"\n        # Multiple similar candidates\n        if len(candidates) > 2:\n            return True\n        \n        # Close similarity scores\n        if len(candidates) >= 2:\n            top_similarity = candidates[0][1]\n            second_similarity = candidates[1][1]\n            if abs(top_similarity - second_similarity) < 0.05:\n                return True\n        \n        # Cross-tier merge\n        for _, _, candidate in candidates:\n            if new_entity.tier != candidate.tier:\n                return True\n        \n        # Conflicting entity types\n        types = set(e.entity_type for _, _, e in candidates if e.entity_type)\n        if new_entity.entity_type and new_entity.entity_type not in types and len(types) > 0:\n            return True\n        \n        return False\n    \n    async def _validate_with_llm(self, new_entity: Entity, \n                              candidates: List[Tuple[str, float, Entity]]) -> ResolutionResult:\n        \"\"\"Validate entity resolution with LLM\"\"\"\n        # Get the top candidate for detailed comparison\n        if not candidates:\n            return ResolutionResult(\n                action='create_new',\n                confidence=1.0,\n                entity=new_entity,\n                reasoning=\"No candidates for comparison\"\n            )\n        \n        top_id, similarity, top_entity = candidates[0]\n        \n        # Format prompt\n        prompt_vars = {\n            \"new_entity_id\": new_entity.entity_id,\n            \"new_entity_label\": new_entity.label,\n            \"new_entity_type\": new_entity.entity_type or \"Unknown\",\n            \"new_entity_description\": self._get_entity_description(new_entity),\n            \"new_entity_attributes\": self._format_entity_attributes(new_entity),\n            \"source\": new_entity.source_uri or \"Unknown\",\n            \"existing_entity_id\": top_entity.entity_id,\n            \"existing_entity_label\": top_entity.label,\n            \"existing_entity_type\": top_entity.entity_type or \"Unknown\",\n            \"existing_entity_description\": self._get_entity_description(top_entity),\n            \"existing_entity_attributes\": self._format_entity_attributes(top_entity),\n            \"existing_entity_tier\": top_entity.tier,\n            \"existing_entity_created\": top_entity.created_at.strftime(\"%Y-%m-%d\"),\n            \"similarity_score\": f\"{similarity:.2f}\"\n        }\n        \n        try:\n            prompt = self.prompt_template.format(**prompt_vars)\n        except KeyError as e:\n            logger.error(f\"Missing prompt variable: {e}\")\n            # Use simplified prompt\n            prompt = f\"\"\"\n            Should these entities be merged?\n            \n            New: {new_entity.label} ({new_entity.entity_type})\n            Existing: {top_entity.label} ({top_entity.entity_type})\n            Similarity: {similarity:.2f}\n            \n            Return JSON with: resolution (MATCH/NO_MATCH), confidence, reasoning\n            \"\"\"\n        \n        # Get LLM response\n        try:\n            response = await self.llm.generate(prompt)\n            decision = self._parse_llm_response(response)\n            \n            # Map resolution to action\n            if decision.get('resolution') == 'MATCH':\n                return ResolutionResult(\n                    action='merge',\n                    confidence=float(decision.get('confidence', similarity)),\n                    target_id=top_id,\n                    reasoning=decision.get('reasoning', ''),\n                    requires_parliament=False\n                )\n            else:\n                return ResolutionResult(\n                    action='keep_separate',\n                    confidence=float(decision.get('confidence', 0.7)),\n                    entity=new_entity,\n                    reasoning=decision.get('reasoning', ''),\n                    requires_parliament=False\n                )\n                \n        except Exception as e:\n            logger.error(f\"Error in LLM validation: {e}\")\n            # Default to keeping separate on error\n            return ResolutionResult(\n                action='keep_separate',\n                confidence=0.5,\n                entity=new_entity,\n                reasoning=f\"Error in LLM validation: {str(e)}\",\n                requires_parliament=False\n            )\n    \n    def _parse_llm_response(self, response: str) -> Dict[str, Any]:\n        \"\"\"Parse LLM response for entity resolution\"\"\"\n        try:\n            # Find JSON in response\n            import re\n            json_match = re.search(r'{.*}', response, re.DOTALL)\n            \n            if json_match:\n                decision = json.loads(json_match.group(0))\n                \n                # Normalize resolution value\n                if 'resolution' in decision:\n                    resolution = decision['resolution'].upper()\n                    if resolution in ['MATCH', 'MERGE', 'YES']:\n                        decision['resolution'] = 'MATCH'\n                    elif resolution in ['NO_MATCH', 'NO_MERGE', 'NO', 'KEEP_SEPARATE']:\n                        decision['resolution'] = 'NO_MATCH'\n                    elif resolution in ['UNCERTAIN', 'MAYBE']:\n                        decision['resolution'] = 'UNCERTAIN'\n                \n                return decision\n            else:\n                logger.warning(\"Could not find JSON in LLM response\")\n                return {\"resolution\": \"NO_MATCH\", \"confidence\": 0.5, \"reasoning\": \"Could not parse response\"}\n                \n        except json.JSONDecodeError as e:\n            logger.error(f\"Error parsing JSON from LLM response: {e}\")\n            return {\"resolution\": \"NO_MATCH\", \"confidence\": 0.5, \"reasoning\": f\"JSON parse error: {str(e)}\"}\n    \n    async def _resolve_with_parliament(self, new_entity: Entity,\n                                     candidates: List[Tuple[str, float, Entity]]) -> ResolutionResult:\n        \"\"\"Resolve entity using parliament decision engine\"\"\"\n        if not self.parliament:\n            logger.warning(\"Parliament engine requested but not available\")\n            return await self._validate_with_llm(new_entity, candidates)\n        \n        # Prepare context for parliament\n        context = {\n            \"decision_type\": \"entity_merge\",\n            \"new_entity\": new_entity.to_dict(),\n            \"candidates\": [\n                {\n                    \"entity_id\": entity_id,\n                    \"similarity\": similarity,\n                    \"entity\": entity.to_dict()\n                }\n                for entity_id, similarity, entity in candidates\n            ]\n        }\n        \n        # Get parliament decision\n        try:\n            decision = await self.parliament.decide(context)\n            \n            # Convert to resolution result\n            if decision.get('action') == 'merge' or decision.get('decision') == 'MERGE':\n                return ResolutionResult(\n                    action='merge',\n                    confidence=float(decision.get('confidence', 0.0)),\n                    target_id=decision.get('target_id') or candidates[0][0],\n                    reasoning=decision.get('reasoning'),\n                    requires_parliament=True\n                )\n            else:\n                return ResolutionResult(\n                    action='keep_separate',\n                    confidence=float(decision.get('confidence', 0.0)),\n                    entity=new_entity,\n                    reasoning=decision.get('reasoning'),\n                    requires_parliament=True\n                )\n        except Exception as e:\n            logger.error(f\"Parliament decision failed: {e}\")\n            return await self._validate_with_llm(new_entity, candidates)\n    \n    async def merge_entities(self, target_entity: Entity, source_entity: Entity) -> Entity:\n        \"\"\"Merge source entity into target entity\n        \n        Args:\n            target_entity: Entity to merge into\n            source_entity: Entity to merge from\n            \n        Returns:\n            Updated target entity\n        \"\"\"\n        # Merge attributes (source overwrites target for conflicts)\n        for key, value in source_entity.attributes.items():\n            if key not in target_entity.attributes:\n                target_entity.attributes[key] = value\n            elif isinstance(value, list) and isinstance(target_entity.attributes[key], list):\n                # Merge lists\n                target_entity.attributes[key] = list(set(target_entity.attributes[key] + value))\n        \n        # Merge evidence IDs\n        target_entity.evidence_ids = list(set(target_entity.evidence_ids + source_entity.evidence_ids))\n        \n        # Update embedding if target doesn't have one\n        if target_entity.embedding is None and source_entity.embedding is not None:\n            target_entity.embedding = source_entity.embedding\n        elif target_entity.embedding is not None and source_entity.embedding is not None:\n            # Average embeddings\n            target_entity.embedding = (target_entity.embedding + source_entity.embedding) / 2\n            # Normalize\n            norm = np.linalg.norm(target_entity.embedding)\n            if norm > 0:\n                target_entity.embedding = target_entity.embedding / norm\n        \n        # Update tier if source is higher confidence\n        tier_priority = {\"peripheral\": 1, \"personal\": 2, \"digested\": 3}\n        if tier_priority.get(source_entity.tier, 0) > tier_priority.get(target_entity.tier, 0):\n            target_entity.tier = source_entity.tier\n        \n        # Track merge in metadata\n        if 'merged_from' not in target_entity.metadata:\n            target_entity.metadata['merged_from'] = []\n            \n        target_entity.metadata['merged_from'].append({\n            'entity_id': source_entity.entity_id,\n            'label': source_entity.label,\n            'timestamp': datetime.now().isoformat()\n        })\n        \n        return target_entity\n```\n\n## 3. Fixed Test Infrastructure (tests/test_entity_resolver.py)\n\n```python\n\"\"\"Test suite for Entity Resolver component\"\"\"\nimport pytest\nimport numpy as np\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Tuple\nimport json\n\nfrom knowledge_garden.extraction.entity_resolver import Entity, EntityResolver, ResolutionResult\nfrom knowledge_garden.extraction.embedding_generator import EmbeddingGenerator\n\n\nclass MockEmbeddingGenerator:\n    \"\"\"Mock embedding generator for testing\"\"\"\n    \n    def __init__(self):\n        self.embedding_dim = 768\n        \n    async def encode(self, text: str) -> np.ndarray:\n        \"\"\"Generate mock embedding based on text hash for consistency\"\"\"\n        # Use hash to generate consistent embeddings for same text\n        hash_val = hash(text)\n        np.random.seed(abs(hash_val) % 2**32)\n        embedding = np.random.randn(self.embedding_dim)\n        # Normalize\n        embedding = embedding / np.linalg.norm(embedding)\n        return embedding\n    \n    def cosine_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:\n        \"\"\"Calculate cosine similarity\"\"\"\n        return float(np.dot(emb1, emb2))\n    \n    def find_similar(self, embedding: np.ndarray, \n                    stored_embeddings: Dict[str, np.ndarray], \n                    threshold: float = 0.85,\n                    top_k: int = 5) -> List[Tuple[str, float]]:\n        \"\"\"Find similar embeddings\"\"\"\n        similarities = []\n        for entity_id, stored_emb in stored_embeddings.items():\n            sim = self.cosine_similarity(embedding, stored_emb)\n            if sim > threshold:\n                similarities.append((entity_id, sim))\n        \n        # Sort by similarity\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return similarities[:top_k]\n\n\nclass MockLLMService:\n    \"\"\"Mock LLM service for testing\"\"\"\n    \n    def __init__(self, responses: Dict[str, Any] = None):\n        self.responses = responses or {}\n        self.call_count = 0\n        \n    async def generate(self, prompt: str) -> str:\n        \"\"\"Generate mock response based on prompt content\"\"\"\n        self.call_count += 1\n        \n        # Check for specific test cases in prompt\n        if \"Machine Learning\" in prompt and \"ML\" in prompt:\n            return json.dumps({\n                \"resolution\": \"MATCH\",\n                \"confidence\": 0.95,\n                \"reasoning\": \"ML is a standard abbreviation for Machine Learning\",\n                \"recommended_label\": \"Machine Learning\"\n            })\n        elif \"Python\" in prompt and \"Programming_Language\" in prompt and \"Animal\" in prompt:\n            return json.dumps({\n                \"resolution\": \"NO_MATCH\",\n                \"confidence\": 0.98,\n                \"reasoning\": \"Different entity types indicate completely different concepts\"\n            })\n        elif \"Deep Learning\" in prompt and \"Neural Networks\" in prompt:\n            return json.dumps({\n                \"resolution\": \"UNCERTAIN\",\n                \"confidence\": 0.60,\n                \"reasoning\": \"Related concepts but distinct - Deep Learning is a subset of Neural Networks\"\n            })\n        \n        # Default response\n        return json.dumps({\n            \"resolution\": \"NO_MATCH\",\n            \"confidence\": 0.70,\n            \"reasoning\": \"Entities appear to be different based on analysis\"\n        })\n\n\n@pytest.fixture\ndef embedding_generator():\n    \"\"\"Create mock embedding generator\"\"\"\n    return MockEmbeddingGenerator()\n\n\n@pytest.fixture\ndef llm_service():\n    \"\"\"Create mock LLM service\"\"\"\n    return MockLLMService()\n\n\n@pytest.fixture\ndef entity_resolver(embedding_generator, llm_service):\n    \"\"\"Create entity resolver with mocks\"\"\"\n    config = {\n        \"similarity_threshold\": 0.85,\n        \"high_confidence_threshold\": 0.95,\n        \"use_parliament_for_complex\": False\n    }\n    return EntityResolver(\n        embedding_generator=embedding_generator,\n        llm_service=llm_service,\n        parliament_engine=None,\n        config=config\n    )\n\n\n@pytest.fixture\nasync def sample_entities(embedding_generator):\n    \"\"\"Create sample entities for testing\"\"\"\n    entities = {}\n    \n    # Machine Learning entity\n    ml_entity = Entity(\n        entity_id=\"entity_ml\",\n        label=\"Machine Learning\",\n        entity_type=\"Concept\",\n        attributes={\"domain\": \"AI\", \"description\": \"Field of AI focused on learning from data\"},\n        tier=\"digested\"\n    )\n    ml_entity.embedding = await embedding_generator.encode(\"Machine Learning\")\n    entities[ml_entity.entity_id] = ml_entity\n    \n    # Python programming entity\n    python_prog = Entity(\n        entity_id=\"entity_python_prog\",\n        label=\"Python\",\n        entity_type=\"Programming_Language\",\n        attributes={\"paradigm\": \"multi-paradigm\", \"year\": \"1991\"},\n        tier=\"digested\"\n    )\n    python_prog.embedding = await embedding_generator.encode(\"Python Programming Language\")\n    entities[python_prog.entity_id] = python_prog\n    \n    # Python snake entity\n    python_snake = Entity(\n        entity_id=\"entity_python_snake\",\n        label=\"Python\",\n        entity_type=\"Animal\",\n        attributes={\"class\": \"Reptilia\", \"family\": \"Pythonidae\"},\n        tier=\"peripheral\"\n    )\n    python_snake.embedding = await embedding_generator.encode(\"Python Snake\")\n    entities[python_snake.entity_id] = python_snake\n    \n    # Neural Networks entity\n    nn_entity = Entity(\n        entity_id=\"entity_nn\",\n        label=\"Neural Networks\",\n        entity_type=\"Concept\",\n        attributes={\"domain\": \"AI\", \"description\": \"Computing systems inspired by biological neural networks\"},\n        tier=\"digested\"\n    )\n    nn_entity.embedding = await embedding_generator.encode(\"Neural Networks\")\n    entities[nn_entity.entity_id] = nn_entity\n    \n    return entities\n\n\nclass TestEntityResolver:\n    \"\"\"Test cases for Entity Resolver\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_abbreviation_match(self, entity_resolver, sample_entities, embedding_generator):\n        \"\"\"Test that abbreviations are correctly matched\"\"\"\n        # Create ML entity\n        new_entity = Entity(\n            entity_id=\"entity_ml_new\",\n            label=\"ML\",\n            entity_type=\"Concept\",\n            attributes={\"description\": \"Machine learning techniques\"}\n        )\n        new_entity.embedding = await embedding_generator.encode(\"ML Machine Learning\")\n        \n        # Resolve\n        result = await entity_resolver.resolve_entity(new_entity, sample_entities)\n        \n        # Should merge with Machine Learning\n        assert result.action == \"merge\"\n        assert result.confidence >= 0.9\n        assert result.target_id == \"entity_ml\"\n        assert \"abbreviation\" in result.reasoning.lower() or \"ML\" in result.reasoning\n    \n    @pytest.mark.asyncio\n    async def test_homonym_no_match(self, entity_resolver, sample_entities, embedding_generator):\n        \"\"\"Test that homonyms with different types are not matched\"\"\"\n        # Create new Python programming entity\n        new_entity = Entity(\n            entity_id=\"entity_python_new\",\n            label=\"Python\",\n            entity_type=\"Programming_Language\",\n            attributes={\"description\": \"High-level programming language\"}\n        )\n        new_entity.embedding = await embedding_generator.encode(\"Python Programming\")\n        \n        # Modify sample to only have snake entity\n        snake_only = {\"entity_python_snake\": sample_entities[\"entity_python_snake\"]}\n        \n        # Resolve\n        result = await entity_resolver.resolve_entity(new_entity, snake_only)\n        \n        # Should not merge\n        assert result.action == \"keep_separate\"\n        assert result.confidence >= 0.9\n        assert \"different\" in result.reasoning.lower()\n    \n    @pytest.mark.asyncio\n    async def test_related_concepts_uncertain(self, entity_resolver, sample_entities, embedding_generator):\n        \"\"\"Test that related but distinct concepts get uncertain resolution\"\"\"\n        # Create Deep Learning entity\n        new_entity = Entity(\n            entity_id=\"entity_dl\",\n            label=\"Deep Learning\",\n            entity_type=\"Concept\",\n            attributes={\"description\": \"Neural networks with multiple layers\"}\n        )\n        new_entity.embedding = await embedding_generator.encode(\"Deep Learning Neural Networks\")\n        \n        # Resolve\n        result = await entity_resolver.resolve_entity(new_entity, sample_entities)\n        \n        # Should get LLM validation with uncertain result\n        assert result.action == \"keep_separate\"  # Based on mock response\n        assert 0.5 <= result.confidence <= 0.7\n        assert \"related\" in result.reasoning.lower() or \"subset\" in result.reasoning.lower()\n    \n    @pytest.mark.asyncio\n    async def test_no_similar_entities(self, entity_resolver, sample_entities, embedding_generator):\n        \"\"\"Test creation of new entity when no similar ones exist\"\"\"\n        # Create completely different entity\n        new_entity = Entity(\n            entity_id=\"entity_quantum\",\n            label=\"Quantum Computing\",\n            entity_type=\"Concept\",\n            attributes={\"description\": \"Computing using quantum phenomena\"}\n        )\n        new_entity.embedding = await embedding_generator.encode(\"Quantum Computing Physics\")\n        \n        # Use high threshold to ensure no matches\n        result = await entity_resolver.resolve_entity(\n            new_entity, sample_entities, similarity_threshold=0.99\n        )\n        \n        # Should create new\n        assert result.action == \"create_new\"\n        assert result.confidence == 1.0\n        assert result.entity == new_entity\n    \n    @pytest.mark.asyncio\n    async def test_exact_match(self, entity_resolver, sample_entities, embedding_generator):\n        \"\"\"Test exact match by label and type\"\"\"\n        # Create entity with same label and type\n        new_entity = Entity(\n            entity_id=\"entity_ml_duplicate\",\n            label=\"Machine Learning\",\n            entity_type=\"Concept\",\n            attributes={\"source\": \"different\"}\n        )\n        new_entity.embedding = await embedding_generator.encode(\"Machine Learning\")\n        \n        # Resolve\n        result = await entity_resolver.resolve_entity(new_entity, sample_entities)\n        \n        # Should merge due to exact match\n        assert result.action == \"merge\"\n        assert result.target_id == \"entity_ml\"\n        assert \"exact match\" in result.reasoning.lower() or \"same\" in result.reasoning.lower()\n    \n    @pytest.mark.asyncio\n    async def test_merge_entities(self, entity_resolver):\n        \"\"\"Test entity merging functionality\"\"\"\n        # Create two entities to merge\n        target = Entity(\n            entity_id=\"entity_target\",\n            label=\"Artificial Intelligence\",\n            entity_type=\"Concept\",\n            attributes={\"field\": \"Computer Science\", \"year\": \"1956\"},\n            evidence_ids=[\"ev1\", \"ev2\"],\n            tier=\"digested\"\n        )\n        target.embedding = np.array([1.0, 0.0, 0.0])\n        \n        source = Entity(\n            entity_id=\"entity_source\",\n            label=\"AI\",\n            entity_type=\"Concept\",\n            attributes={\"applications\": [\"ML\", \"NLP\"], \"year\": \"1956\"},\n            evidence_ids=[\"ev3\", \"ev4\"],\n            tier=\"personal\"\n        )\n        source.embedding = np.array([0.0, 1.0, 0.0])\n        \n        # Merge\n        merged = await entity_resolver.merge_entities(target, source)\n        \n        # Check merge results\n        assert merged.entity_id == \"entity_target\"\n        assert merged.label == \"Artificial Intelligence\"  # Keep target label\n        assert \"field\" in merged.attributes  # Keep target attributes\n        assert \"applications\" in merged.attributes  # Add source attributes\n        assert len(merged.evidence_ids) == 4  # Merge evidence\n        assert merged.tier == \"digested\"  # Keep higher tier\n        \n        # Check merge tracking\n        assert \"merged_from\" in merged.metadata\n        assert len(merged.metadata[\"merged_from\"]) == 1\n        assert merged.metadata[\"merged_from\"][0][\"entity_id\"] == \"entity_source\"\n        \n        # Check embedding averaging\n        expected_embedding = np.array([0.5, 0.5, 0.0])\n        expected_embedding = expected_embedding / np.linalg.norm(expected_embedding)\n        np.testing.assert_allclose(merged.embedding, expected_embedding, rtol=1e-5)\n    \n    @pytest.mark.asyncio  \n    async def test_complex_case_detection(self, entity_resolver, sample_entities):\n        \"\"\"Test detection of complex resolution cases\"\"\"\n        # Create new entity\n        new_entity = Entity(\n            entity_id=\"entity_new\",\n            label=\"Test Entity\",\n            entity_type=\"Concept\",\n            tier=\"personal\"\n        )\n        \n        # Test multiple candidates\n        candidates = [\n            (\"e1\", 0.88, Entity(\"e1\", \"Entity 1\", entity_type=\"Concept\")),\n            (\"e2\", 0.87, Entity(\"e2\", \"Entity 2\", entity_type=\"Concept\")),\n            (\"e3\", 0.86, Entity(\"e3\", \"Entity 3\", entity_type=\"Concept\"))\n        ]\n        assert entity_resolver._is_complex_resolution_case(new_entity, candidates) == True\n        \n        # Test close similarity scores\n        candidates = [\n            (\"e1\", 0.88, Entity(\"e1\", \"Entity 1\", entity_type=\"Concept\")),\n            (\"e2\", 0.85, Entity(\"e2\", \"Entity 2\", entity_type=\"Concept\"))\n        ]\n        assert entity_resolver._is_complex_resolution_case(new_entity, candidates) == True\n        \n        # Test cross-tier merge\n        candidates = [\n            (\"e1\", 0.90, Entity(\"e1\", \"Entity 1\", entity_type=\"Concept\", tier=\"digested\"))\n        ]\n        assert entity_resolver._is_complex_resolution_case(new_entity, candidates) == True\n        \n        # Test conflicting types\n        new_entity.entity_type = \"Person\"\n        candidates = [\n            (\"e1\", 0.88, Entity(\"e1\", \"Entity 1\", entity_type=\"Organization\"))\n        ]\n        assert entity_resolver._is_complex_resolution_case(new_entity, candidates) == True\n    \n    @pytest.mark.asyncio\n    async def test_no_embedding_handling(self, entity_resolver, sample_entities):\n        \"\"\"Test handling of entities without embeddings\"\"\"\n        # Create entity without embedding\n        new_entity = Entity(\n            entity_id=\"entity_no_emb\",\n            label=\"No Embedding Entity\",\n            entity_type=\"Concept\"\n        )\n        \n        # Resolve - should generate embedding\n        result = await entity_resolver.resolve_entity(new_entity, sample_entities)\n        \n        # Should have generated embedding\n        assert new_entity.embedding is not None\n        assert result.action in [\"create_new\", \"keep_separate\", \"merge\"]\n    \n    def test_abbreviation_detection(self, entity_resolver):\n        \"\"\"Test abbreviation detection logic\"\"\"\n        # Test acronym detection\n        assert entity_resolver._is_abbreviation(\"ML\", \"Machine Learning\") == True\n        assert entity_resolver._is_abbreviation(\"AI\", \"Artificial Intelligence\") == True\n        assert entity_resolver._is_abbreviation(\"NLP\", \"Natural Language Processing\") == True\n        \n        # Test known abbreviations\n        assert entity_resolver._is_abbreviation(\"KB\", \"Knowledge Base\") == True\n        assert entity_resolver._is_abbreviation(\"KG\", \"Knowledge Graph\") == True\n        \n        # Test non-abbreviations\n        assert entity_resolver._is_abbreviation(\"Python\", \"Java\") == False\n        assert entity_resolver._is_abbreviation(\"ML\", \"Deep Learning\") == False\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n```\n\n## 4. Fixed Configuration Validator (core/config_validator.py)\n\n```python\n\"\"\"\nConfiguration validation module for Knowledge Garden v2.0\n\nThis module provides functionality to validate configuration files against JSON schemas,\nensuring that all required fields are present and have correct types and values.\n\"\"\"\n\nimport os\nimport json\nimport logging\nimport jsonschema\nimport yaml\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\n# Define schema paths\nSCHEMA_DIR = Path(__file__).parent.parent / \"schemas\"\n\n# Ensure schema directory exists\nSCHEMA_DIR.mkdir(exist_ok=True, parents=True)\n\nclass ConfigValidator:\n    \"\"\"\n    Validates configuration files against JSON schemas\n    \n    This class ensures that configuration files contain all required fields\n    and that values have the correct types and meet any specified constraints.\n    \"\"\"\n    \n    def __init__(self, schema_dir: Optional[Path] = None):\n        \"\"\"\n        Initialize the configuration validator\n        \n        Args:\n            schema_dir: Optional custom directory for JSON schemas\n        \"\"\"\n        self.schema_dir = schema_dir or SCHEMA_DIR\n        self.schemas: Dict[str, Dict[str, Any]] = {}\n        self._create_default_schemas()\n        self.load_schemas()\n    \n    def _create_default_schemas(self) -> None:\n        \"\"\"Create default schemas if they don't exist\"\"\"\n        default_schemas = {\n            \"extraction\": {\n                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n                \"type\": \"object\",\n                \"required\": [\"embedding\", \"similarity\", \"evidence\", \"sentence_extraction\", \"entity_resolution\", \"llm\"],\n                \"properties\": {\n                    \"embedding\": {\n                        \"type\": \"object\",\n                        \"required\": [\"model\", \"dimension\", \"batch_size\", \"device\"],\n                        \"properties\": {\n                            \"model\": {\"type\": \"string\", \"minLength\": 1},\n                            \"dimension\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"batch_size\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"device\": {\"type\": \"string\", \"enum\": [\"auto\", \"cpu\", \"cuda\"]},\n                            \"cache_dir\": {\"type\": \"string\"}\n                        }\n                    },\n                    \"similarity\": {\n                        \"type\": \"object\",\n                        \"required\": [\"exact_match\", \"merge_candidate\", \"potentially_related\"],\n                        \"properties\": {\n                            \"exact_match\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                            \"merge_candidate\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                            \"potentially_related\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n                        }\n                    },\n                    \"evidence\": {\n                        \"type\": \"object\",\n                        \"required\": [\"max_quote_length\", \"context_window\"],\n                        \"properties\": {\n                            \"max_quote_length\": {\"type\": \"integer\", \"minimum\": 10},\n                            \"context_window\": {\"type\": \"integer\", \"minimum\": 0},\n                            \"max_batch_size\": {\"type\": \"integer\", \"minimum\": 1}\n                        }\n                    },\n                    \"sentence_extraction\": {\n                        \"type\": \"object\",\n                        \"required\": [\"min_sentence_length\", \"max_sentence_length\", \"batch_size\", \"mode\"],\n                        \"properties\": {\n                            \"min_sentence_length\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"max_sentence_length\": {\"type\": \"integer\", \"minimum\": 10},\n                            \"context_window\": {\"type\": \"integer\", \"minimum\": 0},\n                            \"batch_size\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"mode\": {\"type\": \"string\", \"enum\": [\"all\", \"filtered\", \"sampled\"]},\n                            \"relevance_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                            \"entity_density_threshold\": {\"type\": \"integer\", \"minimum\": 0},\n                            \"sample_rate\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                            \"min_samples\": {\"type\": \"integer\", \"minimum\": 1}\n                        }\n                    },\n                    \"entity_resolution\": {\n                        \"type\": \"object\",\n                        \"required\": [\"auto_merge_threshold\", \"parliament_threshold\", \"use_parliament_for_complex\"],\n                        \"properties\": {\n                            \"auto_merge_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                            \"parliament_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                            \"use_parliament_for_complex\": {\"type\": \"boolean\"},\n                            \"complexity_indicators\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                    \"multiple_candidates\": {\"type\": \"boolean\"},\n                                    \"conflicting_types\": {\"type\": \"boolean\"},\n                                    \"ambiguous_attributes\": {\"type\": \"boolean\"},\n                                    \"cross_tier_merge\": {\"type\": \"boolean\"}\n                                }\n                            }\n                        }\n                    },\n                    \"llm\": {\n                        \"type\": \"object\",\n                        \"required\": [\"default_model\", \"timeout\", \"max_retries\"],\n                        \"properties\": {\n                            \"default_model\": {\"type\": \"string\", \"minLength\": 1},\n                            \"timeout\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"max_retries\": {\"type\": \"integer\", \"minimum\": 0},\n                            \"sentence_extraction_model\": {\"type\": \"string\"},\n                            \"entity_resolution_model\": {\"type\": \"string\"},\n                            \"complex_resolution_model\": {\"type\": \"string\"}\n                        }\n                    }\n                }\n            },\n            \n            \"parliament\": {\n                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n                \"type\": \"object\",\n                \"required\": [\"core\", \"members\", \"decision_types\", \"llm\"],\n                \"properties\": {\n                    \"core\": {\n                        \"type\": \"object\",\n                        \"required\": [\"enabled\", \"default_mode\", \"num_perspectives\", \"consensus_threshold\"],\n                        \"properties\": {\n                            \"enabled\": {\"type\": \"boolean\"},\n                            \"default_mode\": {\"type\": \"string\", \"enum\": [\"cooperative\", \"adversarial\", \"divergent\"]},\n                            \"num_perspectives\": {\"type\": \"integer\", \"minimum\": 2, \"maximum\": 10},\n                            \"num_rounds\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n                            \"time_limit\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"consensus_threshold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 100},\n                            \"fallback_strategy\": {\"type\": \"string\", \"enum\": [\"majority\", \"weighted\", \"chairperson\"]},\n                            \"summarize\": {\"type\": \"boolean\"},\n                            \"cache_decisions\": {\"type\": \"boolean\"},\n                            \"cache_lifetime\": {\"type\": \"integer\", \"minimum\": 1}\n                        }\n                    }\n                }\n            },\n            \n            \"artifacts\": {\n                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n                \"type\": \"object\",\n                \"required\": [\"dependency_graph\", \"version_manager\", \"storage\", \"staleness\"],\n                \"properties\": {\n                    \"dependency_graph\": {\n                        \"type\": \"object\",\n                        \"required\": [\"persistent\", \"auto_update\"],\n                        \"properties\": {\n                            \"persistent\": {\"type\": \"boolean\"},\n                            \"auto_update\": {\"type\": \"boolean\"},\n                            \"max_depth\": {\"type\": \"integer\", \"minimum\": 1},\n                            \"include_dependency_types\": {\n                                \"type\": \"array\",\n                                \"items\": {\"type\": \"string\"}\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        \n        # Save schemas to files if they don't exist\n        for schema_name, schema in default_schemas.items():\n            schema_path = self.schema_dir / f\"{schema_name}.json\"\n            if not schema_path.exists():\n                with open(schema_path, 'w') as f:\n                    json.dump(schema, f, indent=2)\n                logger.info(f\"Created default schema: {schema_path}\")\n    \n    def load_schemas(self) -> None:\n        \"\"\"Load all JSON schemas from the schema directory\"\"\"\n        if not self.schema_dir.exists():\n            logger.warning(f\"Schema directory not found: {self.schema_dir}\")\n            return\n            \n        for schema_file in self.schema_dir.glob(\"*.json\"):\n            try:\n                with open(schema_file, 'r') as f:\n                    schema = json.load(f)\n                    \n                schema_name = schema_file.stem\n                self.schemas[schema_name] = schema\n                logger.info(f\"Loaded schema: {schema_name}\")\n            except Exception as e:\n                logger.error(f\"Error loading schema {schema_file}: {e}\")\n    \n    def validate_config(self, config: Dict[str, Any], schema_name: str) -> Tuple[bool, List[str]]:\n        \"\"\"\n        Validate a configuration against a schema\n        \n        Args:\n            config: Configuration dictionary to validate\n            schema_name: Name of the schema to validate against\n            \n        Returns:\n            Tuple of (is_valid, error_messages)\n        \"\"\"\n        if schema_name not in self.schemas:\n            logger.error(f\"Schema not found: {schema_name}\")\n            return False, [f\"Schema not found: {schema_name}\"]\n            \n        schema = self.schemas[schema_name]\n        \n        try:\n            # Create validator with custom error messages\n            validator = jsonschema.Draft7Validator(schema)\n            \n            # Collect all validation errors\n            errors = sorted(validator.iter_errors(config), key=lambda e: e.path)\n            \n            if not errors:\n                return True, []\n                \n            # Format error messages\n            error_messages = []\n            for error in errors:\n                # Build path string\n                path_parts = list(error.path)\n                if path_parts:\n                    path = \".\".join(str(p) for p in path_parts)\n                else:\n                    path = \"root\"\n                \n                # Create user-friendly error message\n                if error.validator == \"required\":\n                    missing_fields = error.validator_value\n                    message = f\"{path}: Missing required field(s): {', '.join(missing_fields)}\"\n                elif error.validator == \"type\":\n                    expected = error.validator_value\n                    actual = type(error.instance).__name__\n                    message = f\"{path}: Expected type '{expected}' but got '{actual}'\"\n                elif error.validator == \"enum\":\n                    allowed = error.validator_value\n                    message = f\"{path}: Value must be one of {allowed}, got '{error.instance}'\"\n                elif error.validator == \"minimum\":\n                    message = f\"{path}: Value {error.instance} is below minimum {error.validator_value}\"\n                elif error.validator == \"maximum\":\n                    message = f\"{path}: Value {error.instance} is above maximum {error.validator_value}\"\n                else:\n                    message = f\"{path}: {error.message}\"\n                    \n                error_messages.append(message)\n                \n            return False, error_messages\n            \n        except jsonschema.SchemaError as e:\n            logger.error(f\"Invalid schema: {e}\")\n            return False, [f\"Invalid schema: {str(e)}\"]\n        except Exception as e:\n            logger.error(f\"Validation error: {e}\")\n            return False, [f\"Validation error: {str(e)}\"]\n    \n    def validate_yaml_config(self, yaml_path: Path, schema_name: str) -> Tuple[bool, List[str]]:\n        \"\"\"\n        Validate a YAML configuration file against a schema\n        \n        Args:\n            yaml_path: Path to YAML configuration file\n            schema_name: Name of the schema to validate against\n            \n        Returns:\n            Tuple of (is_valid, error_messages)\n        \"\"\"\n        try:\n            with open(yaml_path, 'r') as f:\n                config = yaml.safe_load(f)\n                \n            if config is None:\n                return False, [\"Empty configuration file\"]\n                \n            return self.validate_config(config, schema_name)\n        except yaml.YAMLError as e:\n            logger.error(f\"Error parsing YAML file {yaml_path}: {e}\")\n            return False, [f\"YAML parsing error: {str(e)}\"]\n        except FileNotFoundError:\n            return False, [f\"Configuration file not found: {yaml_path}\"]\n        except Exception as e:\n            logger.error(f\"Error loading YAML file {yaml_path}: {e}\")\n            return False, [f\"Error loading file: {str(e)}\"]\n    \n    def validate_all_configs(self, config_dir: Path) -> Dict[str, Tuple[bool, List[str]]]:\n        \"\"\"\n        Validate all configuration files in a directory\n        \n        Args:\n            config_dir: Directory containing configuration files\n            \n        Returns:\n            Dictionary mapping file names to validation results\n        \"\"\"\n        results = {}\n        \n        if not config_dir.exists():\n            logger.error(f\"Configuration directory not found: {config_dir}\")\n            return results\n            \n        # Process YAML files\n        for config_file in config_dir.glob(\"*.yaml\"):\n            schema_name = config_file.stem\n            \n            if schema_name in self.schemas:\n                is_valid, errors = self.validate_yaml_config(config_file, schema_name)\n                results[config_file.name] = (is_valid, errors)\n                \n                if is_valid:\n                    logger.info(f\"✓ Valid configuration: {config_file.name}\")\n                else:\n                    logger.error(f\"✗ Invalid configuration: {config_file.name}\")\n                    for error in errors:\n                        logger.error(f\"  - {error}\")\n            else:\n                results[config_file.name] = (False, [f\"No schema defined for {schema_name}\"])\n                logger.warning(f\"No schema for configuration: {config_file.name}\")\n                \n        return results\n    \n    def generate_example_config(self, schema_name: str) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Generate an example configuration that satisfies the schema\n        \n        Args:\n            schema_name: Name of the schema\n            \n        Returns:\n            Example configuration dictionary\n        \"\"\"\n        if schema_name not in self.schemas:\n            return None\n            \n        schema = self.schemas[schema_name]\n        return self._generate_from_schema(schema)\n    \n    def _generate_from_schema(self, schema: Dict[str, Any]) -> Any:\n        \"\"\"Generate example data from a JSON schema\"\"\"\n        schema_type = schema.get(\"type\", \"object\")\n        \n        if schema_type == \"object\":\n            obj = {}\n            properties = schema.get(\"properties\", {})\n            required = schema.get(\"required\", [])\n            \n            for prop, prop_schema in properties.items():\n                if prop in required or len(properties) <= 5:  # Include all if few properties\n                    obj[prop] = self._generate_from_schema(prop_schema)\n                    \n            return obj\n            \n        elif schema_type == \"array\":\n            item_schema = schema.get(\"items\", {\"type\": \"string\"})\n            return [self._generate_from_schema(item_schema)]\n            \n        elif schema_type == \"string\":\n            if \"enum\" in schema:\n                return schema[\"enum\"][0]\n            elif \"minLength\" in schema:\n                return \"example_string\"\n            else:\n                return \"string\"\n                \n        elif schema_type == \"number\":\n            minimum = schema.get(\"minimum\", 0)\n            maximum = schema.get(\"maximum\", 1)\n            return (minimum + maximum) / 2\n            \n        elif schema_type == \"integer\":\n            minimum = schema.get(\"minimum\", 1)\n            return minimum\n            \n        elif schema_type == \"boolean\":\n            return True\n            \n        else:\n            return None\n\n\ndef validate_config_on_startup(config_dir: Optional[Path] = None) -> bool:\n    \"\"\"\n    Validate all configuration files on startup\n    \n    Args:\n        config_dir: Configuration directory path\n        \n    Returns:\n        True if all configurations are valid, False otherwise\n    \"\"\"\n    if config_dir is None:\n        config_dir = Path(__file__).parent.parent / \"config\"\n    \n    # Initialize validator\n    validator = ConfigValidator()\n    \n    # Validate all configuration files\n    results = validator.validate_all_configs(config_dir)\n    \n    # Check if all are valid\n    all_valid = all(is_valid for is_valid, _ in results.values())\n    \n    if all_valid:\n        logger.info(\"✓ All configurations are valid\")\n    else:\n        logger.error(\"✗ Some configurations have validation errors\")\n        \n    # Generate example configs for missing files\n    for schema_name in validator.schemas:\n        config_file = config_dir / f\"{schema_name}.yaml\"\n        if not config_file.exists():\n            logger.warning(f\"Missing configuration file: {config_file.name}\")\n            example = validator.generate_example_config(schema_name)\n            if example:\n                logger.info(f\"Example configuration for {schema_name}:\")\n                print(yaml.dump(example, default_flow_style=False))\n    \n    return all_valid\n\n\nif __name__ == \"__main__\":\n    # Set up logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(levelname)s - %(message)s'\n    )\n    \n    # Validate configurations\n    config_valid = validate_config_on_startup()\n    \n    # Exit with appropriate code\n    sys.exit(0 if config_valid else 1)\n```\n\nThese fixes address the core issues identified:\n\n1. **Improved Prompt Structure** - The entity resolution prompt now includes proper context, examples, reasoning steps, and error handling\n2. **Better Entity Resolution** - Fixed implementation with proper tier handling, abbreviation detection, and improved merging logic\n3. **Comprehensive Testing** - Added proper test suite with various test cases for entity resolution\n4. **Working Config Validation** - The validator now properly creates schemas, validates configs, and provides helpful error messages\n\nThe improvements follow the recommendations in the improvement document and should significantly enhance the system's reliability and performance.",
            "citations": []
          }
        ],
        "updated_at": "2025-06-11T14:09:25.995540Z",
        "attachments": [],
        "files": []
      }
    }
  ]
}