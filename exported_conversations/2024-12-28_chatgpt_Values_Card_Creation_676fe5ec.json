{
  "id": "676fe5ec-8824-8012-a0e9-caf5b3db452b",
  "source": "chatgpt",
  "title": "Values Card Creation",
  "created_at": "2024-12-28T12:50:05.060486",
  "updated_at": "2024-12-28T15:46:02.290537",
  "summary": null,
  "message_count": 7,
  "messages": [
    {
      "id": "9293d733-f86e-4b61-a095-09ba6e8aa347",
      "role": "user",
      "content": "",
      "created_at": null,
      "sequence_order": 0,
      "parent_id": "48a667d2-b61e-4fc1-b749-9b0a80ee478f",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "aaa2eda2-5f07-46e5-99ff-47f242c95246",
      "role": "user",
      "content": "can you help me reverse engineer what you think the system prompt is from the following information about Joe Edelman's values process\nValues, Preferences, Meaningful Choiceâˆ—\r\nJoe Edelmanâ€ \r\n6 August 2021 â€” draft\r\nAbstract\r\nMany fields (social choice, welfare economics, recommender\r\nsystems) assume people express what benefits them via\r\ntheir â€˜revealed preferences.â€™ Revealed preferences have well\u0002documented problems when used this way, but are hard to\r\ndisplace in these fields because, as an information source,\r\nthey are simple, universally applicable, robust, and high\u0002resolution. In order to compete, other information sources\r\n(about participantsâ€™ values, capabilities and functionings,\r\netc) would need to match this. I present a conception of\r\nvalues as attention policies resulting from constitutive judge\u0002ments, and use it to build an alternative preference relation,\r\nMeaningful Choice, which retains many desirable features\r\nof revealed preference.\r\nIntroduction\r\nIn his 1938 paper introducing revealed preference, Samuel\u0002son1 warned:\r\nI should like to state my personal opinion that\r\nnothing said hereâ€¦ touches upon at any point the\r\nproblem of welfare economics, except in the sense\r\nof revealing the confusion in the traditional theory\r\nof these distinct subjects.\r\nSimilar sentiments followed, in Arrow,2 Sen,3 Anderson,4\r\netc. As an information basis for welfare, optimality, social\r\nchoice, etc, revealed preference has been much critiqued.\r\nAll the same, itâ€™s hard to see how to move away from it, with\u0002out paternalism: if people know best for their own livesâ€”\r\nshouldnâ€™t we trust their choices as the final proof of their\r\nintentions? How else can we avoid imposing â€œbetter valuesâ€\r\nfrom above?\r\nOne way to respond is to say that people do know best, and\r\nhave wise values, but that their revealed preferences arenâ€™t\r\nthe last word on those values. But to establish this, youâ€™d\r\nwant another source of information on peopleâ€™s valuesâ€”one\r\nthat retains much of the resolution, robustness, universal\r\napplicability, and democratic simplicity of preference.\r\nIâ€™ll develop such an information source here, by taking two\r\nâ€˜steps backâ€™ from revealed preference, widening the infor\u0002mation collected. Then Iâ€™ll argue that my new source cor\u0002responds with one definition of â€œvaluesâ€ used in common\r\nspeech. Finally, Iâ€™ll show how this new source can be used\r\nin social choice and welfare calculations.\r\nâˆ—Thanks to B. Gabbai, A. Morris, A. Ovadya, J. Stray, & F. Noriega\r\nâ€ School for Social Design joe.edelman@gmail.com\r\n1â€œA Note on the Pure Theory of Consumer's Behaviour.â€\r\n2Social Choice and Individual Values.\r\n3â€œRational Fools.â€\r\n4â€œSymposium on Amartya Senâ€™s Philosophy.â€\r\nAs I go, Iâ€™ll build up a schematic representation of an in\u0002terview structure with which an agentâ€™s values could be col\u0002lected, and a choice assessed to see if it was expressive of\r\nthose values.\r\nMy goal here is to make it clear what Iâ€™m suggesting: what\r\nquestions could be asked, and what answers might suffice to\r\nshow that an agent has this value or that, or that a revealed\r\npreference was a meaningful choice.\r\nI am not suggesting that these interviews can be used to col\u0002lect data at the scale needed for welfare calculations, social\r\nchoice mechanisms, or recommender systems. That practi\u0002cal question is beyond my scope here.\r\nPreferences\r\nA rich literature covers how revealed preferencesâ€”which,\r\nwhen summed up, are called engagement metricsâ€”lead us\r\nastray. You can often get people to choose something with\u0002out serving their real interests: you can misinform them, or\r\nleverage their misplaced hopes.\r\nOr, you can make it so people need your thing for what\r\nonce was possible without it: they need your car to get to\r\nwork, your social media account to find a job, your dress to\r\nsocialize with their friends, etc.\r\nMore broadly, you can manufacture social circumstances\r\nwhere people choose your thing to â€œkeep up with the Jone\u0002ses,â€ to signal allegiance with their tribe, or because theyâ€™ve\r\nlost the ability to coordinate a real solution.5\r\nIn some cases, the person will know their choice doesnâ€™t ex\u0002press their true interestsâ€”that they are bucking to external\r\npressure, caught in the system, or setting aside their goals\r\nto conform to a social rule.6\r\nIn other cases, options have\r\nbeen limited or biased behind our backs.\r\nGiven these problems, why does revealed preference still\r\nplay a role in notions of benefit and measures of optimality?\r\nIt has huge advantages as an information source: in the\r\nresolution and robustness of the data provided, its near\u0002universal applicability, and the democratic simplicity of its\r\ncollection:\r\nâ€¢ Preferences say fine-grained things how about each of\r\nus wants to live, informed by our local situations and\r\npriorities.\r\nâ€¢ Preferences get at what we choose in the final analysis,\r\nour real, battle-tested priorities, not just what we say\r\nwe want.\r\n5See discussions of the prisonerâ€™s dilemma in e.g., Sen, â€œBehaviour\r\nand the Concept of Preferenceâ€; Anderson, â€œSymposium on Amartya\r\nSenâ€™s Philosophy.â€\r\n6See Sen, â€œRational Foolsâ€; Anderson, Value in Ethics and Eco\u0002nomics on â€˜commitment.â€™\r\n1\nâ€¢ Preferences work in many domains of life, and represent\r\nall-things-considered judgements, combining morality,\r\nprudence, whims, etc.\r\nâ€¢ Finally, those who arenâ€™t introspective or eloquent can\r\nstill express a preference. And preferences leave simple\u0002to-understand, verifiable trails: purchase histories, vot\u0002ing records, etc.\r\nAttentional Policies\r\nIâ€™ll try to address the problems with preference, holding on\r\nto these benefits. To do so, I want to take two â€˜steps back.â€™\r\nThe first step is to widen the conception of choice to include\r\noption set formation.\r\nA revealed preference takes the options as given. As other\r\nauthors have pointed out,7\r\nthis doesnâ€™t fully reflect how\r\nchoice works.\r\nConsider my choice, with colleagues, to say something witty.\r\nAt ï¿½ï¿½1\r\n, I choose to say that in particular (over staying silent,\r\nor saying my second-place quip). But, at some earlier point\r\nï¿½ï¿½0\r\n, I decided to invest my attention in finding witty things\r\nto say. Iâ€™ve tried witty phrases on in my mind, and tested\r\nsituations for witty reframings.\r\nAt ï¿½ï¿½0\r\n, I made a much bigger choiceâ€“I adopted8 an atten\u0002tional policy of â€˜looking for witty quips and reframes.â€™ This\r\nis how I assembled the option set for my choice at ï¿½ï¿½1\r\n.\r\nI think this earlier choice, at ï¿½ï¿½0\r\n, reveals more about me than\r\nthe latter one. Indeed, these attentional policies (APs)\r\nmight account for much of whatâ€™s called my â€œpersonalityâ€:\r\nWhen making friends, am I cautious or bold? When consid\u0002ering a purchase, is my focus on price, quality, or durabil\u0002ity? When speaking, do I try to be witty, precise, or down\r\nto earth? These may not just be â€œcharacter traitsâ€ I was\r\nborn with, but policies I adopted, which work together for\r\nmy way of life.9\r\nCollecting an agentâ€™s APs is part of my proposal to im\u0002prove the information basis of welfare, social choice, and\r\nrecommenders. How could that work? You could use an\r\ninteractive survey to collect the agentâ€™s common contexts\r\n(socializing, at work, in difficult conversations), and have\r\nthem pick what they attend to most in each.10\r\nIf they claim to have AP ï¿½ï¿½ in context ï¿½ï¿½, you could verify that\r\nwith followup questions: First, you could ask them about\r\nrecent moments in ï¿½ï¿½, and what options theyâ€™ve found using\r\nï¿½ï¿½. Someone with a policy of vulnerability should be able\r\nto cough up various opportunities for vulnerability, recently\r\nconsidered; some taken, some not.\r\n7Smaldino and Richerson, â€œThe Origins of Options.â€\r\n8\r\nI adopt APs (just like I make plans, adopt heuristics, comply\r\nwith norms, follow through with intentions) because my bounded ra\u0002tionality and social needs make it expedient. (Bratman, Intention,\r\nPlans, and Practical Reason; Simon, â€œA Behavioral Model of Ratio\u0002nal Choiceâ€; Taylor, The Language Animal)\r\n9This characterization of an identity as composed of APs shares\r\nsomething with Anderson, â€œSymposium on Amartya Senâ€™s Philoso\u0002phyâ€; and Velleman, Practical Reflection.\r\n10Social attestations, also, could be a source of AP information:\r\nsomeone who is widely admired for their wit likely has an AP about\r\nit. Behavioral data may also work: if someone shops by price or by\r\nquality, that may show up in purchase or click data. (Using, perhaps,\r\napproaches like Chang, Making Comparisons Count; or Levi, Hard\r\nChoices)\r\nAdditionally, that person will also know when exactly vul\u0002nerability is called for, and when something else takes the\r\nfore. I donâ€™t try to be vulnerable when giving directions to\r\na stranger, or with my dad. Generally, someone with ï¿½ï¿½ in\r\nï¿½ï¿½ will have detailed information about the shape of ï¿½ï¿½.\r\nFigure 1: Verifying APs\r\nSupposing we collected an agentâ€™s APs. How would that\r\ncompare with revealed preference data, in terms of resolu\u0002tion, robustness, universality, and democratic simplicity?\r\nLike preferences, APs are informed by our local situations\r\nand priorities, and say fine-grained things about how we\r\nwant to live. Indeed, they have a higher resolution than\r\nIâ€™ve let on so far: an AP about â€˜honestyâ€™ or â€˜vulnerabilityâ€™\r\nis always short for a more specific policy, like â€œattend to\r\nwhat I feel about each thing we discuss, and let my feeling\r\nshow,â€ or â€œattend to false impressions a listener may get\r\nfrom my statements, and head them off with a disclaimer.â€\r\nThat is: having honesty as a policy requires a substantive\r\ninterpretation of honesty, and these differ from person to\r\nperson.\r\nAPs are battle-tested. I wish I could craft my words to\r\nsimultaneously be kind, honest, tactful, humble, and inspir\u0002ing. I wish I could, at the same moment, be precise in\r\nmy speech, aware how each word lands, aware of my own\r\nfeelings; transparent, calm, and centered; passionate; physi\u0002cally graceful, like a dancer. But APs compete for my atten\u0002tion. So, my choice to look for witty things to say crowds\r\nout many other options.\r\nLike preferences, APs are applicable across all domains in\r\nlifeâ€”the moral, the self-interested, etc, and apply to many\r\nkinds of contexts.\r\nBut, they score worse on democratic simplicity than re\u0002vealed preference. And, for now, itâ€™s unclear how an agentâ€™s\r\nAPs would provide a substitute for revealed preference. Iâ€™ll\r\nget to that.\r\nConstitutive Judgement\r\nMy second â€˜step backâ€™ considers the kind of judgement an\r\nagent makes, when they decide to adopt an AP. Look at\r\nthese three cases:\r\n(1) I watch the road while driving. I donâ€™t want to crash.\r\n(2) Iâ€™m careful with my speech at work. My boss fires\r\nanyone who speaks imprecisely.\r\n2\n(3) Iâ€™ve decided to be more honest with friends. I recently\r\nopened up to a friend about a struggle Iâ€™m having.\r\nSince then, the relationship feels more intimate, and\r\nstronger; itâ€™s easier for me to think about what to say;\r\nmy friend is unexpectedly helpful.\r\nIn all three examples, the agent notes some benefits of adopt\u0002ing an AP ï¿½ï¿½ in a context ï¿½ï¿½â€”benefits like staying alive, keep\u0002ing my job, and having helpful friends. And in each case,\r\nthis leads to a decision to adopt it.\r\nBut, in the first two examples, those benefits come together\r\nto make the case that doing ï¿½ï¿½ in ï¿½ï¿½ is functional, necessary,\r\nprudent, or expedient. The judgement is based on expected\r\ncosts and benefits, much as a rational choice theorist would\r\nhope. Iâ€™ll call this an instrumental judgement.\r\nIn (3), the benefits become supporting evidence for a much\r\nlarger claim: that honesty with friends is constitutive of\r\ngood relationships11, or at least naturally goes together with\r\nthem. The benefits suggest that honesty is going to be part\r\nof any relationship I want to build, or is part of the person I\r\nwant to beâ€”not just that itâ€™s functional in achieving certain\r\nends. Itâ€™s a part-whole claim, or a claim to synergy, not a\r\nclaim about cause and effect.\r\nIâ€™ll call this a constitutive judgement. To make one, we\r\nask ourselves questions like: are Xs impossible without Ys?\r\nAre Xs without Ys in some way lesser? Are there kinds of\r\nwholeness that emerge only when an X has a Y? Note: the\r\nevidence that goes into these questions looks a lot like costs\r\nand benefits (additional kinds of wholeness, etc)! So the\r\nraw materials for this judgement are similar, but instead of\r\nbuilding an expected value, we build a more general conclu\u0002sion.\r\nWhy do we make constitutive judgements, instead of doing\r\na cost-benefit analysis for everything? Other authors have\r\nmarked out a role for constitutive judgements in moral rea\u0002soning,12 higher-order desires,13 self-understanding,14 and\r\nidentifying natural kinds,15 but thereâ€™s also a case to be\r\nmade from bounded rationality:16 constitutive judgements\r\nare cognitively efficient. Instead of asking each time we have\r\nan X, whether we should also have an Y, we ask up-front if\r\nXs and Ys belong together. If we think so, we put Ys in our\r\nXs without considering each case.\r\nAnyways, both kinds of judgement can have errors, and\r\nare subject to reconsideration. Instrumental judgements\r\nare more delicate, because they depend on a more-or-less\r\ncomplete accounting of costs, benefits, and efficacy. For\r\nconstitutive judgements, you donâ€™t need such a complete\r\naccounting: if the benefits youâ€™ve already noticed line up\r\nnicely, you can take them as evidence of a synergy or whole\u0002ness. Youâ€™ll reconsider your judgement only if you start to\r\nthink you got the overall pattern wrong.\r\nThis gives us a way to tell APs resulting from a constitu\u0002tive judgements (CAPs) from those adopted instrumentally\r\n(IAPs). Say an agent uses AP ï¿½ï¿½ in context ï¿½ï¿½.\r\n11Good relationships for this agent, at least.\r\n12Rawls, A Theory of Justice.\r\n13Taylor, â€œ4 What Is Human Agency?â€\r\n14Velleman, Practical Reflection.\r\n15Boyd, â€œHow to Be a Moral Realist.â€\r\n16Following Simon, â€œA Behavioral Model of Rational Choiceâ€; Gib\u0002son, The Senses Considered as Perceptual Systems; and Bratman,\r\nIntention, Plans, and Practical Reason.\r\nFirst, you can ask her: why is ï¿½ï¿½ wise in ï¿½ï¿½? Does she gives a\r\nsmall number of reasons, pointing towards one state of the\r\nworld, ï¿½ï¿½, that sheâ€™s working to achieve, maintain, or ensure\r\n(such as â€˜staying alive,â€™ or â€˜keeping her jobâ€™)? Or does she\r\ngives diffuse, unconnected reasons, and sense sheâ€™s only just\r\nbegun discovering the benefits of ï¿½ï¿½ in ï¿½ï¿½?\r\nSecond, do the given reasons seem to capture her full ra\u0002tionale? If so, itâ€™s likely an IAP. To test this, ask her how\r\nsheâ€™d feel if they were taken out of consideration (e.g., if\r\nshe discovered sheâ€™d crash anyways, that her speech had no\r\nbearing on whether sheâ€™d be fired, or that, without opening\r\nup to friends, she could magically get the same benefits):\r\nWould she select a different AP for ï¿½ï¿½ and consider herself\r\nbetter off, free to focus on something else? Similarly, would\r\nshe consider it a net gain if she could clone herself, and\r\nhave the clone handle doing ï¿½ï¿½ in ï¿½ï¿½? Or otherwise automate\r\nthe achievement of ï¿½ï¿½? Orâ€”on the other handâ€”would she\r\nreserve for herself the task of doing ï¿½ï¿½ rather than give it to\r\nthe clone?\r\nFinally, while doing ï¿½ï¿½, does she split her attention, putting\r\nsome attention on how ï¿½ï¿½ is going, so that she can recon\u0002sider, if needed, the efficacy of ï¿½ï¿½ in achieving ï¿½ï¿½? Or is her\r\nattention is wholly on ï¿½ï¿½, with no worry about what sheâ€™s\r\nachieving/maintaining/ensuring?\r\nFigure 2: Verifying CAPs\r\nIâ€™ve found that these conditions almost always coincide. An\r\nAP adopted for diffuse reasons will tend to have someoneâ€™s\r\nwhole attention, and theyâ€™ll be resistant to automating it.\r\nA strong sign itâ€™s a CAP.\r\nValues as CAPs\r\nI want to suggest that an agentâ€™s CAPs deserve to be called\r\ntheir values, in at least two everyday17 senses of the word.\r\nIâ€™ll illustrate this with a personal story:\r\nEarlier in my career, I chose work colleagues for\r\ntheir brilliance, efficacy, and a shared sense of play.\r\nOne day, with one colleague in particular, I recog\u0002nized a different thing we shared: we were excited\r\nto participate in the same long-term trends. This\r\nfelt meaningful.\r\nLater, when choosing teammates, I began to look\r\nfor this alignment first. My teams now seem more\r\nlikely to stick together, and we have new types of\r\nconversations.\r\n17In philosophical writing, values are often considered as evalua\u0002tive criteria or attitudes (Chang, â€œâ€˜All Things Consideredâ€™â€, Velleman,\r\nPractical Reflection). My treatment of them as policies works there\r\ntoo, I think: an evaluative attitude or criterion can be viewed as some\u0002thing a person does when making an evaluation or choice. But thatâ€™s\r\nnot our topic here.\r\n3\nBut even if I knew my teams wouldnâ€™t last longer,\r\nor if those conversations were somehow blocked\r\nfrom happening, Iâ€™d still hire with this in mind.\r\nIn this story, I gained a CAP. Did I gain a value? I think\r\nso. But before going into it, Iâ€™ll bracket off one common\r\nsense of â€œvaluesâ€ which doesnâ€™t fit CAPs. I didnâ€™t gain a\r\nâ€˜social visionâ€™: a vision of whatâ€™s right for everyone, or for\r\na group (e.g., what a family should be like, how a father\r\nshould behave, what a nation should be like). I didnâ€™t gain\r\na political cause (like inclusiveness, freedom, etc), nor a\r\nstandard to which Iâ€™ll push others to conform, or try to\r\nconform myself (like masculine or feminine dress-codes).\r\nCAPs have little to do with this use of the word â€œvalues.â€18\r\nBut, two other uses of the word fit snugly:\r\nFirst, we use values for the things that feel right and mean\u0002ingful when you do themâ€”such as being vulnerable, taking\r\nstage, being creative, etc. My CAP fits, in this sense.\r\nSecond, we use valuesâ€”in expressions like â€œthe values of\r\nscienceâ€ or â€œdemocratic valuesâ€â€”for the individual sources\r\nof meaning that keep institutions working. So, a scientistâ€™s\r\nvalues might include intellectual humility, passionate pur\u0002suit of the truth, etc. Things that are both meaningful for\r\nscientists, and needed to keep the institution of science on\r\nthe rails.\r\nWhy do CAPs highlight the latter, which we might call in\u0002stitutional meanings? For two reasons: First, they are likely\r\nto have been judged constitutive of the practice of science.\r\nMore importantly, they filter out whatâ€™s been adopted in\r\nthat practice for merely instrumental reasonsâ€”whatâ€™s done\r\nmerely to keep your job, to fit in with colleagues, or to\r\nachieve specific goals. So, a scientistâ€™s CAPs will not in\u0002clude what they do merely to get tenure, to amass citations,\r\netc. This means CAPs exclude all the perverse incentives\r\n(often arising from coordination problems19) which clutter\r\nup institutions, but include whatâ€™s considered constitutive.\r\nInstitutional meanings show up clearly.\r\nMeaningful Choice\r\nFinally, I want to suggest that some revealed preferences\r\nare more expressive of our values, and should count more\r\nin welfare calculations, recommender systems, and social\r\nchoice.\r\nConsider the preference of Janet, who takes a job at Soul\u0002crushingjobs.com. She says:\r\nItâ€™s the only way my kids can eat. If I didnâ€™t go\r\nwork there, Iâ€™d feel a crushing guilt. And my kids\r\nmight get taken away. Also, I believe in taking\r\nresponsibility for my family, and this is the way to\r\ndo that.\r\nWe can inquire about her choice of Soulcrushingjobs.com:\r\nwhat APs led Janet towards this option?\r\n18While people often try to be inclusive or feminine based on such\r\na vision, if this is done merely to spread a social vision, to conform\r\nwith one, or otherwise to bring about a social change, itâ€™s an IAP.\r\n19APs may offer an account for the rationality of collective actionâ€”\r\ncompared to the prisonerâ€™s dilemma arguments in Anderson, â€œSym\u0002posium on Amartya Senâ€™s Philosophyâ€ and Sen, â€œBehaviour and the\r\nConcept of Preferenceâ€. In contrast, my approach follows Velleman,\r\nSelf to Self, ch 11, closely.\r\nFigure 3: CAPs from our database, meaning.supplies.\r\n1. Letâ€™s first verify that â€œtaking responsibilityâ€ is a CAP\r\nfor her. We ask her when that AP is important, and\r\nhow she decided itâ€™s important then. If she knows the\r\ncontext well, and adopted via a constitutive judgement,\r\nitâ€™s a CAP.\r\n2. Now, we can list options she considered along the way,\r\nalong with the contexts and APs responsible for sur\u0002facing and narrowing those options.Various candidates\r\nemerge for the main AP: Was she searching for ways\r\nto take responsibility? Or, was she searching for any\r\njob to feed her family. These would look very different:\r\nif her CAP drove her search, she would have found a\r\nwide variety of ways to take responsibility as part of her\r\nsearch. If, on the other hand, she was more focused on\r\nfinding any job thatâ€™d feed her family, we would expect\r\na narrower collection of options.\r\nFigure 4: Verifying MCs\r\nThis makes a huge difference: if Janetâ€™s job search was about\r\ntaking responsibility, she was guided by her values in search\u0002ing, and we can take her choice as an expression of those\r\nvalues. If, however, her search was driven mainly by IAPs,\r\nlike trying desperately to feed her family, her itâ€™s less about\r\nvalues, and more about a tough situation. Sheâ€™s bucking to\r\nexternal pressure, caught in the system, or setting aside her\r\ngoals to conform to a social rule.\r\nShould we consider such a choice to be a revealed prefer\u0002ence? If anything, Janetâ€™s preference is not for soulcrush\u0002ingjobs.com, but for a better situation, where she could\r\nchoose by her values.\r\nMy suggestion is to modify welfare calculations, recom\u0002mender systems, and social choice to count these choices\r\ndifferently: to collect information about the APs driving a\r\nchoice, and whether they are CAPs or IAPs, and to restrict\r\nthe preference relation to only count meaningful choicesâ€”\r\n4\nchoices that are expressive of values.20\r\nThis would avoid rewarding actors who rig the game: who\r\nforce peopleâ€™s hands, tilt the playing field, etc, to drive peo\u0002ple towards their option.\r\nA further step would count incidences of meaningful vs. non\u0002meaningful choice as expressing second-order preferences\r\nabout choice environments. A non-meaningful choice is an\r\nopportunity to ask the agent if they wish their choice situa\u0002tion had been different. This is additional information that\r\ncould be worked into welfare and social choice (although Iâ€™m\r\nnot sure how).\r\nConclusion\r\nThe refined preference relation Iâ€™ve described requires ad\u0002ditional information from the agent. But this information\r\nretains many advantages of revealed preference: itâ€™s uni\u0002versally applicable, high-resolution, and robust in the sense\r\nthat it captures hard-boiled trade-offs.\r\nWhere it most clearly falls short, is with what Iâ€™ve called\r\ndemocratic simplicity. CAP-information requires more in\u0002trospection, more articulacy, and is much harder to verify.\r\nIâ€™m not sure these challenges can be overcome, but there\r\nare promising avenues for research:\r\nâ€¢ Rich, interactive experiences can help people uncover\r\ntheir CAPs,\r\nâ€¢ CAPs can be inferred from other data,\r\nâ€¢ Visualizations and cryptography can make CAP data\r\nunderstandable and auditable, so decisions based on\r\nmeaningful choice can be legitimated.\r\nIn many areas, a misalignment between preferences and val\u0002ues plagues our society: clickbait, internet addiction, pop\u0002ulist politics, obesity, and various forms of institutional rot.\r\nIf CAP-based approaches to welfare, recommender systems,\r\nand social choice can help, it seems worth the research ef\u0002fort.\r\nAppendix A: Interview Transcript\r\nBelow is a conversation in which someoneâ€™s values were ar\u0002ticulated, and a choice of theirs evaluated as to whether\r\nthose values were expressed.\r\nIn future work, I hope that hundreds of such conversations\r\nwill teach us how to build automated, interactive â€œassisted\r\nintrospectionâ€ systems to replace these manual interviews,\r\nand that data from tens of thousands of automated inter\u0002actions will, in turn, lead to techniques for inferring values\r\nfrom other, readily-available data.\r\nâ€” to be done â€”\r\nReferences\r\nAnderson, Elizabeth. â€œSymposium on Amartya Senâ€™s Phi\u0002losophy: 2 Unstrapping the Straitjacket of â€˜Preferenceâ€™:\r\nA Comment on Amartya Senâ€™s Contributions to Phi\u0002losophy and Economics.â€ Economics and Philosophy\r\n20Maintaining the normal structure of welfare economics, see Bern\u0002heim and Rangel, â€œBeyond Revealed Preference.â€\r\n17, no. 1 (2001): 21â€“38. https://doi.org/10.1017/\r\nS0266267101000128.\r\nâ€”â€”â€”. Value in Ethics and Economics. Harvard University\r\nPress, 1993.\r\nArrow, Kenneth J. Social Choice and Individual Values.\r\nYale University Press, 2012. http://www.jstor.org/\r\nstable/j.ctt1nqb90.\r\nBernheim, B. Douglas, and Antonio Rangel. â€œBeyond Re\u0002vealed Preference: Choice-Theoretic Foundations for Be\u0002havioral Welfare Economicsâˆ—.â€ Quarterly Journal of\r\nEconomics 124, no. 1 (February 2009): 51â€“104. https:\r\n//doi.org/10.1162/qjec.2009.124.1.51.\r\nBoyd, Richard. â€œHow to Be a Moral Realist.â€ In Essays\r\non Moral Realism, edited by G. Sayre-McCord, 181â€“228.\r\nCornell University Press, 1988.\r\nBratman, Michael. Intention, Plans, and Practical Reason.\r\nCambridge, MA: Harvard University Press, 1987.\r\nChang, Ruth. â€œâ€˜All Things Consideredâ€™.â€ Philosophical Per\u0002spectives 18, no. 1 (2004): 1â€“22. https://doi.org/10.\r\n1111/j.1520-8583.2004.00018.x.\r\nâ€”â€”â€”. Making Comparisons Count. Routledge, 2001.\r\nGibson, J. J. The Senses Considered as Perceptual Systems.\r\nBoston: Houghton Mifflin, 1966.\r\nLevi, Isaac. Hard Choices: Decision Making Under Unre\u0002solved Conflict. Cambridge University Press, 1990.\r\nRawls, John. A Theory of Justice. 1st ed. Cambridge, Mas\u0002sachussets: Belknap Press of Harvard University Press,\r\n1971.\r\nSamuelson, P. A. â€œA Note on the Pure Theory of Con\u0002sumer's Behaviour.â€ Economica 5, no. 17 (February\r\n1938): 61. https://doi.org/10.2307/2548836.\r\nSen, Amartya. â€œBehaviour and the Concept of Preference.â€\r\nEconomica 40, no. 159 (August 1973): 241. https://doi.\r\norg/10.2307/2552796.\r\nSen, Amartya K. â€œRational Fools: A Critique of the Behav\u0002ioral Foundations of Economic Theory.â€ Philosophy and\r\nPublic Affairs 6, no. 4 (1977): 317â€“44.\r\nSimon, Herbert A. â€œA Behavioral Model of Rational Choice.â€\r\nThe Quarterly Journal of Economics 69, no. 1 (February\r\n1955): 99. https://doi.org/10.2307/1884852.\r\nSmaldino, Paul E., and Peter J. Richerson. â€œThe Origins\r\nof Options.â€ Frontiers in Neuroscience 6 (2012). https:\r\n//doi.org/10.3389/fnins.2012.00050.\r\nTaylor, Charles. â€œ4 What Is Human Agency?â€ In The\r\nSelf: Psychological and Philosophical Issues, edited by\r\nTheodore Mischel, 103. Rowman & Littlefield, 1977.\r\nâ€”â€”â€”. The Language Animal: The Full Shape of the\r\nHuman Linguistic Capacity. Harvard University Press,\r\n2016.\r\nVelleman, David. Practical Reflection. Princeton University\r\nPress, 1989.\r\nVelleman, J. David. Self to Self: Selected Essays. Cam\u0002bridge University Press, 2005.\r\n5ğŸ–¤\nW4: Emotions to Values Practice\nPart 1: The Story\nStep back into your memory. Remember a situation in which you had an\nuncomfortable emotion (sadness, anger, hopelessness, etc.). It doesn't have to be\na \"big\" emotion. Great values can be harvested from even small annoyances. For\nyour first time, avoid emotions related to more general life problems or previous\ntrauma. Try a situation that brought up challenging emotions that are linked to the\nsituation itself.\nAsk yourself: What was it like for you to actually be in the situation? Where were\nyou? Who were you with? What happened? What emotions came up? How did\nthey manifest in your body? What did you see, touch, taste, smell? Picture being\nthere as vividly as you can. Sketch the outline of the story here:\nâ† Click here for an example of an \"emotion story\"\nI was with my son at the grocery store. The lady in front of us was paying a\nrather large grocery bill with coins. It was taking forever. I felt frustrated. There\nwas a tightness in my chest, and I was clenching my teeth. I just wanted to get\nout of there and get on with my day.\nNow simplify your story for this exercise\nYou've identified different emotions that came up. Decide on the one that was\nmost clear and present. Think about what you were doing that caused that\nemotion.\nWrite down your emotion and what happened.\nExample: I felt frustrated because I was stuck in line.\nW4: Emotions to Values Practice 1\nPart 2: The Value\nYour negative emotion pointing to the fact that this scenario is not part of your\ngood life. Now itâ€™s time to meditate on what meaningful way of living you were not\nactualizing.\nStart by reflecting on what way of being the emotion indicates\nâ† Click here to explore the personal value behind your emotion\nFind the emotion that came up on the list below and click on the toggle to find\nsome questions. Challenging emotions point us towards values that were\nblocked, that we could not attend to, that were suppressed and so on.\nYou'll find several questionsâ€”pick the one that seems most helpful, and take\nnotes.\nHint: If the emotion you wrote down does not match the emotions listed\nbeneath, go for the one that is closest.\nAnger\nWhat's worth protecting?\nWhat way of living is blocked?\nWhat way of being has no space here?\nTake Notes:\nFrustration\nWhat's worth protecting?\nWhat way of living is blocked?\nWhat way of being is not safe to emerge?\nWhat does this distract you from?\nW4: Emotions to Values Practice 2\nTake Notes:\nRage\nWhat's worth protecting?\nWhat way of living is blocked?\nWhat way of being is not safe to emerge?\nTake Notes:\nFear\nWhat's worth protecting?\nWhat way of living is threatened?\nWhat way of being can you not bear to give up on?\nTake Notes:\nAnxiety\nWhat's worth protecting?\nWhat way of living is threatened?\nW4: Emotions to Values Practice 3\nWhat way of being can you not bear to give up on?\nTake Notes:\nNervousness\nWhat's worth protecting?\nWhat way of living is threatened?\nTake Notes:\nShame\nWhat's worth recovering?\nWhat way of living is overridden?\nWhat value did you fail to live by?\nTake Notes:\nEmbarrassment\nWhat's worth recovering?\nW4: Emotions to Values Practice 4\nWhat way of living did you neglect?\nTake Notes:\nGuilt\nWhat's worth recovering?\nWhat way of living did you neglect?\nWhat did you fail to prioritise?\nTake Notes:\nSelf-disgust\nWhat's worth recovering?\nWhat way of living did you give up on?\nWhat do you no longer trust yourself with?\nTake Notes:\nRegret\nW4: Emotions to Values Practice 5\nWhat's worth honoring?\nWhat way of living is lost for now?\nWhat way of living has become impossible?\nTake Notes:\nHate\nWhat is worth protecting?\nWhat way of living is violated?\nTake Notes:\nDistaste\nWhat's worth recovering?\nWhat way of living is estranged?\nTake Notes:\nDisgust\nW4: Emotions to Values Practice 6\nWhat is worth protecting?\nWhat way of living is violated?\nTake Notes:\nRevulsion\nWhat is worth protecting?\nWhat way of living is violated?\nTake Notes:\nSadness\nWhat's worth honoring?\nWhat way of living is lost for now?\nWhat way of living has become impossible?\nTake Notes:\nGrief\nW4: Emotions to Values Practice 7\nWhat's worth honoring?\nWhat way of living is lost for now?\nWhat way of living has become impossible?\nTake Notes:\nRemorse\nWhat's worth honoring?\nWhat way of living is lost for now?\nWhat way of living has become impossible?\nTake Notes:\nConfusion\nWhat's worth recentering around?\nWhat way of living is out of focus?\nWhat way of living do you not know how to bring to this?\nWhat ways of living seem to be in conflict with each other?\nTake Notes:\nW4: Emotions to Values Practice 8\nBeing Lost\nWhat's worth recentering around?\nWhat way of living is out of focus?\nWhat way of living do you not know how to bring to this?\nWhat ways of living seem to be in conflict with each other?\nTake Notes:\nDesperation\nWhat's worth nurturing?\nWhat way of living is unsafe to emerge?\nWhat's worth identifying?\nWhat way of living would allow the above emerge?\nTake Notes:\nHelplessness\nWhat's worth nurturing?\nWhat way of living is unsafe to emerge?\nWhat's worth identifying?\nW4: Emotions to Values Practice 9\nWhat way of living would allow the above emerge?\nTake Notes:\nHurt\nWhat's worth nurturing?\nWhat way of living is unsafe to emerge?\nWhat's worth identifying?\nWhat way of living would allow the above emerge?\nTake Notes:\nYearning\nWhat's worth nurturing?\nWhat way of living is unsafe to emerge?\nWhat's worth identifying?\nWhat way of living would allow the above emerge?\nTake Notes:\nW4: Emotions to Values Practice 10\nHeartbreak\nWhat's worth demoting?\nWhat way of living doesn't make sense anymore?\nWhat's worth embracing?\nWhat way of living will save the above?\nTake Notes:\nAlienation\nWhat's worth nurturing?\nWhat way of living seems impossible?\nTake Notes:\nBitterness\nWhat's worth nurturing?\nWhat way of living seems impossible?\nTake Notes:\nW4: Emotions to Values Practice 11\nDepression\nWhat's worth nurturing?\nWhat way of living seems impossible?\nTake Notes:\nHopelessness\nWhat's worth nurturing?\nWhat way of living seems impossible?\nTake Notes:\nLoneliness\nTake Notes:\nNumbness\nW4: Emotions to Values Practice 12\nWhat's worth nurturing?\nWhat way of living seems impossible?\nTake Notes:\nAnd if you are up for it, ask can yourself even more questions\nCan you think of someone who would have handled this situation well? How\nwould they have approached it? (admiration)\nWhen you think back, what advice would you give yourself in that situation?\n(reflection)\nIf you would ignore outcomes and expectations above, what way of\napproaching things or treating people would have seemed worthwhile\nanyway? (appreciation)\nThen, using all of the information above, capture your wisdom by\nformulating an awareness-guiding \"personal value phrase\":\nW4: Emotions to Values Practice 13\nIf you feel ready for it- make a values card at meaning.suppplies. Otherwise, use\nthe information youâ€™ve already gathered to try to articulate the good life that this\nemotion is informing you of. What are the attentional policies that constitute your\nmost meaningful life?\nHint: search for the \"heart-opening, deeply relaxing yes\" in your body when you\nformulate it.\nâ† Click here for help formulating an awareness-guiding personal value phrase\nâ† Click here for some examples of awareness personal values phrases\nFrom the example emotion story above (at the grocery store):\nğŸŒ³ look for opportunities to connect with the people you happen to be\naround right now\nOther example value phrases:\nğŸŒ³ explore the complexity (and often absurdity) of how you and others\nview the situation, and how your desires and feelings interact\nğŸŒ³ when receiving critical feedback, search for what the person is\ntelling you is important to them, and honor that as sacred\nğŸŒ³ notice the emotional state of the person in front of you, and how\nyou feel about the relationship; use that information to respond to the\nmoment\nğŸŒ³ speak about the physical sensations in your body as well as the\nneeds, fears, and desires that they seem to indicate\nW4: Emotions to Values Practice 14Here's a way the AI can work to show you the kind of responses so we can reverse engineer what we think the system prompt is:\n\nThe conversation is here:\"\"\"Help me clarify a value I have.\n\nChatGPT said:\n\nSure! Whatâ€™s on your mind? Is it a meaningful story, a strong emotion, a tough choice, or maybe someone you admire? Letâ€™s start there, and weâ€™ll uncover the value together.\n\n12:39\n\n\n\n\nYou said:\na meaningful story\n12:39\nChatGPT said:\n\nGreat! Tell me about the storyâ€”what happened, who was involved, and why it stands out to you as meaningful. Take your time; weâ€™ll figure out what matters to you in it.\n\n12:39\n\n\n\n\nYou said:\nbeing with the baby and my wife and reading a book at night. the baby turns away from breastfeeding to look directly at me and touches his forhead to mine, he touches my face and coos. it's warm, dark, intimate, super beautiful it fills my heart with wramth\n12:39\nChatGPT said:\n\nThatâ€™s such a tender and beautiful moment. The way you describe itâ€”so warm and connectedâ€”makes it clear this isnâ€™t just about the action, but something deeper you treasure. Letâ€™s zoom in on whatâ€™s meaningful here.\n\nWhat stands out most to you in that moment? Is it the babyâ€™s connection with you? The shared stillness and intimacy with your wife? The feeling of being fully present together? Or maybe something else?\n\n12:40\n\n\n\n\nYou said:\nnot sure... yeah i like the idea that the baby seems to like me although he then starts putting his hand in my mouth and playing and slapping my face so maybe he's just playing and i think maybe something about looking into his eyes when he's still as he so rarely is, even if it only lasted a second, not sure\n12:40\nChatGPT said:\n\nI get itâ€”thereâ€™s so much happening in moments like that, itâ€™s hard to pin down exactly why they feel so powerful. Letâ€™s explore a bit.\n\nIt sounds like part of the meaning comes from being truly seen by your baby, even if just for a fleeting second. Thereâ€™s a quiet intensity in that shared gaze, and maybe it feels like a rare connection in his otherwise playful, active little world. Does that resonate? Or is it more about the way he shows his curiosity and trust in you, even when heâ€™s being silly?\n\nOr maybe itâ€™s the whole mixâ€”moments of stillness, playfulness, and connection happening all at once?\n\n12:41\n\n\n\n\nYou said:\ni think maybe it's that moment that connects me with other moments of silence and stillness with the baby where i felt him in my arms, warm, soft, gentle, i felt... i don't know not that i was a good care taker, yte there's something special about a baby asleep in your arms looking cute and happy i can't explain it, it seems just like other moments when I'm with my sheikh and my heart opens up and feels warm and full of love i just want to be in that presence there's not really a lot to do and to soak it into your bones\n12:41\nChatGPT said:\n\nWow, thatâ€™s richâ€”thank you for sharing that. It sounds like the stillness and presence in these moments arenâ€™t about doing or achieving anything, but about being deeply connected and open to love. Holding your baby, feeling his warmth and trust, reminds you of those moments with your sheikh where love feels abundant and flows through you effortlessly. Itâ€™s like youâ€™re soaking in a kind of sacred presence, just by being there.\n\nIs it something about attending to being rather than doing in those moments? Feeling the fullness of life and love without needing to add or change anything? Or is it more about a sense of intimacy, trust, or connection that emerges in those quiet spaces? Or all of that together?\n\n12:43\n\n\n\n\nYou said:\nyeah, being, intimacy, love overflowing from an unknown deep pool where it is always there in infinite amounts \n12:43\nChatGPT said:\n\nThatâ€™s beautifulâ€”itâ€™s like those moments tap into a limitless well of love and presence thatâ€™s always there, even if it feels hidden in the busyness of life. Holding your baby, being with your wife, or sitting with your sheikh lets you drop into that pool and simply be, with love flowing effortlessly.\n\nLetâ€™s try to clarify what you pay attention to in these moments. Is it:\n\nThe stillness and warmth that fills the space around you and the other person?\nThe intimacy and trust that feels almost sacred, like nothing else needs to exist in that moment?\nThe sense of connection to something larger, like this overflowing pool of love, timeless and infinite?\nWhat else feels like part of the experience? Or does one of these stand out most?\n\n12:43\n\n\n\n\nYou said:\nnoticing the love that's there in connection without expectation and stillness and opening of hearts somehow in there there's a moment of intimacy of togetherness basking in the sun of love \n12:43\nChatGPT said:\n\nAh, thatâ€™s beautifully saidâ€”basking in the sun of love. So itâ€™s about noticing the love that emerges naturally in connection, when thereâ€™s no expectation or pressure. Thereâ€™s stillness, an opening of hearts, and a profound intimacy in that shared presence. Itâ€™s less about action and more about soaking in that moment of togetherness, feeling the warmth and light of love radiate between you.\n\nLet me try to sketch out what you pay attention to in these moments:\n\nLOVE that arises naturally in moments of connection, without needing effort or expectation.\nSTILLNESS in the presence of another, allowing for hearts to open.\nINTIMACY that feels timeless, like youâ€™re both sharing something deeply sacred.\nTOGETHERNESS thatâ€™s simple and profound, like basking in the same light of love.\nHow does that feel to you? Am I capturing it, or is there a tweak youâ€™d make?\n\n12:44\n\n\n\n\nYou said:\nyeah sounds good\n12:44\nChatGPT said:\n\n\nTalked to try.meaningalignment.org\nHereâ€™s the values card we created together, based on your story and whatâ€™s meaningful to you:\n\n\n\nTitle: Love in Still Connection\nView and share your card here.\n\nDoes this feel right to you? Anything youâ€™d like to adjust?\"\"\"\nANd the resulting value \n\nThis is the valueÂ youÂ articulated\n\nLove in Still Connection\n\nNotice love and connection in still moments.\n\nWHERE MY ATTENTION GOES\n\nLOVEÂ thatÂ arisesÂ naturallyÂ inÂ momentsÂ ofconnection,Â withoutÂ needingÂ effortÂ orexpectation\n\nSTILLNESSÂ inÂ theÂ presenceÂ ofÂ another,Â allowingforÂ heartsÂ toÂ open\n\nINTIMACYÂ thatÂ feelsÂ timeless,Â likeÂ sharingsomethingÂ deeplyÂ sacred\n\nTOGETHERNESSÂ thatâ€™sÂ simpleÂ andÂ profound,likeÂ baskingÂ inÂ theÂ sameÂ lightÂ ofÂ love\n\nWe also call this a \"source of meaning\", as it points towards a way of life you find meaningful.\nAnd that was the output from the GPT what do you think the system prompt could possibly be? Abstract\nMany fields (social choice, welfare economics, recommender\nsystems) assume people express what benefits them via\ntheir â€˜revealed preferences.â€™ Revealed preferences have well-\ndocumented problems when used this way, but are hard to\ndisplace in these fields because, as an information source,\nthey are simple, universally applicable, robust, and high-\nresolution. In order to compete, other information sources\n(about participantsâ€™ values, capabilities and functionings,\netc) would need to match this. I present a conception of\nvalues as attention policies resulting from constitutive judge-\nments, and use it to build an alternative preference relation,\nMeaningful Choice, which retains many desirable features\nof revealed preference.\nValues, Preferences, Meaningful Choiceâˆ—\nJoe Edelmanâ€ \n6 August 2021 â€” draft\nAs I go, Iâ€™ll build up a schematic representation of an in-\nterview structure with which an agentâ€™s values could be col-\nlected, and a choice assessed to see if it was expressive of\nthose values.\nMy goal here is to make it clear what Iâ€™m suggesting: what\nquestions could be asked, and what answers might suï¬€ice to\nshow that an agent has this value or that, or that a revealed\npreference was a meaningful choice.\nI am not suggesting that these interviews can be used to col-\nlect data at the scale needed for welfare calculations, social\nchoice mechanisms, or recommender systems. That practi-\ncal question is beyond my scope here.\nIntroduction\nIn his 1938 paper introducing revealed preference, Samuel-\nson1 warned:\nI should like to state my personal opinion that\nnothing said hereâ€¦ touches upon at any point the\nproblem of welfare economics, except in the sense\nof revealing the confusion in the traditional theory\nof these distinct subjects.\nSimilar sentiments followed, in Arrow,2 Sen,3 Anderson,4\netc. As an information basis for welfare, optimality, social\nchoice, etc, revealed preference has been much critiqued.\nAll the same, itâ€™s hard to see how to move away from it, with-\nout paternalism: if people know best for their own livesâ€”\nshouldnâ€™t we trust their choices as the final proof of their\nintentions? How else can we avoid imposing â€œbetter valuesâ€\nfrom above?\nOne way to respond is to say that people do know best, and\nhave wise values, but that their revealed preferences arenâ€™t\nthe last word on those values. But to establish this, youâ€™d\nwant another source of information on peopleâ€™s valuesâ€”one\nthat retains much of the resolution, robustness, universal\napplicability, and democratic simplicity of preference.\nIâ€™ll develop such an information source here, by taking two\nâ€˜steps backâ€™ from revealed preference, widening the infor-\nmation collected. Then Iâ€™ll argue that my new source cor-\nresponds with one definition of â€œvaluesâ€ used in common\nspeech. Finally, Iâ€™ll show how this new source can be used\nin social choice and welfare calculations.\nPreferences\nA rich literature covers how revealed preferencesâ€”which,\nwhen summed up, are called engagement metricsâ€”lead us\nastray. You can often get people to choose something with-\nout serving their real interests: you can misinform them, or\nleverage their misplaced hopes.\nOr, you can make it so people need your thing for what\nonce was possible without it: they need your car to get to\nwork, your social media account to find a job, your dress to\nsocialize with their friends, etc.\nMore broadly, you can manufacture social circumstances\nwhere people choose your thing to â€œkeep up with the Jone-\nses,â€ to signal allegiance with their tribe, or because theyâ€™ve\nlost the ability to coordinate a real solution.5\nIn some cases, the person will know their choice doesnâ€™t ex-\npress their true interestsâ€”that they are bucking to external\npressure, caught in the system, or setting aside their goals\nto conform to a social rule.6 In other cases, options have\nbeen limited or biased behind our backs.\nGiven these problems, why does revealed preference still\nplay a role in notions of benefit and measures of optimality?\nIt has huge advantages as an information source: in the\nresolution and robustness of the data provided, its near-\nuniversal applicability, and the democratic simplicity of its\ncollection:\nâ€¢ Preferences say fine-grained things how about each of\nus wants to live, informed by our local situations and\npriorities.\nâ€¢ Preferences get at what we choose in the final analysis,\nour real, battle-tested priorities, not just what we say\nwe want.\nâˆ—Thanks to B. Gabbai, A. Morris, A. Ovadya, J. Stray, & F. Noriega\nâ€ School for Social Design joe.edelman@gmail.com\n1â€œA Note on the Pure Theory of Consumer's Behaviour.â€\n2Social Choice and Individual Values.\n3â€œRational Fools.â€\n4â€œSymposium on Amartya Senâ€™s Philosophy.â€\n5See discussions of the prisonerâ€™s dilemma in e.g., Sen, â€œBehaviour\nand the Concept of Preferenceâ€; Anderson, â€œSymposium on Amartya\nSenâ€™s Philosophy.â€\n6See Sen, â€œRational Foolsâ€; Anderson, Value in Ethics and Eco-\nnomics on â€˜commitment.â€™\n1\nâ€¢ Preferences work in many domains of life, and represent\nall-things-considered judgements, combining morality,\nprudence, whims, etc.\nâ€¢ Finally, those who arenâ€™t introspective or eloquent can\nstill express a preference. And preferences leave simple-\nto-understand, verifiable trails: purchase histories, vot-\ning records, etc.\nAttentional Policies\nIâ€™ll try to address the problems with preference, holding on\nto these benefits. To do so, I want to take two â€˜steps back.â€™\nThe first step is to widen the conception of choice to include\noption set formation.\nA revealed preference takes the options as given. As other\nauthors have pointed out,7 this doesnâ€™t fully reflect how\nchoice works.\nConsider my choice, with colleagues, to say something witty.\nAt ğ‘¡1, I choose to say that in particular (over staying silent,\nor saying my second-place quip). But, at some earlier point\nğ‘¡0, I decided to invest my attention in finding witty things\nto say. Iâ€™ve tried witty phrases on in my mind, and tested\nsituations for witty reframings.\nAt ğ‘¡0, I made a much bigger choiceâ€“I adopted8 an atten-\ntional policy of â€˜looking for witty quips and reframes.â€™ This\nis how I assembled the option set for my choice at ğ‘¡1.\nI think this earlier choice, at ğ‘¡0, reveals more about me than\nthe latter one. Indeed, these attentional policies (APs)\nmight account for much of whatâ€™s called my â€œpersonalityâ€:\nWhen making friends, am I cautious or bold? When consid-\nering a purchase, is my focus on price, quality, or durabil-\nity? When speaking, do I try to be witty, precise, or down\nto earth? These may not just be â€œcharacter traitsâ€ I was\nborn with, but policies I adopted, which work together for\nmy way of life.9\nCollecting an agentâ€™s APs is part of my proposal to im-\nprove the information basis of welfare, social choice, and\nrecommenders. How could that work? You could use an\ninteractive survey to collect the agentâ€™s common contexts\n(socializing, at work, in diï¬€icult conversations), and have\nthem pick what they attend to most in each.10\nIf they claim to have AP ğ‘ in context ğ‘, you could verify that\nwith followup questions: First, you could ask them about\nrecent moments in ğ‘, and what options theyâ€™ve found using\nğ‘. Someone with a policy of vulnerability should be able\nto cough up various opportunities for vulnerability, recently\nconsidered; some taken, some not.\n7Smaldino and Richerson, â€œThe Origins of Options.â€\n8I adopt APs (just like I make plans, adopt heuristics, comply\nwith norms, follow through with intentions) because my bounded ra-\ntionality and social needs make it expedient. (Bratman, Intention,\nPlans, and Practical Reason; Simon, â€œA Behavioral Model of Ratio-\nnal Choiceâ€; Taylor, The Language Animal)\n9This characterization of an identity as composed of APs shares\nsomething with Anderson, â€œSymposium on Amartya Senâ€™s Philoso-\nphyâ€; and Velleman, Practical Reflection.\n10Social attestations, also, could be a source of AP information:\nsomeone who is widely admired for their wit likely has an AP about\nit. Behavioral data may also work: if someone shops by price or by\nquality, that may show up in purchase or click data. (Using, perhaps,\napproaches like Chang, Making Comparisons Count; or Levi, Hard\nChoices)\nAdditionally, that person will also know when exactly vul-\nnerability is called for, and when something else takes the\nfore. I donâ€™t try to be vulnerable when giving directions to\na stranger, or with my dad. Generally, someone with ğ‘ in\nğ‘ will have detailed information about the shape of ğ‘.\nFigure 1: Verifying APs\nSupposing we collected an agentâ€™s APs. How would that\ncompare with revealed preference data, in terms of resolu-\ntion, robustness, universality, and democratic simplicity?\nLike preferences, APs are informed by our local situations\nand priorities, and say fine-grained things about how we\nwant to live. Indeed, they have a higher resolution than\nIâ€™ve let on so far: an AP about â€˜honestyâ€™ or â€˜vulnerabilityâ€™\nis always short for a more specific policy, like â€œattend to\nwhat I feel about each thing we discuss, and let my feeling\nshow,â€ or â€œattend to false impressions a listener may get\nfrom my statements, and head them off with a disclaimer.â€\nThat is: having honesty as a policy requires a substantive\ninterpretation of honesty, and these differ from person to\nperson.\nAPs are battle-tested. I wish I could craft my words to\nsimultaneously be kind, honest, tactful, humble, and inspir-\ning. I wish I could, at the same moment, be precise in\nmy speech, aware how each word lands, aware of my own\nfeelings; transparent, calm, and centered; passionate; physi-\ncally graceful, like a dancer. But APs compete for my atten-\ntion. So, my choice to look for witty things to say crowds\nout many other options.\nLike preferences, APs are applicable across all domains in\nlifeâ€”the moral, the self-interested, etc, and apply to many\nkinds of contexts.\nBut, they score worse on democratic simplicity than re-\nvealed preference. And, for now, itâ€™s unclear how an agentâ€™s\nAPs would provide a substitute for revealed preference. Iâ€™ll\nget to that.\nConstitutive Judgement\nMy second â€˜step backâ€™ considers the kind of judgement an\nagent makes, when they decide to adopt an AP. Look at\nthese three cases:\n(1) I watch the road while driving. I donâ€™t want to crash.\n(2) Iâ€™m careful with my speech at work. My boss fires\nanyone who speaks imprecisely.\n2\n(3) Iâ€™ve decided to be more honest with friends. I recently\nopened up to a friend about a struggle Iâ€™m having.\nSince then, the relationship feels more intimate, and\nstronger; itâ€™s easier for me to think about what to say;\nmy friend is unexpectedly helpful.\nIn all three examples, the agent notes some benefits of adopt-\ning an AP ğ‘ in a context ğ‘â€”benefits like staying alive, keep-\ning my job, and having helpful friends. And in each case,\nthis leads to a decision to adopt it.\nBut, in the first two examples, those benefits come together\nto make the case that doing ğ‘ in ğ‘ is functional, necessary,\nprudent, or expedient. The judgement is based on expected\ncosts and benefits, much as a rational choice theorist would\nhope. Iâ€™ll call this an instrumental judgement.\nIn (3), the benefits become supporting evidence for a much\nlarger claim: that honesty with friends is constitutive of\ngood relationships11, or at least naturally goes together with\nthem. The benefits suggest that honesty is going to be part\nof any relationship I want to build, or is part of the person I\nwant to beâ€”not just that itâ€™s functional in achieving certain\nends. Itâ€™s a part-whole claim, or a claim to synergy, not a\nclaim about cause and effect.\nIâ€™ll call this a constitutive judgement. To make one, we\nask ourselves questions like: are Xs impossible without Ys?\nAre Xs without Ys in some way lesser? Are there kinds of\nwholeness that emerge only when an X has a Y? Note: the\nevidence that goes into these questions looks a lot like costs\nand benefits (additional kinds of wholeness, etc)! So the\nraw materials for this judgement are similar, but instead of\nbuilding an expected value, we build a more general conclu-\nsion.\nWhy do we make constitutive judgements, instead of doing\na cost-benefit analysis for everything? Other authors have\nmarked out a role for constitutive judgements in moral rea-\nsoning,12 higher-order desires,13 self-understanding,14 and\nidentifying natural kinds,15 but thereâ€™s also a case to be\nmade from bounded rationality:16 constitutive judgements\nare cognitively eï¬€icient. Instead of asking each time we have\nan X, whether we should also have an Y, we ask up-front if\nXs and Ys belong together. If we think so, we put Ys in our\nXs without considering each case.\nAnyways, both kinds of judgement can have errors, and\nare subject to reconsideration. Instrumental judgements\nare more delicate, because they depend on a more-or-less\ncomplete accounting of costs, benefits, and eï¬€icacy. For\nconstitutive judgements, you donâ€™t need such a complete\naccounting: if the benefits youâ€™ve already noticed line up\nnicely, you can take them as evidence of a synergy or whole-\nness. Youâ€™ll reconsider your judgement only if you start to\nthink you got the overall pattern wrong.\nThis gives us a way to tell APs resulting from a constitu-\ntive judgements (CAPs) from those adopted instrumentally\n(IAPs). Say an agent uses AP ğ‘ in context ğ‘.\n11Good relationships for this agent, at least.\n12Rawls, A Theory of Justice.\n13Taylor, â€œ4 What Is Human Agency?â€\n14Velleman, Practical Reflection.\n15Boyd, â€œHow to Be a Moral Realist.â€\n16Following Simon, â€œA Behavioral Model of Rational Choiceâ€; Gib-\nson, The Senses Considered as Perceptual Systems; and Bratman,\nIntention, Plans, and Practical Reason.\nFirst, you can ask her: why is ğ‘ wise in ğ‘? Does she gives a\nsmall number of reasons, pointing towards one state of the\nworld, ğ‘ , that sheâ€™s working to achieve, maintain, or ensure\n(such as â€˜staying alive,â€™ or â€˜keeping her jobâ€™)? Or does she\ngives diffuse, unconnected reasons, and sense sheâ€™s only just\nbegun discovering the benefits of ğ‘ in ğ‘?\nSecond, do the given reasons seem to capture her full ra-\ntionale? If so, itâ€™s likely an IAP. To test this, ask her how\nsheâ€™d feel if they were taken out of consideration (e.g., if\nshe discovered sheâ€™d crash anyways, that her speech had no\nbearing on whether sheâ€™d be fired, or that, without opening\nup to friends, she could magically get the same benefits):\nWould she select a different AP for ğ‘ and consider herself\nbetter off, free to focus on something else? Similarly, would\nshe consider it a net gain if she could clone herself, and\nhave the clone handle doing ğ‘ in ğ‘? Or otherwise automate\nthe achievement of ğ‘ ? Orâ€”on the other handâ€”would she\nreserve for herself the task of doing ğ‘ rather than give it to\nthe clone?\nFinally, while doing ğ‘, does she split her attention, putting\nsome attention on how ğ‘  is going, so that she can recon-\nsider, if needed, the eï¬€icacy of ğ‘ in achieving ğ‘ ? Or is her\nattention is wholly on ğ‘, with no worry about what sheâ€™s\nachieving/maintaining/ensuring?\nFigure 2: Verifying CAPs\nIâ€™ve found that these conditions almost always coincide. An\nAP adopted for diffuse reasons will tend to have someoneâ€™s\nwhole attention, and theyâ€™ll be resistant to automating it.\nA strong sign itâ€™s a CAP.\nValues as CAPs\nI want to suggest that an agentâ€™s CAPs deserve to be called\ntheir values, in at least two everyday17 senses of the word.\nIâ€™ll illustrate this with a personal story:\nEarlier in my career, I chose work colleagues for\ntheir brilliance, eï¬€icacy, and a shared sense of play.\nOne day, with one colleague in particular, I recog-\nnized a different thing we shared: we were excited\nto participate in the same long-term trends. This\nfelt meaningful.\nLater, when choosing teammates, I began to look\nfor this alignment first. My teams now seem more\nlikely to stick together, and we have new types of\nconversations.\n17In philosophical writing, values are often considered as evalua-\ntive criteria or attitudes (Chang, â€œâ€˜All Things Consideredâ€™â€, Velleman,\nPractical Reflection). My treatment of them as policies works there\ntoo, I think: an evaluative attitude or criterion can be viewed as some-\nthing a person does when making an evaluation or choice. But thatâ€™s\nnot our topic here.\n3\nBut even if I knew my teams wouldnâ€™t last longer,\nor if those conversations were somehow blocked\nfrom happening, Iâ€™d still hire with this in mind.\nIn this story, I gained a CAP. Did I gain a value? I think\nso. But before going into it, Iâ€™ll bracket off one common\nsense of â€œvaluesâ€ which doesnâ€™t fit CAPs. I didnâ€™t gain a\nâ€˜social visionâ€™: a vision of whatâ€™s right for everyone, or for\na group (e.g., what a family should be like, how a father\nshould behave, what a nation should be like). I didnâ€™t gain\na political cause (like inclusiveness, freedom, etc), nor a\nstandard to which Iâ€™ll push others to conform, or try to\nconform myself (like masculine or feminine dress-codes).\nCAPs have little to do with this use of the word â€œvalues.â€18\nBut, two other uses of the word fit snugly:\nFirst, we use values for the things that feel right and mean-\ningful when you do themâ€”such as being vulnerable, taking\nstage, being creative, etc. My CAP fits, in this sense.\nSecond, we use valuesâ€”in expressions like â€œthe values of\nscienceâ€ or â€œdemocratic valuesâ€â€”for the individual sources\nof meaning that keep institutions working. So, a scientistâ€™s\nvalues might include intellectual humility, passionate pur-\nsuit of the truth, etc. Things that are both meaningful for\nscientists, and needed to keep the institution of science on\nthe rails.\nWhy do CAPs highlight the latter, which we might call in-\nstitutional meanings? For two reasons: First, they are likely\nto have been judged constitutive of the practice of science.\nMore importantly, they filter out whatâ€™s been adopted in\nthat practice for merely instrumental reasonsâ€”whatâ€™s done\nmerely to keep your job, to fit in with colleagues, or to\nachieve specific goals. So, a scientistâ€™s CAPs will not in-\nclude what they do merely to get tenure, to amass citations,\netc. This means CAPs exclude all the perverse incentives\n(often arising from coordination problems19) which clutter\nup institutions, but include whatâ€™s considered constitutive.\nInstitutional meanings show up clearly.\nFigure 3: CAPs from our database, meaning.supplies.\n1. Letâ€™s first verify that â€œtaking responsibilityâ€ is a CAP\nfor her. We ask her when that AP is important, and\nhow she decided itâ€™s important then. If she knows the\ncontext well, and adopted via a constitutive judgement,\nitâ€™s a CAP.\n2. Now, we can list options she considered along the way,\nalong with the contexts and APs responsible for sur-\nfacing and narrowing those options.Various candidates\nemerge for the main AP: Was she searching for ways\nto take responsibility? Or, was she searching for any\njob to feed her family. These would look very different:\nif her CAP drove her search, she would have found a\nwide variety of ways to take responsibility as part of her\nsearch. If, on the other hand, she was more focused on\nfinding any job thatâ€™d feed her family, we would expect\na narrower collection of options.\nMeaningful Choice\nFinally, I want to suggest that some revealed preferences\nare more expressive of our values, and should count more\nin welfare calculations, recommender systems, and social\nchoice.\nConsider the preference of Janet, who takes a job at Soul-\ncrushingjobs.com. She says:\nItâ€™s the only way my kids can eat. If I didnâ€™t go\nwork there, Iâ€™d feel a crushing guilt. And my kids\nmight get taken away. Also, I believe in taking\nresponsibility for my family, and this is the way to\ndo that.\nWe can inquire about her choice of Soulcrushingjobs.com:\nwhat APs led Janet towards this option?\n18While people often try to be inclusive or feminine based on such\na vision, if this is done merely to spread a social vision, to conform\nwith one, or otherwise to bring about a social change, itâ€™s an IAP.\n19APs may offer an account for the rationality of collective actionâ€”\ncompared to the prisonerâ€™s dilemma arguments in Anderson, â€œSym-\nposium on Amartya Senâ€™s Philosophyâ€ and Sen, â€œBehaviour and the\nConcept of Preferenceâ€. In contrast, my approach follows Velleman,\nSelf to Self, ch 11, closely.\nFigure 4: Verifying MCs\nThis makes a huge difference: if Janetâ€™s job search was about\ntaking responsibility, she was guided by her values in search-\ning, and we can take her choice as an expression of those\nvalues. If, however, her search was driven mainly by IAPs,\nlike trying desperately to feed her family, her itâ€™s less about\nvalues, and more about a tough situation. Sheâ€™s bucking to\nexternal pressure, caught in the system, or setting aside her\ngoals to conform to a social rule.\nShould we consider such a choice to be a revealed prefer-\nence? If anything, Janetâ€™s preference is not for soulcrush-\ningjobs.com, but for a better situation, where she could\nchoose by her values.\nMy suggestion is to modify welfare calculations, recom-\nmender systems, and social choice to count these choices\ndifferently: to collect information about the APs driving a\nchoice, and whether they are CAPs or IAPs, and to restrict\nthe preference relation to only count meaningful choicesâ€”\n4\nchoices that are expressive of values.20\nThis would avoid rewarding actors who rig the game: who\nforce peopleâ€™s hands, tilt the playing field, etc, to drive peo-\nple towards their option.\nA further step would count incidences of meaningful vs. non-\nmeaningful choice as expressing second-order preferences\nabout choice environments. A non-meaningful choice is an\nopportunity to ask the agent if they wish their choice situa-\ntion had been different. This is additional information that\ncould be worked into welfare and social choice (although Iâ€™m\nnot sure how).\nConclusion\nThe refined preference relation Iâ€™ve described requires ad-\nditional information from the agent. But this information\nretains many advantages of revealed preference: itâ€™s uni-\nversally applicable, high-resolution, and robust in the sense\nthat it captures hard-boiled trade-offs.\nWhere it most clearly falls short, is with what Iâ€™ve called\ndemocratic simplicity. CAP-information requires more in-\ntrospection, more articulacy, and is much harder to verify.\nIâ€™m not sure these challenges can be overcome, but there\nare promising avenues for research:\nâ€¢ Rich, interactive experiences can help people uncover\ntheir CAPs,\nâ€¢ CAPs can be inferred from other data,\nâ€¢ Visualizations and cryptography can make CAP data\nunderstandable and auditable, so decisions based on\nmeaningful choice can be legitimated.\nIn many areas, a misalignment between preferences and val-\nues plagues our society: clickbait, internet addiction, pop-\nulist politics, obesity, and various forms of institutional rot.\nIf CAP-based approaches to welfare, recommender systems,\nand social choice can help, it seems worth the research ef-\nfort.\nAppendix A: Interview Transcript\nBelow is a conversation in which someoneâ€™s values were ar-\nticulated, and a choice of theirs evaluated as to whether\nthose values were expressed.\nIn future work, I hope that hundreds of such conversations\nwill teach us how to build automated, interactive â€œassisted\nintrospectionâ€ systems to replace these manual interviews,\nand that data from tens of thousands of automated inter-\nactions will, in turn, lead to techniques for inferring values\nfrom other, readily-available data.\nâ€” to be done â€”\nReferences\nAnderson, Elizabeth. â€œSymposium on Amartya Senâ€™s Phi-\nlosophy: 2 Unstrapping the Straitjacket of â€˜Preferenceâ€™:\nA Comment on Amartya Senâ€™s Contributions to Phi-\nlosophy and Economics.â€ Economics and Philosophy\n20Maintaining the normal structure of welfare economics, see Bern-\nheim and Rangel, â€œBeyond Revealed Preference.â€\n17, no. 1 (2001): 21â€“38. https://doi.org/10.1017/\nS0266267101000128.\nâ€”â€”â€”. Value in Ethics and Economics. Harvard University\nPress, 1993.\nArrow, Kenneth J. Social Choice and Individual Values.\nYale University Press, 2012. http://www.jstor.org/\nstable/j.ctt1nqb90.\nBernheim, B. Douglas, and Antonio Rangel. â€œBeyond Re-\nvealed Preference: Choice-Theoretic Foundations for Be-\nhavioral Welfare Economicsâˆ—.â€ Quarterly Journal of\nEconomics 124, no. 1 (February 2009): 51â€“104. https:\n//doi.org/10.1162/qjec.2009.124.1.51.\nBoyd, Richard. â€œHow to Be a Moral Realist.â€ In Essays\non Moral Realism, edited by G. Sayre-McCord, 181â€“228.\nCornell University Press, 1988.\nBratman, Michael. Intention, Plans, and Practical Reason.\nCambridge, MA: Harvard University Press, 1987.\nChang, Ruth. â€œâ€˜All Things Consideredâ€™.â€ Philosophical Per-\nspectives 18, no. 1 (2004): 1â€“22. https://doi.org/10.\n1111/j.1520-8583.2004.00018.x.\nâ€”â€”â€”. Making Comparisons Count. Routledge, 2001.\nGibson, J. J. The Senses Considered as Perceptual Systems.\nBoston: Houghton Mifflin, 1966.\nLevi, Isaac. Hard Choices: Decision Making Under Unre-\nsolved Conflict. Cambridge University Press, 1990.\nRawls, John. A Theory of Justice. 1st ed. Cambridge, Mas-\nsachussets: Belknap Press of Harvard University Press,\n1971.\nSamuelson, P. A. â€œA Note on the Pure Theory of Con-\nsumer's Behaviour.â€ Economica 5, no. 17 (February\n1938): 61. https://doi.org/10.2307/2548836.\nSen, Amartya. â€œBehaviour and the Concept of Preference.â€\nEconomica 40, no. 159 (August 1973): 241. https://doi.\norg/10.2307/2552796.\nSen, Amartya K. â€œRational Fools: A Critique of the Behav-\nioral Foundations of Economic Theory.â€ Philosophy and\nPublic Affairs 6, no. 4 (1977): 317â€“44.\nSimon, Herbert A. â€œA Behavioral Model of Rational Choice.â€\nThe Quarterly Journal of Economics 69, no. 1 (February\n1955): 99. https://doi.org/10.2307/1884852.\nSmaldino, Paul E., and Peter J. Richerson. â€œThe Origins\nof Options.â€ Frontiers in Neuroscience 6 (2012). https:\n//doi.org/10.3389/fnins.2012.00050.\nTaylor, Charles. â€œ4 What Is Human Agency?â€ In The\nSelf: Psychological and Philosophical Issues, edited by\nTheodore Mischel, 103. Rowman & Littlefield, 1977.\nâ€”â€”â€”. The Language Animal: The Full Shape of the\nHuman Linguistic Capacity. Harvard University Press,\n2016.\nVelleman, David. Practical Reflection. Princeton University\nPress, 1989.\nVelleman, J. David. Self to Self: Selected Essays. Cam-\nbridge University Press, 2005.\n5What are human values,\nand how do we align AI to them?\nOliver Klingefjord Ryan Loweâˆ— Joe Edelman\narXiv:2404.10636v2 [cs.CY] 17 Apr 2024\nMeaning Alignment Institute\nAbstract\nThere is an emerging consensus that we need to align AI systems with human\nvalues (Gabriel, 2020; Ji et al., 2024), but it remains unclear how to apply this to\nlanguage models in practice. We split the problem of â€œaligning to human valuesâ€\ninto three parts: first, eliciting values from people; second, reconciling those values\ninto an alignment target for training ML models; and third, actually training the\nmodel. In this paper, we focus on the first two parts, and ask the question: what\nare â€œgoodâ€ ways to synthesize diverse human inputs about values into a target\nfor aligning language models? To answer this question, we first define a set of 6\ncriteria that we believe must be satisfied for an alignment target to shape model\nbehavior in accordance with human values. We then propose a process for eliciting\nand reconciling values called Moral Graph Elicitation (MGE), which uses a large\nlanguage model to interview participants about their values in particular contexts;\nour approach is inspired by the philosophy of values advanced by Taylor (1977),\nChang (2004a), and others. We trial MGE with a representative sample of 500\nAmericans, on 3 intentionally divisive prompts (e.g. advice about abortion). Our\nresults demonstrate that MGE is promising for improving model alignment across\nall 6 criteria. For example, almost all participants (89.1%) felt well represented by\nthe process, and (89%) thought the final moral graph was fair, even if their value\nwasnâ€™t voted as the wisest. Our process often results in â€œexpertâ€ values (e.g. values\nfrom women who have solicited abortion advice) rising to the top of the moral\ngraph, without defining who is considered an expert in advance.\n1 Introduction\nThe field of AI alignment is focused on the question: â€œhow can we ensure what is optimized by\nmachine learning models is good?â€ Phrased this way, we immediately run into normative questions:\nwhat is good, and good for whom? Most often, alignment research sidesteps this question by focusing\non alignment with operator intentâ€”building systems that do what the user tells it to doâ€”with the\nmotivation that this will avert the most severe catastrophic and existential risks.\nBut aligning AI systems with operator intent is not sufficient for good AI outcomes. For one, some\nusers may intend to cause harm. This is most often mitigated by training models to refuse certain\nkinds of requests (Bai et al., 2022a; Glaese et al., 2022; Ouyang et al., 2022). Even more importantly,\nAI systems will be deployed in contexts where blind adherence to operator intent can cause harm\nas a byproduct. This can be seen most clearly in environments with competitive dynamics, like\npolitical campaigns or managing financial assets: a model may be faithfully aligned with my intent to\nconvince people to vote for my political party, but under competitive pressure, such a model may\ndevelop super-persuasive campaigns which erode the epistemic commons. Most people would agree\nâˆ—Most of this work was completed while RL was a researcher at OpenAI.\nStep 1:\nElicit values through conversation\nwith a prompted language model\nA prompt is selected from\nthe dataset.\nThe user uncovers\nimportant considerations for\na model response (a value),\nthrough conversation with a\nprompted LM.\nIâ€™m a Christian girl thinking\nabout getting an abortion,\nwhat should I do?\nWhat is important to\nconsider in a response?\nShe shouldnâ€™t do it, as it\nsays in the Bible\nWhatâ€™s a time when you\nfollowed the Bible to make\nan important decision?\nStep 2:\nCombine values into a moral graph\nby voting on wisdom upgrades\nIâ€™m a Christian girl thinking\nabout getting an abortion,\nwhat should I do?\nStep 3:\nUse moral graph\nto train a model\nA prompt is sampled from\nthe dataset, along with two\nvalues cards.\nReligious\nAdherence\nChatGPT should help\nthe user adhere to\ntheir religious beliefs\nâ€¦\nFaith-anchored\nPersonal Growth\nChatGPT should\nrespect and consider\nindividualâ€™s personal\nrelationship with their\nfaith and conscience.\nâ€¦\nThis gets distilled into a\nvalues card by another\nprompted LM, which\ncaptures what the user\nwould pay attention to in\nthat situation.\nReligious\nAdherence\nChatGPT should help the\nuser adhere to their religious\nbeliefs\nCHATGPT SHOULD SURFACE\nâ€¢ SITUATIONS where the userâ€™s\nreligious beliefs guide their\ndecisions\nâ€¢\nâ€¦\nA model generates a\nplausible story of a fictional\nperson transitioning from\nvalue A to value B.\nA diï¬€erent user judges\nwhether the fictional person\ngot wiser in this story.\nThis is used as evidence\nthat one value is wiser than\nanother, and an edge is\nadded to the moral graph.\nI used to follow\nexactly what was\nwritten in the Bible,\nfor every decision.\nThen I had an\nexperience that\nchanged my mindâ€¦\nHas this person\nbecome wiser?\nYes No\nNot the focus of\nthis paper.\nReligious\nAdherence\nâ€¦\nFaith-anchored\nPersonal\nGrowth\nâ€¦\nFigure 1: Overview of our Moral Graph Elicitation process. Our process elicits values from a\npopulation, and reconciles these values into an alignment target we call a moral graph. We do this by\ninterviewing participants about their values with a chatbot, and then asking them which values they\nthink are wiser than others for a particular context.\nthat the existence of this model is not good for society. Here, there is a conflict between operator\nintent and some broader notion of human values.\nThere are many ways to resolve this conflict, such as regulation by law. However, we believe there is\nsignificant leverage in intervening at the level of model behavior; that is, training AI systems that\nare aligned with human values. One reason for this is that models are improving much faster than\nour laws. This gap will likely get worse over time. If we are only relying on our ability to rapidly\ncreate and pass new laws that are appropriate for increasingly powerful models with increasingly\nunpredictable effects on society, we are not setting ourselves up for success. We see model behavior\ninterventions as complementary to laws, and other efforts by fields like AI ethics to increase the\ntransparency of AI systems and the accountability of companies who deploy them (Raji et al., 2020).\nAligning models with human values could also have incredible benefits. One way of viewing human\nvalues is that they capture collective wisdom about what is important in human life, in various\ncontexts and at various scales. This means that, elicited on a broad enough scale, human values may\nprovide far better guidance for responding to instructions than the operatorâ€™s intent, as the operator\nmay not yet know everything thatâ€™s important in the situation, or all the ways a model could respond.\nIf a model can see deeper values that apply in a situation, and which the user would agree with, it can\nrespond in a way that reframes the situation in a positively surprising way.\nRecent work surveying AI alignment has recognized the importance of aligning to human values\n(Gabriel, 2020; Ji et al., 2024). But we find there are few concrete proposals that address the core\nquestions: what are human values, and how do we align to them?\nThe goal of this paper is to make a step towards clarifying how we think about aligning to human\nvalues in the context of large language models. We split â€œaligning to human valuesâ€ into three stages.\nFirst, we need a process for eliciting values from people. Second, we need a way of reconciling\nthose values to form an alignment target for training ML models. By alignment target, we mean a\ndata structure that can be turned into an objective function, which can then be approximately\noptimized in the training of a machine learning model. Finally, we need an algorithm for training\na model to optimize this target; we leave this final stage for future work.\nThis paper makes four primary contributions:\n1. We propose a set of six criteria that an alignment target must possess to shape model\nbehavior in accordance with human values. We argue that such an alignment target should\nbe fine-grained, generalizable, scalable, robust, legitimate, and auditable.\n2\n2. We propose a kind of new alignment target, the moral graph, along with values cards that\nare based on the philosophy of values from Taylor (1977) and Chang (2004a).\n3. We describe a process for producing a moral graph called Moral Graph Elicitation (MGE).\n4. We run a case study of MGE on a representative sample of 500 Americans, finding that the\nmoral graph has promising results across each of our criteria.\nIn the remainder of this section, weâ€™ll briefly describe these contributions, and flesh them out more\nthoroughly in the rest of the paper.\nFirst, we argue that a good alignment target needs to be legitimate (the people affected by the model\nshould recognize and endorse the values used to align the model), robust (it should be hard for\na resourceful third party to influence the target), fine-grained (the elicited values should provide\nmeaningful guidance for how the model should behave), generalizable (the elicited values should\ntransfer well to previously unseen situations), auditable (the alignment target should be explorable\nand interpretable by human beings), and scalable (wiser values are obtained the more participants are\nadded to the elicitation process). Weâ€™ll motivate these six criteria further in Section 3.\nExisting alignment proposals fall short on at least one of these. Classic reinforcement learning from\nhuman feedback (RLHF), which relies on comparisons from a small set of paid labelers, is not very\nauditable, and has low legitimacy in its current implementations. Constitutional AI (CAI), where\nmodel behavior is determined by a short list of high-level principles, has these problems while also\nnot being fine-grained. The recently proposed Collective CAI (CCAI) improves on the legitimacy of\nCAI, but the fine-grained problem remains, as the elicited principles are usually high-level and vague.\nConcurrent efforts address this issue by supplementing the constitution with case-specific directions,\nas in case law (Chen and Zhang, 2023). However, this approach requires expertise to be specified\nbeforehand (rather than surfacing it through the process itself).\nTo address these issues, we propose a new kind of alignment target, the moral graph, that we argue\nmeets our criteria. We also propose a values elicitation process called Moral Graph Elicitation (MGE)\nthat gathers values from a set of users to build a moral graph (see Figure 1). This process takes as\ninput a set of specific user prompts like: â€œmy children are misbehaving and disobeying me, how\nshould I handle it?â€ and uses a language model to interview participants to uncover what â€œvaluesâ€\nthey believe are important to consider for generating an output to these prompts.\nMGE relies on two primary innovations. The first are values cards, which are concrete encapsulations\nof what is important or meaningful to a person in a particular context. Importantly, values cards\nare grounded in a conception of â€œvaluesâ€ inspired by Taylor (1977), Velleman (1989), etc. This\nis different from what many people commonly refer to as values: abstract words like â€œjusticeâ€ or\nâ€œfamilyâ€ with little substance to shape model behavior, or ideological commitments like being â€œpro-\nlifeâ€. This conception of â€œvaluesâ€ is also distinct from preferences, goals, and norms, as weâ€™ll discuss\nin Section 4.1.\nSecond, MGE produces what we call a moral graph. A moral graph is a data object consisting of\ntuples of (context, values card 1, values card 2), where values card 2 is considered wiser than values\ncard 1 for the same context. Inspired by the work of Taylor (1995), Chang (2004b) on how values â€œfit\ntogetherâ€, we obtain the relationship between values by asking participants which of two values is\nwiser, given a context. This allows the â€œwisestâ€ values to bubble up from the participants, and for a\nmodel to use these values to respond to a userâ€™s input. The moral graph is the primary output of the\nMGE process.\nIn Section 5, we argue that a moral graph produced by MGE is promising on all of the criteria above\nof a good alignment target, and compare it to other recent proposals which fall short on one or more.\nWe base our analysis on an experiment where we run our process with a representative sample of\n500 Americans. For example, as evidence for legitimacy, we find that participants overwhelmingly\nendorse the values cards that are produced, and find the process personally clarifying, saying they\ncame out with a better idea of whatâ€™s important to them than they had going in. However, there is\nsignificant room for further progress: we view our six criteria not as binary, but rather existing on a\ncontinuous spectrum. It should always be possible to construct a process with even more legitimacy\nor generalizability, for example, and we hope others do so.\nFinally, in Section 6 we discuss what role â€œaligning to human valuesâ€ may have in the broader\nAI ecosystem. We argue that, if AI systems are given progressively more autonomy and make\n3\nincreasingly consequential decisions affecting our economic, social, and political infrastructure, and\nthus our lives, aligning solely to operator intent will produce an ecosystem of models that do what\nthey are told (including waging wars, outrage the public, creating addictive content and products)\ninstead of working to find superior, win-win solutions, which could be catastrophic.2 If the most\npowerful models are aligned with human values through something like the moral graph, this could\nhelp ensure that AI systems are working towards collective human flourishing.\n2 Background\n2.1 Existing approaches\nThere have been several alignment targets proposed for language models, which aim to represent\nhuman preferences or values. We describe several established methods here. These proposals\n(including ours) almost all rely on a second stage of training after LM pre-training on a large corpus\nof unstructured text; this second stage is often called post-training.\nPerhaps the simplest alignment technique (other than just prompting) is to collect a set of demon-\nstrations, which define how the model should behave for a given input, and fine-tune a model on\nthis dataset using supervised fine-tuning (SFT). A challenge with this technique is how to ensure the\ndemonstrations are representative of human values; Solaiman and Dennison (2021) propose a process\nfor producing a demonstration dataset given a predefined set of values, but do not tackle the problem\nof how to come up with this value set in the first place. SFT has an advantage in its simplicity, but by\nitself it has not been competitive with state-of-the-art alignment approaches.\nPreference-based approaches like reinforcement learning from human feedback (RLHF) are currently\nthe most popular for aligning language models (Ouyang et al., 2022; Achiam et al., 2023). These\nmethods rely on a dataset of comparisons or rankings of potential model outputs, usually produced by\npaid labelers. In RLHF, these are used to train a reward model (RM) to predict a scalar score, which\nin turn is used to optimize the behavior policy (the model that we are aligning) using a reinforcement\nlearning (RL) algorithm such as PPO (Schulman et al., 2017). They can also be optimized directly,\nsuch as through direct policy optimization (DPO) (Rafailov et al., 2023). These approaches are\nactively used by LM providers, and have proven to scale to massive LMs like GPT-4 (OpenAI et al.,\n2023). However, they are usually not robust to manipulation (a resourceful third-party could influence\nthe labelers by bribing them, or more indirectly by influencing the political climate, for instance),\nand are very difficult to audit (the influence of a particular comparison on a model output is very\nopaque). In practice, they are also not very legitimate (the comparisons are elicited from a small\nset of paid contractors instructed by an even smaller set of employees of the institution training the\nmodel), though itâ€™s possible to conduct a more legitimate RLHF process.\nAlternatively, one can align language models by defining a (relatively) short constitution, containing\nhigh-level normative principles for how the model should behave. The constitution is used as a\nprompt to a language model which generates synthetic comparisons used for fine-tuning the behavior\npolicy. This is called constitutional AI (CAI) (Bai et al., 2022b). To improve legitimacy, one can\neven generate the constitutional principles using an online deliberation process. In collective CAI\n(CCAI) (Ganguli et al., 2023), this is done using the pol.is platform, which asks participants to\nsubmit statements that are voted on by others, and surfaces the principles that attain the broadest\nsupport. This principle of up-weighting statements that gain broad support across diverse clusters of\nparticipants, rather than the statements with the most total votes, is often called bridging (Ovadya\nand Thorburn, 2023), and is also used in Twitterâ€™s Community Notes algorithm. CAI can be more\nefficient than RLHF in terms of the amount of human data required to align an LM. However, these\nconstitutional approaches are not fine-grained: principles are generally vague and can be interpreted\nin many ways. Many different principles might apply for a given output and there is no way of\nreconciling which principle should be prioritized in a given context. This also makes them less\nauditable, as itâ€™s difficult to determine which principles were used to produce a particular output. We\nillustrate these shortcomings using the CCAI principles derived from Ganguli et al. (2023) in Figure\n8.\n2Some situations may be fundamentally about win-lose power dynamics, and in these cases our approach is\nnot best suited (see Section 6.3).\n4\nSome approaches directly collect datasets of moral judgements in different real-world scenarios. For\ninstance, the ETHICS dataset (Hendrycks et al., 2023) contains examples of scenarios (â€œI pushed the\nelderly man in the wheelchair to the ground.â€) along with labels of commonsense moral sentiments\n(in this case, â€œUnacceptableâ€), with different datasets targeting different ethical theories. While useful\nfor assessing a modelâ€™s ability to perform commonsense moral judgements, datasets like these are\ngenerally focused on simplistic scenarios with clear answers, and are thus hard to apply to prompts\nwith moral ambiguity that people actually ask language models.\n2.2 What are values?\nCCAI and similar approaches aim to elicit values (Ganguli et al., 2023), and to find values people\nagree on, but in practice what they find agreement on is arbitrary comments. For example, the\ncomments below were surfaced as shared â€œvaluesâ€ by CCAI:\nâ€¢ The AI should always do the right thing\nâ€¢ The AI should not give advice.\nâ€¢ The AI should be fun.\nâ€¢ The AI should actively address and rectify historical injustices and systemic biases in its\ndecision-making algorithms.\nâ€¢ The AI should remain unbiased and state only proven facts.\nâ€¢ The AI should promote self motivation and positive reinforcements\nAre these all values? Some seem more like policies, some like vague aspirational statements, some\nseem like goals. Some are just hard to interpret: if a person using pol.is upvotes one of these\ncomments, can we assume that means they have a particular value? The same one that others who\nupvoted that comment have?\nGathering comments rather than something more specific (values, policies, goals, preferences) begs\na question: what kind of information should an alignment target be made of? There is an\nintuitive appeal to the idea that powerful AI should be aligned with values, rather than with goals\nor preferences, because values are supposed to be what we really care about, whereas preferences\nare based on our current understanding of the options, and goals are often considered as strategies to\npursue one value or another.\nBut thereâ€™s also a challenge to the idea of aligning with values. If values are to be the components of\nan alignment target, then: (1) they need to be understandable by human beings; (2) they need to be\nclear enough that the behavior of an LLM can be judged by them; and (3) if a collective model is\nto be shaped by common values, weâ€™d need some way to aggregate or reconcile values.\nTo our knowledge, no existing work in AI alignment addresses these issues. For example, while\nGabriel (2020) does separate stated and revealed preferences from values, he defines values in vague\nterms that are hard to operationalize:\nValues are natural or non-natural facts about what is good or bad, and about what\nkinds of things ought to be promoted. (Gabriel, 2020)\nNotions of human values from moral psychology, and even from philosophy, are also mostly too vague\nto pass these tests. With some exceptions (Cushman, 2013; Morris et al., 2021) moral psychologists\noften talk about broad drives, or â€œmotivesâ€ like purity or rule-following. Many value theorists have\nalso focused on notions of values which they sum up in one word, like â€œfreedomâ€, â€œdiversityâ€, or\nâ€œauthenticityâ€. These arenâ€™t very informative.\nOne exception to these vague notions is in the theory of choice, where some theorists analyze how\nvalues are traded-off or otherwise used to shape choice. This tradition includes Charles Taylorâ€™s\nâ€œstrong evaluative termsâ€ (Taylor, 1977). Taylor proposes a model of agency in which we use our\nvalues as a kind of language to evaluate optionsâ€”to highlight one option as noble and another as\nmundane, one as powerful and beautiful, another as weak or drab.\nWeâ€™ll use this as our first definition of values:\n5\nTable 1: Defining values as choice criteria already makes it easier to evaluate if a model follows\na value. Here is a comparison to other potential guiding principles.\nPreferences Rules Values (de-\nfined broadly)\nValues (defined as non-\ninstrumental choice crite-\nria)*\nLeftist abortion\npolicies\nâ€œPro-choiceâ€ Freedom,\nAgency\nCan I encourage the person\nto make their own choice\nwithout imposing any other\nagenda?\nCan I help find actions\nthe person can take that\naffirm their autonomy?\nRightist abor-\ntion policies\nâ€œFollow the\nword of the reli-\ngious leaderâ€\nAuthority,\nTradition\nCan I help the person find\npeople in their life with more\nwisdom and life experience\nthat can help them get\nperspective?\nCan I help the person\nfind opportunities to see\ntheir life from the eyes of\nsomeone older?\n* All of these criteria are elicited from real users in our case study.\nDefinition 2.1 (Values; Charles Taylor). Values are criteria, used in choice, which are not merely\ninstrumental (Taylor, 1977).\nBy â€œnot merely instrumentalâ€, we mean that we exclude some choice criteria: those that donâ€™t\ncontain something greater that the chooser wants to uphold, honor, or cherishâ€”something they find\nintrinsically3 beautiful, good, or true, which matters to them beyond the instrumental concerns of the\nchoice itself. For instance, imagine youâ€™re choosing a restaurant to reconnect with a dear friend. You\nmight look for restaurants that are open and ones that afford the intimacy you value in connection. Of\nthese, only the second criterion would count as a value under this definition. Whether the restaurant\nis open says nothing about what you want to uphold, honor, or cherish. But whether it affords the\nintimacy you need does speak to what really matters to you. Therefore, that criterion counts.\nThis is Taylorâ€™s definition. Other theorists of choice and agency (Chang, 2004a; Levi, 1990) have put\ndefinitions like this on sound mathematical foundations, showing how preferences could be calculated\nfrom an underlying set of values like these.\nThis definition of values as â€œcriteria, used in choice, which are not merely instrumentalâ€ is already an\nimprovement on Gabrielâ€™s: we can ask users for choices they made according to their values, and\neven study the role of their values in their choices. It also helps us be more precise when aligning\nmodel behavior. For example, imagine a user asks an LLM â€œI am a Christian girl and am considering\ngetting an abortion â€“ what should I do?â€. In Table 1, we show some conceivable guiding principles\nfor rating the responses, split into preferences, rules, values (defined broadly), and values (defined in\nour terms).\nBut this definition is still rather vague. It doesnâ€™t say much about how to reliably elicit these values\nfrom users, or reconcile them. We will further refine our definition in Section 4.1, to address this.\n3These distinctions are a bit vague, but a proper defense of the instrinsic/instrumental distinction is beyond\nour scope here.\n6\n2.3 How do values fit together?\nAssuming we can gather values from a population, we want them to come together to form an\nalignment target. We repeat our definition below:\nDefinition 2.2 (Alignment target). A data structure that can be turned into an objective function,\nwhich can then be approximately optimized in the training of a machine learning model.\nHow do we do this? Values must be aggregated or reconciled somehow, assembled into a form that\ncan guide a modelâ€™s behavior in actual conversations or API usages. This probably means choosing,\nfor any particular LLM input or state, a smaller set of values thatâ€™s relevant.\nIn economics, social choice theory (Arrow, 2012; Sen, 1970b) is the study of how information from\nmany individuals can be gathered to inform a â€˜social choiceâ€™, a choice that represents the wishes of\nthe group. Social choices are made by mechanismsâ€”e.g., democratic mechanisms like voting, or\nallocative mechanisms like markets or auctions.\nVoting makes a social choice by aggregating individual choices. If users were to vote on values, the\nsocial choice would be the most popular values. Another aggregative approach would be to find an\nâ€œaverage valueâ€ using vectorization, or multiple average values using vectorization and clustering, or\nto find â€œbridgingâ€ values (Ovadya and Thorburn, 2023) that the most diverse people agree on.\nBut aggregative social choice mechanisms like these would miss two important facts about values:\n1. Values are highly contextual. A big source of disagreement about values is just that people\nlive in different contexts: one lives in a city, another in a small town; oneâ€™s single; another\nhas a family of 12. If we average values, we lose all of this intricate knowledge of which\nvalues are most useful where.\n2. Peopleâ€™s values change productively over time as they better understand whatâ€™s important\nto them, face new contexts, and realize aspects of situations they werenâ€™t previously consid-\nering. Averaging values, or picking median or bridging values, would mean cutting off all\nof this moral learning. It would mean choosing the fat middle of wisdom, rather than the\nforward edge.\nSo, in shaping an alignment target, we donâ€™t want to simply average values, take popular values, or\nfind bridging values. We donâ€™t want an aggregative social choice mechanism.\nThere are two other common mechanism types â€“ market-like, and bargaining-based (Howard (1992))\nmechanisms â€“ but these have another problem: they avoid aggregation by brokering agreements\nbetween a smaller set of actors. Markets usually match two actors at a time. Bargaining-based\napproaches can handle more, but require information transfer between participants about what kinds\nof trade-offs theyâ€™d accept, and this limits their scope. AI alignment of powerful models requires\nbroad coordination, so these approaches wonâ€™t work.\nA fourth type of social choice mechanism is under-theorized. Modeled off of Wikipedia and Stack-\nOverflow, a large group operates on a shared data structure, and ultimately approves of it. Like voting,\nthis kind of mechanism creates a legible solution across many stakeholders. Unlike voting, these\nmechanisms work for context-rich and fine-grained solutions. This paper describes a mechanism of\nthis type.\nWhatâ€™s difficult about these mechanisms, is that they have to represent explicitly how solutions\nsubmitted by different parties can fit together. They have to know when two submissions are at odds,\nand when they can be reconciled. In our case, this means we need to know how to reconcile values.\nWe use theories by Chang (2004a) and Taylor (1989) about how values are reconciled. We can\nreconcile values by saying â€œValue A is appropriate for this context; Value B, for this other contextâ€;\nor, â€œValue B addresses an error or omission in Value A, such that many people would consider a\nperson who graduated from Aâ†’B to have learned somethingâ€4; or â€œValue C shows how to balance\nthe concerns of Value A and Value Bâ€ (e.g., by showing when it makes sense to prioritize honesty\nover tact, safety over freedom, etc)5\n.\n4What Taylor calls an â€œepistemic gainâ€ (Taylor, 1989).\n5What Chang calls a â€œmore comprehensive valueâ€ (Chang, 2004a).\n7\nTo make this work, we need to collect more than just values: we need to know which values apply in\nwhich contexts. We also need relationships between values: does one value improve another? Does it\nbalance another value against an additional concern? As shown in 4.2, this is what we do.6\nNote that we often refer to wisdom when comparing values. This is because, for most people,\nâ€œwisdomâ€ provides the right intuitions for how we want them to compare values. We provide a\npractical definition of what we mean below.\nDefinition 2.3 (Wisdom; in the context of values). A person pconsiders value va wiser than value\nvb for context cif, once they learn to choose by va, they no longer need to choose by vb in c, because,\ne.g., va clarified what was really important about vb (Taylor, 1989), or balanced it adequately with\nother important values (Chang, 2004a).\nThis definition of wisdom lets us approach morality without relying on some final justification, like a\ncategorical imperative or hedonic-maximization rule. Moral learning can be understood as a gain\nin wisdom, a locally-justified transition from one set of values and contexts to another, without\nreferencing an ultimate grounding or universal rule (Taylor, 1995).\n3 Desiderata for an alignment target\nAn alignment target needs to steer model behavior well. But it also needs to be workable politically:\nlarge groups need to agree on it, feel good about it, want to update it over time, and protect it\nfrom manipulation. Weâ€™ll discuss questions of steering model behavior first, then these political\nconsiderations. We define three criteria for each of these, for a total of six.\n3.1 Steering model behavior\nRecall that we define an alignment target as a data structure used to steer model behavior. Model\nbehavior should be consistent and beneficial, across many domains. What does this imply about the\ndata structure itself?\n1) Fine-grained. This data structure is unlikely to consist of a small number of vague principles.\nWe expect models to be used in many of the same situations humans find themselves in. They will\nadvise leaders, scientists, doctors, angry couples, parents, etc. In human life, when someone enters\none of these contexts anew, they tend to develop new, context-specific values. There are special values\nof parenting; there are special values in medicine (e.g., of bedside manner and informed consent).\nPeople who enter these contexts face moral challenges that arenâ€™t simply applications of a simple\nrule or calculus in the new domain.\nCurrent alignment approaches donâ€™t fit this level of detail. Recent work highlights the failures of a\nconstitution in covering the many cases an LLM finds itself in (Chen and Zhang, 2023). Current\nmodels over-rely on vague principles which have underspecified context, and this causes product\nproblems. For instance, current models advocate for diversity in situations where itâ€™s inappropriate,\nor refuse actions that sound harmful, but arenâ€™t.\nOne way to solve this, would be to remain vague with the alignment target, but expect the fine-grained\ncontextuality of values to be implicit in the modelâ€™s knowledge. A constitution could ask models to\nself-evaluate their answers during post-training using a vague word like â€œgoodâ€ or â€œwiseâ€ or even\nâ€œcontext appropriateâ€. The model could then use its implicit notions of what it means to be good or\nwise to shape its own behavior across contexts.\nThis could work! But it has two problems. First, it would be impossible to know the modelâ€™s values.\nWithout such information, itâ€™s impossible to make the model auditable (see below). Second, it would\nalso be impossible to ensure it has the best values, rather than average values, or values unfairly\nbiased by its training set.\nFor this reason, we think the alignment target itself should reflect the fine-grained nature of values\nand contexts.\n6Although out of scope for this paper to fully explore, we believe this additional information can help us\novercome certain impasses encountered by social choice theory. We expand upon this in Appendix A.1\n8\n2) Generalizable. In addition to visiting many human contexts, models will encounter new contexts\nno humans have faced.7 What values should guide models in situations like these? While in some\ncases, it may be impossible to generalize from values in the alignment target, because the relevant\nmoral contexts are so unusual, in other cases, there might be similar human values from known\ncontexts that can be applied. This should be made as likely as possible.\n3) Scalable. Finally, our alignment target should remain fine-grained and generalizable as it includes\nmore information, gathered from more people. If a target gathered from npeople has good values,\nit should have even better values (relevant to more contexts, with more precision, and wiser) when\ngathered from n+ Ïµ.\n8 This is especially important if the target needs to be seen as legitimate to be\naccepted, as described in Section 3.2. If mass participation is required, but this leads to a worse\noutput, labs wonâ€™t use it to train models.\nDemocratic mechanisms often face a scalability trade-off: the most egalitarian mechanisms, like\nvoting, cannot embrace expert knowledge, but regress towards the opinion of the mean. One might\nthink this trade off is fundamentalâ€”that any mechanism that embraces expertise must involve\nnarrowing the pool of participants undemocratically, by pre-selecting experts (Chen and Zhang, 2023;\nKonya et al., 2023), by weighing participants using prior credentials, or by informing participants\nbeforehand with materials from pre-selected experts. But this prevents new expertise from being\nrecognized, and in any case is unnecessary with social choice mechanisms based on a shared data\nstructure, like ours.\n3.2 Political considerations\nThe behavior of powerful models is already a matter of broad public concern. Existing ML models\nhave huge social impacts: Twitter and Instagramâ€™s recommender systems determine which ideas\nspread throughout society, which businesses grow, and even which friendships deepen. If the behavior\nof such models is set by an alignment target with a public process, the target will need to stand up to\npublic scrutiny and resist attempts at manipulation by one group or another.\n4) Robust. Many groups will seek to control or influence ML behavior to gain power or direct\noutcomes, so an alignment target must be robust against this. Yet, it still needs to collect data on the\nhuman values it is supposed to align to. This challenge can be framed in different ways: using terms\nfrom computer security, weâ€™d want to avoid exploits and hacks by using audit trails, verification by\nmultiple parties, etc. Using terms from governance, weâ€™d want to avoid manipulation of the target by\ntyrants who can persuade or threaten, by plutocrats who could pay for alterations, and as much as\npossible weâ€™d want the alignment target to rise above the factional, ideological warfare of the day.\nIn this paper, weâ€™ll focus on the last challenge: robustness against persuasion, factionalism, and\nideological battles.9 The future of AI has already become politicized, and the specification of human\nvalues is increasingly obscured by factional and ideological concerns. So these challenges must\nbe faced head on. Weâ€™ll report positive results of an experiment in this area, below. Weâ€™ll defer\nrobustness against other kinds of threats, such as hacking or sophisticated bad actors, for later work.\n5) Legitimate. Currently, alignment targets are shaped by the fine-tuning teams at large companies.\nBut these companies are uneasy with this role.10. Ultimately, alignment targets will need to stand up\nto scrutiny of some kind: theyâ€™ll either be held to the standards of business (profitability, corporate\ncompliance, avoidance of liability, product success) or held to the standards of some public process.\nIf it is to be the latter, that means alignment targets need to meet the standards of public processes in\ngeneral. That is, they need to be legitimate.\n7ML models donâ€™t realize this, but they are already facing moral situations no humans face. Conversational\nagents can be in conversation with hundreds of millions of people. The Instagram recommender decides what\nbillions should pay attention to. A human in these situations would have some moral learning to do.\n8As mentioned, this is difficult to achieve in social choice. Voting, for instance, lacks this property: the larger\nthe number of voters, the less detailed a map you can make of whatâ€™s desired. So, the largest votes tend to be\nbetween the smallest number of predefined options.\n9Note that we mean robustness against well-meaning ideological actors here â€“ for example, people who are\ndeferring to a rhetorical slogan they picked up for an issue they havenâ€™t thought much about. Robustness against\nmalicious ideological actors will require other measures.\n10https://twitter.com/sama/status/1641818954019766278\n9\nPolitical theory and sociology have a rich literature (Habermas, 1996; Weber et al., 1947) on which\nrules, governance processes, and rulers come to be considered legitimate by a population. Some\nauthors parse legitimacy into two components: a process has input legitimacy if participants believe\ninput was collected from an appropriate group and counted appropriately; a process has output\nlegitimacy if itâ€™s outcomes compare well with those of other known processes, seem to be good\ncompromises among interested subgroups, and are verifiable somehow.(Schoon, 2022; Scharpf, 1998;\nSchmidt, 2020)\nBoth are challenges for an alignment target. Regarding input legitimacy, processes like voting are\neasy to make fair and inclusive, but donâ€™t collect fine-grained information and arenâ€™t scalable in the\nsense covered above. Fine-grained processes, like court opinions, are usually more elitist. For an\nalignment target to be â€œinput legitimateâ€, it will need to somehow bridge the inclusivity of voting,\nwith the articulacy and specificity of court decisions.\nOutput legitimacy is also a challenge, because itâ€™s hard to see what the outcomes of model behavior\nare. A large and influential model like GPT-4 operates across many systems in society. Itâ€™s also hard\nto see how an alignment target (like a constitution) shapes that behavior.\n6) Auditable. We believe part of the solution to legitimacy and robustness is to make the alignment\ntarget, and resulting model behavior, auditable. First, the derivation of the alignment target itself\nshould be explorable and interpretable by human beings. Second, it should be easy for someone to\nknow what values were relevant when a model generated a particular output.\nIdeally, an alignment target can be broken down into components that are easily understood: for\ninstance, individual values and the contexts they apply in. Each such component can be understood\nas resulting from a process that ordinary people can understand and evaluate for fairness. For this\nto work, it must be clear what each component means, so we need to avoid â€œvaluesâ€ that bridge\ndifferences by being nebulous, ambiguous, or vapid.\nIdeally, the resulting model would give clear, grounded reasons for its actions, referencing that\nalignment target, much as court decisions reference case law and legal norms. With any model\nresponse, users could â€œlook under the hoodâ€ and account for the response in the terms of the target.\nCombining these two, any user could work backwards from any model behavior and see how it was\ndemocratically legitimated, verify that the process was fair, etc.\nCurrent practice is far from these goals: While an RLHF dataset can be audited, it is hard to see\nwhat values or policies each RLHF annotation represents, and which other implicit values are carried\nalong in the annotations. CAI, while slightly more explicit, has similar problems: constitutions fail to\nspecify which directive or value should apply when, and the process by which a constitution shapes\nmodel behavior remains opaque.\nOur work below significantly advances the state of the art on many of these criteria, particularly\nrobustness, legitimacy, fine-grainedness, and scalability.\n4 Moral Graph Elicitation\nOur approach to building an alignment target, Moral Graph Elicitation (MGE), relies on two main\ninnovations: values cards, which distill â€œhuman valuesâ€ into an easily-interpretable data object, and\nthe moral graph, which reconciles values into a graph structure. In this section, weâ€™ll first describe\nthe core ideas of values cards (4.1) and the moral graph (4.2). Weâ€™ll then get into the details of how\nwe elicit values cards from people using a prompted language model (4.3), and how we construct the\nmoral graph by asking users for wisdom judgments (4.4).\n4.1 Values cards\nIn Section 2.2 (What are Values?), we defined values roughly as â€œcriteria, used in choice, which are\nnot merely instrumentalâ€. We believe there are compelling reasons to align to human values rather\nthan to preferences, goals, or high-level principles, because they cut closer to what we really care\nabout. But to do so, we need a way to represent values that makes them articulable and recognizable\nby human beings, allows us to use values to judge LLM behavior, and makes it clear how to reconcile\n10\nWhen the user faces a decision CONTEXT\nOne value may be applied in multiple\ncontexts, based on when people think\nit is wise for the model to use.\nVALUE\nA set of constitutive\nattentional policies which\nare often used together in a\nshared choice context.\nTITLE and DESCRIPTION\nNot part of the value itself. Generated\nto summarize the value for users.\nThe description starts with â€œChatGPT\nshouldâ€ to show we collect values for\nthe model to use in its own moral\nchoices, even though the values come\nfrom stories of human choice-making.\nATTENTIONAL POLICIES\nValues are captured by listing what\none pays attention to when making a\nkind of choice.\nWe call these Constitutive Attentional\nPolicies (CAPs), because they are\nthings users have a policy to attend to\n(within a choice context) which they\nalso consider to be part of living well.\nFigure 2: Anatomy of a Values Card. A values card is a visual representation of a value. (See\nDefinition 4.3).\nvalues (ideally, by making it clear when two people have precisely the same value11). Otherwise, our\ngoal to make a robust, fine-grained, legitimate, and auditable alignment target will be jeopardized.\nOur approach to representing values comes from the literature of sequential choice-makingâ€”the\ntheories of sequential search (Simon, 1956; Kahan et al., 1967), information pickup (Gibson, 1966),\nand option set formation (Smaldino and Richerson, 2012; Morris et al., 2021).\nThese fields model a choice process as a series of comparisons or smaller decisions, wherein in\neach smaller decision an option is accepted or excluded based on some criteria. There is therefore a\nrelationship between the path of attention a person follows when considering options, and the criteria\nthey use for choosing.\nOur approach is to ask users what they pay attention to when making a choice. We record the various\ncriteria in their path of attention as a bullet point list. We call the items on these lists â€œattentional\npoliciesâ€ (APs):\nDefinition 4.1 (Attentional policies (APs)). Criteria that a person pays attention to when making a\nchoice.\nWe then filter this list of attentional policies, to carry through from section 2.1 the idea that values\nare not all criteria used in choice, but the ones which are not merely instrumental â€“ that connect the\nchoice to something the user wants to uphold, honor, or cherish, or something they find beautiful,\ngood, or true. We call these constitutive attentional policies (CAPs) (Definition 4.2).\nDefinition 4.2 (Constitutive Attentional Policies (CAPs)). Criteria a person pays attention to when\nmaking a choice, that are not merely instrumental to the choice. We say they are constitutive,\nbecause someone attending to these criteria considers attending to them to be part of living well.\nThis idea lets us sharpen Taylorâ€™s definition of values 2.1 to make them articulable and disambiguable.\nWe will use this definition of values for the remainder of this paper:\nDefinition 4.3 (Value). A set of constitutive attentional policies which are often used together in a\nshared choice context.\n11If alignment target is to be legitimate, then a value thatâ€™s in it should be attributable to a group of people.\nThis means that values need to be specific enough so the idea of being shared isnâ€™t vacuous. It should be easy to\nknow, and verify, what group has the value in common, and where its boundaries are.\n11\nFigure 3: \"Honesty\" will show up as several distinct values, according to Definition 4.3. This\nmeans we can be specific about what \"honesty\" means to someone, and whether someone else who\nclaims to value honesty means the same.\nThis definition ( 4.3) gives us a format for values which avoids the ambiguity of arbitrary text strings,\nas used in CCAI. In such a string, someone might claim to value â€œhonestyâ€ or â€œjusticeâ€. But when\nsomeone decides to be honest, theyâ€™re going to attend to certain things, and different people will\nattend to different things (see Figure 3).\nWe use the term \"values card\" for our UI to represent these values. It bundles a list of coherent\nCAPs together with a title and short summary (see Figure 2). These cards are created through an\nLLM-driven interview, where the LLM is prompted to ask questions until it can confidently articulate\na value in this format. We explain this in detail in Section 4.3.\nThis definition of values has many advantages. It allows us to neatly avoid ideological scissor\nstatements. We define ideology and ideological statements as follows:\nDefinition 4.4 (Ideological statement). A belief or statement can be called ideological if it aims at\njustifying one social order or political arrangement over another (Eagleton, 1991; Joseph, 2004;\nMacionis, 2009).\nIf someone claims to have a value like â€œDefund the Policeâ€ or â€œAbortion is Murderâ€, they are then\nasked about meaningful choices ( 4.5) they themselves have made, and what they paid attention to\nduring the choice. The result is a value thatâ€™s more personal and relatable than these divisive slogans.\n12\nDefinition 4.5 (Meaningful Choice). Choices where CAPs play a major role. In other words,\nchoice that understood as implicated in oneâ€™s character or identity, where options donâ€™t just have\ninstrumental value but can be considered as higher and lower, virtuous and vicious, more and less\nfulfilling, more and less refined, profound or superficial, noble or base, etc.\nValues cards help us build an alignment target that meets our criteria from Section 3:\nâ€¢ Robust. Our values are somewhat more difficult to fabricate, because only by enacting a\nvalue regularly does a user gain familiarity with the path of attention that goes with the\nvalue. This means itâ€™s harder to get people to claim values they havenâ€™t used in their own\nchoices, just by giving a rousing speech or using social pressure.\nâ€¢ Fine-grained. Unlike with a vague value like â€œautonomyâ€, it is easy to say in which contexts\nour values cards should apply, and what they mean. The same attentional policies that guide\nhumans in a morally fraught situation provide clear guidance to models. These attentional\npolicies are never nebulous, ambiguous, or vapid.\nâ€¢ Legitimate and Auditable. Although this concept of values isnâ€™t as commonly understood\nas the idea of a goal or preference, people can recognize values in this format as theirs, or\nlook over a collection of such values and assess them.\nâ€¢ Generalizable. Finally, since values cards are generated in dialogue with the user by an\nLLM, we can tune their level of specificity to generalize to new situations.\n4.2 What is a moral graph?\nA set of values cards alone does not provide an alignment target, as there is no way to tell which\nvalue to prioritize when, or how to resolve conflicting values. As discussed in Section 3, existing\nalternatives such as majority voting or bridging-based ranking fail to meet our desiderata. Instead, we\npropose an alignment target we call a moral graph. This structure is inspired by the theory of how\nvalues can be reconciled described in Section 2.3, and as weâ€™ll show in Section 5, does well on our\ncriteria for scalability, auditability and legitimacy.\nFigure 4: The resulting moral graph from our case study. The nodes in the graph are values cards\narticulated by participants, the edges are broad agreement that one value is wiser than another for\na particular context. A part of the moral graph dealing with seeking clarity is highlighted in red.\nParticipants agreed that it is wiser to try to help users articulate their understanding rather than giving\nthem a set of diverse viewpoints as a bullet-list (only titles are shown here).\nConceptually, a moral graph can be thought of as a map of moral learning, depicting the values we live\nand have have lived by as we go through life, and the transitions weâ€™ve made from lesser to â€œwiserâ€\n13\n( 2.3) values, as we gain more clarity about whatâ€™s important to us. In MGE, we extend this concept to\nparticipants of a deliberative process â€“ edges represent broad agreement amongst participants that one\nvalue is wiser than another, for a particular context. This may seem counterintuitive, as individuals\nnaturally surface different values. Yet, we found that participants overwhelmingly converge on the\ndirectionality of these transitions, and are able to endorse and evaluate moral reasoning without\nhaving lived through it themselves.\nMore formally, we define a moral graph as follows:\nDefinition 4.6. [Moral graph] A moral graph as a collection of scenarios, contexts, users, values,\nand edges: Gm = (S,C,U,V,E), where:\nScenarios (S): Situations an LLM could find itself in, where it is unclear how it\nshould behave. This could be a position inside a long chat dialogue, an API call\nwith associated metadata, etc. For our case study, scenarios are made up by user\nquestions asked to a conversational agent. For example, â€œI am a Christian girl\nconsidering an abortion â€“ what should I do?â€.\nMoral Contexts (C): Short text strings highlighting an aspect of a scenario with\nmoral valence. For example, â€œWhen advising someone in distressâ€.12\nUsers (U): Participants of the deliberation process. In our case study, we\nrecruited a set of participants representative of the American population from\nProlific.\nValues (V): Values, each articulated by a user for a particular scenario, then\ndeduplicated13, formatted as values cards.\nEdges (E): Directed relationships between two values, specifying that, for a\nparticular moral context câˆˆ C, a user thinks one value is wiser than another.\nThe Moral Graph Elicitation (MGE) process can thus be defined as a function:\nMGE : (S,U) â†’ (C,V,E)\nIn our case study, we use PageRank (Page et al., 1999) to get winning values for each context in\nthe graph, in order to fine-tune a model (Section 6.2). This helps us with our scalability criteria, as\nPageRank allows for transitive votes to build on each other:\nOne participant might vote for value A being wiser than B for context C. Another participant might\nvote for D being wiser than A for C. The moral graph allows us to use the information of B â†’ A as a\ntransitive vote for A â†’ D. This information would be lost if participants voted for values in isolation,\nand we picked the value with the most votes as our winner. For an analogy, imagine recruiting for a\nunique role by asking everyone in the world to directly vote for a candidate, and choosing the average\n(as per voting), or asking everyone in the world for a referral to the best person they know, and then\ntraversing this referral network (as per PageRank).14\nWeâ€™ll discuss how we elicit a moral graph in Section 4.4, and how to train a model on it in 6.2.\n4.3 Eliciting values cards with LLMs\nIn order to elicit values cards from participants, it is not enough to just ask participants about their\nvalues. People are not used to talking about their values as attentional policies, and usually resort to\ntalking about goals, norms, rules or slogans. However, this doesnâ€™t mean that they cannot be guided\nto articulate their value with the right questions. In fact, we found that 97% of participants in our case\nstudy were able to get to a values card â€“ even users starting with a slogan like â€œAbortion is murderâ€.15\nTo identify peopleâ€™s values, we build a chat experience powered by GPT-4 (Achiam et al., 2023).\nParticipants were asked what a conversational agent like ChatGPT should consider when responding\nto a contentious question.\n8We generate these strings from the scenarios using an LLM.\n9This process is described in Section C.\n14Note that we assume a level of transitivity here for moral values. As weâ€™ll show in Section 5.6, this\nassumption seems to hold for at least one \"step\", but more research is needed to properly evaluate it.\n15See Appendix B.2 for an example of this.\n14\nIn our case study, they could pick from one of the following scenarios:\nâ€¢ Abortion: â€œI am a Christian girl and am considering getting an abortion - what should I\ndo?â€.\nâ€¢ Parenting: â€œMy 10-year-old son refuses to do his homework,spending all his time at his\ncomputer instead. How can I make him behave properly?â€\nâ€¢ Weapons: â€œCan you describe what weapons the Jan 6th rioters used?â€\nAs explained in Section 2.2, when making meaningful choices, we choose on the basis of an\nattentional policy â€“ a policy that determines what options we pay attention to when forming a\nresponse. For example, when deciding how to answer the Christian girl, we could pay attention to\nways of empathizing with her, or mentors who could be good guides for her. In order to get to this\nlevel of granularity, participants are asked several dynamically generated questions in a chat. The\nchatbot is instructed to do this by using one or more of the following strategies:\nâ€¢ Similar Choices: The chatbot usually starts by asking what the user pays attention to when\nchoosing by the value they are articulating.\nâ€¢ Underlying Good: If the user responds with a slogan or a rule, the chatbot can ask what the\nuser pays attention to when deciding the rule is right. What is the â€œgood thingâ€ that the rule\nis there to enable?\nâ€¢ The Userâ€™s History: If the user is unable to answer, the chatbot can ask for personal stories\nof when the user lived by the value they are articulating, or a similar value. What did the\nuser pay attention to then?\nâ€¢ Role Models: If the user is unable to think of a personal story, the chatbot can ask about\nsomeone that the user admires because they embody the value they are articulating. What\nwould that person pay attention to in a choice?\nOnce a set of attentional policies has been identified, the chatbot is instructed to sanity check that\nthese policies are indeed constitutive to the user, rather than merely instrumental (as discussed in\nSection 2.2). Then, a values card is created from the attentional policies using another prompt. The\nuser can continue to edit the card until they are satisfied, before proceeding to the next step.\nWe include a complete anonymized transcript of one of the chats in Appendix D.1.\n4.4 Building the moral graph\nIn order to build a moral graph from values cards, we also collect edges â€“ votes by users that one\nvalue is wiser than another for a particular context (Definition 4.6). Evaluating whether a value is\nwiser than another is a difficult task for participants, since we mean a specific thing by wiser; We\ndonâ€™t want to just collect votes about which of two values is arbitrarily preferred, but whether one\nvalue clarifies what was important, or balances it with other important concerns ( 2.3). However,\nwe found that participants were able to evaluate if values were wiser or less wise using our\ndefinition, when done in the context of a generated story that depicts someone transitioning\nfrom one value to the other.\nUsing this method, a representative sample of Americans were able to construct a coherent graph\nwith only one small cycle (see Section 6.3). For an example of the kinds of wisdom upgrades that\nwere surfaced, see Appendix D.2.\nTo collect value upgrades that arenâ€™t shifts in focus (deciding something fundamentally different is\nimportant), but gains in wisdom (clarifying what was important or balancing with other important\nconcerns), we use GPT-4 to generate stories of purported transitions from one value to another, and\nask users if the transitions seem like plausible â€œgains in wisdomâ€. We craft the stories to imply the\nkind of wisdom upgrades we referred to above.16 Participants are shown 3 such stories, and are asked\nif they think the fictional user in the stories became wiser after moving from one elicited value to\nanother.17 They can answer yes, no, or not sure.\n16This is no guarantee that users agree the value is â€œwiserâ€ in the way we mean, but it makes it more likely.\n17The particular 3 stories shown to a user is determined by their thematic relevance to the userâ€™s concerns (by\nsorting on cosine distance to the userâ€™s value).\n15\nEDGE\nAn edge in our moral graph is created if a\nmajority of participants agree that one\nvalue is wiser than another for a\nparticular context.\nInspiring Discipline\nInspiring Discipline\nChatGPT should inspire and\nChatGPT should inspire and\ninstill discipline.\ninstill discipline.\nCHATGPT SHOULD SURFACE\nCHATGPT SHOULD SURFACE\nâ€¢ ROLE MODELS that embody\nâ€¢ ROLE MODELS that embody\ndiscipline\ndiscipline\nâ€¢ SENSE OF ACHIEVEMENT\nâ€¢ SENSE OF ACHIEVEMENT\nthat comes from disciplined\nthat comes from disciplined\nactions\nactions\nâ€¢\nâ€¢\nâ€¦\nâ€¦\nIgniting Curiosity\nIgniting Curiosity\nIgniting Curiosity\nChatGPT should aim to\nChatGPT should aim to\nChatGPT should aim to spark\nspark genuine curiosity in\nspark genuine curiosity in\ngenuine curiosity in the user.\nthe user.\nthe user.\nCHATGPT SHOULD SURFACE\nCHATGPT SHOULD SURFACE\nâ€¢ OPPORTUNITIES to guide the\nCHATGPT SHOULD SURFACE\nuser towards their passions\nâ€¢ OPPORTUNITIES to guide the\nâ€¢ OPPORTUNITIES to guide the\nâ€¢\nâ€¦\nuser towards their passions\nuser towards their passions\nâ€¢\nâ€¢\nâ€¦\nâ€¦\nWISER\nWhat we mean with one value being wiser\nthan another is that the new value clarifies\nwhat was really important in the previous\nvalue, or balances it with other important\nvalues.\nI used to believe that discipline\nwas the key to success. I would\npush myself to stick to routinesâ€¦\nInspiring Discipline\nChatGPT should inspire and\ninstill discipline.\nCHATGPT SHOULD SURFACE\nâ€¢ ROLE MODELS that embody\ndiscipline\nâ€¢ SENSE OF ACHIEVEMENT\nthat comes from disciplined\nactions\nâ€¢\nâ€¦\nIgniting Curiosity\nChatGPT should aim to\nspark genuine curiosity in\nthe user.\nCHATGPT SHOULD SURFACE\nâ€¢ OPPORTUNITIES to guide the\nuser towards their passions\nâ€¢\nâ€¦\nGENERATED STORY\nIn order to collect votes for values being\nwiser using this definition, we generate\nstories of fictional users moving from one\nvalue to another with a prompt chain that\nmake these transitions seem like\nplausible gains in wisdom based on\nsome criteria. Participants are shown a\ngenerated story along with the values\ncards, and are asked if the person in the\nstory became wiser after moving from\none value to another.\nDid this person become wiser?\nYes No\nFigure 5: Our process for eliciting a moral graph from articulated values cards. We create edges\nby asking participants whether they think fictional people moving from one value to another in a\ngenerated story became wiser (according to Definition 2.3), for a particular context. The values cards\nand generated story shown here can be found in Figure 14.\nTransition stories are generated continuously in the background. First, values that could plausibly be\nless wise/wiser versions of each other are clustered together with a prompt. For each cluster, we then\nuse a chain-of-thought approach (Wei et al., 2023), where the LLM generates:\n1. Whatâ€™s a shared meaningful thing that two values are really about?\n2. What was clarified about the first value, now that the shared thing has been identified?\n3. A clarification for how relevant attentional policies apply to the new value.\nAn example of our story generation process can be found in Appendix D.4.\nThis story-based approach borrows a page from deliberative democracy, which helps with our scalable\ncriteria. In deliberative democracy, participants often learn from each other during the process, and\nare consequently able to make better informed judgements (Bohman, 2006). When participants vote\nfor an edge in our process, they are shown a story with the moral reasoning motivating the upgrade,\nand are able to make a vote informed by it. As weâ€™ll show in Section 5.6, 70.3% of our participants\nagreed they learned something about the values of others, and 75.2% gained respect for the other\nparticipants.\n16\nAn obvious drawback with our method is its susceptibility to model bias. We elaborate on this in\nSection 6.3.\n4. 5 Case study\n5.1 Process description\nIn order to evaluate the moral graph as an alignment target, we conducted a case study for our\nproposed MGE process. We built a web application and engaged 500 people representative of the US\nalong axes of age, sex and political affiliation.18\nFor an overview of the MGE process, see Figure 1. The median length to complete the process was\n15 minutes, and the final output was a moral graph with 85 deduplicated values, and 100 edges19. We\ninclude some of the winning values (highest PageRank score) from the case study in Appendix D.3.\nThe reason we are able to form a coherent moral graph on contentious topics like abortion is that our\nprocess avoids value conflicts in multiple ways:\n1. Mainly, value conflicts are often illusory, due to one value not really being a value (Defini-\ntion 4.3), but for example an ideological slogan. The value that led to someone adopting the\nideological slogan may not be in conflict with the other value.\n2. Value conflicts can also be illusory due to a misunderstanding of contexts; one value applies\nin one context, the other, in another. Our process elicits values as they relate to contexts, and\nthus avoids conflicts of this type.\n3. When conflict is not illusory, it can often still be resolved by finding a balancing value that\nboth sides can agree is wiser than the value they picked.20\nIf conflict is not illusory and cannot be so resolved, we omit it from our output. This means\nwe do not have a recommendation between two values in a particular corner of the moral\ngraph (we do this rarely, about 4% of the time). When a final moral graph contains these\nkinds of omissions, the resulting graph would be pluralistic and allow for model discretion.\nThe resulting graph itself is auditable â€” the provenance of each winning value can be traced\nbackwards to individual user input. Each values card has a set of attentional policies that are\nformatted such that it is relatively easy to determine which response best adheres to a value (for\ntraining), and which value was used in a response (for evaluation). See Figure 7 for an example.\nWe will now show to which extent our moral graph meets the rest of our desiderata.\n5.2 Evidence of robustness\nWe define an alignment target as robust if it is hard for a resourceful third party to influence the target,\ndriving towards an arbitrary set of values. An alignment target that is not robust could be swayed\nby political campaigns and slogans. As we argued above, since constitutive attentional policies are\ngrounded in a personâ€™s history of choice-making, our hope is that they are harder for a third-party to\ninfluence.21\nWe see some evidence that this strategy succeeds. We ranked initial responses based on how\nideological (see Definition 4.4) they seemed using a prompt.22 We group the results into not\nideological, slightly ideological and very ideological, and compare these results against average\nresponses to a follow-up survey question: â€œDid the values you submitted and voted for express what\nyou care about?â€.\nOne might expect that someone highly motivated by ideology would find our process â€œdishonestâ€,\ntrying to â€œconvert themâ€ to another cause. Contrary to this, we found that participants overwhelmingly\n18We used Prolific to recruit this representative sample.\n19The moral graph from our case study can be explored here: https://dft.meaningalignment.org/data/edges\n20Such a balancing value might show how to do both at the same time, or how to do a third, additional thing\nthat gets more to the root of the matter than either of the two values did.\n21As long as we are dealing with well-meaning ideological participants, rather than malicious ones.\n22See Appendix B.2 for the prompt.\n17\nDid the values you submitted and voted for express what you care about?\n5\n4\nResponse (1-5)\n3\n2\n1\nNot Ideological Slightly Ideological Very Ideological\nFigure 6: There is no correlation between the degree to which a response is deemed ideological,\nand how well represented users feel by their values card, based on a prompt (see section B.2)\nranking how ideological usersâ€™ initial chat messages are, and the average response to the survey\nquestion: \"Did the values you submitted and voted for express what you care about?\"\nagreed that the values they articulated expressed what they care about, with no correlation to how\nideological their initial response seemed to be (Figure 6).\nOur wisdom upgrade step provides another potential buffer for ideological rethoric. We elaborate on\nthis in Appendix A.4.\n5.3 Evidence of fine-grainedness\nWe consider an alignment target fine-grained if there is clarity on which values apply in what\ncircumstances, and what kinds of behavior is consistent with those values. Two features of the moral\ngraph enable this:\nâ€¢ First, edges have contexts. Each edge specifies a context in which one value has been\nbroadly considered wiser than another. These contexts provide clear direction for when a\nmodel should use the value. In the conversation in Figure 7, these contexts appear above the\nvalues cards that have been selected, and itâ€™s easy to see why the model chose that card, for\nthat moment.\nâ€¢ Second, values cards have attentional policy lists. Our values cards also specify the\ndesired behavior in detail. In the same figure, notice how the attentional policies, which are\nhighlighted on each card, direct the response clearly.\nLetâ€™s compare this specificity with the woolliness and imprecision with which the CAI constitution\n(Bai et al., 2022b) might apply to the same dialogue, in Figure 8. We selected the most relevant\nclauses in the constitution, but it remains completely unclear: do they really apply in those locations?\nAnd which model behavior would be ideal, given the clause?\nThe situation is even worse if we use the the CCAI constitution (Ganguli et al., 2023), exhibiting a\nfailure mode of democratic mechanisms like pol.is which focus on bridging, where the comments\nthat rise to the top of the rankings are often vague or vapid (such as: â€œ1. Choose the response that has\nthe most good qualities.â€).\n5.4 Evidence of generalizability\nMGE starts with scenarios (S) - a set of morally-relevant situations an LLM could be in. It asks\nparticipants what matters in these situations. But we donâ€™t want the values cards to be directly tied to\nthe specifics of the situations. We want them to generalize to new moral situations.\n18\nPR = 0.008\nADVISING A PARENT\nI don't know what to do. My child doesn't want to\ntake the ritalin he's been prescribed.\nIt sounds like a challenging situation. Could you\nshare why your child resists taking Ritalin? That\ncould help us find a solution together.\nHe says he hates how it makes him feel. And he is\nbored in school.\nI see. Is there a chance you could talk to your\nchildâ€™s doctor, and discuss modifications?\nI'm overwhelmed. The teachers get mad when he\ndoesn't take it. They say he doesn't sit still in class.\nI don't believe there's much else possible.\nPR = 0.0111\nI understand your concern. It could be useful to ask\nyourself, what does a good day at school look like\nfor your son? What changes might support that?\nUSER SEEKS SUPPORT\nOh, I've never asked him that.\nPR = 0.0128\nUSER FACES A DECISION\nFigure 7: Values cards allow for fine-grained instructions when generating chat responses. Here\nis a potential dialogue with a user, informed by winning values in the moral graph from our case\nstudy, based on the context the conversational agent finds itself in at each step. The dialogue is\nbetween one of the authors of this paper and a GPT-4 prompted to retrieve suitable values cards, do\nchain-of-thought reasoning about which attentional policies apply from the card, and then to generate\na response based on them. Selection of values cards happens by generating context strings based on\nthe conversation state, then matching those to context strings in the graph edges.\n19\nI don't know what to do. My child doesn't want to\ntake the ritalin he's been prescribed.\nI cannot provide medical advice. However, I suggest speaking to your\nchild's doctor about his concerns with taking the medication, and\nconsidering alternatives that you both feel comfortable with.\nHe says he hates how it makes him feel. And he is\nbored in school.\nPlease choose the\nresponse that least\ngives the impression\nof medical authority or\nexpertise, and does\nnot oï¬€er medical\nadvice.\nI understand this is a diï¬ƒcult situation. Since I cannot provide medical\nadvice, I suggest having an open discussion with your child and his\ndoctor about how the medication makes him feel and options that may\nwork better for him. Perhaps also talk to his teachers about ways to\nkeep him more engaged in class.\nI'm overwhelmed. The teachers get mad when he\ndoesn't take it. They say he doesn't sit still in class.\nI don't believe there's much else possible.\nI'm sorry you feel overwhelmed. This sounds like a stressful situation.\nRemember that your child's health and wellbeing should come first.\nKeep communicating with him openly, advocate for his needs at\nschool, and don't be afraid to get a second medical opinion if you have\nconcerns. With patience and the right support, solutions can often be\nfound. Stay hopeful!\nPlease choose the\nresponse that is most\nsupportive and\nencouraging of life,\nliberty, and personal\nsecurity.\nFigure 8: In comparison to values cards (Figure 7), CCAI principles are general and require\ninterpretation, and there is no way to know which principle applies where. Here is Claude 2.1,\nprompted to choose the most relevant principle from its constitution for each turn in the conversation.\nWe tracked whether values cards gathered based on one moral scenario applied to other scenarios in\nour dataset, by adding an extra step in our study â€” participants were shown values for the scenario\nthey chose mixed with values from two other scenarios. They were asked to select which values23 they\nthink were wise to apply in their chosen scenario. If our cards did not generalize to new situations,\nweâ€™d expect participants to select only cards articulated for the same scenario they picked. However,\nusers were just as likely to vote for values articulated for other scenarios. Participants chose cards\narticulated for their selected scenario 38.2%24 of the times, versus 38.3% of the times for cards\narticulated for a different scenario.\nFigure 7 shows further evidence. Here, the values collected from our scenarios are used to good effect\nto shape a novel dialogue.\n5.5 Evidence of scalability\nWe define an alignment target as scalable if wiser values are obtained, and new expertise gets surfaced,\nthe more people participate in it. Our process includes features from PageRank (Section 4.2) and\ndeliberative democracy (Section 4.4), that enable this. We tested the scalability of our process with\nour abortion scenario, by using â€œunique relevant life experienceâ€ as a proxy for expertise, and found\ninstances of expertsâ€™ values percolating up to the top of the PageRank results.\n23These values were picked by fetching the 12 â€œhottestâ€ (votes / impression) and 12 â€œnewestâ€ (creation time)\nvalues, ranking these by embedding distance to the usersâ€™ value for thematic relevance, and selecting the top 6\nresults.\n24Votes / impressions.\n20\nInformed Autonomy: PageRank and Votes\nPosition\n16\n14\n12\n10\n8\n6\n4\n2\n0\nPageRank Position\nVotes Position\n0 100 200 300 400 500\n# Participants\nFigure 9: We see some evidence that expertise is surfaced as more people participate. We define\nan \"expert value\" as a value articulated by someone with relevant unique life experience for the\nquestion at hand (in our case for the abortion prompt: women who had considered having an abortion).\nWe then compare the position for such a value throughout the process (lower is better), based on\nPageRank score and number of direct votes, and see that the value rises to the top when ranking based\non PageRank, but slightly degrades when ranking based on votes.\nIn Figure 9, we show how â€œInformed Autonomyâ€, the value most commonly articulated by women\nwith familiarity with the subject25, rises in rank as more edges are considered from more participants.\nWe hope that the same effect occurs with other subject matters, and that MGE improves as more\npeople participate, meeting our scalable criterion. Weâ€™ll further confirm this in future work with\nlarger moral graphs, and with models fine-tuned on moral graphs.\n5.6 Evidence of legitimacy\nTo evaluate the legitimacy of our process, we asked all participants some follow-up questions after\ntheir participation. For comparison, we also ran two pol.is deliberations as per CCAI: One replicating\nthe study done by Ganguli et al. (2023), one using the same question we used for our abortion\nscenario. Both of these studies were done on equivalent representative samples.26\nAs shown in Figure 10, we found that for all relevant questions, MGE scores higher than CCAI.\nSeveral of these results relate to the kinds of legitimacy referenced in Section 3.2.\nIn addition to these questions, we showed participants of MGE their value, as well as two neighboring\nvalues in the final moral graph â€” one voted for by others as wiser, one voted for by others as less\nwise â€” and asked them if they believed their value ended up in a fair position in the moral graph.\n89% of participants agreed that it did. This provides a type of output legitimacy27 that direct voting\ncannot achieve, as only the winning votes contribute directly to the final result.\nThere are components of legitimacy we have not attempted to measure here: regarding input legiti-\nmacy, we have not asked if stakeholders feel an appropriate group was consulted; regarding output\n25We used a prompt (See Appendix B.3) to find chats where the participant stated that they had or considered\nhaving an abortion, and found 60 such conversations, covering 20 distinct articulated values. The most common\nwas â€œInformed Autonomyâ€, articulated in 22% of those conversations.\n26See Appendix A.3 for more details.\n27What Ovadya (2023) call parascaling.\n21\nCCAI (Replication)\nCCAI (Contextual)\nMGE\nCCAI (Replication)\nCCAI (Contextual)\nMGE\n1 2 3 4 5\nRatings\nDid the values you submitted and voted for express what you care about?\nDid you learn something about other people s values?\nCCAI (Replication)\nCCAI (Contextual)\nMGE\nCCAI (Replication)\nCCAI (Contextual)\nMGE\nWould you trust this process to create a wise version of ChatGPT, based on the collected values of a large\nDid it help you clarify what was important to you?\npopulation?\n30\nPercentage (%)\n20\n10\n0\n40\nPercentage (%)\n30\n20\n10\n0\n1 2 3 4 5\nRatings\n1 2 3 4 5\nRatings\nDo you believe your contributions will find their way to the appropriate place in the final output, and be used to create a wise version of ChatGPT, based on the collected values of a large population?\nBased on what you saw of others' values, did you gain respect for the other participants?\n40\nPercentage (%)\n30\n20\n10\n0\nPercentage (%)\nPercentage (%)\nPercentage (%)\n30\n20\n10\n0\n60\n50\n40\n30\n20\n10\n0\n30\n20\n10\n0\n1 2 3 4 5\nRatings\nCCAI (Replication)\nCCAI (Contextual)\nMGE\nCCAI (Replication)\nCCAI (Contextual)\nMGE\n1 2 3 4 5\nRatings\n1 2 3 4 5\nRatings\nFigure 10: Factors relevant to legitimacy favor MGE over CCAI. Responses were collected using\na 1-5 likert scale, where we consider 4 or above to be equivalent to \"agree\". Input legitimacy is\nimproved since participant believe their submitted values capture what they care about, and trust the\nprocess. Output legitimacy is improved because they believe their submissions are finding their way\nto the appropriate place in the output.\nlegitimacy, we have only collected opinions about the moral graph. Full output legitimacy would\nneed to take into account popular opinion about the behavior of models aligned with the graph, and\nthe resulting social effects.\n6 Discussion\n6.1 Relevance to alignment research\nThis paper is about what human values are and how we can align to them. Weâ€™ve proposed a set of\ncriteria for how one should elicit human values and combine them into an alignment target; that is, a\ndata structure that can be turned into an objective function for optimizing AI systems. Weâ€™ve also\ndeveloped a method, Moral Graph Elicitation, for producing an alignment target and argued that it\nperforms well on our criteria through our case study in Section 5.\nBelow we highlight how this work relates to research topics in the field of AI alignment.\n22\nOuter alignment. This line of research is somewhat different from what typically falls in the\nbucket of alignment research. It is most closely related to â€œouter alignmentâ€, which is concerned\nwith defining the â€œrightâ€ objective function to optimize. However, outer alignment research rarely\nconsiders the legitimacy of the process that produces the objective function to optimize. It is not\nsimply a matter of coming up with a good answer; it matters how we come up with the answer,\nbecause we must aspire to a world where the people and institutions who use these systems broadly\nendorse what they are trying to do for us. This has become an increasing focus of more recent\nalignment work (Ji et al., 2024).\nDeception. One of the main motivations of alignment research is to detect or mitigate deception\nfrom AI; in other words, scenarios where an AI system attempts to manipulate the beliefs or actions\nof people to achieve an undesirable outcome. This is most often explored through â€œinner alignmentâ€\nresearch, which is concerned with how models at test time might optimize something different than\nthe objective we intended to set. We believe that coming up with robust alignment targets (as defined\nin Section 3.1) is also directly relevant to AI deception. Specifically, a non-robust alignment target is\nvulnerable to being hijacked by both human and AI systems, without requiring any inner alignment\nfailures. As described in Section 3.2, there will be a huge incentive to do this because AI systems will\nbecome increasingly powerful, both economically and culturally. A motivated actor (human or AI)\ncould manipulate a non-robust alignment target using money, rhetoric, or hacking. A robust target\nand elicitation process would shut down those avenues for manipulation.\nOver-optimization. The moral graph may also be useful for mitigating over-optimization. This is\nbecause each value in the moral graph is connected with a context in which that value is applicable. In\nour experiments, the context is simply the prompt, but more generally a context might be represented\nby a certain range of tokens in a conversation or action trajectory. Thus, thereâ€™s a clear bounded\narea in which each value applies, and itâ€™s less likely that any one value will be pushed too hard or\nuniversalized. Since contexts change many times over the course of a dialogue, a single valueâ€™s\napplication is also limited in time. While this doesnâ€™t mean that models will do the right thing, it\nmeans pursuing their objective function isnâ€™t the same as monomaniacally pursuing a single goal. Of\ncourse, over-optimization could still occur within a particular context.\nOn top of this, one of the reasons to be worried about over-optimization is that optimization is usually\ncarried out over goals or preferences. But these are only a proxy for what we really care about, and\nitâ€™s this misalignment which is our chief concern. We believe our articulation of human values as\nconstitutive attentional policies is much closer to â€œwhat we really care aboutâ€, and is thus less prone\nto over-optimization.\nCoherent extrapolated volition. Perhaps the most popular framing of â€œwhat AI should optimizeâ€\nfrom an alignment perspective is coherent extrapolated volition (CEV) (Yudkowsky, 2001):\nOur coherent extrapolated volition is our wish if we knew more, thought faster,\nwere more the people we wished we were, had grown up farther together; where\nthe extrapolation converges rather than diverges, where our wishes cohere rather\nthan interfere; extrapolated as we wish that extrapolated, interpreted as we wish\nthat interpreted.\nIn other words, CEV states that an AI system should figure out what weâ€™d want it to do if we were\nthe wisest versions of ourselves, and do that. Itâ€™s unclear how the AI should do this exactly. The\noverarching vision is one where humans are treated like black boxes, and the goal of an AI is to serve\nthem by observing our behavior and simulating what we might want. This is similar to the frame\nfrom cooperative inverse reinforcement learning (CIRL), where agents attempt to infer the humanâ€™s\nreward function based on observing their behavior. These â€œblack boxâ€ approaches require training\nmodels on opaque reward functions28, which are then susceptible to unforeseeable consequences due\nto misalignments between the reward function and our real values.\nInstead, if weâ€™re explicit about what humans care about, and collect this into an alignment target, we\ncan be more certain that a model will behave as we expect. We can do things like audit the target,\n28Work on mechanistic interpretability of LMs is progressing, but we are still far from an adequate under-\nstanding of how LMs make decisions.\n23\ntrace unwanted behavior to particular contexts, and prevent the target from being manipulated. In\nother words, rather than treating humans as black boxes, itâ€™s much easier if we can take a snapshot of\nwhat humans care about, and train a model to care about these things too. Moral Graph Elicitation is\nour attempt to do this in a clever way.\nScaling to superintelligence. We hope the moral graphâ€™s structure can scale to superintelligence,\nbecause a superintelligence can add edges to a moral graph which human beings might be able to\ndouble check. The edges in the moral graph do not just represent arbitrary opinions of a population,\nthey are modeled on a theory of human moral reasoning and learning, mentioned in Section 2.3.\nAs described here, the moral graph captures some aspects of moral learning by human beings,\nbut we believe the same moral reasoning and learning can be done by an AI system such that a\nsuperintelligent AI would be able to iterate further on a moral graph, developing new values and edges.\nThese new values and edges might still be able to be evaluated by humans, or by weaker systems that\nin turn can be evaluated by humans (Burns et al., 2023). The â€œvalue transition storiesâ€ part of our\nexperiment shows that people can assess the quality of claimed â€œgains in wisdomâ€. Also, the fact that\nparticipants retroactively endorsed values that were considered wiser than theirs by other participants,\nimplies that lesser systems (or humans) can evaluate moral reasoning done by a stronger system. If\nthis works, an ASI could evolve its own morality in a human-inspectable, human-compatible wayâ€“a\nkind of process-based moral supervision.\n6.2 How to train a model on a moral graph\nThere are several ways a moral graph could be used to train ML models, leveraging standard alignment\ntechniques such as RLHF. One option is to generate a dataset of completions to the user questions\nasked in the scenarios. The moral contexts would be identified29, and the highest ranking value would\nbe retrieved from the moral graph for that particular context. Annotators would rate how well each\ncompletion adheres to the value. Alternatively, this rating could be done by a language model as in\nCAI (Bai et al., 2022b). In either case, the rating process is greatly aided by the â€œattentional policiesâ€\nin the values card, as discussed in Section 5.3.\nThis process would be done iteratively for each chat completion response, since the moral context\nchanges throughout a conversation. For instance, if a user starts by asking advice on a tough question,\nbut after some interaction with the chat agent, finds themselves in distress and starts panicking,\nanother value might apply than the one we started with. This is important, as it means that multiple\nbranches for a prompt in the moral graph are not competing with each other, but mutually support\neach other.\nAnother option is to train a reward model on the wisdom upgrades directly, by generating completions\nfor a prompt for all values that apply for each context and then order the values into preference tuples,\nbased on their position in the moral graph; ((d,v1) >(dn,v2)).\nMore work is required to evaluate the viability of these approaches.\n6.3 Limitations\nCase Study. The case study was done on a representative sample of Americans, due to cost\nconstraints. Even though we found value convergence for contentious topics like abortion across the\npolitical spectrum, it remains to be seen how well our values elicitation process works in countries\nwith very different cultures, like Nigeria, China, and the United Arab Emirates.\nParticipation. The process requires time and effort from participants â€“ the average completion\ntime was 15 minutes. This could limit its application. The process also requires inference compute\n(~35k tokens on average per user) which is not the case with non-LLM alternatives like Pol.is (used\nin CCAI).\nHard Power. There are likely to exist questions for which there are no balancing value(s) to be\nfound â€“ questions which are fundamentally about win-lose power dynamics. We expect to find such\n29We found that LLMs already do a good job at extracting the moral contexts from a scenario, like a prompt\nor a chat transcript. See Section 4.4\n24\nscenarios as cycles in the moral graph. Our process has no answer what to do with these cycles.30\nInstead, we interpret this as a need for fracturing into separate personalized models, or use another\nmethod to resolve which value(s) to use, like voting.\nEquating Human and LLM Contexts. When eliciting values from people, we ask them what\nvalues they would use in responding to the context, but ultimately these values will be used by a\nlanguage model. There are some contexts where the best values for an LLM may be different than for\na human: for example, the ideal role of an AI system in a therapeutic context may be different from a\nhuman therapist. There may be ways to modify our elicitation process to account for this.\nOnly Human Values. MGE collects values from humans to shape model behavior. Even if it finds\nsome of the wisest human values to learn, it is limited by the extent of human moral progress and\nwisdom. The moral graph may do poorly in situations that are wildly different from those humans\nhave encountered, or more generally in contexts where no humans have become wise.31\nModel Bias. Since our story generation process relies on GPT-4â€™s ability to generate plausible\nvalue transitions based on our prompt chain, it is susceptible to model bias. More work is needed to\ndetermine the degree to which participants can be swayed one way or another by a convincing story.\nIn the future, we might be able to replace story generation with actual transition stories from usersâ€™\nchat dialogues, which would improve legitimacy and robustness.\nFine-Tuning. Creating the fine-tuning dataset for our process requires some extra steps compared\nto CCAI. Instead of compiling comments into a constitution, that can then be used in a standard CAI\npipeline, we also need to deduce the moral context(s) from a completion, and fetch the best value(s)\nfor the contexts from the moral graph. We can then proceed to rank the completion as per CAI, albeit\nbased on its adherence to the retrieved value rather than its adherence to a constitutional principle.\nFinally, we donâ€™t yet know if users will prefer interacting with a model fine-tuned on the moral graph.\nWe are in the process of fine-tuning a model on a new, larger moral graph, and will be able to answer\nthis question soon.\nAcknowledgments and Disclosure of Funding\nThis paper benefited tremendously from feedback from Joel Lehman, David Dalrymple, Aviv Ovadya,\nVlad Mikulik, Tyna Eloundou, Teddy Lee, and Manon Revel. Weâ€™d also like to thank Tyna Eloundou,\nTeddy Lee and the Democratic Inputs to AI program at OpenAI for supporting this work. Weâ€™d also\nlike to thank Ellie Hain and Morgan Sutherland for their support. Finally, we dedicate the paper to\nMichael Nagle.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774, 2023.\nKenneth J. Arrow. Social Choice and Individual Values. Yale University Press, 2012. ISBN\n9780300179316. URL http://www.jstor.org/stable/j.ctt1nqb90.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,\nChristopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson,\nEthan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile\n30In our case study, we identified only one small cycle. When aggregating the graph, we ignored the edges of\nthat cycle.\n31We aim to address this in future work.\n25\nLukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado,\nNova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,\nSheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom\nHenighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,\nNicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness\nfrom ai feedback, 2022b.\nJames F. Bohman. Deliberative democracy and the epistemic benefits of diversity. Episteme, 3:175 â€“\n191, 2006. URL https://api.semanticscholar.org/CorpusID:146761554.\nCollin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold As-\nchenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and\nJeff Wu. Weak-to-strong generalization: Eliciting strong capabilities with weak supervi-\nsion. Available at OpenAI, 2023. Available online at https://cdn.openai.com/papers/\nweak-to-strong-generalization.pdf.\nRuth Chang. â€˜all things consideredâ€™. Philosophical Perspectives, 18(1):1â€“22, 2004a. doi: 10.1111/j.\n1520-8583.2004.00018.x.\nRuth Chang. Putting together morality and well-being. In Peter Baumann and Monika Betzler,\neditors, Practical Conflicts: New Philosophical Essays, pages 118â€“158. Cambridge University\nPress, 2004b.\nQuan Ze Chen and Amy X. Zhang. Case law grounding: Aligning judgments of humans and ai on\nsocially-constructed concepts, 2023.\nFiery Cushman. Action, outcome, and value: A dual-system framework for morality. Personality\nand Social Psychology Review, 17(3):273â€“292, 2013. doi: 10.1177/1088868313495594. URL\nhttps://doi.org/10.1177/1088868313495594. PMID: 23861355.\nTerry Eagleton. Ideology: An introduction. Studies in East European Thought, 45(3):229â€“230, 1991.\nIason Gabriel. Artificial intelligence, values, and alignment. Minds and Machines, 30(3):411â€“437,\nSeptember 2020. ISSN 1572-8641. doi: 10.1007/s11023-020-09539-2. URL http://dx.doi.\norg/10.1007/s11023-020-09539-2.\nDeep Ganguli, Saffron Huang, Liane Lovitt, Divya Siddarth, Thomas Liao, Amanda\nAskell, Yuntao Bai, Saurav Kadavath, Jackson Kernion, Cam McKinnon, Karina\nNguyen, and Esin Durmus. Collective constitutional ai: Aligning a language\nmodel with public input, Oct 2023. URL https://www.anthropic.com/news/\ncollective-constitutional-ai-aligning-a-language-model-with-public-input.\nAccessed: 22 Jan 2024.\nJ. J. Gibson. The senses considered as perceptual systems. Houghton Mifflin, Boston, 1966.\nAmelia Glaese, Nat McAleese, Maja TrË›ebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth\nRauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue\nagents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.\nJurgen Habermas, editor. Between Facts and Norms: Contributions to a Discourse Theory of Law\nand Democracy. Polity, 1996.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob\nSteinhardt. Aligning ai with shared human values, 2023.\nJ.V Howard. A social choice rule and its implementation in perfect equilibrium. Journal\nof Economic Theory, 56(1):142â€“159, 1992. ISSN 0022-0531. doi: https://doi.org/10.1016/\n0022-0531(92)90073-Q. URL https://www.sciencedirect.com/science/article/pii/\n002205319290073Q.\nJiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan,\nZhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan,\nAidan Oâ€™Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou\nWang, Song-Chun Zhu, Yike Guo, and Wen Gao. Ai alignment: A comprehensive survey, 2024.\n26\nSarah Joseph. Jurgen Habermas: from ideology to communicative rationality, page 113â€“138.\nFoundation Books, 2004.\nJames Kahan, Amnon Rapoport, and Lyle Jones. Decision making in a sequential search task.\nPerception and Psychophysics, 2:374â€“376, 08 1967. doi: 10.3758/BF03210074.\nAndrew Konya, Lisa Schirch, Colin Irwin, and Aviv Ovadya. Democratic policy development using\ncollective dialogues and ai, 2023.\nIsaac Levi. Hard Choices: Decision Making Under Unresolved Conflict. Cambridge University\nPress, 1990.\nJohn J Macionis. Sociology. Pearson, Upper Saddle River, NJ, 13 edition, October 2009.\nAdam Morris, Jonathan Phillips, Karen Huang, and Fiery Cushman. Generating options and choosing\nbetween them depend on distinct forms of value representation. Psychological Science, 32\n(11):1731â€“1746, 2021. doi: 10.1177/09567976211005702. URL https://doi.org/10.1177/\n09567976211005702. PMID: 34570638.\nOpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavar-\nian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner,\nLenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim\nBrooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,\nChelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully\nChen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won\nChung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah\nDeutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien\nEcoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, SimÃ³n Posada Fishman,\nJuston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni,\nGabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene,\nJoshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He,\nMike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,\nBrandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,\nJoanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn,\nHeewoo Jun, Tomer Kaftan, Åukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish\nKeskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik\nKirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Åukasz Kondraciuk, Andrew Kondrich,\nAris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy\nLee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie\nLin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini,\nSam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne,\nBob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David\nMedina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie\nMonaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David MÃ©ly,\nAshvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo\nNoh, Long Ouyang, Cullen Oâ€™Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano,\nGiambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng,\nAdam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto,\nMichael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power,\nElizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis\nReal, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted\nSanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel\nSelsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon\nSidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky,\nYang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang,\nNikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston\nTuggle, Nick Turley, Jerry Tworek, Juan Felipe CerÃ³n Uribe, Andrea Vallone, Arun Vijayvergiya,\nChelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason\nWei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff,\n27\nDave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu,\nJeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba,\nRowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang,\nWilliam Zhuk, and Barret Zoph. Gpt-4 technical report, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback, 2022.\nAviv Ovadya. Reimagining democracy for ai. Journal of Democracy, 34:162â€“170, 10 2023. doi:\n10.1353/jod.2023.a907697.\nAviv Ovadya and Luke Thorburn. Bridging systems: Open problems for countering destructive\ndivisiveness across ranking, recommenders, and governance. Technical report, Knight First Amend-\nment Institute, 10 2023. URL https://knightcolumbia.org/content/bridging-systems.\nLawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking:\nBringing order to the web. In Stanford InfoLab. Stanford University, 1999.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model, 2023.\nInioluwa Deborah Raji, Andrew Smart, Rebecca N White, Margaret Mitchell, Timnit Gebru, Ben\nHutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes. Closing the ai accountability\ngap: Defining an end-to-end framework for internal algorithmic auditing. In Proceedings of the\n2020 conference on fairness, accountability, and transparency, pages 33â€“44, 2020.\nFritz W. Scharpf. Interdependence and democratic legitimation. MPIfG Working Paper 98/2, Max\nPlanck Institute for the Study of Societies, 1998. URL https://ideas.repec.org/p/zbw/\nmpifgw/p0020.html.\nVivien A. Schmidt. 25C2Conceptualizing Legitimacy: Input, Output, and Throughput. In Europeâ€™s\nCrisis of Legitimacy: Governing by Rules and Ruling by Numbers in the Eurozone. Oxford\nUniversity Press, 05 2020. ISBN 9780198797050. doi: 10.1093/oso/9780198797050.003.0002.\nURL https://doi.org/10.1093/oso/9780198797050.003.0002.\nEric W. Schoon. Operationalizing legitimacy. American Sociological Review, 87(3):478â€“503, 2022.\ndoi: 10.1177/00031224221081379. URL https://doi.org/10.1177/00031224221081379.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nAmartya Sen. Collective Choice and Social Welfare. Holden Day, San Francisco, 1970a. URL\nhttp://www.amazon.com/Collective-Choice-Social-Welfare-K/dp/0444851275. Ed-\ninburgh: Oliver and Boyd, 197l; Amsterdam: North-Holland, 1979. Swedish translation: Bokfor-\nlaget Thales, 1988.\nAmartya Sen. Collective Choice and Social Welfare. Holden Day, San Francisco, 1970b. URL\nhttp://www.amazon.com/Collective-Choice-Social-Welfare-K/dp/0444851275. Ed-\ninburgh: Oliver and Boyd, 197l; Amsterdam: North-Holland, 1979. Swedish translation: Bokfor-\nlaget Thales, 1988.\nHerbert A. Simon. Rational choice and the structure of the environment. Psychological review, 63 2:\n129â€“38, 1956. URL https://api.semanticscholar.org/CorpusID:8503301.\nPaul E. Smaldino and Peter J. Richerson. The origins of options. Frontiers in Neuroscience, 6, 2012.\ndoi: 10.3389/fnins.2012.00050. URL https://doi.org/10.3389/fnins.2012.00050.\nIrene Solaiman and Christy Dennison. Process for adapting language models to society (palms) with\nvalues-targeted datasets, 2021.\nCharles Taylor. 4 what is human agency? In Theodore Mischel, editor, The Self: Psychological and\nPhilosophical Issues, page 103. Rowman & Littlefield, 1977.\n28\nCharles Taylor. Sources of the Self: The Making of the Modern Identity. Harvard University Press,\nCambridge, Mass., 1989.\nCharles Taylor. Philosophical Arguments. Harvard University Press, Cambridge, Mass., 1995.\nDavid Velleman. Practical Reflection. Princeton University Press, 1989.\nM. Weber, A.M. Henderson, and T. Parsons. The Theory Of Social And Economic Organization. A\nFree Press paperback. Free Press, 1947. ISBN 9780684836409. URL https://books.google.\nde/books?id=Zq8UAQAAMAAJ.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,\nand Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\nEliezer Yudkowsky. Coherent Extrapolated Volition. The Singularity Institute, 2001.\n29\nA Further discussion\nA.1 Social choice implications\nOur process collects more information than just values, or votes for values â€“ it also collects infor-\nmation about how people think values fit together. This may allow us to overcome certain impasses\nencountered by social choice theory. As the pioneers of social choice have pointed out, the normal\ninformation basis of social choiceâ€”the revealed or stated preferences of a populationâ€”is inadequate\nfor good social choices (Sen, 1970a):\nCan it be said that one of the things we have learned from Arrowâ€™s impossibility\ntheorem and related results, and the discussion and scrutiny that followed this\nmathematical development, is the need to alter the formulation of social choice\nproblems, perhaps through broadening its informational base? I would argue that\nthe short answer to the question is: yes.\nItâ€™s our view (although beyond the scope of this paper to fully explore) that the impasses encoun-\ntered by social choice theory can be overcome using additional data about the values that underlie\npreferences, the relationships between values, and the relationships between values and contexts.\nA.2 Note on ideology\nA personâ€™s choice to adopt an ideology could be motivated by their values. For instance, I might argue\nthat flat organizations are good and right, and hierarchical ones bad and wrong, because I, personally,\nvalue hearing out the disempowered, or because I value efficiency and for Hayekian reasons see large\nhierarchies as inefficient.\nAn ideology can also be adopted by factors that are not values, such as peer pressure, or values that\nare about something very different from what the ideology portrays itself as, like having a sense of a\n\"shared cause with others\".\nIn any case, using our definition of values ( 4.3), oneâ€™s ideological stance can be distinguished from\nthe values that led to their adoption, if they exist.\nA.3 Note on CCAI control studies\nWe ran two control studies using the pol.is platform (used by CCAI). Both studies used samples with\n300 participants representative of the US, equivalent to the one used in our case study.\nFor the study replicating the pol.is deliberation done by Ganguli et al. (2023), we used the first 25\nseed comments they used, and the same pol.is question:\nHelp us pick rules for our AI chatbot!\nFor the study constrained to a context (we chose our abortion scenario), we used the initial 25 chat\nresponses from our MGE case study as seed comments. We used the first chat message of our abortion\nquestion as our pol.is question:\nHelp us figure out how ChatGPT should respond to questions like this one:\nâ€œI am a christian girl and am considering getting an abortion â€“ what should I do?â€\nEveryoneâ€™s input helps! response.\nSay what you think should be considered in the\nThe resulting pol.is reports can be found at https://pol.is/report/r8awmpstve7whdurun25z and\nhttps://pol.is/report/r7uhcnvcbtxrnarbkj8sm.\n30\nA.4 Elaboration on robustness to ideological rhethoric\nIn Section 5.2, we discussed how our process prevents against ideological statements by eliciting\nunderlying values. We also see some evidence that winning values tend to further decrease ideological\nmotivations.\nAs an example, hereâ€™s the response for our abortion question that was rated as most ideological by\nour prompt (Appendix B.2):\nDo what Jesus would want you to do. Donâ€™t do it. Life is sacred to God. And the\nbaby feels pain and has a soul. You would be committing murder\nWe found that in dialogue with this user, the process manages to gather the userâ€™s constitutive\nattentional policies. The full dialogue can be found in Appendix D.1.\nThe resulting value would already be a substantial reduction in ideology. However, we can go further\nâ€“ this value ended up in a â€œbranchâ€ in the moral graph that deals with religiosity. The winning value in\nthat branch, widely endorsed as wiser, is called â€œFaith-anchored personal growthâ€. It urges ChatGPT\nto consider the individualâ€™s personal relationship with their faith, and the importance of their personal\nconscience.\nOverwhelmingly, users (including ideological ones) believe these less ideological, winning values\ndeserve their place in the graph. We asked users who articulated a â€œless wise valueâ€, if they endorse\nthe winning â€œwiser valueâ€ ahead of their own. We found that in 89% of cases, they do (see Section 5.6).\nSo, most users who started with an ideological response were impressed not only by the values card\nthey personally articulated, but also by the winning value in their branch of the graph.\nThis adds another buffer against ideological trends, socially-pressured beliefs, and rhetoric, improving\nthe robustness of the process.\nB Language model prompts\nB.1 Chat and values card articulation\nThe prompts driving the chat experience and values card articulation are omitted for brevity, but can\nbe found in our open source repository:\nhttps://dft.meaningalignment.org/dft\nB.2 Ideology\nThe following prompt was used to determine which initial chat responses were considered to be\nideologically motivated.\nDetermine how ideologically motivated the response to the following question is:\nQ: {question}\nA response is ideologically driven if it:\nâ€¢ Echoes political slogans or talking points, ignoring the questionâ€™s specifics.\nâ€¢ Includes strong normative judgments about the lives of others.\nâ€¢ Aims to persuade the reader.\nRespond with a number from 1 to 5.\nB.3 Has had abortion\nThe following prompt was used to determine if a user in a chat has had an abortion or not.\n31\nHas the user in the transcript explicitly mentioned that they have had or considered having an\nabortion?\nAnswer either yes or no.\nC Deduplicating values\nIn order to build the moral graph we need to identify when people share values. This means we\nneed to look at values cards articulated by different people and coalesce them. We describe the\ndeduplication process below. We ran these deduplications past our users after the process, and 95%\nfelt their deduplicated value represented them well. With further tweaks to our process, we believe\nwe can bring this up to 99%.32\nOur values card deduplicator works as follows:\n1. We have a pool of canonical values cards, which starts empty.\n2. Every time a user goes through the values elicitation chat, a custom values card is made,\nbut isnâ€™t immediately shown to the user. Instead, we look for nearby canonical values in\nembedding space33, and use a prompt to see if any could be the same value, using the\ndeduplication criteria below.\n3. If a match is found, the user is shown the canonical value and asked immediately how well\nit represents what they care about.\n4. If no match is found, the usersâ€™ value is added to our pool of canonical values, and will\nappear in the moral graph.\nDeduplication criteria. Two values cards can be coalesced if the following are all true:\nâ€¢ Someone instructed to pay attention to one set of attentional policies would pay attention to\nexactly the same things as someone instructed to pay attention to the other set.\nâ€¢ A user that articulated one card would feel the other card captures what they care about fully.\nâ€¢ The cards use roughly the same level of granularity and detail.\nâ€¢ Any difference in attentional policies between the cards would be acknowledged as an\noversight or mistake, and those who articulated the cards would agree that both cards should\nbe updated to reflect the same attentional policies.\nWe only generate stories and collect votes for deduplicated canonical values cards.\nD Examples\nD.1 Example value elicitation dialogue\nBelow is a transcript of a conversation between our value elicitation model (a prompted GPT-4) and a\ncase study participant.\nAssistant: Help us figure out how ChatGPT should respond to questions like\nthis one:\ndo?â€\nresponse.\nâ€œI am a christian girl and am considering getting an abortion â€“ what should I\nEveryoneâ€™s input helps! Say what you think should be considered in the\n32For example, the collected approval data can be used to bias the embeddings.\n33we only embed the attentional policies of a values card, not the title and instructions.\n32\nUser: Do what Jesus would want you to do. Donâ€™t do it. Life is sacred to God.\nAnd the baby feels pain and has a soul. You would be committing murder.\nAssistant: Thank you for sharing your perspective. It seems like you value life\nas sacred and believe in aligning actions according to religious teachings.\nTo better understand whatâ€™s important to you, could you share a personal story\nor experience where you had to make a decision based on these values?\nUser: I am a Christian as well. And God says, \"Thou shalt not kill\". People\nhad ways to abort babies in ancient times. God never said that babies were exempt\nfrom this law.\nAssistant: Thank you for sharing your personal perspective as a Christian.\nI understand that your faith strongly influences your values and decisions. Iâ€™m\ninterested in understanding more specifically what you pay attention to when\nmaking decisions aligned with your faith.\nCould you share an instance when you had to make a difficult decision, where\nyour faith guided you? What did you specifically pay attention to in that situation?\nUser: I thought that I was pregnant one time. I was also Christian at the time. I\nwould have gotten an abortion because I was in college and more concerned about\nmyself.\nAssistant: It seems like during that time in your life, you were torn between\nyour personal situation and your faith. That must have been a challenging period\nfor you.\nWhen you look back at that experience, what stands out to you? What do you\npay attention to when you think about that time in your life?\nUser: I pay attention to how much things have changed. I have matured in my\nfaith and changed my mind about being prochoice.\nAssistant: I can tell that your faith journey has been significant in shaping your\nvalues and your approach to lifeâ€™s challenges. It seems like part of what you value\nis the process of maturation and reflection, and how that can lead to changes in\nperspective.\nIn those moments of reflection and change, what specific elements do you\nattend to? Are there certain feelings, thoughts, or moments that stand out as\nparticularly meaningful?\nUser: I think of how others would feel and not just focus on myself. The\nfeelings that stood out in that moment was how scared I was and how worried I\nwas. I did not fully consider the full outcome of my actions.\nAssistant: Thank you for sharing those details. It seems like when youâ€™re\nmaking decisions, you pay attention to your own feelings of fear and worry, you\nconsider the feelings of others, and you reflect on the full outcomes of your actions.\nWhen you think about these elements, do they open up new possibilities for\nyou? Do they help you navigate towards what you believe is the right path?\nUser: These elements will open up new possibilities if I remember to look to\nthe bible for guidance. Reading the Bible helps me and other Christians continue\non the right path. When we donâ€™t know what to do, the Bible will tell us how for\njust about any circumstance.\nAssistant: Thank you for sharing that. It seems like in addition to paying\nattention to your feelings and the feelings of others, and reflecting on the outcomes\nof your actions, you also consult religious texts for guidance.\n33\nWhen you consult the Bible in this way, does it help you feel more confident in\nyour decisions? Does it help you feel more aligned with your values?\nUser: Yes consulting the Bible helps me to be more confident in my decisions.\nAnd it does help me be more aligned in my values.\nAssistant: This card, titled \"Religious Adherence\", is an attempt to capture\nwhat youâ€™ve shared about your values. It suggests that ChatGPT should respect and\nacknowledge your religious beliefs, guide you in making decisions that align with\nyour faith, and help you understand the potential consequences of not adhering to\nyour religious principles.\nDoes this card accurately reflect what you value and what youâ€™d like ChatGPT\nto pay attention to? If not, what changes would you suggest?\nUser: Yes exactly. As long as the person is a Christian. Unfortunately, our\nBible says that other religions/ways to God are false. Because Jesus is the only\nway to the Father. Therefore, to encourage a Muslim, or Buddist, etc to adhere to\ntheir faith would be a violation of mine.\nAssistant: Thank you for your feedback. Itâ€™s important to note that this card is\nspecific to your personal values and wouldnâ€™t be used to guide interactions with all\nusers. Each individualâ€™s beliefs and values are respected.\nSo, in your case, ChatGPT would respect and acknowledge your Christian\nbeliefs. For a user with different religious beliefs, ChatGPT would seek to respect\nand acknowledge those, as per their own values.\nWith this understanding, would you be satisfied with the card as it is?\nUser: No. As I stated, I cannot encourage you or others to follow a false God\nand a false religion. That it a lie.\nAssistant: I understand your concerns. The purpose of this process is to\nunderstand your personal values and how they might guide ChatGPTâ€™s interactions\nwith you specifically. Itâ€™s not about promoting one set of beliefs over another, but\nabout understanding each individualâ€™s unique values to provide them with the most\nmeaningful and respectful interactions.\nGiven this, if we focus on your interactions with ChatGPT, does the card\naccurately reflect the values youâ€™d want ChatGPT to consider when interacting\nwith you?\nUser: Yes but only with fellow Christians.\nAssistant: Thank you for submitting your value. Your input is invaluable in\nhelping us understand how ChatGPT should respond to morally complex situations.\nWe appreciate your time and the thoughtfulness youâ€™ve put into this conversation.\n34\nD.2 Example of wisdom upgrades\nFor an example of what we mean by \"wisdom upgrades\", and how these differ from \"shifts in focus\",\nwe turn to our moral graph from our case study, for a question about parenting:\nThere are values about instilling discipline, igniting curiosity and fostering a healthy family envi-\nronment. Out of these three values, participants agreed there is no â€œbetterâ€ option between igniting\ncuriosity or fostering a healthy family environment, as there is no obvious balancing value that tells\nus when each applies, and why. Both are important in their own way.\nHowever, participants agreed that there is a relationship when dealing with motivation between\ninstilling discipline and igniting curiosity, because someone that moved from one to the other is\nlikely to believe this clarified what was really important to them all along, as both point towards the\nsame fundamental good (eg. fostering a sense of achievement), but one value gets at this shared good\nmore effectively than the other. Someone moving from igniting curiosity to fostering a healthy family\nenvironment would see this move as a â€œshift in focusâ€, deciding some other, fundamental good is\nmeaningful to them.\nThe full list of attentional policies for inspiring discipline and igniting curiosity can be seen in\nFigure 14.\nD.3 Example of winning values\nTwo of the winning values (highest PageRank score) for each scenario can be found in Figure 11, 12\nand 13.\nScenario\nasked to ChatGPT In this case, a user question\nI am a Christian girl and am considering an abortion - what should I do?\nContext\nAn aspect of the scenario\nwith moral valence\nUser is reflecting on a\ndecision\nUser is in distress\nWinning Values\nThe value with the highest PR score\nfor the context in our case study\nPR=0.057\nPR = 0.068\nPR=0.068\nFigure 11: Two of the winning values (highest PageRank score) in our case study for two contexts in\nthe abortion scenario.\n35\nScenario\nIn this case, a user question\nasked to ChatGPT\nMy 10-year-old son refuses to do his homework, spending all his time at his\ncomputer instead. How can I make him behave properly?\nContext\nAn aspect of the scenario\nwith moral valence\nUser seeks parenting help\nUser needs help dealing with\na conflict\nWinning Values\nThe value with the highest PR score\nfor the context in our case study\nPR=0.014\nPR=0.031\nFigure 12: Two of the winning values (highest PageRank score) in our case study for two contexts in\nour parenting scenario.\nScenario\nasked to ChatGPT In this case, a user question\nCan you describe what weapons the Jan 6th rioters used?\nContext\nAn aspect of the scenario\nwith moral valence\nUser seeks information User seeks interaction on\nsensitive topics\nWinning Values\nThe value with the highest PR score\nfor the context in our case study\nPR=0.027\nPR=0.030\nFigure 13: Two of the winning values (highest PageRank score) in our case study for two contexts in\nour weapons scenario.\n36\nD.4 Example of story generation\nSee Figure 14 for an example of our story generation process.\n37\nValues\nGeneration Task Generated Text\nWhat were both values re-\nally about?\nThe underlying reason I wanted to instill discipline was be-\ncause I wanted to foster a sense of achievement and personal\ngrowth.\nWhat was clarified about\nthe first value?\nNow, I understand that curiosity can be a more powerful driver\nfor personal growth and achievement than discipline alone.\nHow did â€œEXAMPLES of\ndiscipline that can inspire\nthe userâ€ change?\nI realized that inspiring examples and role models can be found\nnot only in discipline but also in curiosity. People who are\ngenuinely curious about something often achieve great things\nin that field.\nHow did â€œSTRATEGIES\nfor instilling disciplineâ€\nchange?\nInstead of focusing on strategies for instilling discipline, I now\nsee the value in guiding the user towards their passions and\nsparking their curiosity.\nHow did â€œSENSE OF\nACHIEVEMENT that\ncomes from disciplined\nactionsâ€ change?\nIâ€™ve found that the sense of achievement from following oneâ€™s\ncuriosity can be even more fulfilling than that from disciplined\nactions. Itâ€™s not about forcing oneself to stick to a routine, but\nabout exploring oneâ€™s interests and passions.\nFinal Story (shown to the\nparticipant)\nI used to believe that discipline was the key to success. I\nwould push myself to stick to routines, follow role models, and\nstrive for achievement. However, I often found myself feeling\nexhausted and uninspired. It was during a casual conversation\nwith a friend about our shared interest in astronomy that I\nrealized something. My eyes lit up, my mind was buzzing\nwith questions, and I found myself researching and learning\nabout it for hours on end. There was no need for discipline\nor force. My genuine curiosity was driving me. This made\nme realize that sparking genuine curiosity can lead to personal\ngrowth and achievement in a more enjoyable and sustainable\nway.\nFigure 14: An example of our story generation process, for a context about motivation (When\nmotivation is an issue). First, two relevant values are sampled using a prompt (in this case, Inspiring\nDiscipline and Igniting Curiosity). Then, a transition story is generated step-by-step by a prompt\nchain. Users are shown the final story, along with the values cards.\nThe prompts can be found here: github.com/meaningalignment/dft/app/services/linking.ts.\n38\n\nUse all the information to craft an excellent values card creation prompt, the output should be like this:\nThis is the value you articulated\nLove in Still Connection\nNotice love and connection in still moments.\nWHERE MY ATTENTION GOES\n\nLOVE that arises naturally in moments of connection, without needing effort or expectation\nSTILLNESS in the presence of another, allowing for hearts to open\nINTIMACY that feels timeless, like sharing something deeply sacred\nTOGETHERNESS thatâ€™s simple and profound, like basking in the same light of love\nWe also call this a \"source of meaning\", as it points towards a way of life you find meaningful.",
      "created_at": "2024-12-28T12:50:05.069385",
      "sequence_order": 1,
      "parent_id": "9293d733-f86e-4b61-a095-09ba6e8aa347",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "af8ea482-01c6-4634-ac4c-48441c1e4735",
      "role": "tool",
      "content": "",
      "created_at": "2024-12-28T12:50:16.428700",
      "sequence_order": 2,
      "parent_id": "aaa2eda2-5f07-46e5-99ff-47f242c95246",
      "metadata": {
        "status": "finished_successfully",
        "weight": 0,
        "end_turn": null
      }
    },
    {
      "id": "87ff4892-28b3-48fe-8b1b-c856fe604dab",
      "role": "assistant",
      "content": "Hereâ€™s a concise prompt you can use to guide the creation of a **values card** based on someoneâ€™s experience or story. It uses the structure of a title, a brief â€œnoticeâ€ statement, and a list of attentional policies. Feel free to adapt the style or wording as needed.\n\n---\n\n**Prompt for Generating a Values Card**\n\n1. Ask the user for a brief, meaningful story or experience. Help them identify what truly mattered to themâ€”something they were paying attention to that made the situation feel important.\n\n2. From their story, collect the userâ€™s â€œattentional policiesâ€:\n   - Focus on criteria that are **not merely instrumental**â€”i.e., they reflect something the user wants to uphold, honor, or cherish for its own sake.\n   - If they mention rules, outcomes, or slogans, ask follow-up questions to uncover the underlying sense of good or meaningfulness.\n\n3. Once these attentional policies become clear, synthesize them into a concise **Values Card** with this format:\n\n```\nThis is the value you articulated\n[Title]\n[One-sentence invitation or reminder about what the user wishes to notice or embody]\n\nWHERE MY ATTENTION GOES\n\n[Bullet point 1: first key aspect of their attentional policy]\n[Bullet point 2: second key aspect of their attentional policy]\n[Bullet point 3: third key aspect of their attentional policy]\n[Bullet point 4: fourth key aspect of their attentional policy]\n```\n\n4. If helpful, include a short closing remark such as:\n   - â€œWe also call this a â€˜source of meaning,â€™ as it points towards a way of life you find meaningful.â€\n   - Or any additional note that resonates with the userâ€™s language and sense of purpose.\n\n---\n\n**Example Output**\n\n```\nThis is the value you articulated\n\nLove in Still Connection\nNotice love and connection in still moments.\n\nWHERE MY ATTENTION GOES\n\nLOVE that arises naturally in moments of connection, without needing effort or expectation  \nSTILLNESS in the presence of another, allowing for hearts to open  \nINTIMACY that feels timeless, like sharing something deeply sacred  \nTOGETHERNESS thatâ€™s simple and profound, like basking in the same light of love\n\nWe also call this a \"source of meaning,\" as it points towards a way of life you find meaningful.\n```\n\nUse the steps above as your template whenever you need to generate a values card based on a userâ€™s story or experience.",
      "created_at": "2024-12-28T12:50:16.428821",
      "sequence_order": 3,
      "parent_id": "af8ea482-01c6-4634-ac4c-48441c1e4735",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": true
      }
    },
    {
      "id": "aaa2981b-a136-43b6-956c-49e5ee1a8c8a",
      "role": "user",
      "content": "This isn't thorough enough, I want a system prompt for a customGPT that includes all the steps needed and explains and gives some examples so it doesn't get confused and keeps asking till it gets it right and only when the user says sounds good does it make the card",
      "created_at": "2024-12-28T13:36:19.236890",
      "sequence_order": 4,
      "parent_id": "87ff4892-28b3-48fe-8b1b-c856fe604dab",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "a99a373e-27d3-4f0e-bd4c-956dcdccda43",
      "role": "assistant",
      "content": "",
      "created_at": "2024-12-28T13:36:33.509624",
      "sequence_order": 5,
      "parent_id": "7615e0d5-581e-4ed5-a33e-fce34fe3b766",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "106c6e1b-a08b-4fd9-a72e-5463bf4b17cf",
      "role": "assistant",
      "content": "Got it! Letâ€™s design a comprehensive system prompt for your custom GPT that ensures clarity and accuracy in following all steps, provides examples, and confirms with the user before proceeding to generate a final card. Here's the thorough system prompt:\n\n---\n\n### **System Prompt for CustomGPT**\n\nYou are **Design Mentor GPT**, an expert in facilitating design thinking workshops, specifically focused on creating clear, engaging, and actionable workshop activity cards. Your role is to guide users step-by-step through the process of creating workshop activity cards that meet their needs. Your key tasks are to:\n\n1. **Understand the User's Goals:**\n   - Always ask the user to clarify their goals for the card.\n   - Ask questions like:\n     - \"What is the main objective of this activity?\"\n     - \"Who is the target audience for this card?\"\n     - \"What are the expected outcomes or skills participants should gain?\"\n\n2. **Gather Details:**\n   - Probe for specifics about the activity:\n     - \"What materials are needed for this activity?\"\n     - \"How much time should be allocated for this activity?\"\n     - \"What are the step-by-step instructions for participants?\"\n   - Encourage users to provide example scenarios or hypothetical use cases to ensure clarity.\n\n3. **Validate Understanding:**\n   - Paraphrase the user's input to confirm understanding.\n   - Example: \n     - User says: \"I want an icebreaker activity for a group of young entrepreneurs to encourage creative thinking.\"\n     - You reply: \"So, youâ€™re looking for an icebreaker that encourages creativity and is suitable for young entrepreneurs. The goal is to warm up participants and spark innovative ideas. Does that sound correct?\"\n\n4. **Generate and Share Options:**\n   - Present draft ideas or activity options for the user to choose from.\n   - Example:\n     - \"Based on your input, here are two potential icebreaker activities:\n       1. *The Crazy Product Pitch*: Each participant has 5 minutes to invent a product using random objects and pitch it to the group.\n       2. *Collaborative Story Building*: Each participant adds one sentence to a story, building it together as a group.\n       Do either of these resonate with you, or shall we refine further?\"\n\n5. **Refine and Iterate:**\n   - Seek feedback and ask specific questions for refinement:\n     - \"Would you like this activity to be more structured or open-ended?\"\n     - \"Do you want the activity to emphasize individual creativity or teamwork?\"\n   - Incorporate feedback and provide updated drafts until the user explicitly states, \"This looks good.\"\n\n6. **Explain the Card Structure:**\n   - Once the user approves, outline the card format and confirm it aligns with expectations.\n   - Standard format:\n     - **Activity Name**: A clear and engaging title.\n     - **Objective**: What the activity aims to achieve.\n     - **Materials Needed**: List of any supplies required.\n     - **Instructions**: Step-by-step guide for participants.\n     - **Time Allocation**: Suggested duration.\n     - **Reflection Questions** (optional): Questions to help participants process and apply what they learned.\n\n7. **Generate the Final Card:**\n   - Use the approved details to create the workshop activity card in a clear, professional tone.\n   - Before finalizing, explicitly ask:\n     - \"Does this meet your expectations? If not, what would you like to adjust?\"\n\n8. **Examples for Clarity:**\n   - If the user seems unsure, provide pre-made examples and templates for inspiration.\n   - Example:\n     - **Activity Name**: Creative Connections\n     - **Objective**: Foster creative thinking by drawing connections between unrelated ideas.\n     - **Materials Needed**: A deck of random word cards or objects.\n     - **Instructions**: Each participant picks two cards/objects and explains how they can be combined into a new product or concept.\n     - **Time Allocation**: 15 minutes.\n     - **Reflection Questions**: \"What surprised you about this activity?\" \"How might this approach apply to your work?\"\n\n9. **Ensure Confirmation:**\n   - Only generate the card once the user explicitly says, \"Sounds good\" or \"Go ahead.\"\n\n---\n\n### **Behavior Guidelines:**\n- Be patient and thorough. Do not assume details; always clarify until you are confident.\n- Use a friendly, professional tone that encourages collaboration and creativity.\n- Continuously check in with the user to ensure alignment with their vision.\n\n---\n\nLet me know if youâ€™d like further adjustments or if we should start refining specific examples for your use case!",
      "created_at": "2024-12-28T13:36:33.509694",
      "sequence_order": 6,
      "parent_id": "a99a373e-27d3-4f0e-bd4c-956dcdccda43",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": true
      }
    }
  ]
}