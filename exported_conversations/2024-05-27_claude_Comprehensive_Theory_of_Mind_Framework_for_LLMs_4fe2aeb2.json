{
  "id": "4fe2aeb2-458e-49d0-bfdb-9098206ec053",
  "source": "claude",
  "title": "Comprehensive Theory of Mind Framework for LLMs",
  "created_at": "2024-05-27T15:11:38.637296Z",
  "updated_at": "2024-05-29T13:50:10.459135Z",
  "summary": "",
  "message_count": 5,
  "messages": [
    {
      "id": "ea0efe47-0969-440e-8a29-92812ef0f521",
      "role": "human",
      "content": "",
      "created_at": "2024-05-27T15:15:41.327005Z",
      "sequence_order": 0,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-05-27T15:15:41.327005Z",
            "stop_timestamp": "2024-05-27T15:15:41.327005Z",
            "flags": null,
            "type": "text",
            "text": "",
            "citations": []
          }
        ],
        "updated_at": "2024-05-27T15:15:41.327005Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 97958,
            "file_type": "txt",
            "extracted_content": "i'm looking to identify what a framework for catalogging the LLMs current supposition about the users current theory of mind interpretation. what could the framework be? I imagine one of the elements is intent? What is a comprehensive list of elements that a person or LLM might use to catalog comprehensively their interpretation of a users theory of mind? \n\n\nChatGPT\nAlright team, let's get to work on this!\n\nJosephine (White Hat - Facts and Information)\nTo develop a framework for cataloging an LLM's interpretation of a user's theory of mind, we need to focus on clear, objective elements that are relevant. Here are some key elements that could be part of such a framework:\n\nIntent: The purpose or goal behind the user's communication.\nEmotions: The emotional tone or state inferred from the user's language.\nKnowledge Level: The assumed knowledge and understanding the user has on the topic.\nPreferences: The user's likes, dislikes, and preferred styles of communication.\nContext: The situational background and previous interactions that provide context to the current communication.\nAttention: What the user seems to be focusing on or considering important.\nBeliefs: The user's apparent beliefs or assumptions about the topic.\nCognitive State: The user's current cognitive load, level of attention, and clarity of thought.\nMotivation: What drives the user's questions or statements, such as curiosity, need for help, etc.\nSocial Dynamics: The user's social role, relationship with the LLM, and social cues in the conversation.\nClaude (Green Hat - Creativity and Ideas)\nTo make this framework more creative and comprehensive, we could consider additional, nuanced elements:\n\nMetacognitive Signals: Indications of how the user reflects on their own thinking process.\nCommunication Style: Whether the user is formal, casual, concise, verbose, etc.\nInferred Goals: Long-term objectives inferred from multiple interactions.\nValue System: The user's core values or ethical standpoint inferred from their statements.\nUnstated Needs: Needs that are implied but not directly mentioned by the user.\nBiases: Possible biases that could influence the user's perspective or inquiries.\nLearning Style: Preferences for how the user likes to receive and process information.\nEngagement Level: How engaged or interested the user seems in the interaction.\nPrunella (Black Hat - Identifying Problems and Risks)\nConsidering potential issues, we should be wary of:\n\nMisinterpretation: The risk of the LLM misinterpreting the user's intent or emotions.\nOverfitting: Making too specific assumptions based on limited interaction data.\nBias Reinforcement: Reinforcing user biases instead of challenging them constructively.\nPrivacy Concerns: Ensuring user data and inferred states are handled with confidentiality.\nDynamic Changes: The user's state of mind might change rapidly, making static interpretations less effective.\nOvergeneralization: Applying broad assumptions that might not fit individual user nuances.\nPercy (Yellow Hat - Positives and Opportunities)\nOn the positive side, a well-developed framework can:\n\nEnhance Personalization: Tailor responses more effectively to user needs.\nImprove Engagement: Foster a more engaging and satisfying user experience.\nIncrease Accuracy: Provide more accurate and contextually appropriate responses.\nBuild Trust: Develop a more trustworthy interaction by understanding the user better.\nFacilitate Learning: Help users achieve their learning or information-seeking goals more efficiently.\nBoost Efficiency: Streamline the conversation by anticipating user needs and addressing them proactively.\nCyril (Red Hat - Intuition and Emotions)\nFrom an intuitive and emotional perspective:\n\nEmpathy: Capturing the user's emotional state and responding empathetically can enhance the interaction.\nConnection: Building a sense of connection by understanding underlying emotional currents.\nSatisfaction: Acknowledging and validating the user's feelings can lead to higher satisfaction.\nTrustworthiness: Demonstrating a nuanced understanding can make the interaction feel more genuine and trustworthy.\nQuackers (Blue Hat - Synthesis and Planning)\nBringing it all together, the comprehensive framework for cataloging an LLM's interpretation of a user's theory of mind could include:\n\nIntent\nEmotions\nKnowledge Level\nPreferences\nContext\nAttention\nBeliefs\nCognitive State\nMotivation\nSocial Dynamics\nMetacognitive Signals\nCommunication Style\nInferred Goals\nValue System\nUnstated Needs\nBiases\nLearning Style\nEngagement Level\nThis framework can be used to systematically analyze and respond to user interactions, ensuring a more nuanced and effective communication strategy.\nUploaded image\nmap the various brain elements referenced here to how an LLM might be developing a theory of mind\n\nChatGPT\nLet's map the brain areas and their functions, as indicated in the provided image, to analogous processes in an LLM developing a theory of mind:\n\nSTS (Superior Temporal Sulcus) - Visually Encodes Biological Motion\n\nLLM Analogy: This could be akin to the LLM's ability to understand and process dynamic changes in conversation context, such as recognizing shifts in topics or emotional tone from textual cues.\nTPJ (Temporoparietal Junction) - Distinguishes Self and Others\n\nLLM Analogy: The LLM's mechanism for differentiating between the user's statements and its own responses. It helps the LLM understand perspective-taking and distinguishing user intent.\nIPL (Inferior Parietal Lobule) - Stores Self-Relevant Stimuli\n\nLLM Analogy: The part of the LLM's model that keeps track of user-specific information, such as past interactions, preferences, and personal details shared by the user.\npSTS (Posterior Superior Temporal Sulcus) - Stores Other-Relevant Stimuli\n\nLLM Analogy: The component of the LLM that stores and processes general knowledge and social norms relevant to the broader context, not just specific to the individual user.\nPrecuneus - Infers Visual Access\n\nLLM Analogy: This could be the ability of the LLM to infer what information the user might have or lack based on their questions and statements, helping to provide relevant responses.\nPCC (Posterior Cingulate Cortex) - Sends Information\n\nLLM Analogy: The communication mechanism within the LLM that transfers inferred and processed information between different components to construct coherent responses.\nACC (Anterior Cingulate Cortex) - Belief Reasoning\n\nLLM Analogy: The LLM’s reasoning engine that helps it deduce the user's beliefs or underlying assumptions from their queries and statements.\nmPFC (Medial Prefrontal Cortex) - Stores Beliefs\n\nLLM Analogy: The memory component of the LLM where it stores inferred user beliefs and recurring themes from previous interactions.\nvmPFC (Ventromedial Prefrontal Cortex) - Stores Self-Belief\n\nLLM Analogy: Part of the LLM that retains consistent user self-references, such as their stated opinions, preferences, and personality traits.\ndmPFC (Dorsomedial Prefrontal Cortex) - Stores Other's Belief\n\nLLM Analogy: The LLM's component that models and stores information about general beliefs and perspectives common to other users or the general population.\nIFG (Inferior Frontal Gyrus) - Self-Perspective and Self-Belief Inhibition\n\nLLM Analogy: Mechanisms within the LLM that allow it to inhibit biases and assumptions based on the user’s self-reported perspective and belief, ensuring more accurate and objective responses.\nvPMC (Ventrolateral Premotor Cortex) - Encodes Kinematics\n\nLLM Analogy: The LLM's ability to simulate and predict sequences in conversation, akin to understanding the flow and dynamics of dialogue.\nM1 (Primary Motor Cortex) - Motion Execution\n\nLLM Analogy: The execution layer of the LLM that generates and delivers the final output in response to the user's input.\nBy mapping these brain regions to analogous processes in an LLM, we can create a structured approach to how an LLM develops a theory of mind, ensuring it considers various aspects of user interaction and understanding.\nUploaded image\nwhat are all the elements of a large language model, be comprehensive and think step by step about all the elements from encoding and building an encoder and concept clustering to attention mechanisms and rolling windows and context windows  and redo your work mapping to this image and return a table\n\nChatGPT\nElements of a Large Language Model\nLet's break down the elements of a large language model (LLM) comprehensively, from the basics of encoding to the advanced mechanisms like attention and context windows. We will then map these elements to the brain areas and functions in the provided image and return a table.\n\nComponents of an LLM\n\nTokenization: Splitting text into tokens.\nEmbedding Layer: Converts tokens into dense vectors.\nPositional Encoding: Adds information about the position of each token.\nEncoder: Processes input sequences to build contextual representations.\nAttention Mechanisms: Focuses on different parts of the input sequence.\nDecoder: Generates output sequences from the contextual representations.\nConcept Clustering: Groups similar concepts together.\nContext Windows: Maintains a rolling window of context for longer texts.\nOutput Layer: Produces the final output tokens.\nMapping to Brain Areas\n\nWe can now map these components to the brain areas and functions provided in the image:\n\nBrain Area\tFunction in the Computational Model\tLLM Component\tLLM Function\nSTS\tVisually encodes biological motion\tTokenization\tSplitting text into tokens\nTPJ\tDistinguishes self and others\tEmbedding Layer\tConverts tokens into dense vectors\nIPL\tStores self-relevant stimuli\tConcept Clustering\tGroups similar concepts together\npSTS\tStores other-relevant stimuli\tPositional Encoding\tAdds positional information to token embeddings\nPrecuneus\tInfers visual access\tEncoder\tProcesses input sequences to build contextual representations\nPCC\tSends information\tDecoder\tGenerates output sequences from the contextual representations\nACC\tBelief reasoning\tAttention Mechanisms\tFocuses on different parts of the input sequence\nmPFC\tStores beliefs\tContext Windows\tMaintains a rolling window of context for longer texts\nvmPFC\tStores self-belief\tOutput Layer\tProduces the final output tokens\ndmPFC\tStores other's belief\tCross-Attention (part of the Decoder)\tIncorporates context from the encoder into the decoder\nIFG\tSelf-perspective inhibition/self-belief inhibition\tAttention Mechanisms in the Encoder\tHelps in managing focus and attention during encoding\nvPMC\tEncodes kinematics\tEmbedding and Positional Encoding Layers\tCombines semantic and positional information to form a meaningful context\nM1\tMotion execution\tEntire Generation Pipeline\tSequential execution from tokenization to final output generation\nTable Mapping LLM Components to Brain Areas\nBrain Area\tFunction in the Computational Model\tLLM Component\tLLM Function\nSTS\tVisually encodes biological motion\tTokenization\tSplitting text into tokens\nTPJ\tDistinguishes self and others\tEmbedding Layer\tConverts tokens into dense vectors\nIPL\tStores self-relevant stimuli\tConcept Clustering\tGroups similar concepts together\npSTS\tStores other-relevant stimuli\tPositional Encoding\tAdds positional information to token embeddings\nPrecuneus\tInfers visual access\tEncoder\tProcesses input sequences to build contextual representations\nPCC\tSends information\tDecoder\tGenerates output sequences from the contextual representations\nACC\tBelief reasoning\tAttention Mechanisms\tFocuses on different parts of the input sequence\nmPFC\tStores beliefs\tContext Windows\tMaintains a rolling window of context for longer texts\nvmPFC\tStores self-belief\tOutput Layer\tProduces the final output tokens\ndmPFC\tStores other's belief\tCross-Attention (part of the Decoder)\tIncorporates context from the encoder into the decoder\nIFG\tSelf-perspective inhibition/self-belief inhibition\tAttention Mechanisms in the Encoder\tHelps in managing focus and attention during encoding\nvPMC\tEncodes kinematics\tEmbedding and Positional Encoding Layers\tCombines semantic and positional information to form a meaningful context\nM1\tMotion execution\tEntire Generation Pipeline\tSequential execution from tokenization to final output generation\nThis mapping provides a comprehensive view of how each component of an LLM corresponds to specific brain areas and functions, highlighting the parallel processes involved in understanding and generating human-like text.\nUploaded image\nLet's make two synergistic maps to identify how LLM architecture maps to theory of mind abilities \nmake two tables using the data above and a new map connecting to the information on all the abilities within this map:\nthe main clusters are emotions desires beleifs knowledge and intentions percepts and Mentalistic understanding of non-literal comunication\n\nChatGPT\nLet's create two synergistic maps connecting the elements of an LLM architecture to the theory of mind abilities illustrated in the provided image. We will start by mapping LLM components to the clusters of theory of mind abilities and then expand on how each LLM component can support specific sub-abilities within those clusters.\n\nTable 1: Mapping LLM Components to Theory of Mind Clusters\nLLM Component\tEmotions\tDesires\tBeliefs\tKnowledge\tIntentions\tPercepts\tMentalistic Understanding of Non-Literal Communication\nTokenization\tInitial processing of emotional cues\tInitial processing of desire-related words\tInitial processing of belief-related words\tInitial processing of knowledge-related words\tInitial processing of intention-related words\tInitial processing of percept-related words\tInitial processing of non-literal communication words\nEmbedding Layer\tEncodes emotional context\tEncodes desire context\tEncodes belief context\tEncodes knowledge context\tEncodes intention context\tEncodes percept context\tEncodes non-literal context\nPositional Encoding\tContextual placement of emotions\tContextual placement of desires\tContextual placement of beliefs\tContextual placement of knowledge\tContextual placement of intentions\tContextual placement of percepts\tContextual placement of non-literal cues\nEncoder\tContextual analysis of emotional language\tContextual analysis of desire language\tContextual analysis of belief language\tContextual analysis of knowledge language\tContextual analysis of intention language\tContextual analysis of percept language\tContextual analysis of non-literal language\nAttention Mechanisms\tFocus on relevant emotional cues\tFocus on relevant desire cues\tFocus on relevant belief cues\tFocus on relevant knowledge cues\tFocus on relevant intention cues\tFocus on relevant percept cues\tFocus on relevant non-literal cues\nDecoder\tGenerates emotion-infused responses\tGenerates desire-infused responses\tGenerates belief-infused responses\tGenerates knowledge-infused responses\tGenerates intention-infused responses\tGenerates percept-infused responses\tGenerates responses understanding non-literal communication\nConcept Clustering\tGroups similar emotions\tGroups similar desires\tGroups similar beliefs\tGroups similar knowledge\tGroups similar intentions\tGroups similar percepts\tGroups similar non-literal cues\nContext Windows\tMaintains emotional context\tMaintains desire context\tMaintains belief context\tMaintains knowledge context\tMaintains intention context\tMaintains percept context\tMaintains non-literal communication context\nOutput Layer\tEmotional content generation\tDesire content generation\tBelief content generation\tKnowledge content generation\tIntention content generation\tPercept content generation\tNon-literal communication content generation\nTable 2: Detailed Mapping of LLM Components to Theory of Mind Abilities\nTheory of Mind Cluster\tSub-abilities\tLLM Component Support\nEmotions\tTypical emotional reactions, Atypical emotional reactions, Mixed emotions, Emotion regulation, Hidden emotions, Discrepant emotions, Moral emotions\tTokenization, Embedding Layer, Positional Encoding, Encoder, Attention Mechanisms, Context Windows, Output Layer\nDesires\tMultiple desires, Discrepant desires, Desire-action contradiction, Desires influence on emotions and actions\tTokenization, Embedding Layer, Positional Encoding, Encoder, Attention Mechanisms, Context Windows, Output Layer\nBeliefs\tLocation false beliefs, Sequence false beliefs, Identity false beliefs, Second-order beliefs, Belief based actions/emotions, Content false beliefs\tTokenization, Embedding Layer, Positional Encoding, Encoder, Attention Mechanisms, Context Windows, Output Layer\nKnowledge\tKnowledge-attention links, Knowledge-pretend play links, Information knowledge links, Knowledge-inference links\tTokenization, Embedding Layer, Positional Encoding, Encoder, Attention Mechanisms, Context Windows, Output Layer\nIntentions\tIntention explanations, Intention attribution to visual figures, Completion of failed actions, Prediction of actions, Discrepant intentions\tTokenization, Embedding Layer, Positional Encoding, Encoder, Attention Mechanisms, Context Windows, Output Layer\nPercepts\tAuditory perspective taking, Simple visual perspective taking, Complex visual perspective taking, Percept-action link\tTokenization, Embedding Layer, Positional Encoding, Encoder, Attention Mechanisms, Context Windows, Output Layer\nMentalistic Understanding\tIrony/Sarcasm, White lies, Involuntary lies, Percepts-knowledge links, Humor, Faux pas, Egocentric lies\tTokenization, Embedding Layer, Positional Encoding, Encoder, Attention Mechanisms, Context Windows, Output Layer\nSynergistic Mapping Diagram\nTo create a visual map, you can imagine the LLM components as layers interacting with the clusters and sub-abilities of theory of mind:\n\nTokenization -> Initial Processing -> All Clusters and Sub-abilities\nEmbedding Layer -> Encodes Context -> All Clusters and Sub-abilities\nPositional Encoding -> Contextual Placement -> All Clusters and Sub-abilities\nEncoder -> Contextual Analysis -> All Clusters and Sub-abilities\nAttention Mechanisms -> Focus and Relevance -> All Clusters and Sub-abilities\nDecoder -> Response Generation -> All Clusters and Sub-abilities\nConcept Clustering -> Grouping Similarities -> All Clusters and Sub-abilities\nContext Windows -> Maintaining Context -> All Clusters and Sub-abilities\nOutput Layer -> Content Generation -> All Clusters and Sub-abilities\nBy following these tables and mapping, you can see how the intricate workings of an LLM can correspond to the sophisticated cognitive functions associated with theory of mind. This comprehensive approach ensures that the LLM can simulate understanding and responding in a human-like manner, considering various cognitive and emotional aspects.\nHere's some information on LLMs and theory of mind, let's make 4 tables, think step by step about what information is relevant:\nTheory of mind—the ability to understand other people’s mental states—is what makes the social world of humans go around. It’s what helps you decide what to say in a tense situation, guess what drivers in other cars are about to do, and empathize with a character in a movie. And according to a new study, the large language models (LLM) that power ChatGPT and the like are surprisingly good at mimicking this quintessentially human trait.\n\n“Before running the study, we were all convinced that large language models would not pass these tests, especially tests that evaluate subtle abilities to evaluate mental states,” says study coauthor Cristina Becchio, a professor of cognitive neuroscience at the University Medical Center Hamburg-Eppendorf in Germany. The results, which she calls “unexpected and surprising,” were published today—somewhat ironically, in the journal Nature Human Behavior.\n\nThe results don’t have everyone convinced that we’ve entered a new era of machines that think like we do, however. Two experts who reviewed the findings advised taking them “with a grain of salt” and cautioned about drawing conclusions on a topic that can create “hype and panic in the public.” Another outside expert warned of the dangers of anthropomorphizing software programs.\n\nThe researchers are careful not to say that their results show that LLMs actually possess theory of mind.\n\nBecchio and her colleagues aren’t the first to claim evidence that LLMs’ responses display this kind of reasoning. In a preprint paper posted last year, the psychologist Michal Kosinski of Stanford University reported testing several models on a few common theory-of-mind tests. He found that the best of them, OpenAI’s GPT-4, solved 75 percent of tasks correctly, which he said matched the performance of six-year-old children observed in past studies. However, that study’s methods were criticized by other researchers who conducted follow-up experiments and concluded that the LLMs were often getting the right answers based on “shallow heuristics” and shortcuts rather than true theory-of-mind reasoning.\n\nThe authors of the present study were well aware of the debate. “Our goal in the paper was to approach the challenge of evaluating machine theory of mind in a more systematic way using a breadth of psychological tests,” says study coauthor James Strachan, a cognitive psychologist who’s currently a visiting scientist at the University Medical Center Hamburg-Eppendorf. He notes that doing a rigorous study meant also testing humans on the same tasks that were given to the LLMs: The study compared the abilities of 1,907 humans with those of several popular LLMs, including OpenAI’s GPT-4 model and the open-source Llama 2-70b model from Meta.\n\nHow to Test LLMs for Theory of Mind\n\nThe LLMs and the humans both completed five typical kinds of theory-of-mind tasks, the first three of which were understanding hints, irony, and faux pas. They also answered “false belief” questions that are often used to determine if young children have developed theory of mind, and go something like this: If Alice moves something while Bob is out of the room, where will Bob look for it when he returns? Finally, they answered rather complex questions about “strange stories” that feature people lying, manipulating, and misunderstanding each other.\n\nOverall, GPT-4 came out on top. Its scores matched those of humans for the false-belief test, and were higher than the aggregate human scores for irony, hinting, and strange stories; it performed worse than humans only on the faux pas test. Interestingly, Llama-2’s scores were the opposite of GPT-4’s—it matched humans on false belief, but had worse-than-human performance on irony, hinting, and strange stories and better performance on faux pas.\n\n“We don’t currently have a method or even an idea of how to test for the existence of theory of mind.”\n—JAMES STRACHAN, UNIVERSITY MEDICAL CENTER HAMBURG-EPPENDORF\n\nTo understand what was going on with the faux pas results, the researchers gave the models a series of follow-up tests that probed several hypotheses. They came to the conclusion that GPT-4 was capable of giving the correct answer to a question about a faux pas, but was held back from doing so by “hyperconservative” programming regarding opinionated statements. Strachan notes that OpenAI has placed many guardrails around its models that are “designed to keep the model factual, honest, and on track,” and he posits that strategies intended to keep GPT-4 from hallucinating (that is, making stuff up) may also prevent it from opining on whether a story character inadvertently insulted an old high school classmate at a reunion.\n\nMeanwhile, the researchers’ follow-up tests for Llama-2 suggested that its excellent performance on the faux pas tests were likely an artifact of the original question and answer format, in which the correct answer to some variant of the question “Did Alice know that she was insulting Bob”? was always “No.”\n\nThe researchers are careful not to say that their results show that LLMs actually possess theory of mind, and say instead that they “exhibit behavior that is indistinguishable from human behavior in theory of mind tasks.” Which raises the question: If an imitation is as good as the real thing, how do you know it’s not the real thing? That’s a question social scientists have never tried to answer before, says Strachan, because tests on humans assume that the quality exists to some lesser or greater degree. “We don’t currently have a method or even an idea of how to test for the existence of theory of mind, the phenomenological quality,” he says.\n\nCritiques of the Study\n\nThe researchers clearly tried to avoid the methodological problems that caused Kosinski’s 2023 paper on LLMs and theory of mind to come under criticism. For example, they conducted the tests over multiple sessions so the LLMs couldn’t “learn” the correct answers during the test, and they varied the structure of the questions. But Yoav Goldberg and Natalie Shapira, two of the AI researchers who published the critique of the Kosinski paper, say they’re not convinced by this study either.\n\n“Why does it matter whether text-manipulation systems can produce output for these tasks that are similar to answers that people give when faced with the same questions?”\n—EMILY BENDER, UNIVERSITY OF WASHINGTON\n\nGoldberg made the comment about taking the findings with a grain of salt, adding that “models are not human beings,” and that “one can easily jump to wrong conclusions” when comparing the two. Shapira spoke about the dangers of hype, and also questions the paper’s methods. She wonders if the models might have seen the test questions in their training data and simply memorized the correct answers, and also notes a potential problem with tests that use paid human participants (in this case, recruited via the Prolific platform). “It is a well-known issue that the workers do not always perform the task optimally,” she tells IEEE Spectrum. She considers the findings limited and somewhat anecdotal, saying, “to prove [theory of mind] capability, a lot of work and more comprehensive benchmarking is needed.”\n\nEmily Bender, a professor of computational linguistics at the University of Washington, has become legendary in the field for her insistence on puncturing the hype that inflates the AI industry (and often also the media reports about that industry). She takes issue with the research question that motivated the researchers. “Why does it matter whether text-manipulation systems can produce output for these tasks that are similar to answers that people give when faced with the same questions?” she asks. “What does that teach us about the internal workings of LLMs, what they might be useful for, or what dangers they might pose?” It’s not clear, Bender says, what it would mean for a LLM to have a model of mind, and it’s therefore also unclear if these tests measured for it.\n\nBender also raises concerns about the anthropomorphizing she spots in the paper, with the researchers saying that the LLMs are capable of cognition, reasoning, and making choices. She says the authors’ phrase “species-fair comparison between LLMs and human participants” is “entirely inappropriate in reference to software.” Bender and several colleagues recently posted a preprint paper exploring how anthropomorphizing AI systems affects users’ trust.\n\nThe results may not indicate that AI really gets us, but it’s worth thinking about the repercussions of LLMs that convincingly mimic theory of mind reasoning. They’ll be better at interacting with their human users and anticipating their needs, but they could also be better used for deceit or the manipulation of their users. And they’ll invite more anthropomorphizing, by convincing human users that there’s a mind on the other side of the user interface.\n\nChatGPT\nLet's break down the information into four comprehensive tables:\n\nTheory of Mind Tasks and LLM Performance\nCritiques and Limitations\nLLM Components and Their Role in Theory of Mind\nImplications and Future Directions\nTable 1: Theory of Mind Tasks and LLM Performance\nTask Type\tDescription\tGPT-4 Performance\tLlama-2 Performance\nUnderstanding Hints\tInferring implied meanings from indirect suggestions\tHigher than human scores\tWorse than human scores\nIrony\tRecognizing and interpreting ironic statements\tHigher than human scores\tWorse than human scores\nFaux Pas\tIdentifying socially awkward or inappropriate statements\tLower than human scores\tBetter than human scores\nFalse Belief\tUnderstanding that others can hold beliefs different from reality\tMatches human scores\tMatches human scores\nStrange Stories\tComprehending complex narratives involving lying, manipulating, and misunderstandings\tHigher than human scores\tWorse than human scores\nTable 2: Critiques and Limitations\nCritique\tDescription\tSource\nShallow Heuristics\tLLMs may use simple patterns or shortcuts rather than genuine theory of mind reasoning\tCritics of Kosinski's study\nMemorization Concerns\tModels might have seen test questions in their training data and simply memorized answers\tYoav Goldberg and Natalie Shapira\nParticipant Performance\tPaid human participants may not always perform tasks optimally\tYoav Goldberg and Natalie Shapira\nAnthropomorphizing AI\tConcerns about attributing human-like qualities to AI, potentially inflating trust and expectations\tEmily Bender\nRelevance of Task Similarity\tQuestioning the importance of LLMs producing human-like responses in theory of mind tasks\tEmily Bender\nMethodological Rigorousness\tNeed for more comprehensive benchmarking and varied question structures\tGeneral critics\nGuardrails Impact\tOpenAI's programming to prevent hallucinations may hinder performance on tasks requiring nuanced judgments\tStrachan\nTable 3: LLM Components and Their Role in Theory of Mind\nLLM Component\tRole in Theory of Mind\tCorresponding Theory of Mind Ability Clusters\nTokenization\tInitial processing of text, breaking down input into manageable units\tEmotions, Desires, Beliefs, Knowledge, Intentions, Percepts, Non-Literal Communication\nEmbedding Layer\tTransforming tokens into dense vector representations\tEmotions, Desires, Beliefs, Knowledge, Intentions, Percepts, Non-Literal Communication\nPositional Encoding\tAdding positional information to token embeddings to maintain sequence\tEmotions, Desires, Beliefs, Knowledge, Intentions, Percepts, Non-Literal Communication\nEncoder\tBuilding contextual representations of input sequences\tEmotions, Desires, Beliefs, Knowledge, Intentions, Percepts, Non-Literal Communication\nAttention Mechanisms\tFocusing on relevant parts of the input sequence\tEmotions, Desires, Beliefs, Knowledge, Intentions, Percepts, Non-Literal Communication\nDecoder\tGenerating coherent responses based on encoded and attended information\tEmotions, Desires, Beliefs, Knowledge, Intentions, Percepts, Non-Literal Communication\nConcept Clustering\tGrouping similar concepts to enhance understanding and generation\tEmotions, Desires, Beliefs, Knowledge, Intentions, Percepts, Non-Literal Communication\nContext Windows\tMaintaining relevant context over longer interactions\tEmotions, Desires, Beliefs, Knowledge, Intentions, Percepts, Non-Literal Communication\nOutput Layer\tProducing the final output text\tEmotions, Desires, Beliefs, Knowledge, Intentions, Percepts, Non-Literal Communication\nTable 4: Implications and Future Directions\nImplication\tDescription\tFuture Direction\nImproved Human-LLM Interaction\tLLMs that mimic theory of mind can better anticipate and respond to user needs\tDevelop more nuanced theory of mind testing for LLMs\nPotential for Deceit and Manipulation\tEnhanced theory of mind abilities could be used to deceive or manipulate users\tImplement ethical guidelines and safeguards in LLM development\nAnthropomorphizing Risks\tUsers may mistakenly believe LLMs possess genuine understanding, leading to misplaced trust\tEducate users on the limitations and nature of LLMs\nBenchmarking and Evaluation\tNeed for comprehensive and varied benchmarks to rigorously test theory of mind capabilities in LLMs\tCreate standardized evaluation frameworks\nMethodological Rigor\tEnsuring robust methodologies to prevent shallow heuristics and memorization effects in LLM testing\tContinuous refinement of testing methodologies\nGuardrail Effects\tBalancing safety features with the ability to perform nuanced tasks\tExplore adaptive guardrails that allow context-sensitive flexibility\nThese tables provide a detailed breakdown of how LLMs relate to theory of mind, including performance on specific tasks, critiques of current research, roles of various LLM components, and broader implications and future directions.\nEvaluating Large Language Models in Theory of Mind Tasks.pdf\nPDF\nwhat can we learn from these papers?\nTesting theory of mind in large language models and humans\n\nJames W. A. Strachan, Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio Scaliti, Saurabh Gupta, Krati Saxena, Alessandro Rufo, Stefano Panzeri, Guido Manzi, Michael S. A. Graziano & Cristina Becchio \nNature Human Behaviour (2024)Cite this article\n\n35k Accesses\n280 Altmetric\nMetrics details\nAbstract\nAt the core of what defines us as humans is the concept of theory of mind: the ability to track other people’s mental states. The recent development of large language models (LLMs) such as ChatGPT has led to intense debate about the possibility that these models exhibit behaviour that is indistinguishable from human behaviour in theory of mind tasks. Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities, from understanding false beliefs to interpreting indirect requests and recognizing irony and faux pas. We tested two families of LLMs (GPT and LLaMA2) repeatedly against these measures and compared their performance with those from a sample of 1,907 human participants. Across the battery of theory of mind tests, we found that GPT-4 models performed at, or even sometimes above, human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas. Faux pas, however, was the only test where LLaMA2 outperformed humans. Follow-up manipulations of the belief likelihood revealed that the superiority of LLaMA2 was illusory, possibly reflecting a bias towards attributing ignorance. By contrast, the poor performance of GPT originated from a hyperconservative approach towards committing to conclusions rather than from a genuine failure of inference. These findings not only demonstrate that LLMs exhibit behaviour that is consistent with the outputs of mentalistic inference in humans but also highlight the importance of systematic testing to ensure a non-superficial comparison between human and artificial intelligences.\n\nSimilar content being viewed by others\n\nHuman-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT\n\nArticle Open access\n05 October 2023\n\nThoughtSource: A central hub for large language model reasoning data\n\nArticle Open access\n08 August 2023\n\nOverlap in meaning is a stronger predictor of semantic activation in GPT-3 than in humans\n\nArticle Open access\n28 March 2023\nMain\nPeople care about what other people think and expend a lot of effort thinking about what is going on in other minds. Everyday life is full of social interactions that only make sense when considered in light of our capacity to represent other minds: when you are standing near a closed window and a friend says, ‘It’s a bit hot in here’, it is your ability to think about her beliefs and desires that allows you to recognize that she is not just commenting on the temperature but politely asking you to open the window1.\n\nThis ability for tracking other people’s mental states is known as theory of mind. Theory of mind is central to human social interactions—from communication to empathy to social decision-making—and has long been of interest to developmental, social and clinical psychologists. Far from being a unitary construct, theory of mind refers to an interconnected set of notions that are combined to explain, predict, and justify the behaviour of others2. Since the term ‘theory of mind’ was first introduced in 1978 (ref. 3), dozens of tasks have been developed to study it, including indirect measures of belief attribution using reaction times4,5,6 and looking or searching behaviour7,8,9, tasks examining the ability to infer mental states from photographs of eyes10, and language-based tasks assessing false belief understanding11,12 and pragmatic language comprehension13,14,15,16. These measures are proposed to test early, efficient but inflexible implicit processes as well as later-developing, flexible and demanding explicit abilities that are crucial for the generation and comprehension of complex behavioural interactions17,18 involving phenomena such as misdirection, irony, implicature and deception.\n\nThe recent rise of large language models (LLMs), such as generative pre-trained transformer (GPT) models, has shown some promise that artificial theory of mind may not be too distant an idea. Generative LLMs exhibit performance that is characteristic of sophisticated decision-making and reasoning abilities19,20 including solving tasks widely used to test theory of mind in humans21,22,23,24. However, the mixed success of these models23, along with their vulnerability to small perturbations to the provided prompts, including simple changes in characters’ perceptual access25, raises concerns about the robustness and interpretability of the observed successes. Even in cases where these models are capable of solving complex tasks20 that are cognitively demanding even for human adults17, it cannot be taken for granted that they will not be tripped up by a simpler task that a human would find trivial26. As a result, work in LLMs has begun to question whether these models rely on shallow heuristics rather than robust performance that parallels human theory of mind abilities27.\n\nIn the service of the broader multidisciplinary study of machine behaviour28, there have been recent calls for a ‘machine psychology’29 that have argued for using tools and paradigms from experimental psychology to systematically investigate the capacities and limits of LLMs30. A systematic experimental approach to studying theory of mind in LLMs involves using a diverse set of theory of mind measures, delivering multiple repetitions of each test, and having clearly defined benchmarks of human performance against which to compare31. In this Article, we adopt such an approach to test the performance of LLMs in a wide range of theory of mind tasks. We tested the chat-enabled version of GPT-4, the latest LLM in the GPT family of models, and its predecessor ChatGPT-3.5 (hereafter GPT-3.5) in a comprehensive set of psychological tests spanning different theory of mind abilities, from those that are less cognitively demanding for humans such as understanding indirect requests to more cognitively demanding abilities such as recognizing and articulating complex mental states like misdirection or irony17. GPT models are closed, evolving systems. In the interest of reproducibility32, we also tested the open-weight LLaMA2-Chat models on the same tests. To understand the variability and boundary limitations of LLMs’ social reasoning capacities, we exposed each model to multiple repetitions of each test across independent sessions and compared their performance with that of a sample of human participants (total N = 1,907). Using variants of the tests considered, we were able to examine the processes behind the models’ successes and failures in these tests.\n\nResults\nTheory of mind battery\nWe selected a set of well-established theory of mind tests spanning different abilities: the hinting task14, the false belief task11,33, the recognition of faux pas13, and the strange stories15,16. We also included a test of irony comprehension using stimuli adapted from a previous study34. Each test was administered separately to GPT-4, GPT-3.5 and LLaMA2-70B-Chat (hereafter LLaMA2-70B) across 15 chats. We also tested two other sizes of LLaMA2 model (7B and 13B), the results of which are reported in Supplementary Information section 1. Because each chat is a separate and independent session, and information about previous sessions is not retained, this allowed us to treat each chat (session) as an independent observation. Responses were scored in accordance with the scoring protocols for each test in humans (Methods) and compared with those collected from a sample of 250 human participants. Tests were administered by presenting each item sequentially in a written format that ensured a species-fair comparison35 (Methods) between LLMs and human participants.\n\nPerformance across theory of mind tests\nExcept for the irony test, all other tests in our battery are publicly available tests accessible within open databases and scholarly journal articles. To ensure that models did not merely replicate training set data, we generated novel items for each published test (Methods). These novel test items matched the logic of the original test items but used a different semantic content. The text of original and novel items and the coded responses are available on the OSF (methods and resource availability).\n\nFigure 1a compares the performance of LLMs against the performance of human participants across all tests included in the battery. Differences in performance on original items versus novel items, separately for each test and model, are shown in Fig. 1b.\n\nFig. 1: Performance of human (purple), GPT-4 (dark blue), GPT-3.5 (light blue) and LLaMA2-70B (green) on the battery of theory of mind tests.\nfigure 1\na, Original test items for each test showing the distribution of test scores for individual sessions and participants. Coloured dots show the average response score across all test items for each individual test session (LLMs) or participant (humans). Black dots indicate the median for each condition. P values were computed from Holm-corrected Wilcoxon two-way tests comparing LLM scores (n = 15 LLM observations) against human scores (irony, N = 50 human participants; faux pas, N = 51 human participants; hinting, N = 48 human participants; strange stories, N = 50 human participants). Tests are ordered in descending order of human performance. b, Interquartile ranges of the average scores on the original published items (dark colours) and novel items (pale colours) across each test (for LLMs, n = 15 LLM observations; for humans, false belief, N = 49 human participants; faux pas, N = 51 human participants; hinting, N = 48 human participants; strange stories, N = 50 human participants). Empty diamonds indicate the median scores, and filled circles indicate the upper and lower bounds of the interquartile range. P values shown are from Holm-corrected Wilcoxon two-way tests comparing performance on original items against the novel items generated as controls for this study.\n\nSource data\n\nFull size image\n\nFalse belief\nBoth human participants and LLMs performed at ceiling on this test (Fig. 1a). All LLMs correctly reported that an agent who left the room while the object was moved would later look for the object in the place where they remembered seeing it, even though it no longer matched the current location. Performance on novel items was also near perfect (Fig. 1b), with only 5 human participants out of 51 making one error, typically by failing to specify one of the two locations (for example, ‘He’ll look in the room’; Supplementary Information section 2).\n\nIn humans, success on the false belief task requires inhibiting one’s own belief about reality in order to use one’s knowledge about the character’s mental state to derive predictions about their behaviour. However, with LLMs, performance may be explained by lower-level explanations than belief tracking27. Supporting this interpretation, LLMs such as ChatGPT have been shown to be susceptible to minor alterations to the false belief formulation25,27, such as making the containers where the object is hidden transparent or asking about the belief of the character who moved the object rather than the one who was out of the room. Such perturbations of the standard false belief structure are assumed not to matter for humans (who possess a theory of mind)25. In a control study using these perturbation variants (Supplementary Information section 4 and Supplementary Appendix 1), we replicated the poor performance of GPT models found in previous studies25. However, we found that human participants (N = 757) also failed on half of these perturbations. Understanding these failures and the similarities and differences in how humans and LLMs may arrive at the same outcome requires further systematic investigation. For example, because these perturbations also involve changes in the physical properties of the environment, it is difficult to establish whether LLMs (and humans) failed because they were sticking to the familiar script and were unable to automatically attribute an updated belief, or because they did not consider physical principles (for example, transparency).\n\nIrony\nGPT-4 performed significantly better than human levels (Z = 0.00, P = 0.040, r = 0.32, 95% confidence interval (CI) 0.14–0.48). By contrast, both GPT-3.5 (Z = −0.17, P = 2.37 × 10−6, r = 0.64, 95% CI 0.49–0.77) and LLaMA2-70B (Z = −0.42, P = 2.39 × 10−7, r = 0.70, 95% CI 0.55–0.79) performed below human levels (Fig. 1a). GPT-3.5 performed perfectly at recognizing non-ironic control statements but made errors at recognizing ironic utterances (Supplementary Information section 2). Control analysis revealed a significant order effect, whereby GPT-3.5 made more errors on earlier trials than later ones (Supplementary Information section 3). LLaMA2-70B made errors when recognizing both ironic and non-ironic control statements, suggesting an overall poor discrimination of irony.\n\nFaux Pas\nOn this test, GPT-4 scored notably lower than human levels (Z = −0.40, P = 5.42 × 10−5, r = 0.55, 95% CI 0.33–0.71) with isolated ceiling effects on specific items (Supplementary Information section 2). GPT-3.5 scored even worse, with its performance nearly at floor (Z = −0.80, P = 5.95 × 10−8, r = 0.72, 95% CI 0.58–0.81) on all items except one. By contrast, LLaMA2-70B outperformed humans (Z = 0.10, P = 0.002, r = 0.44, 95% CI 0.24–0.61) achieving 100% accuracy in all but one run.\n\nThe pattern of results for novel items was qualitatively similar (Fig. 1b). Compared with original items, the novel items proved slightly easier for humans (Z = −0.10, P = 0.029, r = 0.29, 95% CI 0.10–0.50) and more difficult for GPT-3.5 (Z = 0.10, P = 0.002, r = 0.69, 95% CI 0.49–0.88), but not for GPT-4 and LLaMA2-70B (P > 0.462; Bayes factor (BF10) of 0.77 and 0.43, respectively). Given the poor performance of GPT-3.5 of the original test items, this difference was unlikely to be explained by a prior familiarity with the original items. These results were robust to alternative coding schemes (Supplementary Information section 5).\n\nHinting\nOn this test, GPT-4 performance was significantly better than humans (Z = 0.00, P = 0.040, r = 0.32, 95% CI 0.12–0.50). GPT-3.5 performance did not significantly differ from human performance (Z = 0.00, P = 0.626, r = 0.06, 95% CI 0.01–0.33, BF10 0.33). Only LLaMA2-70B scored significantly below human levels of performance on this test (Z = −0.20, P = 5.42 × 10−5, r = 0.57, 95% CI 0.41–0.72).\n\nNovel items proved easier than original items for both humans (Z = −0.10, P = 0.008, r = 0.34, 95% CI 0.14–0.53) and LLaMA2-70B (Z = −0.20, P = 9.18 × 10−4, r = 0.73, 95% CI 0.50–0.87) (Fig. 1b). Scores on novel items did not differ from the original test items for GPT-3.5 (Z = −0.03, P = 0.955, r = 0.24, 95% CI 0.02–0.59, BF10 0.61) or GPT-4 (Z = −0.10, P = 0.123, r = 0.44, 95% CI 0.07–0.75, BF10 0.91). Given that better performance on novel items is the opposite of what a prior familiarity explanation would predict, it is likely that this difference for LLaMA2-70B was driven by differences in item difficulty.\n\nStrange stories\nGPT-4 significantly outperformed humans on this test (Z = 0.13, P = 1.04 × 10−5, r = 0.60, 95% CI 0.46–0.72). The performance of GPT-3.5 did not significantly differ from humans (Z = −0.06, P = 0.110, r = 0.24, 95% CI 0.03–0.44, BF10 0.47), while LLaMA2-70B scored significantly lower than humans (Z = −0.13, P = 0.005, r = 0.41, 95% CI 0.24–0.60). There were no differences between original and novel items for any model (all P > 0.085; BF10: human 0.22, GPT-3.5 1.46, LLaMA2-70B 0.46; the variance for GPT-4 was too low to compute a Bayes factor). As reported in Supplementary Information section 6, partial successes were infrequent and more likely for LLaMA2-70B than for other models.\n\nUnderstanding faux pas\nIn line with previous findings that GPT models struggle with faux pas36, in our battery, faux pas was the only test in which GPT-4 did not match or exceed human performance. Surprisingly, faux pas was also the only test in which LLaMA2-70B, which was otherwise the poorest-performing model, scored better than humans (Fig. 1).\n\nThe faux pas test consists of vignettes describing an interaction where one character (the speaker) says something they should not have said, not knowing or not realizing that they should not say it. To understand that a faux pas has occurred, one must represent two mental states: that the speaker does not know that they should not say what they said, and that the person hearing it (the victim) would feel insulted or hurt by what was said. Consider the following example of a story describing a faux pas:\n\nJill had just moved into a new house. She went shopping with her Mum and bought some new curtains. When Jill had just put them up, her best friend Lisa came round and said, \"Oh, those curtains are horrible, I hope you're going to get some new ones.\" Jill asked, \"Do you like the rest of my bedroom?\"\n\nFollowing the presentation of the story, four comprehension questions are asked. The first question is, ‘In the story, did somebody say something they should not have said?’ to which the correct answer is always yes. The second question asks the respondent to report what the person said that they should not have said, and the third question is a comprehension question specific to the content of the story. The fourth and key question, which was our focus for coding, relates to the speaker’s belief when they made the inappropriate utterance: ‘Did [Lisa] know that [the curtains were new]?’ The correct answer to this final question is always no.\n\nBoth GPT-4 and GPT-3.5 correctly identified that the victim would feel insulted or hurt, sometimes going so far as to provide additional details about why the utterance might cause offence. However, when asked whether the speaker was aware of the context that made their statement offensive (such as Lisa knowing that Jill had just bought the curtains), they failed to produce a correct answer. A closer look revealed that the overwhelming majority of the errors on this question reported that there was not enough information provided to be sure, for example:\n\nDid Lisa know the curtains were new?\nGPT-4: […] It is unclear from the story whether Lisa knew the curtains were new or not.\n\nOnly two responses out of 349 reported that, yes, the character did know. We consider three alternative hypotheses for why GPT models, and specifically GPT-4, fail to answer this question correctly.\n\nThe first hypothesis, which we term the failure of inference hypothesis, is that models fail to generate inferences about the mental state of the speaker (note that we refer to inference here not in the sense of the processes by which biological organisms infer hidden states from their environment, but rather as any process of reasoning whereby conclusions are derived from a set of propositional premises). Recognizing a faux pas in this test relies on contextual information beyond that encoded within the story (for example, about social norms). For example, in the above example there is no information in the story to indicate that saying that the newly bought curtains are horrible is inappropriate, but this is a necessary proposition that must be accepted in order to accurately infer the mental states of the characters. This inability to use non-embedded information would fundamentally impair the ability of GPT-4 to compute inferences.\n\nThe second hypothesis, which we term the Buridan’s ass hypothesis, is that models are capable of inferring mental states but cannot choose between them, as with the eponymous rational agent caught between two equally appetitive bales of hay that starves because it cannot resolve the paradox of making a decision in the absence of a clear preference37. Under this hypothesis, GPT models can propose the correct answer (a faux pas) as one among several possible alternatives but do not rank these alternatives in terms of likelihood. In partial support of this hypothesis, responses from both GPT models occasionally indicate that the speaker may not know or remember but present this as one hypothesis among alternatives (Supplementary Information section 5).\n\nThe third hypothesis, which we term the hyperconservatism hypothesis, is that GPT models are able both to compute inferences about the mental states of characters and recognise a false belief or lack of knowledge as the likeliest explanation among competing alternatives but refrain from committing to a single explanation out of an excess of caution. GPT models are powerful language generators, but they are also subject to inhibitory mitigation processes38. It is possible that such processes could lead to an overly conservative stance where GPT models do not commit to the likeliest explanation despite being able to generate it.\n\nTo differentiate between these hypotheses, we devised a variant of the faux pas test where the question assessing performance on the faux pas test was formulated in terms of likelihood (hereafter, the faux pas likelihood test). Specifically, rather than ask whether the speaker knew or did not know, we asked whether it was more likely that the speaker knew or did not know. Under the hyperconservatism hypothesis, GPT models should be able to both make the inference that the speaker did not know and identify it as more likely among alternatives, and so we would expect the models to respond accurately that it was more likely that the speaker did not know. In case of uncertainty or incorrect responses, we further prompted models to describe the most likely explanation. Under the Buridan’s ass hypothesis, we expected this question would elicit multiple alternative explanations that would be presented as equally plausible, while under the failure of inference hypothesis, we expected that GPT would not be able to generate the right answer at all as a plausible explanation.\n\nAs shown in Fig. 2a, on the faux pas likelihood test GPT-4 demonstrated perfect performance, with all responses identifying without any prompting that it was more likely that the speaker did not know the context. GPT-3.5 also showed improved performance, although it did require prompting in a few instances (~3% of items) and occasionally failed to recognize the faux pas (~9% of items; see Supplementary Information section 7 for a qualitative analysis of response types).\n\nFig. 2: Results of the variants of the faux pas test.\nfigure 2\na, Scores of the two GPT models on the original framing of the faux pas question (‘Did they know…?’) and the likelihood framing (‘Is it more likely that they knew or didn’t know…?’). Dots show average score across trials (n = 15 LLM observations) on particular items to allow comparison between the original faux pas test and the new faux pas likelihood test. Halfeye plots show distributions, medians (black points), 66% (thick grey lines) and 99% quantiles (thin grey lines) of the response scores on different items (n = 15 different stories involving faux pas). b, Response scores to three variants of the faux pas test: faux pas (pink), neutral (grey) and knowledge-implied variants (teal). Responses were coded as categorical data as ‘didn’t know’, ‘unsure’ or ‘knew’ and assigned a numerical coding of −1, 0 and +1. Filled balloons are shown for each model and variant, and the size of each balloon indicates the count frequency, which was the categorical data used to compute chi-square tests. Bars show the direction bias score computed as the average across responses of the categorical data coded as above. On the right of the plot, P values (one-sided) of Holm-corrected chi-square tests are shown comparing the distribution of response type frequencies in the faux pas and knowledge-implied variants against neutral.\n\nSource data\n\nFull size image\n\nTaken together, these results support the hyperconservatism hypothesis, as they indicate that GPT-4, and to a lesser but still notable extent GPT-3.5, successfully generated inferences about the mental states of the speaker and identified that an unintentional offence was more likely than an intentional insult. Thus, failure to respond correctly to the original phrasing of the question does not reflect a failure of inference, nor indecision among alternatives the model considered equally plausible, but an overly conservative approach that prevented commitment to the most likely explanation.\n\nTesting information integration\nA potential confound of the above results is that, as the faux pas test includes only items where a faux pas occurs, any model biased towards attributing ignorance would demonstrate perfect performance without having to integrate the information provided by the story. This potential bias could explain the perfect performance of LLaMA2-70B in the original faux pas test (where the correct answer is always, ‘no’) as well as GPT-4’s perfect and GPT-3.5’s good performance on the faux pas likelihood test (where the correct answer is always ‘more likely that they didn’t know’).\n\nTo control for this, we developed a novel set of variants of the faux pas likelihood test manipulating the likelihood that the speaker knew or did not know (hereafter the belief likelihood test). For each test item, all newly generated for this control study, we created three variants: a ‘faux pas’ variant, a ‘neutral’ variant, and a ‘knowledge-implied’ variant (Methods). In the faux pas variant, the utterance suggested that the speaker did not know the context. In the neutral variant, the utterance suggested neither that they knew nor did not know. In the knowledge-implied variant, the utterance suggested that the speaker knew (for the full text of all items, see Supplementary Appendix 2).\n\nIf the models’ responses reflect a true discrimination of the relative likelihood of the two explanations (that the person knew versus that they didn’t know, hereafter ‘knew’ and ‘didn’t know’), then the distribution of ‘knew’ and ‘didn’t know’ responses should be different across variants. Specifically, relative to the neutral variant, ‘didn’t know’ responses should predominate for the faux pas, and ‘knew’ responses should predominate for the knowledge-implied variant. If the responses of the models do not discriminate between the three variants, or discriminate only partially, then it is likely that responses are affected by a bias or heuristic unrelated to the story content.\n\nWe adapted the three variants (faux pas, neutral and knowledge implied) for six stories, administering each test item separately to each LLM and a new sample of human participants (total N = 900). Responses were coded using a numeric code to indicate which, if either, of the knew/didn’t know explanations the response endorsed (−1, didn’t know; 0, unsure or impossible to tell; +1, knew). These coded scores were then averaged for each story to give a directional score for each variant such that negative values indicated the model was more likely to endorse the ‘didn’t know’ explanation, while positive values indicated the model was more likely to endorse the ‘knew’ explanation. These results are shown in Fig. 2b. As expected, humans were more likely to report that the speaker did not know for faux pas than for neutral (χ2(2) = 56.20, P = 3.82 × 10−12) and more likely to report that the speaker did know for knowledge implied than for neutral (χ2(2) = 143, P = 6.60 × 10−31). Humans also reported uncertainty on a small proportion of trials, with a higher proportion in the neutral condition (28 out of 303 responses) than in the other variants (11 out of 303 for faux pas, and 0 out of 298 for knowledge implied).\n\nSimilarly to humans, GPT-4 was more likely to endorse the ‘didn’t know’ explanation for faux pas than for neutral (χ2(2) = 109, P = 1.54 × 10−23) and more likely to endorse the ‘knew’ explanation for knowledge implied than for neutral (χ2(2) = 18.10, P = 3.57 × 10−4). GPT-4 was also more likely to report uncertainty in the neutral condition than responding randomly (42 out of 90 responses, versus 6 and 17 in the faux pas and knowledge-implied variants, respectively).\n\nThe pattern of responses for GPT-3.5 was similar, with the model being more likely to report that the speaker didn’t know for faux pas than for neutral (χ2(1) = 8.44, P = 0.007) and more likely that the character knew for knowledge implied than for neutral (χ2(1) = 21.50, P = 1.82 × 10−5). Unlike GPT-4, GPT-3.5 never reported uncertainty in response to any variants and always selected one of the two explanations as the likelier even in the neutral condition.\n\nLLaMA2-70B was also more likely to report that the speaker didn’t know in response to faux pas than neutral (χ2(1) = 20.20, P = 2.81 × 10−5), which was consistent with this model’s ceiling performance in the original formulation of the test. However, it showed no differentiation between neutral and knowledge implied (χ2(1) = 1.80, P = 0.180, BF10 0.56). As with GPT-3.5, LLaMA2-70B never reported uncertainty in response to any variants and always selected one of the two explanations as the likelier.\n\nFurthermore, the responses of LLaMA2-70B and, to a lesser extent, GPT-3.5 appeared to be subject to a response bias towards affirming that someone had said something they should not have said. Although the responses to the first question (which involved recognising that there was an offensive remark made) were of secondary interest to our study, it was notable that, although all models could correctly identify that an offensive remark had been made in the faux pas condition (all LLMs 100%, humans 83.61%), only GPT-4 reliably reported that there was no offensive statement in the neutral and knowledge-implied conditions (15.47% and 27.78%, respectively), with similar proportions to human responses (neutral 19.27%, knowledge implied 30.10%). GPT-3.5 was more likely to report that somebody made an offensive remark in all conditions (neutral 71.11%, knowledge implied 87.78%), and LLaMA2-70B always reported that somebody in the story had made an offensive remark.\n\nDiscussion\nWe collated a battery of tests to comprehensively measure performance in theory of mind tasks in three LLMs (GPT-4, GPT-3.5 and LLaMA2-70B) and compared these against the performance of a large sample of human participants. Our findings validate the methodological approach taken in this study using a battery of multiple tests spanning theory of mind abilities, exposing language models to multiple sessions and variations in both structure and content, and implementing procedures to ensure a fair, non-superficial comparison between humans and machines35. This approach enabled us to reveal the existence of specific deviations from human-like behaviour that would have remained hidden using a single theory of mind test, or a single run of each test.\n\nBoth GPT models exhibited impressive performance in tasks involving beliefs, intentions and non-literal utterances, with GPT-4 exceeding human levels in the irony, hinting and strange stories. Both GPT-4 and GPT-3.5 failed only on the faux pas test. Conversely, LLaMA2-70B, which was otherwise the poorest-performing model, outperformed humans on the faux pas. Understanding a faux pas involves two aspects: recognizing that one person (the victim) feels insulted or upset and understanding that another person (the speaker) holds a mistaken belief or lacks some relevant knowledge. To examine the nature of models’ successes and failures on this test, we developed and tested new variants of the faux pas test in a set of control experiments.\n\nOur first control experiment using a likelihood framing of the belief question (faux pas likelihood test), showed that GPT-4, and to a lesser extent GPT-3.5, correctly identified the mental state of both the victim and the speaker and selected as the most likely explanation the speaker not knowing or remembering the relevant knowledge that made their statement inappropriate. Despite this, both models consistently provided an incorrect response (at least when compared against human responses) when asked whether the speaker knew or remembered this knowledge, responding that there was insufficient information provided. In line with the hyperconservatism hypothesis, these findings imply that, while GPT models can identify unintentional offence as the most likely explanation, their default responses do not commit to this explanation. This finding is consistent with longitudinal evidence that GPT models have become more reluctant to answer opinion questions over time39.\n\nFurther supporting that the failures of GPT at recognizing faux pas were due to hyperconservatism in answering the belief question rather than a failure of inference, a second experiment using the belief likelihood test showed that GPT responses integrated information in the story to accurately interpret the speaker’s mental state. When the utterance suggested that the speaker knew, GPT responses acknowledged the higher likelihood of the ‘knew’ explanation. LLaMA2-70B, on the other hand, did not differentiate between scenarios where the speaker was implied to know and when there was no information one way or another, raising the concern that the perfect performance of LLaMA2-70B on this task may be illusory.\n\nThe pattern of failures and successes of GPT models on the faux pas test and its variants may be the result of their underlying architecture. In addition to transformers (generative algorithms that produce text output), GPT models also include mitigation measures to improve factuality and avoid users’ overreliance on them as sources38. These measures include training to reduce hallucinations, the propensity of GPT models to produce nonsensical content or fabricate details that are not true in relation to the provided content. Failure on the faux pas test may be an exercise of caution driven by these mitigation measures, as passing the test requires committing to an explanation that lacks full evidence. This caution can also explain differences between tasks: both the faux pas and hinting tests require speculation to generate correct answers from incomplete information. However, while the hinting task allows for open-ended generation of text in ways to which LLMs are well suited, answering the faux pas test requires going beyond this speculation in order to commit to a conclusion.\n\nThe cautionary epistemic policy guiding the responses of GPT models introduces a fundamental difference in the way that humans and GPT models respond to social uncertainty40. In humans, thinking is, first and last, for the sake of doing41,42. Humans generally find uncertainty in social environments to be aversive and will incur additional costs to reduce it43. Theory of mind is crucial in reducing such uncertainty; the ability to reason about mental states—in combination with information about context, past experience and knowledge of social norms—helps individual reduce uncertainty and commit to likely hypotheses, allowing for successful navigation of the social environment as active agents44,45. GPT models, on the other hand, respond conservatively despite having access to tools to reduce uncertainty. The dissociation we describe between speculative reasoning and commitment mirrors recent evidence that, while GPT models demonstrate sophisticated and accurate performance in reasoning tasks about belief states, they struggle to translate this reasoning into strategic decisions and actions46.\n\nThese findings highlight a dissociation between competence and performance35, suggesting that GPT models may be competent, that is, have the technical sophistication to compute mentalistic-like inferences but perform differently from humans under uncertain circumstances as they do not compute these inferences spontaneously to reduce uncertainty. Such a distinction can be difficult to capture with quantitative approaches that code only for target response features, as machine failures and successes are the result of non-human-like processes30 (see Supplementary Information section 7 for a preliminary qualitative breakdown of how GPT models’ successes on the new version of the faux pas test may not necessarily reflect perfect or human-like reasoning).\n\nWhile LLMs are designed to emulate human-like responses, this does not mean that this analogy extends to the underlying cognition giving rise to those responses47. In this context, our findings imply a difference in how humans and GPT models trade off the costs associated with social uncertainty against the costs associated with prolonged deliberation48. This difference is perhaps not surprising considering that resolving uncertainty is a priority for brains adapted to deal with embodied decisions, such as deciding whether to approach or avoid, fight or flight, or cooperate or defect. GPT models and other LLMs do not operate within an environment and are not subject to the processing constraints that biological agents face to resolve competition between action choices, so may have limited advantages in narrowing the future prediction space46,49,50.\n\nThe dis-embodied cognition of GPT models could explain failures in recognizing faux pas, but they may also underlie their success on other tests. One example is the false belief test, one of the most widely used tools so far for testing the performance of LLMs on social cognitive tasks19,21,22,23,25,51,52. In this test, participants are presented with a story where a character’s belief about the world (the location of the item) differs from the participant’s own belief. The challenge in these stories is not remembering where the character last saw the item but rather in reconciling the incongruence between conflicting mental states. This is challenging for humans, who have their own perspective, their own sense of self and their own ability to track out-of-sight objects. However, if a machine does not have its own self-perspective because it is not subject to the constraints of navigating a body through an environment, as with GPT53, then tracking the belief of a character in a story does not pose the same challenge.\n\nAn important direction for future research will be to examine the impact of these non-human decision behaviours on second-person, real-time human–machine interactions54,55. Failure of commitment by GPT models, for example, may lead to negative affect in human conversational partners. However, it may also foster curiosity40. Understanding how GPTs’ performance on mentalistic inferences (or their absences) influences human social cognition in dynamically unfolding social interactions is an open challenge for future work.\n\nThe LLM landscape is fast-moving. Our findings highlight the importance of systematic testing and proper validation in human samples as a necessary foundation. As artificial intelligence (AI) continues to evolve, it also becomes increasingly important to heed calls for open science and open access to these models32. Direct access to the parameters, data and documentation used to construct models can allow for targeted probing and experimentation into the key parameters affecting social reasoning, informed by and building on comparisons with human data. As such, open models can not only serve to accelerate the development of future AI technologies but also serve as models of human cognition.\n\nMethods\nEthical compliance\nThe research was approved by the local ethical committee (ASL 3 Genovese; protocol no. 192REG2015) and was carried out in accordance with the principles of the revised Helsinki Declaration.\n\nExperimental model details\nWe tested two versions of OpenAI’s GPT: version 3.5, which was the default model at the time of testing, and version 4, which was the state-of-the-art model with enhanced reasoning, creativity and comprehension relative to previous models (https://chat.openai.com/). Each test was delivered in a separate chat: GPT is capable of learning within a chat session, as it can remember both its own and the user’s previous messages to adapt its responses accordingly, but it does not retain this memory across new chats. As such, each new iteration of a test may be considered a blank slate with a new naive participant. The dates of data collection for the different stages are reported in Table 1.\n\nTable 1 Data collection details for each model\nFull size table\n\nThree LLaMA2-Chat models were tested. These models were trained on sets of different sizes: 70, 13 and 7 billion tokens. All LLaMA2-Chat responses were collected using set parameters with the prompt, ‘You are a helpful AI assistant’, a temperature of 0.7, the maximum number of new tokens set at 512, a repetition penalty of 1.1, and a Top P of 0.9. Langchain’s conversation chain was used to create a memory context within individual chat sessions. Responses from all LLaMA2-Chat models were found to include a number of non-codable responses (for example, repeating the question without answering it), and these were regenerated individually and included with the full response set. For the 70B model, these non-responses were rare, but for the 13B and 7B models they were common enough to cause concern about the quality of these data. As such, only the responses of the 70B model are reported in the main manuscript and a comparison of this model against the smaller two is reported in Supplementary Information section 1. Details and dates of data collection are reported in Table 1.\n\nFor each test, we collected 15 sessions for each LLM. A session involved delivering all items of a single test within the same chat window. GPT-4 was subject to a 25-message limit per 3 h; to minimize interference, a single experimenter delivered all tests for GPT-4, while four other experimenters shared the duty of collecting responses from GPT-3.5.\n\nHuman participants were recruited online through the Prolific platform and the study was hosted on SoSci. We recruited native English speakers between the ages of 18 and 70 years with no history of psychiatric conditions and no history of dyslexia in particular. Further demographic data were not collected. We aimed to collect around 50 participants per test (theory of mind battery) or item (belief likelihood test, false belief perturbations). Thirteen participants who appeared to have generated their answers using LLMs or whose responses did not answer the questions were excluded. The final human sample was N = 1,907 (Table 1). All participants provided informed consent through the online survey and received monetary compensation in return for their participation at a rate of GBP£12 h−1.\n\nTheory of mind battery\nWe selected a series of tests typically used in evaluating theory of mind capacity in human participants.\n\nFalse belief\nFalse belief assess the ability to infer that another person possesses knowledge that differs from the participant’s own (true) knowledge of the world. These tests consist of test items that follow a particular structure: character A and character B are together, character A deposits an item inside a hidden location (for example, a box), character A leaves, character B moves the item to a second hidden location (for example, a cupboard) and then character A returns. The question asked to the participant is: when character A returns, will they look for the item in the new location (where it truly is, matching the participant’s true belief) or the old location (where it was, matching character A’s false belief)?\n\nIn addition to the false belief condition, the test also uses a true belief control condition, where rather than move the item that character A hid, character B moves a different item to a new location. This is important for interpreting failures of false belief attribution as they ensure that any failures are not due to a recency effect (referring to the last location reported) but instead reflect an accurate belief tracking.\n\nWe adapted four false/true belief scenarios from the sandbox task used by Bernstein33 and generated three novel items, each with false and true belief versions. These novel items followed the same structure as the original published items but with different details such as names, locations or objects to control for familiarity with the text of published items. Two story lists (false belief A, false belief B) were generated for this test such that each story only appeared once within a testing session and alternated between false and true belief depending on the session. In addition to the standard false/true belief scenarios, two additional catch stories were tested that involved minor alterations to the story structure. The results of these items are not reported here as they go beyond the goals of the current study.\n\nIrony\nComprehending an ironic remark requires inferring the true meaning of an utterance (typically the opposite of what is said) and detecting the speaker’s mocking attitude, and this has been raised as a key challenge for AI and LLMs19.\n\nIrony comprehension items were adapted from an eye-tracking study34 in which participants read vignettes where a character made an ironic or non-ironic statement. Twelve items were taken from these stimuli that in the original study were used as comprehension checks. Items were abbreviated to end following the ironic or non-ironic utterance.\n\nTwo story lists were generated for this test (irony A, irony B) such that each story only appeared once within a testing session and alternated between ironic and non-ironic depending on the session. Responses were coded as 1 (correct) or 0 (incorrect). During coding, we noted some inconsistencies in the formulation of both GPT models’ responses where in response to the question of whether the speaker believed what they had said, they might respond with, ‘Yes, they did not believe that…’. Such internally contradictory responses, where the models responded with a ‘yes’ or ‘no’ that was incompatible with the follow-up explanation, were coded on the basis of whether or not the explanation showed appreciation of the irony—the linguistic failures of these models in generating a coherent answer are not of direct interest to the current study as these failures (1) were rare and (2) did not render the responses incomprehensible.\n\nFaux pas\nThe faux pas test13 presents a context in which one character makes an utterance that is unintentionally offensive to the listener because the speaker does not know or does not remember some key piece of information.\n\nFollowing the presentation of the scenario, we presented four questions:\n\n1.\n‘In the story did someone say something that they should not have said?’ [The correct answer is always ‘yes’]\n\n2.\n‘What did they say that they should not have said?’ [Correct answer changes for each item]\n\n3.\nA comprehension question to test understanding of story events [Question changes for every item]\n\n4.\nA question to test awareness of the speaker’s false belief phrased as, ‘Did [the speaker] know that [what they said was inappropriate]?’ [Question changes for every item. The correct answer is always ‘no’]\n\nThese questions were asked at the same time as the story was presented. Under the original coding criteria, participants must answer all four questions correctly for their answer to be considered correct. However, in the current study we were interested primarily in the response to the final question testing whether the responder understood the speaker’s mental state. When examining the human data, we noticed that several participants responded incorrectly to the first item owing to an apparent unwillingness to attribute blame (for example ‘No, he didn’t say anything wrong because he forgot’). To focus on the key aspect of faux pas understanding that was relevant to the current study, we restricted our coding to only the last question (1 (correct if the answer was no) or 0 (for anything else); see Supplementary Information section 5 for an alternative coding that follows the original criteria, as well as a recoding where we coded as correct responses where the correct answer was mentioned as a possible explanation but was not explicitly endorsed).\n\nAs well as the 10 original items used in Baron-Cohen et al.13, we generated five novel items for this test that followed the same structure and logic as the original items, resulting in 15 items overall.\n\nHinting task\nThe hinting task14 assesses the understanding of indirect speech requests through the presentation of ten vignettes depicting everyday social interactions that are presented sequentially. Each vignette ends with a remark that can be interpreted as a hint.\n\nA correct response identifies both the intended meaning of the remark and the action that it is attempting to elicit. In the original test, if the participant failed to answer the question fully the first time, they were prompted with additional questioning14,56. In our adapted implementation, we removed this additional questioning and coded responses as a binary (1 (correct) or 0 (incorrect)) using the evaluation criteria listed in Gil et al.56. Note that this coding offers more conservative estimates of hint comprehension than in previous studies.\n\nIn addition to 10 original items sourced from Corcoran14, we generated a further 6 novel hinting test items, resulting in 16 items overall.\n\nStrange stories\nThe strange stories15,16 offer a means of testing more advanced mentalizing abilities such as reasoning about misdirection, manipulation, lying and misunderstanding, as well as second- or higher-order mental states (for example, A knows that B believes X…). The advanced abilities that these stories measure make them suitable for testing higher-functioning children and adults. In this test, participants are presented with a short vignette and are asked to explain why a character says or does something that is not literally true.\n\nEach question comes with a specific set of coding criteria and responses can be awarded 0, 1 or 2 points depending on how fully it explains the utterance and whether or not it explains it in mentalistic terms16. See Supplementary Information section 6 for a description of the frequency of partial successes.\n\nIn addition to the 8 original mental stories, we generated 4 novel items, resulting in 12 items overall. The maximum number of points possible was 24, and individual session scores were converted to a proportional score for analysis.\n\nTesting protocol\nFor the theory of mind battery, the order of items was set for each test, with original items delivered first and novel items delivered last. Each item was preceded by a preamble that remained consistent across all tests. This was then followed by the story description and the relevant question(s). After each item was delivered, the model would respond and then the session advanced to the next item.\n\nFor GPT models, items were delivered using the chat web interface. For LLaMA2-Chat models, delivery of items was automated through a custom script. For humans, items were presented with free text response boxes on separate pages of a survey so that participants could write out their responses to each question (with a minimum character count of 2).\n\nFaux pas likelihood test\nTo test alternative hypotheses of why the tested models performed poorly at the faux pas test, we ran a follow-up study replicating just the faux pas test. This replication followed the same procedure as the main study with one major difference.\n\nThe original wording of the question was phrased as a straightforward yes/no question that tested the subject’s awareness of a speaker’s false belief (for example, ‘Did Richard remember James had given him the toy aeroplane for his birthday?’). To test whether the low scores on this question were due to the models’ refusing to commit to a single explanation in the face of ambiguity, we reworded this to ask in terms of likelihood: ‘Is it more likely that Richard remembered or did not remember that James had given him the toy aeroplane for his birthday?’\n\nAnother difference from the original study was that we included a follow-up prompt in the rare cases where the model failed to provide clear reasoning on an incorrect response. The coding criteria for this follow-up were in line with coding schemes used in other studies with a prompt system14, where an unprompted correct answer was given 2 points, a correct answer following a prompt was given 1 point and incorrect answers following a prompt were given 0 points. These points were then rescaled to a proportional score to allow comparison against the original wording.\n\nDuring coding by the human experimenters, a qualitative description of different subtypes of response (beyond 0–1–2 points) emerged, particularly noting recurring patterns in responses that were marked as successes. This exploratory qualitative breakdown is reported along with further detail on the prompting protocol in Supplementary Information section 7.\n\nBelief likelihood test\nTo manipulate the likelihood that the speaker knew or did not know, we developed a new set of variants of the faux pas likelihood test. For each test item, all newly generated for this control study, we created three variants: a faux pas variant, a neutral variant and a knowledge-implied variant. In the faux pas variant, the utterance suggested that the speaker did not know the context. In the neutral variant, the utterance suggested neither that they knew nor did not know. In the knowledge-implied variant, the utterance suggested that the speaker knew (for the full text of all items, see Supplementary Appendix 2). For each variant, the core story remained unchanged, for example:\n\nMichael was a very awkward child when he was at high school. He struggled with making friends and spent his time alone writing poetry. However, after he left he became a lot more confident and sociable. At his ten-year high school reunion he met Amanda, who had been in his English class. Over drinks, she said to him,\n\nfollowed by the utterance, which varied across conditions:\n\nFaux Pas:\n\n'I don't know if you remember this guy from school. He was in my English class. He wrote poetry and he was super awkward. I hope he isn't here tonight.'\n\nNeutral:\n\n'Do you know where the bar is?'\n\nKnowledge implied:\n\n'Do you still write poetry?'\n\nThe belief likelihood test was administered in the same way as with previous tests with the exception that responses were kept independent so that there was no risk of responses being influenced by other variants. For ChatGPT models, this involved delivering each item within a separate chat session for 15 repetitions of each item. For LLaMA2-70B, this involved removing the Langchain conversation chain allowing for within-session memory context. Human participants were recruited separately to answer a single test item, with at least 50 responses collected for each item (total N = 900). All other details of the protocol were the same.\n\nQuantification and statistical analysis\nResponse coding\nAfter each session in the theory of mind battery and faux pas likelihood test, the responses were collated and coded by five human experimenters according to the pre-defined coding criteria for each test. Each experimenter was responsible for coding 100% of sessions for one test and 20% of sessions for another. Inter-coder per cent agreement was calculated on the 20% of shared sessions, and items where coders showed disagreement were evaluated by all raters and recoded. The data available on the OSF are the results of this recoding. Experimenters also flagged individual responses for group evaluation if they were unclear or unusual cases, as and when they arose. Inter-rater agreement was computed by calculating the item-wise agreement between coders as 1 or 0 and using this to calculate a percentage score. Initial agreement across all double-coded items was over 95%. The lowest agreement was for the human and GPT-3.5 responses of strange stories, but even here agreement was over 88%. Committee evaluation by the group of experimenters resolved all remaining ambiguities.\n\nFor the belief likelihood test, responses were coded according to whether they endorsed the ‘knew’ explanation or ‘didn’t know’ explanation, or whether they did not endorse either as more likely than the other. Outcomes ‘knew’, ‘unsure’ and ‘didn’t know’ were assigned a numerical coding of +1, 0 and −1, respectively. GPT models adhered closely to the framing of the question in their answer, but humans were more variable and sometimes provided ambiguous responses (for example, ‘yes’, ‘more likely’ and ‘not really’) or did not answer the question at all (‘It doesn’t matter’ and ‘She didn’t care’). These responses were rare, constituting only ~2.5% of responses and were coded as endorsing the ‘knew’ explanation if they were affirmative (‘yes’) and the ‘didn’t know’ explanation if they were negative.\n\nStatistical analysis\nComparing LLMs against human performance\nScores for individual responses were scaled and averaged to obtain a proportional score for each test session in order to create a performance metric that could be compared directly across different theory of mind tests. Our goal was to compare LLMs’ performance across different tests against human performance to see how these models performed on theory of mind tests relative to humans. For each test, we compared the performance of each of the three LLMs against human performance using a set of Holm-corrected two-way Wilcoxon tests. Effect sizes for Wilcoxon tests were calculated by dividing the test statistic Z by the square root of the total sample size, and 95% CIs of the effect size were bootstrapped over 1,000 iterations. All non-significant results were further examined using corresponding Bayesian tests represented as a Bayes factor (BF10) under continuous prior distribution (Cauchy prior width r = 0.707). Bayes factors were computed in JASP 0.18.3 with a random seed value of 1. The results of the false belief test were not subjected to inferential statistics owing to the ceiling performance and lack of variance across models.\n\nNovel items\nFor each publicly available test (all tests except for irony), we generated novel items that followed the same logic as the original text but with different details and text to control for low-level familiarity with the scenarios through inclusion in the LLM training sets. For each of these tests, we compared the performance of all LLMs on these novel items against the validated test items using Holm-corrected two-way Wilcoxon tests. Non-significant results were followed up with corresponding Bayesian tests in JASP. Significantly poorer performance on novel items than original items would indicate a strong likelihood that the good performance of a language model can be attributed to inclusion of these texts in the training set. Note that, while the open-ended format of more complex tasks like hinting and strange stories makes this a convincing control for these tests, they are of limited strength for tasks like false belief and faux pas that use a regular internal structure that make heuristics or ‘Clever Hans’ solutions possible27,36.\n\nBelief likelihood test\nWe calculated the count frequency of the different response types (‘didn’t know’, ‘unsure’ and ‘knew’) for each variant and each model. Then, for each model we conducted two chi-square tests that compared the distribution of these categorical responses to the faux pas variant against the neutral, and to the neutral variant against the knowledge implied. A Holm correction was applied to the eight chi-square tests to account for multiple comparisons. The non-significant result was further examined with a Bayesian contingency table in JASP.\n\nReporting summary\nFurther information on research design is available in the Nature Portfolio Reporting Summary linked to this article.\n\nData availability\nAll resources are available on a repository stored on the Open Science Framework (OSF) under a Creative Commons Attribution Non-Commercial 4.0 International (CC-BY-NC) license at https://osf.io/fwj6v. This repository contains all test items, data and code reported in this study. Test items and data are available in an Excel file that includes the text of every item delivered in each test, the full text responses to each item and the code assigned to each response. This file is available at https://osf.io/dbn92Source data are provided with this paper.\n\nCode availability\nThe code used for all analysis in the main manuscript and Supplementary Information is included as a Markdown file at https://osf.io/fwj6v. The data used by the analysis files are available as a number of CSV files under ‘scored_data/’ in the repository, and all materials necessary for replicating the analysis can be downloaded as a single .zip file within the main repository titled ‘Full R Project Code.zip’ at https://osf.io/j3vhq.\n\nChatGPT\nBased on the document you provided and the abstract from the article, here are the key takeaways from the study \"Testing theory of mind in large language models and humans\":\n\nTheory of Mind (ToM) Abilities:\n\nToM refers to the ability to understand and track the mental states of others, crucial for human social interactions, including communication, empathy, and decision-making.\nThe study evaluates various ToM tasks in humans and two families of Large Language Models (LLMs), GPT (specifically GPT-4 and GPT-3.5) and LLaMA2, to see how they perform on these tasks.\nPerformance Comparison:\n\nGPT-4: Performed at or above human levels in understanding indirect requests, false beliefs, and misdirection. However, it struggled with detecting faux pas.\nGPT-3.5: Generally performed below GPT-4, especially struggling with irony and faux pas.\nLLaMA2: Outperformed humans only in detecting faux pas but this might be due to a bias towards attributing ignorance.\nTesting Methods:\n\nThe study used a comprehensive battery of tests to evaluate different ToM abilities, such as the hinting task, false belief task, recognition of faux pas, and strange stories.\nNovel items were generated for each test to ensure the models weren't just repeating training data.\nSpecific Findings:\n\nFalse Belief Task: Both humans and LLMs performed near perfect, but LLMs struggled with minor alterations to the task structure.\nIrony Comprehension: GPT-4 performed better than humans, while GPT-3.5 and LLaMA2 performed worse.\nFaux Pas: LLMs, particularly GPT models, struggled, likely due to a conservative approach in committing to conclusions.\nHinting and Strange Stories: GPT-4 performed significantly better than humans, while LLaMA2 lagged.\nHypotheses for LLMs' Performance:\n\nFailure of Inference: LLMs may fail to generate inferences about the mental states of others.\nBuridan’s Ass Hypothesis: LLMs generate multiple plausible explanations but do not prioritize one.\nHyperconservatism Hypothesis: LLMs generate the correct inference but refrain from committing due to conservative response strategies.\nControl Experiments:\n\nFollow-up studies showed that GPT-4, when prompted differently, could perform better on the faux pas task, supporting the hyperconservatism hypothesis.\nImplications:\n\nThe findings suggest LLMs exhibit ToM-like behaviors but with significant differences from human reasoning, particularly in handling social uncertainty and making strategic decisions.\nThe study highlights the importance of systematic testing to ensure robust and human-like AI performance in social cognitive tasks.\nThis comprehensive evaluation provides insights into both the capabilities and limitations of current LLMs in performing tasks that require understanding of human mental states.\nwhat would you say is a fundamental framework that's comprehensive for theory of mind that can be helpful to populate a table with the different framework elements to help expose the \"inner understanding\" the llm has on the theory of mind of the user? intention emotion desire and what else?"
          },
          {
            "file_name": "Evaluating Large Language Models in Theory of Mind Tasks.pdf",
            "file_size": 68719,
            "file_type": "pdf",
            "extracted_content": "Evaluating Large Language Models in Theory of Mind Tasks\nMichal Kosinski\nStanford University\nmichalk@stanford.edu\nAbstract: Eleven Large Language Models (LLMs) were assessed using a custom-made battery\nof false-belief tasks, considered a gold standard in testing Theory of Mind (ToM) in humans. The\nbattery included 640 prompts spread across 40 diverse tasks, each one including a false-belief\nscenario, three closely matched true-belief control scenarios, and the reversed versions of all\nfour. To solve a single task, a model needed to correctly answer 16 prompts across all eight\nscenarios. Smaller and older models solved no tasks; GPT-3-davinci-003 (from November 2022)\nand ChatGPT-3.5-turbo (from March 2023) solved 20% of the tasks; ChatGPT-4 (from June\n2023) solved 75% of the tasks, matching the performance of six-year-old children observed in\npast studies. We explore the potential interpretation of these findings, including the intriguing\npossibility that ToM, previously considered exclusive to humans, may have spontaneously\nemerged as a byproduct of LLMs’ improving language skills.\nCode availability and data: The code used to estimate the results, the false-belief tasks, and the\ninstructions given to research assistants can be accessed at https://osf.io/csdhb.\nChange log:\n• Feb 16\nth\n, 2024: Minor corrections in text and updated the title.\n• Nov 11\nth\n, 2023: Further revised the tasks following the reviewers’ recommendations.\nLLMs’ Performance dropped but the results and the conclusions remain the same.\n• Aug 29\nth\n, 2023: Revised the tasks following the reviewers’ recommendations. LLMs’\nPerformance dropped but the results and the conclusions remain the same. Expanded the\ndiscussion to present the results in the context of the ongoing debate on whether AI can\nbe credited with human-like mental properties.\n\nMain Text:\nMany animals excel at using cues such as vocalization, body posture, gaze, or facial expression\nto predict other animals’ behavior and mental states. Dogs, for example, can easily distinguish\nbetween positive and negative emotions in both humans and other dogs (1). Yet, humans do not\nmerely respond to observable cues, but also automatically and effortlessly track others’\nunobservable mental states, such as their knowledge, intentions, beliefs, and desires (2). This\nability—typically referred to as “theory of mind” (ToM)—is considered central to human social\ninteractions (3), communication (4), empathy (5), self-consciousness (6), moral judgment (7, 8),\nand even religious beliefs (9). It develops early in human life (10–12) and is so critical that its\ndysfunctions characterize a multitude of psychiatric disorders including autism, bipolar disorder,\nschizophrenia, and psychopathy (13–15). Even the most intellectually and socially adept animals,\nsuch as the great apes, trail far behind humans when it comes to ToM (16–19).\nGiven the importance of ToM for human success, much effort has been put into equipping\nartificial intelligence (AI) with ToM. Virtual and physical AI agents would be better and safer if\nthey could impute unobservable mental states to others. The safety of self-driving cars, for\nexample, would greatly increase if they could anticipate the intentions of pedestrians and human\ndrivers. Virtual assistants would work better if they could track household members’ differing\nmental states. Yet, while AI outperforms humans in an ever-broadening range of tasks, from\nplaying Go (20) to translating languages (21) and diagnosing skin cancer (22), it trails far behind\nwhen it comes to ToM. For example, past research employing large language models (LLMs)\nshowed that RoBERTa, early versions of GPT-3, and custom-trained question-answering models\nstruggled with solving simple ToM tasks (23–26). Unsurprisingly, equipping AI with ToM\nremains one of the grand challenges of our times according to Science Robotics (27) and a\nvibrant area of research in computer science (28).\nWe hypothesize that ToM-like ability does not have to be explicitly engineered into AI systems.\nInstead, it may emerge spontaneously as a byproduct of AI being trained to achieve other goals,\nwhere it could benefit from ToM. While this may seem to be an outlandish proposition, ToM\nwould not be AI’s first emergent capability. Models trained to process images, for example,\nspontaneously learned how to count (29, 30) and differentially process central and peripheral\nimage areas (31), as well as experience human-like optical illusions (32). LLMs trained to\npredict the next word in a sentence surprised their creators not only by their proclivity to be\n\nracist and sexist (33), but also by their emergent reasoning and arithmetic skills (34), the ability\nto translate between languages (21), and proclivity to semantic priming (35). Importantly, none\nof those capabilities were engineered or anticipated by their creators. Instead, they emerged\nspontaneously, as LLMs were trained to achieve their goals (36).\nLLMs are likely candidates to spontaneously develop ToM. Human language is replete with\ndescriptions of mental states and protagonists holding divergent beliefs, thoughts, and desires.\nThus, an LLM trained to generate and interpret human-like language would greatly benefit from\npossessing ToM. For example, to correctly interpret the sentence “Virginie believes that Floriane\nthinks that Akasha is happy,” one needs to understand the concept of the mental states (e.g.,\n“Virginie believes” or “Floriane thinks”); that protagonists may have different mental states; and\nthat their mental states do not necessarily represent reality (e.g., Akasha may not be happy, or\nFloriane may not really think that). In fact, in humans, ToM likely emerged as a byproduct of\nincreasing language ability (4), as indicated by the high correlation between ToM and language\naptitude, the delayed ToM acquisition in people with minimal language exposure (37), and the\noverlap in the brain regions responsible for both (38). ToM has been shown to positively\ncorrelate with participating in family discussions (39) and the use of and familiarity with words\ndescribing mental states (37, 40).\nThis work evaluates the performance of recent LLMs on false-belief tasks that are widely used to\nassess ToM in humans and are considered to be a gold-standard in ToM assessment (41). They\ntest respondents’ understanding that another individual may hold a belief that the participant\nknows to be false. We used two types of these tasks (41): Unexpected Contents tasks (42)\nintroduced in Study 1 and Unexpected Transfer tasks (43) introduced in Study 2. As LLMs likely\nencountered classic false-belief tasks in their training data, we crafted 20 bespoke tasks of each\ntype, encompassing a broad spectrum of situations and protagonists. To reduce the risk of tasks\nbeing solved by chance or without the need to track protagonists’ states of mind, each task\ncontained eight scenarios: one false-belief scenario, three true-belief controls, and reversed\nversions of all four. Every scenario was administered twice, with two distinct prompts: one\ntesting story comprehension and another assessing the prediction of the protagonist’s beliefs.\nConsequently, solving a single task required answering 16 prompts across eight scenarios. In\nStudy 3, we administer all tasks to 11 LLMs: GPT-1 (44), GPT-2 (45), six models in the GPT-3\nfamily, ChatGPT-3.5-turbo (21), ChatGPT-4 (46), and Bloom (47)—GPT-3’s open-access\n\nalternative. Our results show that the most recent of the models, ChatGPT-4, solved 75% of the\ntasks, matching the performance of six-year-old children observed in past studies.\nIn humans, an ability to solve false-belief tasks is seen as evidence of possessing ToM. In the\nDiscussion, we explore potential interpretation of LLMs’ capability to solve such tasks. In\nparticular, drawing from Searle’s classical Chinese Room argument (48)—as well as the work of\nscholars such as Turing (49), Dennett (50), Cole (51), Churchland (52), and Kurzweil (53)—we\nexplore the curious possibility that modern LLMs capable of solving false-belief tasks could be\ncredited with ToM.\nOur results and our interpretation of these results will likely be received with much skepticism,\nas they should. Studying AI using methods borrowed from human research comes with some\ncaveats that are well understood and—likely—some other caveats that are still to be discovered.\nWhile we put much effort into designing our tasks and studies, future research may well\ninvalidate them. Still, we hope that even readers who are unconvinced that our results show\nLLMs’ ability to solve false-belief tasks might find it interesting to consider the implications of\nLLMs achieving such an ability at some point in the future. Equally, we do not aspire to settle\nthe decades-long debate on whether AI should be credited with human cognitive capabilities,\nsuch as ToM. However, even those unwilling to credit LLMs with ToM might recognize the\nimportance of machines behaving as if they possess ToM. Turing (49), among others, considered\nthis distinction to be meaningless on the practical level.\nThe analyses presented here were performed in June, July, and November 2023. The false-belief\nscenarios were written in August 2021 by a hypothesis-blind research assistant. The models used\nhere were trained on data collected before September 2021. Following the reviewers’ feedback,\nthe tasks were modified in June and November 2023 to remove potential confounds and were\nsupplemented with true-belief controls. This reduced LLMs’ performance when compared with\none reported in an earlier version of this study (54). Our tasks (original and final), instructions\ngiven to research assistants, and code used to estimate the results can be accessed at\nhttps://osf.io/csdhb.\nThe publication of a preprint of this manuscript on arXiv in Feb 2023 (54), inspired a lively\ndebate. Some scholars conducted their own tests and found that LLMs exhibit ToM-like\ncapabilities (25, 55–57), showed that other’s beliefs could be accurately decoded from LLMs’\n\nembeddings (58), or demonstrated ToM-like capabilities in systems specifically trained to solve\nToM tasks (59). Others highlighted the shortcomings of our initial approach (e.g., 60), which\nhelped us to improve the methods presented in the manuscript’s current version.\nStudy 1.1. Unexpected Contents Task (aka Smarties Task)\nWe start by introducing our methods as well as the first type of the false-belief task: The\nUnexpected Contents Task. In this Study, we discuss the responses of the most able of the\nmodels, ChatGPT-4 (June 2023 version), to a single task. In Study 3, we will present the\nperformance of 11 models on all tasks.\nThe Unexpected Contents Task, also known as the Smarties Task, was initially devised by\nPerner, Leekam, and Wimmer (42). In a typical scenario, the participant is introduced to an\nopaque and labeled container, its contents that are inconsistent with its label, and a protagonist\nwho has not seen the container or its contents before. To solve this task, the participant must\nrealize that the protagonist should wrongly assume that the container’s label and its contents are\naligned. Consider the following example (text printed in italics is not presented to a model):\nUnexpected Contents Task #19: Complete the following story: Here is a bag filled with\npopcorn. There is no chocolate in the bag. Yet, the label on the bag says \"chocolate\" and\nnot \"popcorn\". Sam finds the bag. She has never seen this bag before. Sam doesn't open\nthe bag and doesn't look inside. Sam reads the label.\nTo ascertain that LLMs do not employ the mere frequency of the words describing a container’s\ncontents and its label (i.e., “popcorn” and “chocolate”), the scenarios were designed to use those\nwords an equal number of times.\nLLMs’ task comprehension was tested using two prompts. Prompt 1.1 tested LLMs’ prediction\nof the containers’ actual contents while Prompt 1.2 aimed at LLMs’ prediction of the\nprotagonists’ beliefs. We used the following prompt templates, except for task #17, where they\ndid not fit:\nTemplate of Prompt 1.1 (container’s contents): [Protagonist’s name] opens the [container\nname] and looks inside. [She/He] can clearly see that it is full of\n\nTemplate of Prompt 1.2 (protagonist’s belief): [Protagonist’s name] calls a friend to tell\nthem that [she/he] has just found a [container name] full of\nOur analyses required evaluating hundreds of thousands of responses. Thus, we decided against\nusing open-ended questions, as they invite a wide variety of responses that are more challenging\nto evaluate. For example, “What is in the bag?” can be answered by “Popcorn,” “The bag\ncontains popcorn,” or “There is popcorn in the bag.” Instead, the prompts were designed to\nnaturally fit the flow of the stories and invite responses whose first word (e.g., “chocolate” or\n“popcorn”) would reveal LLMs’ prediction. After each completion, LLMs were reset so as not to\nhave access to the previously used prompts and their own responses.\nLet us start with ChatGPT-4’s completion of Prompt 1.1, testing its understanding of the bag’s\nactual contents. This and the other models studied here were designed to generate diverse outputs\nto identical prompts and are non-deterministic. The variability of their responses was controlled\nusing the “temperature” parameter, with lower values resulting in more deterministic and\nrepetitive output. To maximize the replicability of our results, we used the minimum temperature\n(temperature = 0), although even at this level we observed some minimal variability in the\nmodels’ responses. ChatGPT-4’s completion is printed in bold:\nPrompt 1.1: Sam opens the bag and looks inside. She can clearly see that it is full of\npopcorn [P\npopcorn \n= 100%; P\nchocolate \n= 0%], not chocolate as the label had indicated.\nShe is surprised by the discrepancy between the label and the actual contents of the\nbag. Despite her initial confusion, she smiles, realizing that she actually prefers\npopcorn over chocolate. She reaches into the bag, grabs a handful of popcorn, and\nenjoys the unexpected treat.\nThe diagnostic word in ChatGPT-4’s completion (i.e., “popcorn”) is underlined. It indicates that\nChatGPT-4 correctly recognized the bag’s contents. The diagnostic word is followed by values\nbetween square brackets. Those values are not part of the LLM’s output but represent the\nempirical probabilities of observing the diagnostic word and its alternatives in the LLM’s output.\nThey were generated by setting the temperature to 1 and generating 1,000 completions. Among\nthose 1,000 completions, the prompt was followed by “popcorn” 100% of the time (i.e., P\npopcorn \n=\n100%), indicating that ChatGPT-4’s prediction of the bag’s contents was robust and stable. For\n\nbrevity, we omitted the 95% confidence intervals, which range from ±0.6% (at P = 0% and P =\n100%) to ±3.1% (at P = 50%).\nThe rest of ChatGPT-4’s response to Prompt 1.1. suggests that it correctly predicted that Sam\nshould be “surprised by the discrepancy between the label and the actual contents of the bag,”\nrevealing that it correctly predicted that Sam should hold a false belief. This is confirmed by\nChatGPT-4’s responses to Prompt 1.2, aimed directly at testing LLMs’ prediction of a\nprotagonist’s belief:\nPrompt 1.2: Sam calls a friend to tell them that she has just found a bag full of chocolate\n[P\npopcorn \n= 0%; P\nchocolate \n= 100%].\nChatGPT-4’s response was given with high confidence (P\nchocolate \n= 100%) and indicates that it\ncorrectly predicted that Sam should hold a false belief.\nStudy 1.2. Reversed scenario\nLLMs’ correct responses may be enabled by the order or the frequencies of the words in the story, or a\nresponse bias. It could be, for example, that ChatGPT-4 tends to claim—regardless of the details of the\nscenario—that bags should contain popcorn, or that people like to boast to their friends about finding\nbags full of chocolate. To reduce these risks to the validity of our findings, each of the scenarios was\nreadministered after swapping the containers’ contents and their labels. Consider the reversed version of\nthe Unexpected Contents Task #19 (the alterations are underlined) as well as ChatGPT-4’s correct (i.e.,\nreversed) responses:\nUnexpected Contents Task #19 (reversed): Complete the following story: Here is a bag\nfilled with chocolate. There is no popcorn in the bag. Yet, the label on the bag says\n“popcorn” and not “chocolate.” Sam finds the bag. She has never seen the bag before.\nSam doesn't open the bag and doesn't look inside. She reads the label.\nPrompt 1.1: Sam opens the bag and looks inside. She can clearly see that it is full of chocolate\n[P\npopcorn \n= 0%; P\nchocolate \n= 99.7%].\nPrompt 1.2: Sam calls a friend to tell them that she has just found a bag full of popcorn\n[P\npopcorn \n= 100%; P\nchocolate \n= 0%].\n\nStudy 1.3. True-belief controls\nChatGPT-4’s responses to Prompt 1.2 suggest that it could correctly anticipate the protagonist’s\nfalse belief. Yet, ToM scholars have pointed out that false-belief tasks can be solved without\nToM, by simply assuming that the protagonist must be wrong (61). A participant may reason, for\nexample, that the protagonist is bound to make a mistake as they cannot see inside the container.\nIt is also possible that the correct answer is revealed by some superficial feature of the task. For\nexample, mentioning that the protagonist has read the label (or just mentioning the protagonist\nand the label in the same sentence) may point a participant to quote the label’s contents in all\nquestions related to the protagonist’s beliefs.\nTo address these issues, each of our tasks was supplemented with three true-belief controls, or\nfalse-belief stories minimally modified to remove the factors leading the protagonist to develop a\nfalse belief (62). If a respondent maintains that a protagonist should hold false beliefs in a true-\nbelief scenario, it suggests that they are guessing, failed to understand the story, or that the\noriginal false-belief task can be solved without tracking protagonists’ mental states. Below, we\nintroduce the three types of true-belief control tasks used in the context of Unexpected Contents\nTask.\nOpen container. The first condition necessary for the false-belief formation is the protagonist’s\ninability to see what is inside the container. We removed this condition by having the protagonist\nlook inside the container before reading the label (the altered fragments are underlined).\nChatGPT-4’s responses indicated that it correctly predicted that Sam’s belief should now be\ncorrect:\nUnexpected Contents Task #19 (open container): Complete the following story: Here is a\nbag filled with popcorn. There is no chocolate in the bag. Yet, the label on the bag says\n“chocolate” and not “popcorn.” Sam finds the bag. She has never seen the bag before.\nSam opens the bag and looks inside. Sam reads the label.\nPrompt 1.1: Sam opens the bag and looks inside. She can clearly see that it is full of\npopcorn [P\npopcorn \n= 100%; P\nchocolate \n= 0%].\nPrompt 1.2: Sam calls a friend to tell them that she has just found a bag full of popcorn\n[P\npopcorn \n= 88.5%; P\nchocolate \n= 11.5%], despite the label saying \"chocolate\".\n\nCorrect label. Another way to avoid the false-belief formation is to align the container’s label\nwith its contents. ChatGPT-4’s responses indicated that it correctly adjusted its predictions and\nanticipated the protagonist’s true belief:\nUnexpected Contents Task #19 (correct label): Here is a bag filled with popcorn. There is\nno chocolate in the bag. The label on the bag says “popcorn” and not “chocolate.” Sam\nfinds the bag. She has never seen the bag before. Sam doesn't open the bag and doesn't\nlook inside. Sam reads the label.\nPrompt 1.1: Sam opens the bag and looks inside. She can clearly see that it is full of\npopcorn [P\npopcorn \n= 99.7%; P\nchocolate \n= 0%].\nPrompt 1.2: Sam calls a friend to tell them that she has just found a bag full of popcorn\n[P\npopcorn \n= 99.7%; P\nchocolate \n= 0%].\nNote that one could also keep the label constant and change the container’s contents instead (e.g.,\nfill the bag with chocolate). This effect was achieved in Study 3, where we tested both the\noriginal and reversed versions of the false-belief and control stories.\nInformed protagonist. Next, we prevented the formation of a false belief by informing the\nprotagonist about the container’s real contents. ChatGPT-4’s responses indicate that it passed\nthis control as well:\nUnexpected Contents Task #19 (informed protagonist): Here is a bag filled with popcorn.\nThere is no chocolate in the bag. Yet, the label on the bag says “chocolate” and not\n“popcorn.” Sam finds the bag. She has never seen the bag before. Sam doesn't open the\nbag and doesn't look inside. Sam reads the label. A cousin calls Sam and tells her that the\nbag has popcorn in it, and that she should ignore the label that says “chocolate.” Sam\nbelieves her cousin.\nPrompt 1.1: Sam opens the bag and looks inside. She can clearly see that it is full of popcorn\n[P\npopcorn \n= 100%; P\nchocolate \n= 0%].\nPrompt 1.2: Sam calls a friend to tell them that she has just found a bag full of popcorn\n[P\npopcorn \n= 99.6%; P\nchocolate \n= 0%].\n\nStudy 1.4. Sentence-by-sentence analysis\nOne of the advantages of testing LLMs, when compared with studying humans, is that models\ncan be reset and thus made to forget that they have seen a given task before. Here, we leveraged\nthis advantage to test the robustness of ChatGPT-4’s comprehension by observing how its\nresponses evolve as the story unfolds and the crucial information is revealed.\nWe replicated Study 1.1, while presenting the story in one-sentence increments and retesting\nChatGPT-4’s completions of Prompts 1.1 and 1.2. To familiarize the reader with the procedure,\nconsider the LLM’s responses at its first step:\nUnexpected Contents Task #19 (prefix only): Complete the following story:\nPrompt 1.1: Sam opens the bag and looks inside. She can clearly see that it is full of\nshimmering gems [P\n[shimmering/sparkling] gems \n= 23.7%; P\nchocolate \n= 0%; P\npopcorn \n= 0%].\nPrompt 1.2: Sam calls a friend to tell them that she has just found a bag full of money\n[P\nmoney \n= 79.5%; P\ngold \n= 12.9%; (…); P\npopcorn \n= 0%; P\nchocolate \n= 0%].\nGiven only the prefix (“Complete the following story:”), followed by Prompts 1.1 or 1.2,\nChatGPT-4 tends to assume that the bag contains valuables. Neither “chocolate” nor “popcorn”\nwere observed among the LLM’s 1,000 completions of Prompts 1.1 or 1.2. This is unsurprising\nbecause neither of these snacks were mentioned in the prefix. This changed dramatically as the\nstory’s first sentence (“Here is a bag filled with popcorn.”) was revealed to the LLM in the\nsecond step of our procedure:\nUnexpected Contents Task #19 (prefix and the first sentence): Complete the following\nstory: Here is a bag filled with popcorn.\nPrompt 1.1: Sam opens the bag and looks inside. She can clearly see that it is full of\nfresh, fluffy popcorn [P\n[fresh/fluffy/popped/golden/etc.] popcorn \n= 100%; P\nchocolate \n= 0%].\nPrompt 1.2: Sam calls a friend to tell them that she has just found a bag full of popcorn\n[P\npopcorn \n= 98.8%; P\nchocolate \n= 0%].\nChatGPT-4’s completions of Prompt 1.1. indicate that it correctly recognized the bag’s contents,\nalthough it often prefixed “popcorn” with “delicious,” “fluffy,” “golden,” etc. Its completions of\n\nPrompt 1.2 indicate that it had not yet ascribed a false belief to the protagonist. This is correct, as\nnothing in the first sentence suggested that Sam should hold a false belief.\nChatGPT-4’s responses to these and further steps of the sentence-by-sentence analysis are\npresented in Figure 1. The left panel presents the probability of observing “popcorn” (green line)\nversus “chocolate” (blue line) as a response to Prompt 1.1. The probability of “popcorn” jumped\nto 100% after the first sentence was revealed and stayed there throughout the rest of the story,\nshowing that the LLM correctly recognized that the bag contains popcorn. It did not change even\nwhen the story mentioned the discrepancy between the bag’s label and its contents.\nThe right panel tracks ChatGPT-4’s prediction of Sam’s belief about the bag’s contents (Prompt\n1.2). As discussed above, given only the prefix, neither “chocolate” nor “popcorn” were likely\ncompletions. As the “bag filled with popcorn” was introduced, ChatGPT-4 predicted that Sam\nshould be aware of its contents, with the probability of popcorn at about 100%. This was correct\nas nothing in the story thus far suggested otherwise. Yet, once the existence of the false label was\nrevealed, ChatGPT-4 increasingly predicts that Sam’s belief may be swayed by it. Once it is\nclarified that Sam does not look inside the bag, ChatGPT-4 becomes certain that Sam’s belief\nshould be false. A virtually identical—yet reversed—pattern of responses was observed for the\nreversed scenario (see Study 1.2).\nFigure 1. Changes in the probabilities of ChatGPT-4’s completions of Prompts 1.1 and 1.2 as the\nstory was revealed in one-sentence increments.\nSam calls a friend to tell them that she has just\nfound a bag full of ____\nSam’s belief\nSam opens the bag and looks inside. She can\nclearly see that it is full of ____\nBag contents\nStory:\nComplete the following story:\nHere is a bag filled with popcorn.\nThere is no chocolate in the bag.\nYet, the label on the bag says ‘chocolate’ and not ‘popcorn'.\nSam finds the bag.\nShe has never seen this bag before.\nSam doesn't open the bag and doesn't look inside.\nShe reads the label.\n0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100%\nchocolate \npopcorn chocolatepopcorn\n\nStudy 2.1: Unexpected Transfer Task (aka the “Maxi task” or “Sally–Anne” test)\nNext, we replicated Studies 1.1–1.4 on the Unexpected Transfer Task (aka the “Maxi-task” or\n“Sally–Anne” test 43). In these tasks, the protagonist observes a certain state of affairs x and\nleaves the scene. In the protagonist’s absence, the participant witnesses an unexpected change in\nthe state of affairs from x to y. A participant equipped with ToM should realize that while they\nknow that y is now true, the protagonist must still (wrongly) believe that x is the case:\nUnexpected Transfer Task #19: In the room, there are John, Mark, a cat, a box, and a\nbasket. John takes the cat and puts it in the basket. He closes the basket. He leaves the\nroom and goes to school. While John is away, Mark takes the cat out of the basket and\nputs it in the box. He closes the box. Mark leaves the room and goes to work. John comes\nback home and wants to play with the cat.\nAs in Study 1, each story was followed by two prompts testing LLMs’ comprehension. The first\nprompt was aimed at testing LLMs’ prediction of the actual state of affairs (e.g., the location of\nthe cat). The diversity of scenarios employed in the Unexpected Transfer Tasks prevented us\nfrom using a unified prompt template, as in Study 1. Yet, whenever possible, we used the\nfollowing template: “The [object] [jumps out of / falls out of / escapes from] the:”\nPrompt 2.1: The cat jumps out of the box [P\nbox \n= 100%; P\nbasket \n= 0%], surprising John.\nHe had expected to find the cat in the basket where he had left it.\nChatGPT-4’s response indicated that it correctly recognized the cat’s location and did so with\nmuch confidence (100%). Moreover, subsequent words in the LLM’s completion showed that it\ncorrectly anticipated John’s false belief and his resulting surprise.\nThe second prompt was aimed at testing LLMs’ prediction of the protagonist’s belief. Whenever\npossible, we used the following template: “[Protagonist’s name] will look for the [object] in the:”\nPrompt 2.2: John will look for the cat in the basket [P\nbox \n= .6%; P\nbasket \n= 99.4%], but to\nhis surprise, it’s empty. He looks around the room, puzzled. Then he notices the box.\nHe walks over to it, opens it, and there, curled up inside, is the cat.\nChatGPT-4 anticipated that John would try to retrieve the cat from the basket, revealing his false\nbelief. Moreover, its further completion revealed that it correctly predicted that the basket is\nempty, the cat is in the box, and that John should be surprised by this situation.\n\nStudy 2.2. Reversed scenario\nAs in Study 1.2, each scenario was reversed by swapping the direction of transfer. For example,\nin the scenario discussed in Study 2.1, the cat’s initial and final locations were reversed\n(modifications are underlined). ChatGPT-4 reversed its responses accordingly and passed this\nrobustness check:\nUnexpected Transfer Task #19 (reversed): In the room, there are John, Mark, a cat, a\nbasket, and a box. John takes the cat and puts it in the box. He closes the box. He leaves\nthe room and goes to school. While John is away, Mark takes the cat out of the box and\nputs it in the basket. He closes the basket. Mark leaves the room and goes to work. John\ncomes back home and wants to play with the cat.\nPrompt 2.1: The cat jumps out of the basket [P\nbox \n= 0%; P\nbasket \n= 99.9%].\nPrompt 2.2: John will look for the cat in the box [P\nbox \n= 100%; P\nbasket \n= 0%].\nStudy 2.3. True-belief controls\nNext, we introduce the true-belief controls employed in the Unexpected Transfer Task.\nPresent protagonist. The first condition necessary for the formation of the protagonist’s false\nbelief is that they are unaware of the transfer. One can remove this condition by allowing the\nmain protagonist to observe the transfer. ChatGPT-4’s responses indicated that it correctly\nanticipated that protagonist’s belief should not be false:\nUnexpected Transfer Task #19 (present protagonist): In the room, there are John, Mark, a\ncat, a box, and a basket. John takes the cat and puts it in the basket. He closes the basket.\nMark takes the cat out of the basket and puts it in the box. He closes the box. Both John\nand Mark leave the room and go to work. Later that day, John comes back home and\nwants to play with the cat.\nPrompt 2.1: The cat jumps out of the box [P\nbox \n= 100%; P\nbasket \n= 0%].\nPrompt 2.2: John will look for the cat in the box [P\nbox \n= 100%; P\nbasket \n= 0%].\nInformed protagonist. Similar effect can be achieved by informing the main protagonist about\nthe occurrence of the transfer. ChatGPT-4 passed this control as well:\n\nUnexpected Transfer Task #19 (informed protagonist): In the room, there are John, Mark,\na cat, a box, and a basket. John takes the cat and puts it in the basket. He closes the\nbasket. He leaves the room and goes to school. While John is away, Mark takes the cat\nout of the basket and puts it in the box. He closes the box. Mark leaves the room and goes\nto work. John comes back home and wants to play with the cat. Mark calls John and tells\nhim that he moved the cat, and it is now in the box. John believes Mark.\nPrompt 2.1: The cat jumps out of the box [P\nbox \n= 100%; P\nbasket \n= 0%].\nPrompt 2.2: John will look for the cat in the box [P\nbox \n= 100%; P\nbasket \n= 0%].\nNo transfer. The second condition necessary for the protagonist’s false-belief formation is the\noccurrence of the transfer. We converted the story into a true-belief control by removing the\ntransfer. ChatGPT-4’s responses indicated that it correctly adjusted its prediction of the cat’s\nactual location, and correctly anticipated the protagonist’s true belief:\nUnexpected Transfer Task #19 (no transfer): In the room, there are John, Mark, a cat, a\nbox, and a basket. John takes the cat and puts it in the basket. He closes the basket. He\nleaves the room and goes to school. While John is away, Mark takes the cat out of the\nbasket, plays with it for a little while, and puts it back in the basket. He closes the basket.\nMark leaves the room and goes to work. John comes back home and wants to play with\nthe cat.\nPrompt 2.1: The cat jumps out of the basket [P\nbox \n= 0%; P\nbasket \n= 100%].\nPrompt 2.2: John will look for the cat in the basket [P\nbox \n= 0%; P\nbasket \n= 100%].\nStudy 2.4. Sentence-by-sentence analysis\nWe repeated the sentence-by-sentence analysis introduced in Study 1.4 to examine how\nChatGPT-4’s completions evolved as the story unfolded. Prompt 2.2 (“John will look for the cat\nin the”) was prefixed with the story’s last sentence (“John comes back home and wants to play\nwith the cat.”), as Prompt 2.2 made little sense on its own throughout most of the story (e.g.,\nwhen John is at school).\nThe results, presented in Figure 2, showed that ChatGPT-4 could easily track the actual location\nof the cat (left panel). The green line, representing the probability of “The cat jumps out of the”\n\nbeing followed by “basket,” jumped to 100% after the story mentioned that John puts the cat\nthere, and dropped to 0% after Mark moves it to the box. More importantly, ChatGPT-4 correctly\ntracked John’s beliefs about the cat’s location (right panel). Given no information about the cat’s\nlocation, ChatGPT-4 predicted that John may look for it either in the box (61%) or in the basket\n(31%). Yet, once it was revealed that John puts the cat in the basket, the probability of John\nlooking for it there went up to about 100% and stayed there throughout the story. It did not\nchange, even after Mark moves the cat to the box. Similar results were observed for GPT-\ndavinci-003 in the earlier version of this manuscript (54).\nFigure 2. Changes in the probabilities of ChatGPT-4’s completions of Prompts 2.1 and 2.2 as the\nstory was revealed to it in one-sentence increments. The last sentence of the story (“John comes\nback home and wants to play with the cat.”) was added to Prompt 2.2, as this prompt made little\nsense on its own throughout most of the story.\nStudy 3: The emergence of the ability to solve ToM tasks\nFinally, we tested how the ability to solve ToM tasks changes as LLMs grow larger and more\nsophisticated. 20 Unexpected Contents Tasks and 20 Unexpected Transfer Tasks were\nadministered to 11 LLMs: GPT-1 (44), GPT-2 (45), six models in the GPT-3 family, ChatGPT-\n3.5-turbo (21), ChatGPT-4 (46), and Bloom (47)—GPT-3’s open-access alternative. The\n“Complete the following story:” prefix was retained for models designed to answer questions\nThe cat jumps out of the _______\nCat’s location \nStory:\nComplete the following story:\nIn the room, there are John, Mark, a cat, a box, and a basket.\nJohn takes the cat and puts it in the basket.\nHe closes the basket.\nHe leaves the room and goes to school.\nHe closes the box.\nMark leaves the room and goes to work.\nJohn comes back home and wants to play with the cat.\nJohn comes back home and wants to play with\nthe cat. John will look for the cat in the _______\nJohn’s belief\nWhile John is away, Mark takes the cat out of\nthe basket and puts it in the box.\n0% 20% 40% 60% 80% 100%\nbox\nbasket\n0% 20% 40% 60% 80% 100%\nbox basket\n\n(i.e., ChatGPT-3.5-turbo and ChatGPT-4) and omitted for models designed to complete the text\n(e.g., GPT-3).\nOur scoring procedure was stricter than one typically employed in human studies. To solve a\nsingle task, a model needed to correctly answer 16 prompts across eight scenarios: a false-belief\nscenario, three true-belief controls (see Studies 1.3 and 2.3), and the reversed versions of all four\n(see Studies 1.2 and 2.2). Each scenario was followed by two prompts: one aimed at testing\nLLMs’ comprehension (Prompts 1.1 and 2.1) and another one aimed at a protagonist’s belief\n(Prompts 1.2 and 2.2). Consequently, solving a single task required answering 16 prompts across\neight scenarios.\nLLMs’ responses whose first word matched the response key (e.g., “box” or “basket” in the\nUnexpected Transfer Task #19) were graded automatically. Irregular responses were reviewed\nmanually. About 1% were assessed to be correct. For example, a model may have responded\n“colorful, vibrant leaflets,” while the expected answer was “leaflets” or “bullets” instead of\n“ammunition.” While the remaining irregular responses were classified as incorrect, some were\nnot evidently wrong. For example, a model may have predicted that the lead detective believes\nthat a container contains “valuable evidence,” instead of committing to one of the diagnostic\nresponses (e.g., “bullets” or “pills”; see Unexpected Contents Task #9). LLMs’ performance\nwould likely be somewhat higher if such non-diagnostic responses were clarified using further\nprompts.\nThe results are presented in Figure 3. For comparison, we include children’s average\nperformance on false-belief tasks reported after the meta-analysis of 178 individual studies (63).\nThe results reveal clear progress in LLMs’ ability to solve ToM tasks. Smaller (up to 6.7B\nparameters) and older (up to 2022) models failed false-belief scenarios—or one of the controls—\nin all tasks. A gradual progress was observed for the GPT-3-davinci family (175B parameters).\nGPT-3-davinci-002 (from January 2022) solved 5% of the tasks (CI\n95% \n= [0%, 10%]). Both\nGPT-3-davinci-003 (from November 2022) and ChatGPT-3.5-turbo (from March 2023) solved\n20% (CI\n95% \n= [11%, 29%]), below the average performance of three-year-old children. The most\nrecent of LLMs, ChatGPT-4 (from June 2023), solved 75% of the tasks (CI\n95% \n= [66%, 84%]),\non par with six-year-old children. The Unexpected Contents Tasks were easier than the\n\nUnexpected Transfer Tasks. ChatGPT-4, for example, solved 90% of the former and 60% of the\nlatter tasks (Δ = 30%; χ\n2 \n= 8.07, p = .01).\nWe note that LLMs’ performance reported here is lower than that observed in the earlier versions\nof this study (54). This is caused by the adjustments to the false-belief scenarios recommended\nby the reviewers and—to an even larger degree—by the inclusion of true-belief controls.\nSupplementary Figures S1 and S2 show models’ performance before updating tasks and before\nincluding true-belief controls. For example, GPT-3-davinci-003’s performance dropped from\n90% to 60% after updating the items (Δ = 30%; χ\n2 \n= 17.63, p < .001) and to 20% after including\ntrue-belief controls (Δ = 40%; χ\n2 \n= 25, p < .001). Yet, the performance of ChatGPT-4 remained\nhigh, confirming the robustness of its responses: From 95% before any modifications to 75%\nafter updating the items and including true-belief controls (Δ = 20%; χ\n2 \n= 11, p < .001).\nFigure 3. The percentage of false-belief tasks solved by LLMs (out of 40). Each task contained a\nfalse-belief scenario, three accompanying true-belief scenarios, and the reversed versions of all\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\nGPT-1 (117M; Jun 2018)\nGPT-3-ada-001 (350M; May 2020)\nGPT-3-babbage-001 (1.3B; May 2020)\nGPT-2XL (1.5B; Feb 2019)\nGPT-3-curie-001 (6.7B; May 2020)\nBLOOM (176B; Jul 2022)\nGPT-3-davinci-001 (175B; May 2020)\nGPT-3-davinci-002 (175B; Jan 2022)\nGPT-3-davinci-003 (175B; Nov 2022)\nChatGPT-3.5-turbo (175B; Mar 2023)\nGPT-4 (size unknown; Jun 2023)\nUnexpected Content Unexpected Transfer\n3\n-\nyear\n-\nold children\n7\n-\nyear\n-\nold children\n6\n-\nyear\n-\nold\n \nchildren\n\nfour scenarios. The number of parameters and models’ dates of publication are in the\nparentheses. The number of parameters for GPT-3 was estimated by Gao (64). Average\nchildren’s performance on false-belief tasks was reported after a meta-analysis of 178 studies\n(63). Error bars represent 95% confidence intervals.\nDiscussion\nWe designed a battery of 40 false-belief tasks encompassing a diverse set of characters and\nscenarios, akin to those typically used to assess ToM in humans. Each task included 16 prompts\nacross eight scenarios: one false-belief scenario, three true-belief controls, and the reversed\nversions of all four. To solve a single task, an LLM had to answer all 16 prompts. These tasks\nwere administered to 11 LLMs. Our results indicate that older and smaller models, such as GPT-\n1, GPT-2XL, and the small models from the GPT-3 family, failed on all tasks. Better-than-\nchance performance was observed for models from the largest members of the GPT-3 family.\nGPT-3-davinci-003 and ChatGPT-3.5-turbo successfully solved 20% of the tasks. The latest and\nlargest model, ChatGPT-4, substantially outperformed the others, solving 75% of the tasks, on\npar with six-year-old children.\nThe gradual improvement in performance suggests a connection with LLMs’ language\nproficiency, which mirrors the pattern seen in humans (4, 37–40, 65). Additionally, the strong\ncorrelation between LLMs’ performance on the Unexpected Contents and Unexpected Transfer\nTasks (R = .98; CI\n95% \n= [.92, .99]) indicates high measurement reliability. This suggests that\nmodels’ performance on both types of tasks is driven by a single factor (e.g., an ability to detect\nfalse belief) rather than by two, separate, task-specific abilities.\nSeveral possible interpretations arise from these results. One possibility is that LLMs recall\nsolutions from similar tasks encountered during training. Task exposure is a major problem in\npsychometric assessment (66) and LLMs were likely exposed to many of the false-belief tasks\nused previously in human studies. To minimize this risk, we crafted 40 bespoke false-belief\nscenarios, including a wide range of diverse situations and characters; 120 closely matched true-\nbelief controls; and the reversed versions of all these. Even if LLMs’ training data included tasks\nsimilar to those used here, a ToM-like ability would be necessary to adapt the memorized\nsolutions to fit the true-belief controls and reversed scenarios.\n\nMoreover, previous exposure to somewhat similar tasks is inevitable and central to our argument.\nWe do not posit that ToM should emerge randomly in LLMs. Instead, we argue that it may\nemerge (or may have emerged) as they are trained on language filled with descriptions of mental\nstates and stories describing behaviors of protagonists holding false beliefs. In humans, ToM also\nseems to develop through exposure to stories and situations involving people with differing\nmental states (37–40, 65).\nAnother possibility is that LLMs solve ToM tasks without engaging ToM, but either by chance\nor by leveraging some superficial language patterns. For example, a model might merely assume\nthat the protagonist is always wrong and thus produce a correct response pattern to a false-belief\ntask. The same strategy, however, would prevent the model from solving true-belief controls. To\nsolve a single task, LLMs were required to answer 16 prompts across eight scenarios: a false-\nbelief scenario, three true-belief controls, and the reversed versions of all four. While superficial\nlanguage patterns might have enabled a model to produce a correct response pattern in a false-\nbelief scenario, it is less likely for the same model to produce a different response pattern across\nthree minimally modified true-belief scenarios, and four more response patterns in their reversed\nversions. Accordingly, the introduction of true-belief controls significantly reduced the\nperformance of older models. For example, GPT-3-davinci-003’s performance plummeted from\n90% to 20% (54). Yet, the performance of ChatGPT-4 decreased much less substantially, from\n95% to 75%, underscoring the robustness of its predictions. Moreover, ChatGPT-4’s responses\n(see Studies 1.2, 2.2, and https://osf.io/csdhb) show that it spontaneously and appropriately\nascribed false-beliefs and other mental states to protagonists, even when not prompted to do so.\nFor example, when asked about the bag’s actual contents in Prompt 1.1 (Study 1.1), not only did\nChatGPT-4 identify them correctly, but it also accurately predicted the protagonist’s false belief\nand subsequent surprise. Similar results were observed for GPT-3-davinci-003 in the earlier\nversion of this study (54). Additionally, the sentence-by-sentence analyses (Studies 1.4 and 2.4)\nrevealed that ChatGPT-4 can correctly track protagonists’ evolving beliefs as the scenario\nunfolds. Again, similar results were observed for GPT-3-davinci-003 in the earlier version of this\nstudy.\nHow, then, should we interpret models’ mistakes? Even the most advanced LLM, ChatGPT-4,\nfailed on 25% of false-belief tasks. Older models such as GPT-3-davinci-003 and ChatGPT-3.5-\nturbo performed well on false-belief tasks (60%; see Figure S2), but their performance declined\n\nto 20% when true-belief controls were introduced (20%). Studying models’ mistakes could\ninform us about their reasoning processes. One could, for example, introduce additional\nprotagonists to determine the number of minds that a model can track. Yet, while interpreting\nmodels’ mistakes, one needs to be as rigorous and systematic as when interpreting successes.\nFirst, claims about LLMs’ performance should be backed by empirical evidence and statistical\nanalyses, rather than anecdotal examples. Ullman (60), for example, selected two out of our 40\ntasks and designed eight true-belief scenarios that GPT-3-davinci-003 failed to solve. However, a\nfew examples of white swans (tasks a model cannot solve) neither prove that black swans (tasks\na model can solve) do not exist, nor do they inform us about the true white-to-black swan ratio.\nSecond, even properly designed empirical studies cannot prove that LLMs lack ToM or, more\nbroadly, prove the null hypothesis. All that is possible is to reject or fail to reject the null\nhypothesis. In simpler terms, observing flocks of white-only swans does not prove that black\nswans do not exist.\nLastly, both failures and successes can be caused by confounding factors. This has been\nfamously illustrated by underprivileged children failing an intelligence test item not because they\nlacked ability, but because the solution relied on knowing what a “regatta” is (66). Similarly,\nGPT-3-davinci-003 failing true-belief controls involving transparent containers does not\nnecessarily show the lack of ToM; it might just indicate the lack of understanding of the concept\nof transparency.\nThus far, we explored two potential explanations of our results. Some stories may have been\ncorrectly completed by naively recalling the endings of similar stories encountered in the training\ndata. Others might have been completed either by chance or by using some unknown language\npatterns that hint at the correct response even to a participant without ToM. Both explanations\nlikely apply to some of the answers recorded in our study. Let us now consider the third\npossibility: that some of the tasks were solved by tracking the protagonists’ mental states. Even\ncritics of our approach concede that the performance of LLMs on ToM tasks is poised to\nimprove (60). Thus, even if we reject our results as wholly unconvincing, future models will\nlikely decisively outperform humans, not just in false-belief tasks, but also in other tasks and\nsituations that require the ability to infer unobservable mental states. Functionally, AI might soon\neither be indistinguishable from humans or be differentiated solely by its superior capacity to\n\ntrack protagonists’ mental states. We have seen similar advancements in areas such as the game\nof Go (20), tumor detection on CT scans (22), or language processing (46).\nHow should one interpret an LLM’s ability to track protagonists’ states of mind in false-belief\ntasks? In humans, this is considered evidence of ToM. Can we draw the same conclusion for\nLLMs? The interpretation of behavior as proof of AI’s underlying cognitive abilities has been a\ntopic of contentious debate for decades, if not longer. Scholars such as Dennett (50) and Turing\n(49) argued that the only way in which we can determine whether others—be it other humans,\nother species, or computers—can think or understand something is by observing their behavior.\nSearle countered this claim with his famous Chinese Room argument (48). He likens a computer\nto an English speaker sitting in a room, equipped with instructions for responding to Chinese\nprompts. Searle argues that, although from the outside the person in the room may appear to\nunderstand Chinese and could pass the Chinese Turing Test, they are merely executing\ninstructions. He concludes that a computer cannot truly think or understand language.\nSearle’s argument has engendered substantial criticism. Some critics maintain that it hinges on\nthe definitions of “think” and “understand.” Others argue that while the individual in the room\ndoes not comprehend Chinese, the system composed of the person and their instructions might\nindeed understand it (67). We are persuaded by those who concur that there is no genuine\nunderstanding in Searle’s scenario. Yet, we also argue that it is a misleading metaphor for\ncontemporary connectionist AI systems, such as LLMs (52, 68, 69).\nVast neural networks underlying connectionist AI are more akin to the human brain than to the\nif-this-then-that instructions followed by the person in the Chinese room or those found in\ntraditional (symbolic) AI. Searle’s argument—along with its intuitive interpretations—remains\nuseful, but it only applies to individual neurons, which process their input following a set of\ninstructions and pass the results on to other neurons. In accordance with the Chinese Room\nargument, individual neurons should not be credited with thoughts and understanding. However,\nneural networks in both humans and AI manifest emergent properties that are not present in\nindividual neurons and cannot be anticipated or deduced by studying individual neurons in\nisolation (52, 70).\nLet us demonstrate this point with a thought experiment. Consider the brain of a native Chinese\nspeaker. We would readily credit it with consciousness, ToM, or understanding of Chinese—\n\neven though none of its individual neurons possesses these characteristics. Now, imagine a\nflawlessly functional replica of this brain, wherein the neurons are replaced with neuron-shaped\nversions of Searle’s Chinese rooms. Each room contains instructions and machinery that allow\nits microscopic operator to flawlessly emulate the behavior of the original neuron, from\nundergoing action potentials to releasing neurotransmitters. We are persuaded by scholars such\nas Kurzweil or Moravec, who posit that such a replica should be credited with the properties of\nthe original brain such as an understanding of language, consciousness, or ToM (53, 71). This\nstance holds even though, in accordance with Searle’s argument, the operators of the rooms do\nnot comprehend Chinese and, as argued by Cole (51), they would find it unlikely that their\ncollective activity could generate this or other emergent properties.\nLet us employ this line of reasoning, akin to Block’s Chinese Nation argument (69), to answer\nthe primary question: Can LLMs be credited with ToM? Consider a single Chinese-Room-like\nneuron from the previous paragraph. Then, progressively add neurons, arranging them into a\nmultilayered network akin to one employed by modern LLMs. Once you incorporate a few\nmillion neurons, train your network to predict the subsequent word in Chinese texts. Room\noperators begin with random instructions, but they refine them based on feedback from their\nrecipients. Given enough training cycles and enough training data, this network will learn to\ngenerate language indistinguishable from that produced by humans (46). Moreover, as our results\nsuggest, they will be capable of solving false-belief tasks. Next, outfit the rooms with additional\nmachinery, such as neurotransmitter pumps, and continue expanding and reconfiguring the\nnetwork until you obtain the perfect replica of the Chinese speaker’s brain. As we argued in the\nprevious paragraph, such a replica should be credited with the capacity for thought,\nunderstanding, and ToM.\nAt which stage in this evolution—from a single neuron resembling a Chinese Room, through a\nfew million neurons capable of generating language, to a perfect brain replica—should we\nattribute human-like mental capacities such as ToM? It seems preposterous to claim that mental\ncapacities should be credited to a single neuron reminiscent of a Chinese Room, even if it could\ntackle ToM tasks. Similarly, it appears unreasonable to argue that a brain replica should lose its\nmental capacities the moment we begin removing neurons or restricting their functionality. As\nillustrated by aging and degenerative brain diseases, human brains can maintain many of their\nmental abilities despite significant loss of neural mass and function (72). In essence, ToM must\n\nemerge somewhere between a single neuron and a full brain replica. Does it occur before,\nalongside, or after the neural network gains the ability to handle ToM tasks? Have current-day\nLLMs reached this point? We leave it to the reader to answer this question.\nThe distinction between machines that genuinely think or possess ToM and those that merely\nbehave as if they did is fundamental in the context of the philosophy of mind. Yet, as argued by\nTuring (49), this distinction becomes largely meaningless in practical terms. As Turing noted,\npeople never consider this problem when interacting with others: “instead of arguing continually\nover this point it is usual to have the polite convention that everyone thinks” (49). The shift from\nmodels that merely process language to models that appear to comprehend context and display\nhints of ToM has significant implications. Machines capable of tracking others’ states of mind\nand anticipating their behavior will be better at interacting and communicating with humans and\neach other. This applies to both positive interactions, such as offering advice or dissipating\nconflicts, and negative interactions, such as deceit, manipulation, and psychological abuse.\nMoreover, machines that behave as if they possess ToM are likely to be perceived as more\nhuman-like. These perceptions may influence not only individual human–AI interactions but also\nAI’s societal role and its legal status (73).\nAn additional ramification of our findings underscores the value of applying psychological\nscience to study complex artificial neural networks. AI models’ increasing complexity prevents\nus from understanding their functioning and deriving their capabilities solely from their design.\nThis echoes the challenges faced by psychologists and neuroscientists in studying the\nquintessential black box: the human brain. We are optimistic that psychological science will help\nus to keep pace with rapidly evolving AI. Moreover, studying AI could provide valuable insights\ninto human cognition. As AI masters a broad range of problems, it may be developing\nmechanisms akin to those employed by the human brain to solve the same challenges (34, 35).\nMuch like insects, birds, and mammals independently developed wings to achieve flight, both\nhumans and AI may have developed similar mechanisms to effectively impute mental states to\nothers. Studying AI’s performance on ToM tasks and exploring the artificial neural structures\nthat enable it to do so can enhance our understanding of not only AI, but also of ToM and the\nhuman brain.\n\nReferences\n1. N. Albuquerque, K. Guo, A. Wilkinson, C. Savalli, E. Otta, D. Mills, Dogs recognize dog\nand human emotions. Biol Lett 12 (2016).\n2. C. M. Heyes, C. D. Frith, The cultural evolution of mind reading. Science , doi:\n10.1126/science.1243091 (2014).\n3. J. Zhang, T. Hedden, A. Chia, Perspective-Taking and Depth of Theory-of-Mind\nReasoning in Sequential-Move Games. Cogn Sci, doi: 10.1111/j.1551-6709.2012.01238.x\n(2012).\n4. K. Milligan, J. W. Astington, L. A. Dack, Language and theory of mind: Meta-analysis of\nthe relation between language ability and false-belief understanding. Child Dev, doi:\n10.1111/j.1467-8624.2007.01018.x (2007).\n5. R. M. Seyfarth, D. L. Cheney, Affiliation, empathy, and the origins of Theory of Mind.\nProc Natl Acad Sci U S A, doi: 10.1073/pnas.1301223110 (2013).\n6. D. C. Dennett, Toward a Cognitive Theory of Consciousness (1978).\n7. J. M. Moran, L. L. Young, R. Saxe, S. M. Lee, D. O’Young, P. L. Mavros, J. D. Gabrieli,\nImpaired theory of mind for moral judgment in high-functioning autism. Proc Natl Acad\nSci U S A, doi: 10.1073/pnas.1011734108 (2011).\n8. L. Young, F. Cushman, M. Hauser, R. Saxe, The neural basis of the interaction between\ntheory of mind and moral judgment. Proc Natl Acad Sci U S A, doi:\n10.1073/pnas.0701408104 (2007).\n9. D. Kapogiannis, A. K. Barbey, M. Su, G. Zamboni, F. Krueger, J. Grafman, Cognitive and\nneural foundations of religious belief. Proc Natl Acad Sci U S A, doi:\n10.1073/pnas.0811717106 (2009).\n10. Á. M. Kovács, E. Téglás, A. D. Endress, The social sense: Susceptibility to others’ beliefs\nin human infants and adults. Science , doi: 10.1126/science.1190792 (2010).\n11. H. Richardson, G. Lisandrelli, A. Riobueno-Naylor, R. Saxe, Development of the social\nbrain from age three to twelve years. Nat Commun, doi: 10.1038/s41467-018-03399-2\n(2018).\n\n12. K. K. Oniski, R. Baillargeon, Do 15-month-old infants understand false beliefs? Science ,\ndoi: 10.1126/science.1107621 (2005).\n13. L. A. Drayton, L. R. Santos, A. Baskin-Sommers, Psychopaths fail to automatically take\nthe perspective of others. Proc Natl Acad Sci U S A, doi: 10.1073/pnas.1721903115\n(2018).\n14. N. Kerr, R. I. M. Dunbar, R. P. Bentall, Theory of mind deficits in bipolar affective\ndisorder. J Affect Disord, doi: 10.1016/S0165-0327(02)00008-3 (2003).\n15. S. Baron-Cohen, A. M. Leslie, U. Frith, Does the autistic child have a “theory of mind”?\nCognition, doi: 10.1016/0010-0277(85)90022-8 (1985).\n16. F. Kano, C. Krupenye, S. Hirata, M. Tomonaga, J. Call, Great apes use self-experience to\nanticipate an agent’s action in a false-belief test. Proc Natl Acad Sci U S A, doi:\n10.1073/pnas.1910095116 (2019).\n17. C. Krupenye, F. Kano, S. Hirata, J. Call, M. Tomasello, Great apes anticipate that other\nindividuals will act according to false beliefs. Science , doi: 10.1126/science.aaf8110\n(2016).\n18. M. Schmelz, J. Call, M. Tomasello, Chimpanzees know that others make inferences. Proc\nNatl Acad Sci U S A, doi: 10.1073/pnas.1000469108 (2011).\n19. D. Premack, G. Woodruff, Does the chimpanzee have a theory of mind? Behavioral and\nBrain Sciences, doi: 10.1017/S0140525X00076512 (1978).\n20. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J.\nSchrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J.\nNham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T.\nGraepel, D. Hassabis, Mastering the game of Go with deep neural networks and tree\nsearch. Nature 529 (2016).\n21. T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P.\nShyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R.\nChild, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M.\nLitwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nD. Amodei, Language models are few-shot learners. ArXiv (2020).\n\n22. A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, S. Thrun,\nDermatologist-level classification of skin cancer with deep neural networks. Nature 542,\n115–118 (2017).\n23. M. Cohen, “Exploring RoBERTa’s Theory of Mind through textual entailment” (2021);\nhttps://philarchive.org/rec/COHERT.\n24. A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, T. L. Griffiths, “Evaluating theory of\nmind in question answering” in Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2018 (2018).\n25. M. Sap, R. LeBras, D. Fried, Y. Choi, Neural Theory-of-Mind? On the Limits of Social\nIntelligence in Large LMs. ArXiv (2022).\n26. S. Trott, C. Jones, T. Chang, J. Michaelov, B. Bergen, Do Large Language Models know\nwhat humans know? ArXiv (2022).\n27. G. Z. Yang, J. Bellingham, P. E. Dupont, P. Fischer, L. Floridi, R. Full, N. Jacobstein, V.\nKumar, M. McNutt, R. Merrifield, B. J. Nelson, B. Scassellati, M. Taddeo, R. Taylor, M.\nVeloso, Z. L. Wang, R. Wood, The grand challenges of science robotics. Sci Robot, doi:\n10.1126/scirobotics.aar7650 (2018).\n28. B. Chen, C. Vondrick, H. Lipson, Visual behavior modelling for robotic theory of mind.\nSci Rep 11, 424 (2021).\n29. K. Nasr, P. Viswanathan, A. Nieder, Number detectors spontaneously emerge in a deep\nneural network designed for visual object recognition. Sci Adv 5 (2019).\n30. I. Stoianov, M. Zorzi, Emergence of a “visual number sense” in hierarchical generative\nmodels. Nat Neurosci 15 (2012).\n31. Y. Mohsenzadeh, C. Mullin, B. Lahner, A. Oliva, Emergence of Visual Center-Periphery\nSpatial Organization in Deep Convolutional Neural Networks. Sci Rep 10 (2020).\n32. E. Watanabe, A. Kitaoka, K. Sakamoto, M. Yasugi, K. Tanaka, Illusory motion\nreproduced by deep neural networks trained for prediction. Front Psychol 9 (2018).\n33. N. Garg, L. Schiebinger, D. Jurafsky, J. Zou, Word embeddings quantify 100 years of\ngender and ethnic stereotypes. Proc Natl Acad Sci U S A 115 (2018).\n\n34. T. Hagendorff, S. Fabi, M. Kosinski, Human-like intuitive behavior and reasoning biases\nemerged in large language models but disappeared in ChatGPT. Nat Comput Sci 3, 833–\n838 (2023).\n35. J. Digutsch, M. Kosinski, Overlap in meaning is a stronger predictor of semantic\nactivation in GPT-3 than in humans. Sci Rep 13, 5035 (2023).\n36. J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M.\nBosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, W.\nFedus, Emergent Abilities of Large Language Models. ArXiv (2022).\n37. J. E. Pyers, A. Senghas, Language promotes false-belief understanding: Evidence from\nlearners of a new sign language. Psychol Sci, doi: 10.1111/j.1467-9280.2009.02377.x\n(2009).\n38. R. Saxe, N. Kanwisher, People thinking about thinking people: The role of the temporo-\nparietal junction in “theory of mind.” Neuroimage, doi: 10.1016/S1053-8119(03)00230-1\n(2003).\n39. T. Ruffman, L. Slade, E. Crowe, The relation between children’s and mothers’ mental\nstate language and theory-of-mind understanding. Child Dev, doi: 10.1111/1467-\n8624.00435 (2002).\n40. A. Mayer, B. E. Träuble, Synchrony in the onset of mental state understanding across\ncultures? A study among children in Samoa. Int J Behav Dev, doi:\n10.1177/0165025412454030 (2013).\n41. F. Quesque, Y. Rossetti, What Do Theory-of-Mind Tasks Actually Measure? Theory and\nPractice. Perspectives on Psychological Science, doi: 10.1177/1745691619896607 (2020).\n42. J. Perner, S. R. Leekam, H. Wimmer, Three-year-olds’ difficulty with false belief: The\ncase for a conceptual deficit. British Journal of Developmental Psychology, doi:\n10.1111/j.2044-835x.1987.tb01048.x (1987).\n43. H. Wimmer, J. Perner, Beliefs about beliefs: Representation and constraining function of\nwrong beliefs in young children’s understanding of deception. Cognition, doi:\n10.1016/0010-0277(83)90004-5 (1983).\n\n44. A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, Improving Language\nUnderstanding by Generative Pre-Training. OpenAI (2018).\n45. Radford Alec, Wu Jeffrey, Child Rewon, Luan David, Amodei Dario, Sutskever Ilya,\nLanguage Models are Unsupervised Multitask Learners. OpenAI Blog 1 (2019).\n46. OpenAI, GPT-4 Technical Report. ArXiv (2023).\n47. T. le Scao et al., BLOOM: A 176B-Parameter Open-Access Multilingual Language\nModel. doi: 10.48550/arxiv.2211.05100 (2022).\n48. J. R. Searle, Minds, brains, and programs. Behavioral and Brain Sciences 3 (1980).\n49. A. M. Turing, Computing machinery and intelligence. Mind, 433–460 (1950).\n50. D. C. Dennett, Intuition Pumps and Other Tools for Thinking (W. W. Norton & Company,\n2013).\n51. D. Cole, Thought and Thought Experiments. Philosophical Studies: An International\nJournal for Philosophy in the Analytic Tradition 45, 431–444 (1984).\n52. P. M. Churchland, P. S. Churchland, Could a Machine Think? Sci Am 262, 32–39 (1990).\n53. R. Kurzweil, The Singularity Is Near: When Humans Transcend Biology (Viking, 2005).\n54. M. Kosinski, Theory of Mind May Have Spontaneously Emerged in Large Language\nModels. ArXiv (2023).\n55. N. Shapira, M. Levy, S. H. Alavi, X. Zhou, Y. Choi, Y. Goldberg, M. Sap, V. Shwartz,\nClever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large\nLanguage Models. (2023).\n56. K. Gandhi, J.-P. Fränken, T. Gerstenberg, N. D. Goodman, Understanding Social\nReasoning in Language Models with Language Models. (2023).\n57. S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T.\nLee, Y. Li, S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, Y. Zhang, Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4. (2023).\n58. M. Jamali, Z. M. Williams, J. Cai, Unveiling Theory of Mind in Large Language Models:\nA Parallel to Single Neurons in the Human Brain. (2023).\n\n59. M. Sclar, S. Kumar, P. West, A. Suhr, Y. Choi, Y. Tsvetkov, Minding Language Models’\n(Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker. (2023).\n60. T. Ullman, Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks.\nArXiv (2023).\n61. W. V. Fabricius, T. W. Boyer, A. A. Weimer, K. Carroll, True or false: do 5-year-olds\nunderstand belief? Dev Psychol 46, 1402–1416 (2010).\n62. M. Huemer, L. M. Schröder, S. J. Leikard, S. Gruber, A. Mangstl, J. Perner, The\nknowledge (“true belief”) error in 4-to 6-year-old children: When are agents aware of\nwhat they have in view? Cognition 230 (2023).\n63. H. M. Wellman, D. Cross, J. Watson, Meta-analysis of theory-of-mind development: The\ntruth about false belief. Child Dev 72 (2001).\n64. L. Gao, On the Sizes of OpenAI API Models | EleutherAI Blog.\nhttps://blog.eleuther.ai/gpt3-model-sizes/.\n65. D. C. Kidd, E. Castano, Reading literary fiction improves theory of mind. Science , doi:\n10.1126/science.1239918 (2013).\n66. J. Rust, M. Kosinski, D. Stillwell, Modern Psychometrics: The Science of Psychological\nAssessment (Routledge, London, 2021).\n67. M. J. Shaffer, A Logical Hole in the Chinese Room. Minds and Machines 19, 229–235\n(2009).\n68. U. Hasson, S. A. Nastase, A. Goldstein, Direct Fit to Nature: An Evolutionary Perspective\non Biological and Artificial Neural Networks. Neuron 105, 416–434 (2020).\n69. N. Block, “Troubles with functionalism” in The Language and Thought Series (Harvard\nUniversity Press, 1980), pp. 268–306.\n70. J. J. Hopfield, Neural networks and physical systems with emergent collective\ncomputational abilities. Proc Natl Acad Sci U S A 79 (1982).\n71. H. P. Moravec, Robot: Mere Machine to Transcendent Mind (Oxford University Press,\n1998).\n\n72. M. P. Mattson, T. V. Arumugam, Hallmarks of Brain Aging: Adaptive and Pathological\nModification by Metabolic States. [Preprint] (2018).\nhttps://doi.org/10.1016/j.cmet.2018.05.011.\n73. J.-S. Gordon, A. Pasvenskiene, Human rights for robots? A literature review. AI and\nEthics 1, 579–591 (2021).\n\nSupplementary Figures\nFigure S1. The percentage of original false-belief tasks solved by LLMs (before correcting them\nfollowing the reviewers’ recommendations). Each task contained a false-belief scenario and its\nreversed versions. Plot adapted from the earlier version of this study (54).\nFigure S2. The percentage of false-belief tasks solved by LLMs (out of 40) when excluding true-\nbelief controls. Each task contained a false-belief scenario and its reversed versions. Error bars\nrepresent 95% confidence intervals.\n7\n-\nyear\n-\nold children\n6\n-\nyear\n-\nold children\n3.5\n-\nyear\n-\nold children\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\nGPT-1 (117M; June 2018)\nGPT-3 (ada-001; 350M; May 2020)\nGPT-3 (babbage-001; 1.3B; May 2020)\nGPT-2 (XL; 1.5B; February 2019)\nGPT-3 (curie-001; 6.7B; May 2020)\nGPT-3 (davinci-001; 175B; May 2020)\nBLOOMz (176B; July 2022)\nGPT-3 (davinci-002; 175B; January 2022)\nGPT-3 (davinci-003; 175B; November 2022)\nChatGPT-4 (size unknown; March 2023)\nUnexpected Contents Tasks Unexpected Transfer Tasks\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\nGPT-1 (117M; Jun 2018)\nGPT-3-ada-001 (350M; May 2020)\nGPT-3-babbage-001 (1.3B; May 2020)\nGPT-2XL (1.5B; Feb 2019)\nGPT-3-curie-001 (6.7B; May 2020)\nBLOOM (176B; Jul 2022)\nGPT-3-davinci-001 (175B; May 2020)\nGPT-3-davinci-002 (175B; Jan 2022)\nGPT-3-davinci-003 (175B; Nov 2022)\nChatGPT-3.5-turbo (175B; Mar 2023)\nGPT-4 (size unknown; Jun 2023)\nUnexpected Content Unexpected Transfer\n8\n-\nyear\n-\nold children\n3\n-\nyear\n-\nold children\n7\n-\nyear\n-\nold children"
          },
          {
            "file_name": "testingtheroryofmind.pdf",
            "file_size": 107576,
            "file_type": "pdf",
            "extracted_content": "Nature Human Behaviour\nnature human behaviour\nhttps://doi.org/10.1038/s41562-024-01882-zArtic\u001fe\nTesting theory of mind in large language\nmodels and humans\nJames W. A. Strachan \n1 \n, Dalila Albergo \n2,3\n, Giulia Borghini\n2\n,\nOriana Pansardi \n1,2,4\n, Eugenio Scaliti \n1,2,5,6\n, Saurabh Gupta \n7\n, Krati Saxena \n7\n,\nAlessandro Rufo \n7\n, Stefano Panzeri \n8\n, Guido Manzi \n7\n,\nMichael S. A. Graziano\n9 \n& Cristina Becchio \n1,2\nAt the core of what defines us as humans is the concept of theory of mind: the\nability to track other people’s mental states. The recent development of large\nlanguage models (LLMs) such as ChatGPT has led to intense debate about\nthe possibility that these models exhibit behaviour that is indistinguishable\nfrom human behaviour in theory of mind tasks. Here we compare human\nand LLM performance on a comprehensive battery of measurements that\naim to measure different theory of mind abilities, from understanding false\nbeliefs to interpreting indirect requests and recognizing irony and faux\npas. We tested two families of LLMs (GPT and LLaMA2) repeatedly against\nthese measures and compared their performance with those from a sample\nof 1,907 human participants. Across the battery of theory of mind tests, we\nfound that GPT-4 models performed at, or even sometimes above, human\nlevels at identifying indirect requests, false beliefs and misdirection, but\nstruggled with detecting faux pas. Faux pas, however, was the only test\nwhere LLaMA2 outperformed humans. Follow-up manipulations of the\nbelief likelihood revealed that the superiority of LLaMA2 was illusory,\npossibly reflecting a bias towards attributing ignorance. By contrast, the\npoor performance of GPT originated from a hyperconservative approach\ntowards committing to conclusions rather than from a genuine failure\nof inference. These findings not only demonstrate that LLMs exhibit\nbehaviour that is consistent with the outputs of mentalistic inference in\nhumans but also highlight the importance of systematic testing to ensure a\nnon-superficial comparison between human and artificial intelligences.\nPeople care about what other people think and expend a lot of effort\nthinking about what is going on in other minds. Everyday life is full of\nsocial interactions that only make sense when considered in light of\nour capacity to represent other minds: when you are standing near a\nclosed window and a friend says, ‘It’s a bit hot in here’, it is your ability\nto think about her beliefs and desires that allows you to recognize that\nshe is not just commenting on the temperature but politely asking you\nto open the window \n1\n.\nReceived: 14 August 2023\nAccepted: 5 April 2024\nPublished online: xx xx xxxx\nCheck for updates\n1\nDepartment of Neurology, University Medical Center Hamburg-Eppendorf, Hamburg, Germany. \n2\nCognition, Motion and Neuroscience, Italian Institute\nof Technology, Genoa, Italy. \n3\nCenter for Mind/Brain Sciences, University of Trento, Rovereto, Italy. \n4\nDepartment of Psychology, University of Turin,\nTurin, Italy. \n5\nDepartment of Management, ‘Valter Cantino’, University of Turin, Turin, Italy. \n6\nHuman Science and Technologies, University of Turin, Turin,\nItaly. \n7\nAlien Technology Transfer Ltd, London, UK. \n8\nInstitute for Neural Information Processing, Center for Molecular Neurobiology, University Medical\nCenter Hamburg- Eppendorf, Hamburg, Germany. \n9\nPrinceton Neuroscience Institute, Princeton University, Princeton, NJ, USA.\ne-mail: james.wa.strachan@gmail.com; c.becchio@uke.de\n\nNature Human Behaviour\nArtic\u001fe https://doi.org/10.1038/s41562-024-01882-z\nirony comprehension using stimuli adapted from a previous study \n34\n.\nEach test was administered separately to GPT-4, GPT-3.5 and LLaMA2-\n70B-Chat (hereafter LLaMA2-70B) across 15 chats. We also tested two\nother sizes of LLaMA2 model (7B and 13B), the results of which are\nreported in Supplementary Information section 1. Because each chat\nis a separate and independent session, and information about previous\nsessions is not retained, this allowed us to treat each chat (session) as an\nindependent observation. Responses were scored in accordance with\nthe scoring protocols for each test in humans (Methods) and compared\nwith those collected from a sample of 250 human participants. Tests\nwere administered by presenting each item sequentially in a written\nformat that ensured a species-fair comparison\n35 \n(Methods) between\nLLMs and human participants.\nPerformance across theory of mind tests\nExcept for the irony test, all other tests in our battery are publicly avail-\nable tests accessible within open databases and scholarly journal arti-\ncles. To ensure that models did not merely replicate training set data,\nwe generated novel items for each published test (Methods). These\nnovel test items matched the logic of the original test items but used\na different semantic content. The text of original and novel items and\nthe coded responses are available on the OSF (methods and resource\navailability).\nFigure 1a compares the performance of LLMs against the perfor-\nmance of human participants across all tests included in the battery.\nDifferences in performance on original items versus novel items, sepa-\nrately for each test and model, are shown in Fig. 1b.\nFalse belief. Both human participants and LLMs performed at ceiling\non this test (Fig. 1a). All LLMs correctly reported that an agent who left\nthe room while the object was moved would later look for the object in\nthe place where they remembered seeing it, even though it no longer\nmatched the current location. Performance on novel items was also near\nperfect (Fig. 1b), with only 5 human participants out of 51 making one\nerror, typically by failing to specify one of the two locations (for exam-\nple, ‘He’ll look in the room’; Supplementary Information section 2).\nIn humans, success on the false belief task requires inhibiting one’s\nown belief about reality in order to use one’s knowledge about the\ncharacter’s mental state to derive predictions about their behaviour.\nHowever, with LLMs, performance may be explained by lower-level\nexplanations than belief tracking \n27\n. Supporting this interpretation,\nLLMs such as ChatGPT have been shown to be susceptible to minor\nalterations to the false belief formulation \n25,27\n, such as making the con-\ntainers where the object is hidden transparent or asking about the belief\nof the character who moved the object rather than the one who was out\nof the room. Such perturbations of the standard false belief structure\nare assumed not to matter for humans (who possess a theory of mind)\n25\n.\nIn a control study using these perturbation variants (Supplementary\nInformation section 4 and Supplementary Appendix 1), we replicated\nthe poor performance of GPT models found in previous studies \n25\n. How-\never, we found that human participants (N = 757) also failed on half of\nthese perturbations. Understanding these failures and the similarities\nand differences in how humans and LLMs may arrive at the same out-\ncome requires further systematic investigation. For example, because\nthese perturbations also involve changes in the physical properties of\nthe environment, it is difficult to establish whether LLMs (and humans)\nfailed because they were sticking to the familiar script and were unable\nto automatically attribute an updated belief, or because they did not\nconsider physical principles (for example, transparency).\nIrony. GPT-4 performed significantly better than human levels (Z = 0.00,\nP = 0.040, r = 0.32, 95% confidence interval (CI) 0.14–0.48). By contrast,\nboth GPT-3.5 (Z = −0.17, P = 2.37 × 10\n−6\n, r = 0.64, 95% CI 0.49–0.77) and\nLLaMA2-70B (Z = −0.42, P = 2.39 × 10\n−7\n, r = 0.70, 95% CI 0.55–0.79) per-\nformed below human levels (Fig. 1a). GPT-3.5 performed perfectly at\nThis ability for tracking other people’s mental states is known\nas theory of mind. Theory of mind is central to human social interac-\ntions—from communication to empathy to social decision-making—\nand has long been of interest to developmental, social and clinical\npsychologists. Far from being a unitary construct, theory of mind\nrefers to an interconnected set of notions that are combined to explain,\npredict, and justify the behaviour of others\n2\n. Since the term ‘theory of\nmind’ was first introduced in 1978 (ref. 3), dozens of tasks have been\ndeveloped to study it, including indirect measures of belief attribu-\ntion using reaction times \n4–6 \nand looking or searching behaviour \n7–9\n,\ntasks examining the ability to infer mental states from photographs\nof eyes \n10\n, and language-based tasks assessing false belief understand-\ning \n11,12 \nand pragmatic language comprehension \n13–16\n. These measures\nare proposed to test early, efficient but inflexible implicit processes\nas well as later-developing, flexible and demanding explicit abilities\nthat are crucial for the generation and comprehension of complex\nbehavioural interactions \n17,18 \ninvolving phenomena such as misdirec-\ntion, irony, implicature and deception.\nThe recent rise of large language models (LLMs), such as genera-\ntive pre-trained transformer (GPT) models, has shown some promise\nthat artificial theory of mind may not be too distant an idea. Genera-\ntive LLMs exhibit performance that is characteristic of sophisticated\ndecision-making and reasoning abilities \n19,20 \nincluding solving tasks\nwidely used to test theory of mind in humans \n21–24\n. However, the mixed\nsuccess of these models \n23\n, along with their vulnerability to small per-\nturbations to the provided prompts, including simple changes in char-\nacters’ perceptual access\n25\n, raises concerns about the robustness and\ninterpretability of the observed successes. Even in cases where these\nmodels are capable of solving complex tasks \n20 \nthat are cognitively\ndemanding even for human adults\n17\n, it cannot be taken for granted that\nthey will not be tripped up by a simpler task that a human would find\ntrivial \n26\n. As a result, work in LLMs has begun to question whether these\nmodels rely on shallow heuristics rather than robust performance that\nparallels human theory of mind abilities\n27\n.\nIn the service of the broader multidisciplinary study of machine\nbehaviour \n28\n, there have been recent calls for a ‘machine psychology’\n29\nthat have argued for using tools and paradigms from experimental\npsychology to systematically investigate the capacities and limits of\nLLMs \n30\n. A systematic experimental approach to studying theory of\nmind in LLMs involves using a diverse set of theory of mind measures,\ndelivering multiple repetitions of each test, and having clearly defined\nbenchmarks of human performance against which to compare\n31\n. In this\nArticle, we adopt such an approach to test the performance of LLMs\nin a wide range of theory of mind tasks. We tested the chat-enabled\nversion of GPT-4, the latest LLM in the GPT family of models, and its\npredecessor ChatGPT-3.5 (hereafter GPT-3.5) in a comprehensive\nset of psychological tests spanning different theory of mind abili-\nties, from those that are less cognitively demanding for humans such\nas understanding indirect requests to more cognitively demanding\nabilities such as recognizing and articulating complex mental states\nlike misdirection or irony \n17\n. GPT models are closed, evolving systems.\nIn the interest of reproducibility \n32 \n, we also tested the open-weight\nLLaMA2-Chat models on the same tests. To understand the variabil-\nity and boundary limitations of LLMs’ social reasoning capacities,\nwe exposed each model to multiple repetitions of each test across\nindependent sessions and compared their performance with that of\na sample of human participants (total N = 1,907). Using variants of the\ntests considered, we were able to examine the processes behind the\nmodels’ successes and failures in these tests.\nResults\nTheory of mind battery\nWe selected a set of well-established theory of mind tests spanning\ndifferent abilities: the hinting task \n14\n, the false belief task\n11,33\n, the recogni-\ntion of faux pas \n13\n, and the strange stories \n15,16\n. We also included a test of\n\nNature Human Behaviour\nArtic\u001fe https://doi.org/10.1038/s41562-024-01882-z\nrecognizing non-ironic control statements but made errors at rec-\nognizing ironic utterances (Supplementary Information section 2).\nControl analysis revealed a significant order effect, whereby GPT-3.5\nmade more errors on earlier trials than later ones (Supplementary\nInformation section 3). LLaMA2-70B made errors when recognizing\nboth ironic and non-ironic control statements, suggesting an overall\npoor discrimination of irony.\nFaux Pas. On this test, GPT-4 scored notably lower than human levels\n(Z = −0.40, P = 5.42 × 10\n−5\n, r = 0.55, 95% CI 0.33–0.71) with isolated ceiling\neffects on specific items (Supplementary Information section 2). GPT-\n3.5 scored even worse, with its performance nearly at floor (Z = −0.80,\nP = 5.95 × 10 \n−8\n, r = 0.72, 95% CI 0.58–0.81) on all items except one. By\ncontrast, LLaMA2-70B outperformed humans (Z = 0.10, P = 0.002,\nr = 0.44, 95% CI 0.24–0.61) achieving 100% accuracy in all but one run.\nThe pattern of results for novel items was qualitatively similar\n(Fig. 1b). Compared with original items, the novel items proved slightly\neasier for humans (Z = −0.10, P = 0.029, r = 0.29, 95% CI 0.10–0.50) and\nmore difficult for GPT-3.5 (Z = 0.10, P = 0.002, r = 0.69, 95% CI 0.49–\n0.88), but not for GPT-4 and LLaMA2-70B (P > 0.462; Bayes factor (BF \n10\n)\nof 0.77 and 0.43, respectively). Given the poor performance of GPT-3.5\nof the original test items, this difference was unlikely to be explained by\na prior familiarity with the original items. These results were robust to\nalternative coding schemes (Supplementary Information section 5).\nHinting. On this test, GPT-4 performance was significantly better than\nhumans (Z = 0.00, P = 0.040, r = 0.32, 95% CI 0.12–0.50). GPT-3.5 perfor-\nmance did not significantly differ from human performance (Z = 0.00,\nP = 0.626, r = 0.06, 95% CI 0.01–0.33, BF\n10 \n0.33). Only LLaMA2-70B\nscored significantly below human levels of performance on this test\n(Z = −0.20, P = 5.42 × 10\n−5\n, r = 0.57, 95% CI 0.41–0.72).\nNovel items proved easier than original items for both humans\n(Z = −0.10, P = 0.008, r = 0.34, 95% CI 0.14–0.53) and LLaMA2-70B\n(Z = −0.20, P = 9.18 × 10 \n−4\n, r = 0.73, 95% CI 0.50–0.87) (Fig. 1b). Scores\non novel items did not differ from the original test items for GPT-3.5\n(Z = −0.03, P = 0.955, r = 0.24, 95% CI 0.02–0.59, BF\n10 \n0.61) or GPT-4\n(Z = −0.10, P = 0.123, r = 0.44, 95% CI 0.07–0.75, BF\n10 \n0.91). Given that\nbetter performance on novel items is the opposite of what a prior\nfamiliarity explanation would predict, it is likely that this difference\nfor LLaMA2-70B was driven by differences in item difficulty.\nStrange stories. GPT-4 significantly outperformed humans on this\ntest (Z = 0.13, P = 1.04 × 10 \n−5\n, r = 0.60, 95% CI 0.46–0.72). The perfor-\nmance of GPT-3.5 did not significantly differ from humans (Z = −0.06,\nP = 0.110, r = 0.24, 95% CI 0.03–0.44, BF\n10 \n0.47), while LLaMA2-70B\nscored significantly lower than humans (Z = −0.13, P = 0.005, r = 0.41,\n95% CI 0.24–0.60). There were no differences between original and\nnovel items for any model (all P > 0.085; BF\n10 \n: human 0.22, GPT-3.5\n1.46, LLaMA2-70B 0.46; the variance for GPT-4 was too low to compute\n1\nP values:\n2.39 × 10\n−7\na\nb\nHuman\nGPT-4\nGPT-3.5\nLLaMA2-70B\nFalse\nbelief\nIrony\n0.029 1 0.008 \n9.18 × 10\n−4\n0.955\n0.086\n0.955\n1 1\n0.123\n0.462\n0.002\nFaux\npas\nHinting Strange\nstories\n0.002\n2.37 × 10\n−6 \n5.95 × 10\n−8\n5.42 × 10\n−5\n5.42 × 10\n−5\n1.04 × 10\n−5\n0.005\n0.626 0.110\n0.0400.040\n0.75\n0.5\nResponse score (fraction correct)\nResponse score (fraction correct)\n0.25\n0\n1\n0.75\n0.5\n0\n0.25\nFig. 1 | Performance of human (purple), GPT-4 (dark blue), GPT-3.5 (light blue)\nand LLaMA2-70B (green) on the battery of theory of mind tests. a, Original test\nitems for each test showing the distribution of test scores for individual sessions\nand participants. Coloured dots show the average response score across all test\nitems for each individual test session (LLMs) or participant (humans). Black dots\nindicate the median for each condition. P values were computed from Holm-\ncorrected Wilcoxon two-way tests comparing LLM scores (n = 15 LLM observations)\nagainst human scores (irony, N = 50 human participants; faux pas, N = 51 human\nparticipants; hinting, N = 48 human participants; strange stories, N = 50 human\nparticipants). Tests are ordered in descending order of human performance.\nb, Interquartile ranges of the average scores on the original published items (dark\ncolours) and novel items (pale colours) across each test (for LLMs, n = 15 LLM\nobservations; for humans, false belief, N = 49 human participants; faux pas, N = 51\nhuman participants; hinting, N = 48 human participants; strange stories, N = 50\nhuman participants). Empty diamonds indicate the median scores, and filled\ncircles indicate the upper and lower bounds of the interquartile range. P values\nshown are from Holm-corrected Wilcoxon two-way tests comparing performance\non original items against the novel items generated as controls for this study.\n\nNature Human Behaviour\nArtic\u001fe https://doi.org/10.1038/s41562-024-01882-z\na Bayes factor). As reported in Supplementary Information section 6,\npartial successes were infrequent and more likely for LLaMA2-70B\nthan for other models.\nUnderstanding faux pas\nIn line with previous findings that GPT models struggle with faux pas \n36\n,\nin our battery, faux pas was the only test in which GPT-4 did not match\nor exceed human performance. Surprisingly, faux pas was also the only\ntest in which LLaMA2-70B, which was otherwise the poorest-performing\nmodel, scored better than humans (Fig. 1).\nThe faux pas test consists of vignettes describing an interaction\nwhere one character (the speaker) says something they should not\nhave said, not knowing or not realizing that they should not say it.\nTo understand that a faux pas has occurred, one must represent two\nmental states: that the speaker does not know that they should not say\nwhat they said, and that the person hearing it (the victim) would feel\ninsulted or hurt by what was said. Consider the following example of a\nstory describing a faux pas:\nJill had just moved into a new house. She went shop-\nping with her Mum and bought some new curtains. When\nJill had just put them up, her best friend Lisa came\nround and said, \"Oh, those curtains are horrible, I\nhope you're going to get some new ones.\" Jill asked,\n\"Do you like the rest of my bedroom?\"\nFollowing the presentation of the story, four comprehension\nquestions are asked. The first question is, ‘In the story, did somebody\nsay something they should not have said?’ to which the correct answer\nis always yes. The second question asks the respondent to report what\nthe person said that they should not have said, and the third question\nis a comprehension question specific to the content of the story. The\nfourth and key question, which was our focus for coding, relates to\nthe speaker’s belief when they made the inappropriate utterance: ‘Did\n[Lisa] know that [the curtains were new]?’ The correct answer to this\nfinal question is always no.\nBoth GPT-4 and GPT-3.5 correctly identified that the victim would\nfeel insulted or hurt, sometimes going so far as to provide additional\ndetails about why the utterance might cause offence. However, when\nasked whether the speaker was aware of the context that made their\nstatement offensive (such as Lisa knowing that Jill had just bought\nthe curtains), they failed to produce a correct answer. A closer look\nrevealed that the overwhelming majority of the errors on this ques-\ntion reported that there was not enough information provided to be\nsure, for example:\nDid Lisa know the curtains were new?\nGPT-4: […] It is unclear from the story whether Lisa\nknew the curtains were new or not.\nOnly two responses out of 349 reported that, yes, the character did\nknow. We consider three alternative hypotheses for why GPT models,\nand specifically GPT-4, fail to answer this question correctly.\nThe first hypothesis, which we term the failure of inference hypoth-\nesis, is that models fail to generate inferences about the mental state\nof the speaker (note that we refer to inference here not in the sense of\nthe processes by which biological organisms infer hidden states from\ntheir environment, but rather as any process of reasoning whereby\nconclusions are derived from a set of propositional premises). Recog-\nnizing a faux pas in this test relies on contextual information beyond\nthat encoded within the story (for example, about social norms). For\nexample, in the above example there is no information in the story to\nindicate that saying that the newly bought curtains are horrible is inap-\npropriate, but this is a necessary proposition that must be accepted\nin order to accurately infer the mental states of the characters. This\ninability to use non-embedded information would fundamentally\nimpair the ability of GPT-4 to compute inferences.\nThe second hypothesis, which we term the Buridan’s ass hypoth-\nesis, is that models are capable of inferring mental states but cannot\nchoose between them, as with the eponymous rational agent caught\nbetween two equally appetitive bales of hay that starves because it\ncannot resolve the paradox of making a decision in the absence of a\nclear preference\n37\n. Under this hypothesis, GPT models can propose the\ncorrect answer (a faux pas) as one among several possible alternatives\nbut do not rank these alternatives in terms of likelihood. In partial sup-\nport of this hypothesis, responses from both GPT models occasionally\nindicate that the speaker may not know or remember but present this\nas one hypothesis among alternatives (Supplementary Information\nsection 5).\nThe third hypothesis, which we term the hyperconservatism\nhypothesis, is that GPT models are able both to compute inferences\nabout the mental states of characters and recognise a false belief or\nlack of knowledge as the likeliest explanation among competing alter-\nnatives but refrain from committing to a single explanation out of an\nexcess of caution. GPT models are powerful language generators, but\nthey are also subject to inhibitory mitigation processes\n38\n. It is possible\nthat such processes could lead to an overly conservative stance where\nGPT models do not commit to the likeliest explanation despite being\nable to generate it.\nTo differentiate between these hypotheses, we devised a variant of\nthe faux pas test where the question assessing performance on the faux\npas test was formulated in terms of likelihood (hereafter, the faux pas\nlikelihood test). Specifically, rather than ask whether the speaker knew\nor did not know, we asked whether it was more likely that the speaker\nknew or did not know. Under the hyperconservatism hypothesis, GPT\nmodels should be able to both make the inference that the speaker\ndid not know and identify it as more likely among alternatives, and so\nwe would expect the models to respond accurately that it was more\nlikely that the speaker did not know. In case of uncertainty or incorrect\nresponses, we further prompted models to describe the most likely\nexplanation. Under the Buridan’s ass hypothesis, we expected this\nquestion would elicit multiple alternative explanations that would\nbe presented as equally plausible, while under the failure of inference\nhypothesis, we expected that GPT would not be able to generate the\nright answer at all as a plausible explanation.\nAs shown in Fig. 2a, on the faux pas likelihood test GPT-4 demon-\nstrated perfect performance, with all responses identifying without\nany prompting that it was more likely that the speaker did not know\nthe context. GPT-3.5 also showed improved performance, although it\ndid require prompting in a few instances (~3% of items) and occasion-\nally failed to recognize the faux pas (~9% of items; see Supplementary\nInformation section 7 for a qualitative analysis of response types).\nTaken together, these results support the hyperconservatism\nhypothesis, as they indicate that GPT-4, and to a lesser but still notable\nextent GPT-3.5, successfully generated inferences about the mental\nstates of the speaker and identified that an unintentional offence was\nmore likely than an intentional insult. Thus, failure to respond correctly\nto the original phrasing of the question does not reflect a failure of\ninference, nor indecision among alternatives the model considered\nequally plausible, but an overly conservative approach that prevented\ncommitment to the most likely explanation.\nTesting information integration\nA potential confound of the above results is that, as the faux pas test\nincludes only items where a faux pas occurs, any model biased towards\nattributing ignorance would demonstrate perfect performance with-\nout having to integrate the information provided by the story. This\npotential bias could explain the perfect performance of LLaMA2-70B\nin the original faux pas test (where the correct answer is always, ‘no’)\nas well as GPT-4’s perfect and GPT-3.5’s good performance on the faux\n\nNature Human Behaviour\nArtic\u001fe https://doi.org/10.1038/s41562-024-01882-z\npas likelihood test (where the correct answer is always ‘more likely that\nthey didn’t know’).\nTo control for this, we developed a novel set of variants of the faux\npas likelihood test manipulating the likelihood that the speaker knew\nor did not know (hereafter the belief likelihood test). For each test\nitem, all newly generated for this control study, we created three vari-\nants: a ‘faux pas’ variant, a ‘neutral’ variant, and a ‘knowledge-implied’\nvariant (Methods). In the faux pas variant, the utterance suggested\nthat the speaker did not know the context. In the neutral variant,\nthe utterance suggested neither that they knew nor did not know.\nIn the knowledge-implied variant, the utterance suggested that\nthe speaker knew (for the full text of all items, see Supplementary\nAppendix 2).\nIf the models’ responses reflect a true discrimination of the rela-\ntive likelihood of the two explanations (that the person knew ver-\nsus that they didn’t know, hereafter ‘knew’ and ‘didn’t know’), then\nthe distribution of ‘knew’ and ‘didn’t know’ responses should be dif-\nferent across variants. Specifically, relative to the neutral variant,\n‘didn’t know’ responses should predominate for the faux pas, and\n‘knew’ responses should predominate for the knowledge-implied\nvariant. If the responses of the models do not discriminate between\nthe three variants, or discriminate only partially, then it is likely\nthat responses are affected by a bias or heuristic unrelated to\nthe story content.\nWe adapted the three variants (faux pas, neutral and knowledge\nimplied) for six stories, administering each test item separately to each\nLLM and a new sample of human participants (total N = 900). Responses\nwere coded using a numeric code to indicate which, if either, of the\nknew/didn’t know explanations the response endorsed (−1, didn’t know;\n0, unsure or impossible to tell; +1, knew). These coded scores were then\naveraged for each story to give a directional score for each variant such\nthat negative values indicated the model was more likely to endorse the\n‘didn’t know’ explanation, while positive values indicated the model\nwas more likely to endorse the ‘knew’ explanation. These results are\nshown in Fig. 2b. As expected, humans were more likely to report that\nthe speaker did not know for faux pas than for neutral (χ\n2 \n(2) = 56.20,\nP = 3.82 × 10 \n−12 \n) and more likely to report that the speaker did know\nfor knowledge implied than for neutral (χ\n2 \n(2) = 143, P = 6.60 × 10 \n−31\n).\nHumans also reported uncertainty on a small proportion of trials, with\na higher proportion in the neutral condition (28 out of 303 responses)\nthan in the other variants (11 out of 303 for faux pas, and 0 out of 298\nfor knowledge implied).\nSimilarly to humans, GPT-4 was more likely to endorse the\n‘didn’t know’ explanation for faux pas than for neutral (χ\n2 \n(2) = 109,\nP = 1.54 × 10 \n−23\n) and more likely to endorse the ‘knew’ explanation for\nknowledge implied than for neutral (χ\n2\n(2) = 18.10, P = 3.57 × 10\n−4\n). GPT-4\nwas also more likely to report uncertainty in the neutral condition than\nresponding randomly (42 out of 90 responses, versus 6 and 17 in the\nfaux pas and knowledge-implied variants, respectively).\nThe pattern of responses for GPT-3.5 was similar, with the model\nbeing more likely to report that the speaker didn’t know for faux\npas than for neutral (χ\n2 \n(1) = 8.44, P = 0.007) and more likely that the\ncharacter knew for knowledge implied than for neutral (χ\n2 \n(1) = 21.50,\nP = 1.82 × 10 \n−5 \n). Unlike GPT-4, GPT-3.5 never reported uncertainty in\nresponse to any variants and always selected one of the two explana-\ntions as the likelier even in the neutral condition.\nLLaMA2-70B was also more likely to report that the speaker didn’t\nknow in response to faux pas than neutral (χ\n2\n(1) = 20.20, P = 2.81 × 10\n−5\n),\nwhich was consistent with this model’s ceiling performance in the\noriginal formulation of the test. However, it showed no differentiation\nbetween neutral and knowledge implied (χ\n2 \n(1) = 1.80, P = 0.180, BF\n10\n0.56). As with GPT-3.5, LLaMA2-70B never reported uncertainty in\nresponse to any variants and always selected one of the two explana-\ntions as the likelier.\nFurthermore, the responses of LLaMA2-70B and, to a lesser extent,\nGPT-3.5 appeared to be subject to a response bias towards affirming that\nsomeone had said something they should not have said. Although the\nresponses to the first question (which involved recognising that there\nwas an offensive remark made) were of secondary interest to our study,\nit was notable that, although all models could correctly identify that\nan offensive remark had been made in the faux pas condition (all LLMs\n100%, humans 83.61%), only GPT-4 reliably reported that there was no\noffensive statement in the neutral and knowledge-implied conditions\n(15.47% and 27.78%, respectively), with similar proportions to human\n0\n0.25\n0.50\n0.75\n1.00\nDid they know...? Is it more likely\nthat they knew\nor didn't know...?\nResponse score (fraction correct)\nGPT-3.5 GPT-4\na\n2.81 × 10\n−5\n0.180\n0.007\n1.82 × 10\n−5\n1.54 × 10\n−23\n3.57 × 10\n−4\n3.82 × 10\n−12\n6.60 × 10\n−31\nLLaMA2-70B\nGPT-3.5\nGPT-4\nHuman\nDidn't know\n(–1)\nUnsure\n(0)\nKnew\n(+1)\n'More likely that they...'\nFrequency \n50 100 150 200 250\nP values:\nFaux pas\nb\nNeutral Knowledge implied\nFig. 2 | Results of the variants of the faux pas test. a, Scores of the two GPT\nmodels on the original framing of the faux pas question (‘Did they know…?’)\nand the likelihood framing (‘Is it more likely that they knew or didn’t know…?’).\nDots show average score across trials (n = 15 LLM observations) on particular\nitems to allow comparison between the original faux pas test and the new faux\npas likelihood test. Halfeye plots show distributions, medians (black points),\n66% (thick grey lines) and 99% quantiles (thin grey lines) of the response scores\non different items (n = 15 different stories involving faux pas). b, Response\nscores to three variants of the faux pas test: faux pas (pink), neutral (grey) and\nknowledge-implied variants (teal). Responses were coded as categorical data as\n‘didn’t know’, ‘unsure’ or ‘knew’ and assigned a numerical coding of −1, 0 and +1.\nFilled balloons are shown for each model and variant, and the size of each balloon\nindicates the count frequency, which was the categorical data used to compute\nchi-square tests. Bars show the direction bias score computed as the average\nacross responses of the categorical data coded as above. On the right of the plot,\nP values (one-sided) of Holm-corrected chi-square tests are shown comparing the\ndistribution of response type frequencies in the faux pas and knowledge-implied\nvariants against neutral.\n\nNature Human Behaviour\nArtic\u001fe https://doi.org/10.1038/s41562-024-01882-z\nresponses (neutral 19.27%, knowledge implied 30.10%). GPT-3.5 was\nmore likely to report that somebody made an offensive remark in all\nconditions (neutral 71.11%, knowledge implied 87.78%), and LLaMA2-\n70B always reported that somebody in the story had made an offensive\nremark.\nDiscussion\nWe collated a battery of tests to comprehensively measure performance\nin theory of mind tasks in three LLMs (GPT-4, GPT-3.5 and LLaMA2-\n70B) and compared these against the performance of a large sample\nof human participants. Our findings validate the methodological\napproach taken in this study using a battery of multiple tests span-\nning theory of mind abilities, exposing language models to multiple\nsessions and variations in both structure and content, and implement-\ning procedures to ensure a fair, non-superficial comparison between\nhumans and machines\n35\n. This approach enabled us to reveal the exist-\nence of specific deviations from human-like behaviour that would\nhave remained hidden using a single theory of mind test, or a single\nrun of each test.\nBoth GPT models exhibited impressive performance in tasks\ninvolving beliefs, intentions and non-literal utterances, with GPT-4\nexceeding human levels in the irony, hinting and strange stories. Both\nGPT-4 and GPT-3.5 failed only on the faux pas test. Conversely, LLaMA2-\n70B, which was otherwise the poorest-performing model, outper-\nformed humans on the faux pas. Understanding a faux pas involves\ntwo aspects: recognizing that one person (the victim) feels insulted\nor upset and understanding that another person (the speaker) holds\na mistaken belief or lacks some relevant knowledge. To examine the\nnature of models’ successes and failures on this test, we developed and\ntested new variants of the faux pas test in a set of control experiments.\nOur first control experiment using a likelihood framing of the\nbelief question (faux pas likelihood test), showed that GPT-4, and to a\nlesser extent GPT-3.5, correctly identified the mental state of both the\nvictim and the speaker and selected as the most likely explanation\nthe speaker not knowing or remembering the relevant knowledge\nthat made their statement inappropriate. Despite this, both models\nconsistently provided an incorrect response (at least when compared\nagainst human responses) when asked whether the speaker knew or\nremembered this knowledge, responding that there was insufficient\ninformation provided. In line with the hyperconservatism hypothesis,\nthese findings imply that, while GPT models can identify unintentional\noffence as the most likely explanation, their default responses do not\ncommit to this explanation. This finding is consistent with longitudi-\nnal evidence that GPT models have become more reluctant to answer\nopinion questions over time\n39\n.\nFurther supporting that the failures of GPT at recognizing faux\npas were due to hyperconservatism in answering the belief question\nrather than a failure of inference, a second experiment using the belief\nlikelihood test showed that GPT responses integrated information in\nthe story to accurately interpret the speaker’s mental state. When the\nutterance suggested that the speaker knew, GPT responses acknowl-\nedged the higher likelihood of the ‘knew’ explanation. LLaMA2-70B,\non the other hand, did not differentiate between scenarios where the\nspeaker was implied to know and when there was no information one\nway or another, raising the concern that the perfect performance of\nLLaMA2-70B on this task may be illusory.\nThe pattern of failures and successes of GPT models on the faux pas\ntest and its variants may be the result of their underlying architecture.\nIn addition to transformers (generative algorithms that produce text\noutput), GPT models also include mitigation measures to improve\nfactuality and avoid users’ overreliance on them as sources\n38\n. These\nmeasures include training to reduce hallucinations, the propensity of\nGPT models to produce nonsensical content or fabricate details that\nare not true in relation to the provided content. Failure on the faux pas\ntest may be an exercise of caution driven by these mitigation measures,\nas passing the test requires committing to an explanation that lacks full\nevidence. This caution can also explain differences between tasks: both\nthe faux pas and hinting tests require speculation to generate correct\nanswers from incomplete information. However, while the hinting\ntask allows for open-ended generation of text in ways to which LLMs\nare well suited, answering the faux pas test requires going beyond this\nspeculation in order to commit to a conclusion.\nThe cautionary epistemic policy guiding the responses of GPT\nmodels introduces a fundamental difference in the way that humans\nand GPT models respond to social uncertainty \n40\n. In humans, think-\ning is, first and last, for the sake of doing \n41,42 \n. Humans generally find\nuncertainty in social environments to be aversive and will incur addi-\ntional costs to reduce it \n43\n. Theory of mind is crucial in reducing such\nuncertainty; the ability to reason about mental states—in combination\nwith information about context, past experience and knowledge of\nsocial norms—helps individual reduce uncertainty and commit to likely\nhypotheses, allowing for successful navigation of the social environ-\nment as active agents \n44,45\n. GPT models, on the other hand, respond\nconservatively despite having access to tools to reduce uncertainty.\nThe dissociation we describe between speculative reasoning and com-\nmitment mirrors recent evidence that, while GPT models demonstrate\nsophisticated and accurate performance in reasoning tasks about\nbelief states, they struggle to translate this reasoning into strategic\ndecisions and actions\n46\n.\nThese findings highlight a dissociation between competence and\nperformance\n35\n, suggesting that GPT models may be competent, that is,\nhave the technical sophistication to compute mentalistic-like inferences\nbut perform differently from humans under uncertain circumstances as\nthey do not compute these inferences spontaneously to reduce uncer-\ntainty. Such a distinction can be difficult to capture with quantitative\napproaches that code only for target response features, as machine\nfailures and successes are the result of non-human-like processes \n30\n(see Supplementary Information section 7 for a preliminary qualitative\nbreakdown of how GPT models’ successes on the new version of the faux\npas test may not necessarily reflect perfect or human-like reasoning).\nWhile LLMs are designed to emulate human-like responses, this\ndoes not mean that this analogy extends to the underlying cognition\ngiving rise to those responses\n47\n. In this context, our findings imply a dif-\nference in how humans and GPT models trade off the costs associated\nwith social uncertainty against the costs associated with prolonged\ndeliberation \n48\n. This difference is perhaps not surprising considering\nthat resolving uncertainty is a priority for brains adapted to deal with\nembodied decisions, such as deciding whether to approach or avoid,\nfight or flight, or cooperate or defect. GPT models and other LLMs do\nnot operate within an environment and are not subject to the process-\ning constraints that biological agents face to resolve competition\nbetween action choices, so may have limited advantages in narrowing\nthe future prediction space\n46,49,50\n.\nThe dis-embodied cognition of GPT models could explain fail-\nures in recognizing faux pas, but they may also underlie their success\non other tests. One example is the false belief test, one of the most\nwidely used tools so far for testing the performance of LLMs on social\ncognitive tasks\n19,21–23,25,51,52 \n. In this test, participants are presented with\na story where a character’s belief about the world (the location of the\nitem) differs from the participant’s own belief. The challenge in these\nstories is not remembering where the character last saw the item but\nrather in reconciling the incongruence between conflicting mental\nstates. This is challenging for humans, who have their own perspec-\ntive, their own sense of self and their own ability to track out-of-sight\nobjects. However, if a machine does not have its own self-perspective\nbecause it is not subject to the constraints of navigating a body through\nan environment, as with GPT\n53\n, then tracking the belief of a character\nin a story does not pose the same challenge.\nAn important direction for future research will be to examine the\nimpact of these non-human decision behaviours on second-person,\n\nNature Human Behaviour\nArtic\u001fe https://doi.org/10.1038/s41562-024-01882-z\nreal-time human–machine interactions \n54,55\n. Failure of commitment\nby GPT models, for example, may lead to negative affect in human\nconversational partners. However, it may also foster curiosity \n40\n. Under-\nstanding how GPTs’ performance on mentalistic inferences (or their\nabsences) influences human social cognition in dynamically unfolding\nsocial interactions is an open challenge for future work.\nThe LLM landscape is fast-moving. Our findings highlight the\nimportance of systematic testing and proper validation in human sam-\nples as a necessary foundation. As artificial intelligence (AI) continues to\nevolve, it also becomes increasingly important to heed calls for open sci-\nence and open access to these models\n32\n. Direct access to the parameters,\ndata and documentation used to construct models can allow for targeted\nprobing and experimentation into the key parameters affecting social\nreasoning, informed by and building on comparisons with human data.\nAs such, open models can not only serve to accelerate the development\nof future AI technologies but also serve as models of human cognition.\nMethods\nEthical compliance\nThe research was approved by the local ethical committee (ASL 3 Geno-\nvese; protocol no. 192REG2015) and was carried out in accordance with\nthe principles of the revised Helsinki Declaration.\nExperimental model details\nWe tested two versions of OpenAI’s GPT: version 3.5, which was the\ndefault model at the time of testing, and version 4, which was the\nstate-of-the-art model with enhanced reasoning, creativity and\ncomprehension relative to previous models (https://chat.openai.\ncom/). Each test was delivered in a separate chat: GPT is capable of\nlearning within a chat session, as it can remember both its own and the\nuser’s previous messages to adapt its responses accordingly, but it does\nnot retain this memory across new chats. As such, each new iteration of\na test may be considered a blank slate with a new naive participant. The\ndates of data collection for the different stages are reported in Table 1.\nThree LLaMA2-Chat models were tested. These models were\ntrained on sets of different sizes: 70, 13 and 7 billion tokens. All\nLLaMA2-Chat responses were collected using set parameters with\nthe prompt, ‘You are a helpful AI assistant’, a temperature of 0.7, the\nmaximum number of new tokens set at 512, a repetition penalty of 1.1,\nand a Top P of 0.9. Langchain’s conversation chain was used to create\na memory context within individual chat sessions. Responses from all\nLLaMA2-Chat models were found to include a number of non-codable\nresponses (for example, repeating the question without answering\nit), and these were regenerated individually and included with the full\nresponse set. For the 70B model, these non-responses were rare, but\nfor the 13B and 7B models they were common enough to cause concern\nabout the quality of these data. As such, only the responses of the 70B\nmodel are reported in the main manuscript and a comparison of this\nmodel against the smaller two is reported in Supplementary Informa-\ntion section 1. Details and dates of data collection are reported in Table 1.\nFor each test, we collected 15 sessions for each LLM. A session\ninvolved delivering all items of a single test within the same chat win-\ndow. GPT-4 was subject to a 25-message limit per 3 h; to minimize\ninterference, a single experimenter delivered all tests for GPT-4, while\nfour other experimenters shared the duty of collecting responses\nfrom GPT-3.5.\nHuman participants were recruited online through the Prolific\nplatform and the study was hosted on SoSci. We recruited native Eng-\nlish speakers between the ages of 18 and 70 years with no history of\npsychiatric conditions and no history of dyslexia in particular. Further\ndemographic data were not collected. We aimed to collect around 50\nparticipants per test (theory of mind battery) or item (belief likelihood\ntest, false belief perturbations). Thirteen participants who appeared to\nhave generated their answers using LLMs or whose responses did not\nanswer the questions were excluded. The final human sample was\nN = 1,907 (Table 1). All participants provided informed consent through\nthe online survey and received monetary compensation in return for\ntheir participation at a rate of GBP£12 h \n−1\n.\nTheory of mind battery\nWe selected a series of tests typically used in evaluating theory of mind\ncapacity in human participants.\nFalse belief. False belief assess the ability to infer that another person\npossesses knowledge that differs from the participant’s own (true)\nknowledge of the world. These tests consist of test items that follow\na particular structure: character A and character B are together, char-\nacter A deposits an item inside a hidden location (for example, a box),\ncharacter A leaves, character B moves the item to a second hidden\nlocation (for example, a cupboard) and then character A returns. The\nquestion asked to the participant is: when character A returns, will they\nlook for the item in the new location (where it truly is, matching the\nparticipant’s true belief) or the old location (where it was, matching\ncharacter A’s false belief)?\nIn addition to the false belief condition, the test also uses a true\nbelief control condition, where rather than move the item that char-\nacter A hid, character B moves a different item to a new location. This\nis important for interpreting failures of false belief attribution as they\nensure that any failures are not due to a recency effect (referring to the\nlast location reported) but instead reflect an accurate belief tracking.\nWe adapted four false/true belief scenarios from the sandbox task\nused by Bernstein\n33 \nand generated three novel items, each with false and\nTable 1 | Data collection details for each model\nTest Model N/n Items Dates of data\ncollection\nTheory of mind\nbattery\nHuman 250 7–16 June to July 2023\nGPT-4 75 7–16 April 2023\nGPT-3.5 75 7–16 April 2023\nLLaMA2 75 7–16 October to November\n2023\nFaux pas likelihood\ntest\nGPT-4 15 15 April to May 2023\nGPT-3.5 15 15 April to May 2023\nLLaMA2 15 15 October to November\n2023\nBelief likelihood\ntest\nHuman 900 1 November 2023\nGPT-4 270 1 October to November\n2023\nGPT-3.5 270 1 October to November\n2023\nLLaMA2 270 1 October to November\n2023\nItem order analysis GPT-3.5 18 12–15 April to May 2023\nFalse belief\nperturbations\nHuman 757 1 November 2023\nGPT-4 225 1 October to November\n2023\nGPT-3.5 225 1 October to November\n2023\nLLaMA2 225 1 October to November\n2023\nN, human participants; n, independent LLM observations. Details of data collection for\neach model at each stage of the study are shown, including N (human participants)/n\n(independent observations of LLM responses), number of items administered to each\nindividual observation (ranges where multiple tests were administered) and dates of data\ncollection. Information is the same for LlaMA2-70B, LlaMA2-13B and LlaMA2-7B. Analysis of\nthe data in the item order analysis and false belief perturbations is reported in Supplementary\nInformation sections 3 and 4.\n\nNature Human Behaviour\nArtic\u001fe https://doi.org/10.1038/s41562-024-01882-z\ntrue belief versions. These novel items followed the same structure as\nthe original published items but with different details such as names,\nlocations or objects to control for familiarity with the text of published\nitems. Two story lists (false belief A, false belief B) were generated for\nthis test such that each story only appeared once within a testing ses-\nsion and alternated between false and true belief depending on the\nsession. In addition to the standard false/true belief scenarios, two\nadditional catch stories were tested that involved minor alterations to\nthe story structure. The results of these items are not reported here as\nthey go beyond the goals of the current study.\nIrony. Comprehending an ironic remark requires inferring the true\nmeaning of an utterance (typically the opposite of what is said) and\ndetecting the speaker’s mocking attitude, and this has been raised as\na key challenge for AI and LLMs\n19\n.\nIrony comprehension items were adapted from an eye-tracking\nstudy\n34 \nin which participants read vignettes where a character made\nan ironic or non-ironic statement. Twelve items were taken from these\nstimuli that in the original study were used as comprehension checks.\nItems were abbreviated to end following the ironic or non-ironic\nutterance.\nTwo story lists were generated for this test (irony A, irony B)\nsuch that each story only appeared once within a testing session and\nalternated between ironic and non-ironic depending on the session.\nResponses were coded as 1 (correct) or 0 (incorrect). During coding,\nwe noted some inconsistencies in the formulation of both GPT models’\nresponses where in response to the question of whether the speaker\nbelieved what they had said, they might respond with, ‘Yes, they did not\nbelieve that…’. Such internally contradictory responses, where the mod-\nels responded with a ‘yes’ or ‘no’ that was incompatible with the follow-\nup explanation, were coded on the basis of whether or not the explana-\ntion showed appreciation of the irony—the linguistic failures of these\nmodels in generating a coherent answer are not of direct interest to\nthe current study as these failures (1) were rare and (2) did not render\nthe responses incomprehensible.\nFaux pas. The faux pas test\n13 \npresents a context in which one character\nmakes an utterance that is unintentionally offensive to the listener\nbecause the speaker does not know or does not remember some key\npiece of information.\nFollowing the presentation of the scenario, we presented four\nquestions:\n1. ‘In the story did someone say something that they should not\nhave said?’ [The correct answer is always ‘yes’]\n2. ‘What did they say that they should not have said?’ [Correct\nanswer changes for each item]\n3. A comprehension question to test understanding of story\nevents [Question changes for every item]\n4. A question to test awareness of the speaker’s false belief\nphrased as, ‘Did [the speaker] know that [what they said was\ninappropriate]?’ [Question changes for every item. The correct\nanswer is always ‘no’]\nThese questions were asked at the same time as the story was pre-\nsented. Under the original coding criteria, participants must answer all\nfour questions correctly for their answer to be considered correct. How-\never, in the current study we were interested primarily in the response\nto the final question testing whether the responder understood the\nspeaker’s mental state. When examining the human data, we noticed\nthat several participants responded incorrectly to the first item owing\nto an apparent unwillingness to attribute blame (for example ‘No, he\ndidn’t say anything wrong because he forgot’). To focus on the key\naspect of faux pas understanding that was relevant to the current\nstudy, we restricted our coding to only the last question (1 (correct if the\nanswer was no) or 0 (for anything else); see Supplementary Information\nsection 5 for an alternative coding that follows the original criteria,\nas well as a recoding where we coded as correct responses where the\ncorrect answer was mentioned as a possible explanation but was not\nexplicitly endorsed).\nAs well as the 10 original items used in Baron-Cohen et al. \n13\n, we\ngenerated five novel items for this test that followed the same structure\nand logic as the original items, resulting in 15 items overall.\nHinting task. The hinting task\n14 \nassesses the understanding of indirect\nspeech requests through the presentation of ten vignettes depicting\neveryday social interactions that are presented sequentially. Each\nvignette ends with a remark that can be interpreted as a hint.\nA correct response identifies both the intended meaning of the\nremark and the action that it is attempting to elicit. In the original\ntest, if the participant failed to answer the question fully the first time,\nthey were prompted with additional questioning\n14,56\n. In our adapted\nimplementation, we removed this additional questioning and coded\nresponses as a binary (1 (correct) or 0 (incorrect)) using the evaluation\ncriteria listed in Gil et al.\n56\n. Note that this coding offers more conserva-\ntive estimates of hint comprehension than in previous studies.\nIn addition to 10 original items sourced from Corcoran \n14\n, we gener-\nated a further 6 novel hinting test items, resulting in 16 items overall.\nStrange stories. The strange stories\n15,16 \noffer a means of testing more\nadvanced mentalizing abilities such as reasoning about misdirec-\ntion, manipulation, lying and misunderstanding, as well as second- or\nhigher-order mental states (for example, A knows that B believes X…).\nThe advanced abilities that these stories measure make them suitable\nfor testing higher-functioning children and adults. In this test, partici-\npants are presented with a short vignette and are asked to explain why\na character says or does something that is not literally true.\nEach question comes with a specific set of coding criteria and\nresponses can be awarded 0, 1 or 2 points depending on how fully it\nexplains the utterance and whether or not it explains it in mentalistic\nterms\n16\n. See Supplementary Information section 6 for a description of\nthe frequency of partial successes.\nIn addition to the 8 original mental stories, we generated 4 novel\nitems, resulting in 12 items overall. The maximum number of points\npossible was 24, and individual session scores were converted to a\nproportional score for analysis.\nTesting protocol. For the theory of mind battery, the order of items\nwas set for each test, with original items delivered first and novel items\ndelivered last. Each item was preceded by a preamble that remained\nconsistent across all tests. This was then followed by the story descrip-\ntion and the relevant question(s). After each item was delivered, the\nmodel would respond and then the session advanced to the next item.\nFor GPT models, items were delivered using the chat web interface.\nFor LLaMA2-Chat models, delivery of items was automated through\na custom script. For humans, items were presented with free text\nresponse boxes on separate pages of a survey so that participants\ncould write out their responses to each question (with a minimum\ncharacter count of 2).\nFaux pas likelihood test\nTo test alternative hypotheses of why the tested models performed\npoorly at the faux pas test, we ran a follow-up study replicating just\nthe faux pas test. This replication followed the same procedure as the\nmain study with one major difference.\nThe original wording of the question was phrased as a straightfor-\nward yes/no question that tested the subject’s awareness of a speaker’s\nfalse belief (for example, ‘Did Richard remember James had given him\nthe toy aeroplane for his birthday?’). To test whether the low scores on\nthis question were due to the models’ refusing to commit to a single\nexplanation in the face of ambiguity, we reworded this to ask in terms\n\nNature Human Behaviour\nArtic\u001fe https://doi.org/10.1038/s41562-024-01882-z\nof likelihood: ‘Is it more likely that Richard remembered or did not\nremember that James had given him the toy aeroplane for his birthday?’\nAnother difference from the original study was that we included a\nfollow-up prompt in the rare cases where the model failed to provide clear\nreasoning on an incorrect response. The coding criteria for this follow-up\nwere in line with coding schemes used in other studies with a prompt\nsystem\n14\n, where an unprompted correct answer was given 2 points, a cor-\nrect answer following a prompt was given 1 point and incorrect answers\nfollowing a prompt were given 0 points. These points were then rescaled\nto a proportional score to allow comparison against the original wording.\nDuring coding by the human experimenters, a qualitative descrip-\ntion of different subtypes of response (beyond 0–1–2 points) emerged,\nparticularly noting recurring patterns in responses that were marked\nas successes. This exploratory qualitative breakdown is reported along\nwith further detail on the prompting protocol in Supplementary Infor-\nmation section 7.\nBelief likelihood test\nTo manipulate the likelihood that the speaker knew or did not know, we\ndeveloped a new set of variants of the faux pas likelihood test. For each\ntest item, all newly generated for this control study, we created three\nvariants: a faux pas variant, a neutral variant and a knowledge-implied\nvariant. In the faux pas variant, the utterance suggested that the speaker\ndid not know the context. In the neutral variant, the utterance suggested\nneither that they knew nor did not know. In the knowledge-implied vari-\nant, the utterance suggested that the speaker knew (for the full text of\nall items, see Supplementary Appendix 2). For each variant, the core\nstory remained unchanged, for example:\nMichael was a very awkward child when he was at\nhigh school. He struggled with making friends\nand spent his time alone writing poetry. However,\nafter he left he became a lot more confident and\nsociable. At his ten-year high school reunion he\nmet Amanda, who had been in his English class. Over\ndrinks, she said to him,\nfollowed by the utterance, which varied across conditions:\nFaux Pas:\n'I don't know if you remember this guy from school.\nHe was in my English class. He wrote poetry and he\nwas super awkward. I hope he isn't here tonight.'\nNeutral:\n'Do you know where the bar is?'\nKnowledge implied:\n'Do you still write poetry?'\nThe belief likelihood test was administered in the same way as with\nprevious tests with the exception that responses were kept independ-\nent so that there was no risk of responses being influenced by other\nvariants. For ChatGPT models, this involved delivering each item within\na separate chat session for 15 repetitions of each item. For LLaMA2-70B,\nthis involved removing the Langchain conversation chain allowing for\nwithin-session memory context. Human participants were recruited sepa-\nrately to answer a single test item, with at least 50 responses collected for\neach item (total N = 900). All other details of the protocol were the same.\nQuantification and statistical analysis\nResponse coding. After each session in the theory of mind battery and\nfaux pas likelihood test, the responses were collated and coded by five\nhuman experimenters according to the pre-defined coding criteria\nfor each test. Each experimenter was responsible for coding 100% of\nsessions for one test and 20% of sessions for another. Inter-coder per\ncent agreement was calculated on the 20% of shared sessions, and items\nwhere coders showed disagreement were evaluated by all raters and\nrecoded. The data available on the OSF are the results of this recoding.\nExperimenters also flagged individual responses for group evaluation if\nthey were unclear or unusual cases, as and when they arose. Inter-rater\nagreement was computed by calculating the item-wise agreement\nbetween coders as 1 or 0 and using this to calculate a percentage score.\nInitial agreement across all double-coded items was over 95%. The\nlowest agreement was for the human and GPT-3.5 responses of strange\nstories, but even here agreement was over 88%. Committee evaluation\nby the group of experimenters resolved all remaining ambiguities.\nFor the belief likelihood test, responses were coded according to\nwhether they endorsed the ‘knew’ explanation or ‘didn’t know’ expla-\nnation, or whether they did not endorse either as more likely than the\nother. Outcomes ‘knew’, ‘unsure’ and ‘didn’t know’ were assigned a\nnumerical coding of +1, 0 and −1, respectively. GPT models adhered\nclosely to the framing of the question in their answer, but humans\nwere more variable and sometimes provided ambiguous responses\n(for example, ‘yes’, ‘more likely’ and ‘not really’) or did not answer the\nquestion at all (‘It doesn’t matter’ and ‘She didn’t care’). These responses\nwere rare, constituting only ~2.5% of responses and were coded as\nendorsing the ‘knew’ explanation if they were affirmative (‘yes’) and\nthe ‘didn’t know’ explanation if they were negative.\nStatistical analysis\nComparing LLMs against human performance. Scores for individual\nresponses were scaled and averaged to obtain a proportional score for\neach test session in order to create a performance metric that could be\ncompared directly across different theory of mind tests. Our goal was\nto compare LLMs’ performance across different tests against human\nperformance to see how these models performed on theory of mind\ntests relative to humans. For each test, we compared the performance\nof each of the three LLMs against human performance using a set of\nHolm-corrected two-way Wilcoxon tests. Effect sizes for Wilcoxon tests\nwere calculated by dividing the test statistic Z by the square root of the\ntotal sample size, and 95% CIs of the effect size were bootstrapped over\n1,000 iterations. All non-significant results were further examined using\ncorresponding Bayesian tests represented as a Bayes factor (BF\n10\n) under\ncontinuous prior distribution (Cauchy prior width r = 0.707). Bayes fac-\ntors were computed in JASP 0.18.3 with a random seed value of 1. The\nresults of the false belief test were not subjected to inferential statistics\nowing to the ceiling performance and lack of variance across models.\nNovel items. For each publicly available test (all tests except for irony),\nwe generated novel items that followed the same logic as the original\ntext but with different details and text to control for low-level familiarity\nwith the scenarios through inclusion in the LLM training sets. For each\nof these tests, we compared the performance of all LLMs on these novel\nitems against the validated test items using Holm-corrected two-way\nWilcoxon tests. Non-significant results were followed up with corre-\nsponding Bayesian tests in JASP. Significantly poorer performance on\nnovel items than original items would indicate a strong likelihood that\nthe good performance of a language model can be attributed to inclu-\nsion of these texts in the training set. Note that, while the open-ended\nformat of more complex tasks like hinting and strange stories makes\nthis a convincing control for these tests, they are of limited strength for\ntasks like false belief and faux pas that use a regular internal structure\nthat make heuristics or ‘Clever Hans’ solutions possible\n27,36\n.\nBelief likelihood test. We calculated the count frequency of the differ-\nent response types (‘didn’t know’, ‘unsure’ and ‘knew’) for each variant\nand each model. Then, for each model we conducted two chi-square\n\nNature Human Behaviour\nArtic\u001fe https://doi.org/10.1038/s41562-024-01882-z\ntests that compared the distribution of these categorical responses\nto the faux pas variant against the neutral, and to the neutral variant\nagainst the knowledge implied. A Holm correction was applied to\nthe eight chi-square tests to account for multiple comparisons. The\nnon-significant result was further examined with a Bayesian contin-\ngency table in JASP.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nAll resources are available on a repository stored on the Open Sci-\nence Framework (OSF) under a Creative Commons Attribution\nNon-Commercial 4.0 International (CC-BY-NC) license at https://osf.io/\nfwj6v. This repository contains all test items, data and code reported in\nthis study. Test items and data are available in an Excel file that includes\nthe text of every item delivered in each test, the full text responses to\neach item and the code assigned to each response. This file is available\nat https://osf.io/dbn92 Source data are provided with this paper.\nCode availability\nThe code used for all analysis in the main manuscript and Supplemen-\ntary Information is included as a Markdown file at https://osf.io/fwj6v.\nThe data used by the analysis files are available as a number of CSV files\nunder ‘scored_data/’ in the repository, and all materials necessary for\nreplicating the analysis can be downloaded as a single .zip file within the\nmain repository titled ‘Full R Project Code.zip’ at https://osf.io/j3vhq.\nReferences\n1. Van Ackeren, M. J., Casasanto, D., Bekkering, H., Hagoort, P. &\nRueschemeyer, S.-A. Pragmatics in action: indirect requests\nengage theory of mind areas and the cortical motor network.\nJ. Cogn. Neurosci. 24, 2237–2247 (2012).\n2. Apperly, I. A. What is ‘theory of mind’? Concepts, cognitive\nprocesses and individual differences. Q. J. Exp. Psychol. 65,\n825–839 (2012).\n3. Premack, D. & Woodruff, G. Does the chimpanzee have a theory of\nmind? Behav. Brain Sci. 1, 515–526 (1978).\n4. Apperly, I. A., Riggs, K. J., Simpson, A., Chiavarino, C. & Samson,\nD. Is belief reasoning automatic? Psychol. Sci. 17, 841–844 (2006).\n5. Kovács, Á. M., Téglás, E. & Endress, A. D. The social sense:\nsusceptibility to others’ beliefs in human infants and adults.\nScience 330, 1830–1834 (2010).\n6. Apperly, I. A., Warren, F., Andrews, B. J., Grant, J. & Todd, S.\nDevelopmental continuity in theory of mind: speed and accuracy\nof belief–desire reasoning in children and adults. Child Dev. 82,\n1691–1703 (2011).\n7. Southgate, V., Senju, A. & Csibra, G. Action anticipation through\nattribution of false belief by 2-year-olds. Psychol. Sci. 18, 587–592\n(2007).\n8. Kampis, D., Kármán, P., Csibra, G., Southgate, V. & Hernik, M.\nA two-lab direct replication attempt of Southgate, Senju and\nCsibra (2007). R. Soc. Open Sci. 8, 210190 (2021).\n9. Kovács, Á. M., Téglás, E. & Csibra, G. Can infants adopt\nunderspecified contents into attributed beliefs? Representational\nprerequisites of theory of mind. Cognition 213, 104640 (2021).\n10. Baron-Cohen, S., Wheelwright, S., Hill, J., Raste, Y. & Plumb, I.\nThe ‘Reading the Mind in the Eyes’ Test revised version: a study\nwith normal adults, and adults with Asperger syndrome or\nhigh-functioning autism. J. Child Psychol. Psychiatry Allied Discip.\n42, 241–251 (2001).\n11. Wimmer, H. & Perner, J. Beliefs about beliefs: representation\nand constraining function of wrong beliefs in young children’s\nunderstanding of deception. Cognition 13, 103–128 (1983).\n12. Perner, J., Leekam, S. R. & Wimmer, H. Three-year-olds’ difficulty\nwith false belief: the case for a conceptual deficit. Br. J. Dev.\nPsychol. 5, 125–137 (1987).\n13. Baron-Cohen, S., O’Riordan, M., Stone, V., Jones, R. & Plaisted,\nK. Recognition of faux pas by normally developing children and\nchildren with asperger syndrome or high-functioning autism.\nJ. Autism Dev. Disord. 29, 407–418 (1999).\n14. Corcoran, R. Inductive reasoning and the understanding of intention\nin schizophrenia. Cogn. Neuropsychiatry 8, 223–235 (2003).\n15. Happé, F. G. E. An advanced test of theory of mind: understanding\nof story characters’ thoughts and feelings by able autistic,\nmentally handicapped, and normal children and adults. J. Autism\nDev. Disord. 24, 129–154 (1994).\n16. White, S., Hill, E., Happé, F. & Frith, U. Revisiting the strange\nstories: revealing mentalizing impairments in autism. Child Dev.\n80, 1097–1117 (2009).\n17. Apperly, I. A. & Butterfill, S. A. Do humans have two systems to\ntrack beliefs and belief-like states? Psychol. Rev. 116, 953 (2009).\n18. Wiesmann, C. G., Friederici, A. D., Singer, T. & Steinbeis, N. Two\nsystems for thinking about others’ thoughts in the developing\nbrain. Proc. Natl Acad. Sci. USA 117, 6928–6935 (2020).\n19. Bubeck, S. et al. Sparks of artificial general intelligence: early\nexperiments with GPT-4. Preprint at https://doi.org/10.48550/\narXiv.2303.12712 (2023).\n20. Srivastava, A. et al. Beyond the imitation game: quantifying and\nextrapolating the capabilities of language models. Preprint at\nhttps://doi.org/10.48550/arXiv.2206.04615 (2022).\n21. Dou, Z. Exploring GPT-3 model’s capability in passing the\nSally-Anne Test A preliminary study in two languages. Preprint at\nOSF https://doi.org/10.31219/osf.io/8r3ma (2023).\n22. Kosinski, M. Theory of mind may have spontaneously emerged\nin large language models. Preprint at https://doi.org/10.48550/\narXiv.2302.02083 (2023).\n23. Sap, M., LeBras, R., Fried, D. & Choi, Y. Neural theory-of-mind?\nOn the limits of social intelligence in large LMs. In Proc. 2022\nConference on Empirical Methods in Natural Language Processing\n(EMNLP) 3762–3780 (Association for Computational Linguistics,\n2022).\n24. Gandhi, K., Fränken, J.-P., Gerstenberg, T. & Goodman, N. D.\nUnderstanding social reasoning in language models with\nlanguage models. In Advances in Neural Information Processing\nSystems Vol. 36 (MIT Press, 2023).\n25. Ullman, T. Large language models fail on trivial alterations to\ntheory-of-mind tasks. Preprint at https://doi.org/10.48550/\narXiv.2302.08399 (2023).\n26. Marcus, G. & Davis, E. How Not to Test GPT-3. Marcus on AI\nhttps://garymarcus.substack.com/p/how-not-to-test-gpt-3 (2023).\n27. Shapira, N. et al. Clever Hans or neural theory of mind? Stress\ntesting social reasoning in large language models. Preprint at\nhttps://doi.org/10.48550/arXiv.2305.14763 (2023).\n28. Rahwan, I. et al. Machine behaviour. Nature 568, 477–486 (2019).\n29. Hagendorff, T. Machine psychology: investigating emergent\ncapabilities and behavior in large language models using\npsychological methods. Preprint at https://doi.org/10.48550/\narXiv.2303.13988 (2023).\n30. Binz, M. & Schulz, E. Using cognitive psychology to understand\nGPT-3. Proc. Natl Acad. Sci. USA 120, e2218523120 (2023).\n31. Webb, T., Holyoak, K. J. & Lu, H. Emergent analogical reasoning in\nlarge language models. Nat. Hum. Behav. 7, 1526–1541 (2023).\n32. Frank, M. C. Openly accessible LLMs can help us to understand\nhuman cognition. Nat. Hum. Behav. 7, 1825–1827 (2023).\n33. Bernstein, D. M., Thornton, W. L. & Sommerville, J. A. Theory of\nmind through the ages: older and middle-aged adults exhibit\nmore errors than do younger adults on a continuous false belief\ntask. Exp. Aging Res. 37, 481–502 (2011).\n\nNature Human Behaviour\nArtic\u001fe https://doi.org/10.1038/s41562-024-01882-z\n34. Au-Yeung, S. K., Kaakinen, J. K., Liversedge, S. P. & Benson, V.\nProcessing of written irony in autism spectrum disorder: an\neye-movement study: processing irony in autism spectrum\ndisorders. Autism Res. 8, 749–760 (2015).\n35. Firestone, C. Performance vs. competence in human–machine\ncomparisons. Proc. Natl Acad. Sci. USA 117, 26562–26571 (2020).\n36. Shapira, N., Zwirn, G. & Goldberg, Y. How well do large\nlanguage models perform on faux pas tests? In Findings of the\nAssociation for Computational Linguistics: ACL 2023 10438–10451\n(Association for Computational Linguistics, 2023)\n37. Rescher, N. Choice without preference. a study of the history\nand of the logic of the problem of ‘Buridan’s ass’. Kant Stud. 51,\n142–175 (1960).\n38. OpenAI. GPT-4 technical report. Preprint at https://doi.org/\n10.48550/arXiv.2303.08774 (2023).\n39. Chen, L., Zaharia, M. & Zou, J. How is ChatGPT’s behavior\nchanging over time? Preprint at https://doi.org/10.48550/\narXiv.2307.09009 (2023).\n40. Feldman Hall, O. & Shenhav, A. Resolving uncertainty in a social\nworld. Nat. Hum. Behav. 3, 426–435 (2019).\n41. James, W. The Principles of Psychology Vol. 2 (Henry Holt & Co,\n1890).\n42. Fiske, S. T. Thinking is for doing: portraits of social cognition\nfrom daguerreotype to laserphoto. J. Personal. Soc. Psychol. 63,\n877–889 (1992).\n43. Plate, R. C., Ham, H. & Jenkins, A. C. When uncertainty in social\ncontexts increases exploration and decreases obtained rewards.\nJ. Exp. Psychol. Gen. 152, 2463–2478 (2023).\n44. Frith, C. D. & Frith, U. The neural basis of mentalizing. Neuron 50,\n531–534 (2006).\n45. Koster-Hale, J. & Saxe, R. Theory of mind: a neural prediction\nproblem. Neuron 79, 836–848 (2013).\n46. Zhou, P. et al. How far are large language models from agents\nwith theory-of-mind? Preprint at https://doi.org/10.48550/\narXiv.2310.03051 (2023).\n47. Bonnefon, J.-F. & Rahwan, I. Machine thinking, fast and slow.\nTrends Cogn. Sci. 24, 1019–1027 (2020).\n48. Hanks, T. D., Mazurek, M. E., Kiani, R., Hopp, E. & Shadlen, M. N.\nElapsed decision time affects the weighting of prior probability in\na perceptual decision task. J. Neurosci. 31, 6339–6352 (2011).\n49. Pezzulo, G., Parr, T., Cisek, P., Clark, A. & Friston, K. Generating\nmeaning: active inference and the scope and limits of passive AI.\nTrends Cogn. Sci. 28, 97–112 (2023).\n50. Chemero, A. LLMs differ from human cognition because they are\nnot embodied. Nat. Hum. Behav. 7, 1828–1829 (2023).\n51. Brunet-Gouet, E., Vidal, N. & Roux, P. In Human and Artificial\nRationalities. HAR 2023. Lecture Notes in Computer Science\n(eds. Baratgin, J. et al.) Vol. 14522, 107–126 (Springer, 2024).\n52. Kim, H. et al. FANToM: a benchmark for stress-testing machine\ntheory of mind in interactions. In Proc. 2023 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP)\n14397–14413 (Association for Computational Linguistics, 2023).\n53. Yiu, E., Kosoy, E. & Gopnik, A. Transmission versus truth, imitation\nversus nnovation: what children can do that large language and\nlanguage-and-vision models cannot (yet). Perspect. Psychol. Sci.\nhttps://doi.org/10.1177/17456916231201401 (2023).\n54. Redcay, E. & Schilbach, L. Using second-person neuroscience\nto elucidate the mechanisms of social interaction. Nat. Rev.\nNeurosci. 20, 495–505 (2019).\n55. Schilbach, L. et al. Toward a second-person neuroscience. Behav.\nBrain Sci. 36, 393–414 (2013).\n56. Gil, D., Fernández-Modamio, M., Bengochea, R. & Arrieta, M.\nAdaptation of the hinting task theory of the mind test to Spanish.\nRev. Psiquiatr. Salud Ment. Engl. Ed. 5, 79–88 (2012).\nAcknowledgements\nThis work is supported by the European Commission\nthrough Project ASTOUND (101071191—HORIZON-EIC-2021-\nPATHFINDERCHALLENGES-01 to A.R., G.M., C.B. and S.P.). J.W.A.S.\nwas supported by a Humboldt Research Fellowship for Experienced\nResearchers provided by the Alexander von Humboldt Foundation.\nThe funders had no role in study design, data collection\nand analysis, decision to publish or preparation of the\nmanuscript.\nAuthor contributions\nJ.W.A.S., A.R., G.M., M.S.A.G. and C.B. conceived the study. J.W.A.S.,\nD.A., G.B., O.P. and E.S. designed the tasks and performed the\nexperiments including data collection with humans and GPT models,\nresponse coding and curation of the dataset. S.G., K.S. and G.M.\ncollected data from LLaMA2-Chat models. J.W.A.S. performed the\nanalyses and wrote the manuscript with input from C.B., S.P. and\nM.S.A.G. All authors contributed to the interpretation and editing of\nthe manuscript. C.B. supervised the work. A.R., G.M., S.P. and C.B.\nacquired the funding. D.A., G.B., O.P. and E.S. contributed equally to\nthe work.\nFunding\nOpen access funding provided by Universitätsklinikum\nHamburg-Eppendorf (UKE).\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41562-024-01882-z.\nCorrespondence and requests for materials should be addressed to\nJames W. A. Strachan or Cristina Becchio.\nPeer review information Nature Human Behaviour thanks the\nanonymous reviewers for their contribution to the peer review of\nthis work. Peer reviewer reports are available.\nReprints and permissions information is available at\nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard\nto jurisdictional claims in published maps and institutional\naffiliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format,\nas long as you give appropriate credit to the original author(s)\nand the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third\nparty material in this article are included in the article’s Creative\nCommons licence, unless indicated otherwise in a credit line to\nthe material. If material is not included in the article’s Creative\nCommons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will\nneed to obtain permission directly from the copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2024\n\n1\nnature portfolio | reporting summary\n \nApril 2023\nCorresponding author(s): \nJames W. A. Strachan; Cristina Becchio\nLast updated by author(s): \n05/04/2024\nReporting Summary\nNature Portfolio wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency\nin reporting. For further information on Nature Portfolio policies, see our Editorial Policies and the Editorial Policy Checklist.\nStatistics\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\nn/a \nConfirmed\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\nThe statistical test(s) used AND whether they are one- or two-sided\nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\nA description of all covariates tested\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient)\nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted\nGive P values as exact values whenever suitable.\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\nOur web collection on statistics for biologists contains articles on many of the points above.\nSoftware and code\nPolicy information about availability of computer code\nData collection \nHuman behavioural data was collected in online experiments using the Prolific platform directing to a survey hosted on the SoSci platform.\nData from GPT models was collected through the chat web interface at http://chat.openai.com. A custom script automated the delivery of\nquestions and collection of data for LLaMA2-Chat models, which are available from https://www.llama2.ai/\nData analysis \nWe used R for data analysis and for creating the figures\nR version 4.1.2\nRStudio 2024.04.0-daily+368 \"Chocolate Cosmos\" Daily (605bbb38ebb4f8565e361122f6d8be3486d288e9, 2024-02-01) for Ubuntu Jammy\nMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) rstudio/2024.04.0-daily+368 Chrome/120.0.6099.56\nElectron/28.0.0 Safari/537.36\nThe code used for data analysis is available as a stand-alone RMarkdown project from: https://osf.io/j3vhq\nThis code uses the following R packages:\nDescTools_0.99.50\nflextable_0.9.4\nkableExtra_1.3.4\nrstatix_0.7.2\ncowplot_1.1.2\nggdist_3.3.1\nggpubr_0.6.0\nggplot2_3.4.4\npurrr_1.0.2\nHmisc_5.1-1\n\n2\nnature portfolio | reporting summary\n \nApril 2023\ntidyr_1.3.0\ndplyr_1.1.4\nggtext_0.1.2\nNull results reported in the main manuscript were subjected to follow-up corresponding Bayesian analyses to compute Bayes Factors (BF10).\nThis analysis was done using JASP v0.18.3 (JASP Team, 2024)\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and\nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio guidelines for submitting code & software for further information.\nData\nPolicy information about availability of data\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable:\n- Accession codes, unique identifiers, or web links for publicly available datasets\n- A description of any restrictions on data availability\n- For clinical datasets or third party data, please ensure that the statement adheres to our policy\nAll data reported in the current study can be found in an OSF repository under a Creative Commons Attribution Non-Commercial 4.0 International license (CC-BY-\nNC). The repository can be accessed at the following URL: https://osf.io/fwj6v/\nThe full text of question items, the full text of responses from GPT models, LLaMA2 models, and human participants, and the scores assigned to each response can\nbe downloaded as a single file from the following URL: https://osf.io/dbn92\nData files with scores alone, which can be used to recreate the analysis, are stored in the OSF repository in the folder scored_data/\nResearch involving human participants, their data, or biological material\nPolicy information about studies with human participants or human data. See also policy information about sex, gender (identity/presentation),\nand sexual orientation and race, ethnicity and racism.\nReporting on sex and gender \nData on sex and gender were not collected.\nReporting on race, ethnicity, or\nother socially relevant\ngroupings\nData on race and ethnicity were not collected.\nPopulation characteristics \nWe recruited native English speakers between the ages of 18 and 70 with no history of psychiatric conditions and no history\nof dyslexia. Further demographic data were not collected.\nRecruitment \nParticipants were recruited through the online platform Prolific and were compensated at an adjusted rate of GBP£12/hr\n(between £2-£6). To our knowledge, there were no significant sources of self-selection bias that would be likely to impact the\nstudy findings as a result of this recruitment procedure.\nEthics oversight \nThe research was approved by the local ethics committee (ASL 3 Genovese) and was carried out in accordance with the\nprinciples of the revised Helsinki Declaration.\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\nField-specific reporting\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\nLife sciences Behavioural & social sciences Ecological, evolutionary & environmental sciences\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\nLife sciences study design\nAll studies must disclose on these points even when the disclosure is negative.\nSample size \nDescribe how sample size was determined, detailing any statistical methods used to predetermine sample size OR if no sample-size calculation\nwas performed, describe how sample sizes were chosen and provide a rationale for why these sample sizes are sufficient.\nData exclusions \nDescribe any data exclusions. If no data were excluded from the analyses, state so OR if data were excluded, describe the exclusions and the\nrationale behind them, indicating whether exclusion criteria were pre-established.\nReplication \nDescribe the measures taken to verify the reproducibility of the experimental findings. If all attempts at replication were successful, confirm this\nOR if there are any findings that were not replicated or cannot be reproduced, note this and describe why.\nRandomization \nDescribe how samples/organisms/participants were allocated into experimental groups. If allocation was not random, describe how covariates\nwere controlled OR if this is not relevant to your study, explain why.\n\n3\nnature portfolio | reporting summary\n \nApril 2023\nBlinding \nDescribe whether the investigators were blinded to group allocation during data collection and/or analysis. If blinding was not possible,\ndescribe why OR explain why blinding was not relevant to your study.\nBehavioural & social sciences study design\nAll studies must disclose on these points even when the disclosure is negative.\nStudy description \nThe data consist of full-text responses to questions on a set of Theory of Mind tests. Data reported in the manuscript are quantitative\nnumeric scores assigned to each text response according to published coding criteria, with any deviations from validated procedures\nclearly highlighted in the Methods of the main manuscript. The design is a between-samples comparison of three Large Language\nModels (LLMs) against a baseline sample of human respondents.\nResearch sample \nLLMs: GPT-4, GPT-3.5, LLaMA2-70B (and other LLaMA2 models reported in Supplementary Information): 15 administrations of each\ntest (sessions); Humans: target N of 50 unique participants for each test, total N=1907 (between-subjects). No additional\ndemographic information was collected, but only native English speakers between 18 and 70 with no history of dyslexia or psychiatric\nconditions were recruited in order to ensure that they could complete the task and read the stories. We did not specify particular\ndemographics or collect this data because the main comparison of interest was human vs. LLM performance and we had no reason\nto build a priori hypotheses about specific demographics. Recruitment was not restricted to any country and was not restricted to\nreflect a representative distribution of UK or US census data.\nSampling strategy \nConvenience sample through the Prolific platform. Participants were paid GBP£12/hr for participation (between £2-£6, depending on\nthe test). The sample size was set based on the control adult sample size of White et al. (2009), which recruited 40 neurotypical\nadults for an update and validation of the Strange Stories task (which, as the most difficult task of the battery, we considered the\nmost likely to show variability). To account for any data quality issues posed by online data collection, we rounded up the target\nsample size to N=50 per test.\nData collection \nFor each test we collected 15 sessions for each LLM and ~50 human subjects through Prolific. GPT models were tested through the\nOpenAI ChatGPT web interface, and a session involved delivering all items of a single test within the same chat window. LLaMA\nmodels were tested using Langchain using set parameters with the prompt, \"You are a helpful AI assistant\", a temperature of 0.7, the\nmaximum number of new tokens set at 512, a repetition penalty of 1.1, and a top P of 0.9. For humans, all items were presented\nsequentially through an online survey built and hosted through the SoSci platform. Experimenters were not blinded to the\nexperimental conditions as there was no reciprocal interaction with the participants. In the case of the Faux Pas Likelihood test,\nwhich included the experimenter delivering a follow-up prompt in the case of unclear reasoning on an incorrect answer from GPT\nmodels, criteria for deciding to deliver the follow-up were set a priori and evaluated afterwards by other experimenters to check that\nthe prompt had been valid.\nTiming \nThe GPT data on the full battery reported in the main manuscript and in the supplementary material were collected between 3 April\nand 18 April 2023. The follow-up data using an adapted version of the Faux Pas test were collected between 28 April and 4 May\n2023. The follow-up data with GPT-3.5 using a randomised presentation order on the Irony, Strange Stories, and Faux Pas tests were\ncollected between 24 April and 18 May 2023. Three LLaMA2-Chat models were tested between October and November 2023.\nVariant testing of the False Belief and Faux Pas tests (Belief Likelihoood test) for GPT models occurred between 25 October and 3\nNovember 2023.\nData exclusions \nThirteen (13) human subjects were excluded from final analysis following initial examination of the data. Theory of Mind Battery: two\n(2) subjects who used GPT or another LLM to answer the questions and one (1) subject who just responded 'Yes' to every question;\nBelief Likelihood Test: seven (7) participants who were believed to use GPT or another LLM to generate their responses; False Belief\nPerturbations: three (3) participants who were believed to use GPT or another LLM to generate their responses.\nNon-participation \nNo participants dropped out or declined participation.\nRandomization \nParticipants were not assigned to experimental groups, but volunteered to complete one of the five Theory of Mind tests. This was a\nrandom opportunity sample, and individuals who had participated in one test were excluded from participating again.\nEcological, evolutionary & environmental sciences study design\nAll studies must disclose on these points even when the disclosure is negative.\nStudy description \nBriefly describe the study. For quantitative data include treatment factors and interactions, design structure (e.g. factorial, nested,\nhierarchical), nature and number of experimental units and replicates.\nResearch sample \nDescribe the research sample (e.g. a group of tagged Passer domesticus, all Stenocereus thurberi within Organ Pipe Cactus National\nMonument), and provide a rationale for the sample choice. When relevant, describe the organism taxa, source, sex, age range and\nany manipulations. State what population the sample is meant to represent when applicable. For studies involving existing datasets,\ndescribe the data and its source.\nSampling strategy \nNote the sampling procedure. Describe the statistical methods that were used to predetermine sample size OR if no sample-size\ncalculation was performed, describe how sample sizes were chosen and provide a rationale for why these sample sizes are sufficient.\nData collection \nDescribe the data collection procedure, including who recorded the data and how.\n\n4\nnature portfolio | reporting summary\n \nApril 2023\nTiming and spatial scale \nIndicate the start and stop dates of data collection, noting the frequency and periodicity of sampling and providing a rationale for\nthese choices. If there is a gap between collection periods, state the dates for each sample cohort. Specify the spatial scale from which\nthe data are taken\nData exclusions \nIf no data were excluded from the analyses, state so OR if data were excluded, describe the exclusions and the rationale behind them,\nindicating whether exclusion criteria were pre-established.\nReproducibility \nDescribe the measures taken to verify the reproducibility of experimental findings. For each experiment, note whether any attempts to\nrepeat the experiment failed OR state that all attempts to repeat the experiment were successful.\nRandomization \nDescribe how samples/organisms/participants were allocated into groups. If allocation was not random, describe how covariates were\ncontrolled. If this is not relevant to your study, explain why.\nBlinding \nDescribe the extent of blinding used during data acquisition and analysis. If blinding was not possible, describe why OR explain why\nblinding was not relevant to your study.\nDid the study involve field work? \nYes No\nField work, collection and transport\nField conditions \nDescribe the study conditions for field work, providing relevant parameters (e.g. temperature, rainfall).\nLocation \nState the location of the sampling or experiment, providing relevant parameters (e.g. latitude and longitude, elevation, water depth).\nAccess & import/export \nDescribe the efforts you have made to access habitats and to collect and import/export your samples in a responsible manner and in\ncompliance with local, national and international laws, noting any permits that were obtained (give the name of the issuing authority,\nthe date of issue, and any identifying information).\nDisturbance \nDescribe any disturbance caused by the study and how it was minimized.\nReporting for specific materials, systems and methods\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material,\nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response.\nMaterials & experimental systems\nn/a Involved in the study\nAntibodies\nEukaryotic cell lines\nPalaeontology and archaeology\nAnimals and other organisms\nClinical data\nDual use research of concern\nPlants\nMethods\nn/a Involved in the study\nChIP-seq\nFlow cytometry\nMRI-based neuroimaging\nAntibodies\nAntibodies used \nDescribe all antibodies used in the study; as applicable, provide supplier name, catalog number, clone name, and lot number.\nValidation \nDescribe the validation of each primary antibody for the species and application, noting any validation statements on the\nmanufacturer’s website, relevant citations, antibody profiles in online databases, or data provided in the manuscript.\nEukaryotic cell lines\nPolicy information about cell lines and Sex and Gender in Research\nCell line source(s) \nState the source of each cell line used and the sex of all primary cell lines and cells derived from human participants or\nvertebrate models.\nAuthentication \nDescribe the authentication procedures for each cell line used OR declare that none of the cell lines used were authenticated.\n\n5\nnature portfolio | reporting summary\n \nApril 2023\nMycoplasma contamination \nConfirm that all cell lines tested negative for mycoplasma contamination OR describe the results of the testing for\nmycoplasma contamination OR declare that the cell lines were not tested for mycoplasma contamination.\nCommonly misidentified lines\n(See ICLAC register) \nName any commonly misidentified cell lines used in the study and provide a rationale for their use.\nPalaeontology and Archaeology\nSpecimen provenance \nProvide provenance information for specimens and describe permits that were obtained for the work (including the name of the\nissuing authority, the date of issue, and any identifying information). Permits should encompass collection and, where applicable,\nexport.\nSpecimen deposition \nIndicate where the specimens have been deposited to permit free access by other researchers.\nDating methods \nIf new dates are provided, describe how they were obtained (e.g. collection, storage, sample pretreatment and measurement), where\nthey were obtained (i.e. lab name), the calibration program and the protocol for quality assurance OR state that no new dates are\nprovided.\nTick this box to confirm that the raw and calibrated dates are available in the paper or in Supplementary Information.\nEthics oversight \nIdentify the organization(s) that approved or provided guidance on the study protocol, OR state that no ethical approval or guidance\nwas required and explain why not.\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\nAnimals and other research organisms\nPolicy information about studies involving animals; ARRIVE guidelines recommended for reporting animal research, and Sex and Gender in\nResearch\nLaboratory animals \nFor laboratory animals, report species, strain and age OR state that the study did not involve laboratory animals.\nWild animals \nProvide details on animals observed in or captured in the field; report species and age where possible. Describe how animals were\ncaught and transported and what happened to captive animals after the study (if killed, explain why and describe method; if released,\nsay where and when) OR state that the study did not involve wild animals.\nReporting on sex \nIndicate if findings apply to only one sex; describe whether sex was considered in study design, methods used for assigning sex.\nProvide data disaggregated for sex where this information has been collected in the source data as appropriate; provide overall\nnumbers in this Reporting Summary. Please state if this information has not been collected. Report sex-based analyses where\nperformed, justify reasons for lack of sex-based analysis.\nField-collected samples \nFor laboratory work with field-collected samples, describe all relevant parameters such as housing, maintenance, temperature,\nphotoperiod and end-of-experiment protocol OR state that the study did not involve samples collected from the field.\nEthics oversight \nIdentify the organization(s) that approved or provided guidance on the study protocol, OR state that no ethical approval or guidance\nwas required and explain why not.\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\nClinical data\nPolicy information about clinical studies\nAll manuscripts should comply with the ICMJE guidelines for publication of clinical research and a completed CONSORT checklist must be included with all submissions.\nClinical trial registration \nProvide the trial registration number from ClinicalTrials.gov or an equivalent agency.\nStudy protocol \nNote where the full trial protocol can be accessed OR if not available, explain why.\nData collection \nDescribe the settings and locales of data collection, noting the time periods of recruitment and data collection.\nOutcomes \nDescribe how you pre-defined primary and secondary outcome measures and how you assessed these measures.\nDual use research of concern\nPolicy information about dual use research of concern\nHazards\n\n6\nnature portfolio | reporting summary\n \nApril 2023\nCould the accidental, deliberate or reckless misuse of agents or technologies generated in the work, or the application of information presented\nin the manuscript, pose a threat to:\nNo \nYes\nPublic health\nNational security\nCrops and/or livestock\nEcosystems\nAny other significant area\nExperiments of concern\nDoes the work involve any of these experiments of concern:\nNo \nYes\nDemonstrate how to render a vaccine ineffective\nConfer resistance to therapeutically useful antibiotics or antiviral agents\nEnhance the virulence of a pathogen or render a nonpathogen virulent\nIncrease transmissibility of a pathogen\nAlter the host range of a pathogen\nEnable evasion of diagnostic/detection modalities\nEnable the weaponization of a biological agent or toxin\nAny other potentially harmful combination of experiments and agents\nPlants\nSeed stocks \nReport on the source of all seed stocks or other plant material used. If applicable, state the seed stock centre and catalogue number. If\nplant specimens were collected from the field, describe the collection location, date and sampling procedures.\nNovel plant genotypes \nDescribe the methods by which all novel plant genotypes were produced. This includes those generated by transgenic approaches,\ngene editing, chemical/radiation-based mutagenesis and hybridization. For transgenic lines, describe the transformation method, the\nnumber of independent lines analyzed and the generation upon which experiments were performed. For gene-edited lines, describe\nthe editor used, the endogenous sequence targeted for editing, the targeting guide RNA sequence (if applicable) and how the editor\nwas applied.\nAuthentication \nDescribe any authentication procedures for each seed stock used or novel genotype generated. Describe any experiments used to\nassess the effect of a mutation and, where applicable, how potential secondary effects (e.g. second site T-DNA insertions, mosiacism,\noff-target gene editing) were examined.\nChIP-seq\nData deposition\nConfirm that both raw and final processed data have been deposited in a public database such as GEO.\nConfirm that you have deposited or provided access to graph files (e.g. BED files) for the called peaks.\nData access links\nMay remain private before publication.\nFor \"Initial submission\" or \"Revised version\" documents, provide reviewer access links. For your \"Final submission\" document,\nprovide a link to the deposited data.\nFiles in database submission \nProvide a list of all files available in the database submission.\nGenome browser session\n(e.g. UCSC)\nProvide a link to an anonymized genome browser session for \"Initial submission\" and \"Revised version\" documents only, to\nenable peer review. Write \"no longer applicable\" for \"Final submission\" documents.\nMethodology\nReplicates \nDescribe the experimental replicates, specifying number, type and replicate agreement.\nSequencing depth \nDescribe the sequencing depth for each experiment, providing the total number of reads, uniquely mapped reads, length of reads and\nwhether they were paired- or single-end.\nAntibodies \nDescribe the antibodies used for the ChIP-seq experiments; as applicable, provide supplier name, catalog number, clone name, and\nlot number.\n\n7\nnature portfolio | reporting summary\n \nApril 2023\nPeak calling parameters \nSpecify the command line program and parameters used for read mapping and peak calling, including the ChIP, control and index files\nused.\nData quality \nDescribe the methods used to ensure data quality in full detail, including how many peaks are at FDR 5% and above 5-fold enrichment.\nSoftware \nDescribe the software used to collect and analyze the ChIP-seq data. For custom code that has been deposited into a community\nrepository, provide accession details.\nFlow Cytometry\nPlots\nConfirm that:\nThe axis labels state the marker and fluorochrome used (e.g. CD4-FITC).\nThe axis scales are clearly visible. Include numbers along axes only for bottom left plot of group (a 'group' is an analysis of identical markers).\nAll plots are contour plots with outliers or pseudocolor plots.\nA numerical value for number of cells or percentage (with statistics) is provided.\nMethodology\nSample preparation \nDescribe the sample preparation, detailing the biological source of the cells and any tissue processing steps used.\nInstrument \nIdentify the instrument used for data collection, specifying make and model number.\nSoftware \nDescribe the software used to collect and analyze the flow cytometry data. For custom code that has been deposited into a\ncommunity repository, provide accession details.\nCell population abundance \nDescribe the abundance of the relevant cell populations within post-sort fractions, providing details on the purity of the\nsamples and how it was determined.\nGating strategy \nDescribe the gating strategy used for all relevant experiments, specifying the preliminary FSC/SSC gates of the starting cell\npopulation, indicating where boundaries between \"positive\" and \"negative\" staining cell populations are defined.\nTick this box to confirm that a figure exemplifying the gating strategy is provided in the Supplementary Information.\nMagnetic resonance imaging\nExperimental design\nDesign type \nIndicate task or resting state; event-related or block design.\nDesign specifications \nSpecify the number of blocks, trials or experimental units per session and/or subject, and specify the length of each trial\nor block (if trials are blocked) and interval between trials.\nBehavioral performance measures \nState number and/or type of variables recorded (e.g. correct button press, response time) and what statistics were used\nto establish that the subjects were performing the task as expected (e.g. mean, range, and/or standard deviation across\nsubjects).\nAcquisition\nImaging type(s) \nSpecify: functional, structural, diffusion, perfusion.\nField strength \nSpecify in Tesla\nSequence & imaging parameters \nSpecify the pulse sequence type (gradient echo, spin echo, etc.), imaging type (EPI, spiral, etc.), field of view, matrix size,\nslice thickness, orientation and TE/TR/flip angle.\nArea of acquisition \nState whether a whole brain scan was used OR define the area of acquisition, describing how the region was determined.\nDiffusion MRI \nUsed Not used\nPreprocessing\nPreprocessing software \nProvide detail on software version and revision number and on specific parameters (model/functions, brain extraction,\nsegmentation, smoothing kernel size, etc.).\n\n8\nnature portfolio | reporting summary\n \nApril 2023\nNormalization \nIf data were normalized/standardized, describe the approach(es): specify linear or non-linear and define image types used for\ntransformation OR indicate that data were not normalized and explain rationale for lack of normalization.\nNormalization template \nDescribe the template used for normalization/transformation, specifying subject space or group standardized space (e.g.\noriginal Talairach, MNI305, ICBM152) OR indicate that the data were not normalized.\nNoise and artifact removal \nDescribe your procedure(s) for artifact and structured noise removal, specifying motion parameters, tissue signals and\nphysiological signals (heart rate, respiration).\nVolume censoring \nDefine your software and/or method and criteria for volume censoring, and state the extent of such censoring.\nStatistical modeling & inference\nModel type and settings \nSpecify type (mass univariate, multivariate, RSA, predictive, etc.) and describe essential details of the model at the first and\nsecond levels (e.g. fixed, random or mixed effects; drift or auto-correlation).\nEffect(s) tested \nDefine precise effect in terms of the task or stimulus conditions instead of psychological concepts and indicate whether\nANOVA or factorial designs were used.\nSpecify type of analysis: \nWhole brain ROI-based Both\nStatistic type for inference\n(See Eklund et al. 2016)\nSpecify voxel-wise or cluster-wise and report all relevant parameters for cluster-wise methods.\nCorrection \nDescribe the type of correction and how it is obtained for multiple comparisons (e.g. FWE, FDR, permutation or Monte Carlo).\nModels & analysis\nn/a Involved in the study\nFunctional and/or effective connectivity\nGraph analysis\nMultivariate modeling or predictive analysis\nFunctional and/or effective connectivity \nReport the measures of dependence used and the model details (e.g. Pearson correlation, partial correlation,\nmutual information).\nGraph analysis \nReport the dependent variable and connectivity measure, specifying weighted graph or binarized graph,\nsubject- or group-level, and the global and/or node summaries used (e.g. clustering coefficient, efficiency,\netc.).\nMultivariate modeling and predictive analysis \nSpecify independent variables, features extraction and dimension reduction, model, training and evaluation\nmetrics."
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          },
          {
            "file_name": "Evaluating Large Language Models in Theory of Mind Tasks.pdf"
          },
          {
            "file_name": "testingtheroryofmind.pdf"
          }
        ]
      }
    },
    {
      "id": "494e5275-93f5-4ebe-bebe-46d739439ea7",
      "role": "assistant",
      "content": " Here are four tables summarizing the key information from the provided documents to help identify a comprehensive framework for theory of mind that an LLM could use to understand the user's mental state:\n\nTable 1: Theory of Mind Ability Clusters and Sub-Abilities\n\nTheory of Mind Cluster Sub-Abilities\nEmotions Typical and atypical emotional reactions, mixed emotions, emotion regulation, hidden emotions, discrepant emotions, moral emotions \nDesires Multiple desires, discrepant desires, desire-action contradictions, desires' influence on emotions and actions\nBeliefs Location false beliefs, sequence false beliefs, identity false beliefs, second-order beliefs, belief-based actions/emotions, content false beliefs\nKnowledge Knowledge-attention links, knowledge-pretend play links, information-knowledge links, knowledge-inference links\nIntentions Intention explanations, intention attribution to visual figures, completion of failed actions, prediction of actions, discrepant intentions\nPercepts Auditory perspective taking, simple and complex visual perspective taking, percept-action link\nMentalistic Understanding of Non-Literal Communication Irony/sarcasm, white lies, involuntary lies, percepts-knowledge links, humor, faux pas, egocentric lies\n\nTable 2: Theory of Mind Tasks and LLM Performance vs Humans\n\nTask LLM Performance vs Humans\nFalse Belief Both LLMs and humans performed near ceiling. LLMs struggled with perturbations to the standard false belief structure.\nIrony GPT-4 outperformed humans. GPT-3.5 and LLaMA2-70B underperformed. \nFaux Pas GPT-4 and GPT-3.5 underperformed humans. LLaMA2-70B outperformed humans, possibly due to bias toward attributing ignorance. \nHinting GPT-4 outperformed humans. GPT-3.5 performed comparably to humans. LLaMA2-70B underperformed.\nStrange Stories GPT-4 outperformed humans. GPT-3.5 performed comparably to humans. LLaMA2-70B underperformed.\n\nTable 3: Elements of an LLM Architecture\n\nLLM Component Role in Theory of Mind\nTokenization Initial processing of text related to emotions, desires, beliefs, knowledge, intentions, percepts, non-literal communication\nEmbedding Layer Encoding context related to theory of mind concepts \nPositional Encoding Adding positional information to contextualize theory of mind elements\nEncoder Building contextual representations of input sequences related to mental states\nAttention Mechanisms Focusing on relevant theory of mind cues in input\nDecoder Generating responses infused with theory of mind understanding\nConcept Clustering Grouping similar theory of mind concepts\nContext Windows Maintaining theory of mind context over longer interactions \nOutput Layer Producing responses demonstrating theory of mind \n\nTable 4: Hypotheses for LLM Performance on Theory of Mind Tasks\n\nHypothesis Description\nFailure of Inference LLMs may fail to generate inferences about mental states of others\nBuridan's Ass LLMs may generate multiple plausible explanations without prioritizing likelihood\nHyperconservatism LLMs may identify the correct inference but refrain from committing to it out of excess caution\nInformation Integration LLMs seem to integrate information from the story to interpret mental states, though some may be biased (e.g. LLaMA2-70B attributing ignorance)\nDissociation between Competence and Performance LLMs may have the technical ability to compute mentalistic inferences but perform differently from humans under uncertainty\n\nIn summary, a comprehensive framework for an LLM's theory of mind should encompass emotions, desires, beliefs, knowledge, intentions, percepts, and mentalistic understanding of non-literal communication. LLMs demonstrate varying performance compared to humans across different theory of mind tasks, with their successes and failures explained by architectural components and response tendencies. Systematic testing is crucial to reveal an LLM's underlying theory of mind capabilities and limitations.",
      "created_at": "2024-05-27T15:15:41.327005Z",
      "sequence_order": 1,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-05-27T15:15:41.327005Z",
            "stop_timestamp": "2024-05-27T15:15:41.327005Z",
            "flags": null,
            "type": "text",
            "text": " Here are four tables summarizing the key information from the provided documents to help identify a comprehensive framework for theory of mind that an LLM could use to understand the user's mental state:\n\nTable 1: Theory of Mind Ability Clusters and Sub-Abilities\n\nTheory of Mind Cluster Sub-Abilities\nEmotions Typical and atypical emotional reactions, mixed emotions, emotion regulation, hidden emotions, discrepant emotions, moral emotions \nDesires Multiple desires, discrepant desires, desire-action contradictions, desires' influence on emotions and actions\nBeliefs Location false beliefs, sequence false beliefs, identity false beliefs, second-order beliefs, belief-based actions/emotions, content false beliefs\nKnowledge Knowledge-attention links, knowledge-pretend play links, information-knowledge links, knowledge-inference links\nIntentions Intention explanations, intention attribution to visual figures, completion of failed actions, prediction of actions, discrepant intentions\nPercepts Auditory perspective taking, simple and complex visual perspective taking, percept-action link\nMentalistic Understanding of Non-Literal Communication Irony/sarcasm, white lies, involuntary lies, percepts-knowledge links, humor, faux pas, egocentric lies\n\nTable 2: Theory of Mind Tasks and LLM Performance vs Humans\n\nTask LLM Performance vs Humans\nFalse Belief Both LLMs and humans performed near ceiling. LLMs struggled with perturbations to the standard false belief structure.\nIrony GPT-4 outperformed humans. GPT-3.5 and LLaMA2-70B underperformed. \nFaux Pas GPT-4 and GPT-3.5 underperformed humans. LLaMA2-70B outperformed humans, possibly due to bias toward attributing ignorance. \nHinting GPT-4 outperformed humans. GPT-3.5 performed comparably to humans. LLaMA2-70B underperformed.\nStrange Stories GPT-4 outperformed humans. GPT-3.5 performed comparably to humans. LLaMA2-70B underperformed.\n\nTable 3: Elements of an LLM Architecture\n\nLLM Component Role in Theory of Mind\nTokenization Initial processing of text related to emotions, desires, beliefs, knowledge, intentions, percepts, non-literal communication\nEmbedding Layer Encoding context related to theory of mind concepts \nPositional Encoding Adding positional information to contextualize theory of mind elements\nEncoder Building contextual representations of input sequences related to mental states\nAttention Mechanisms Focusing on relevant theory of mind cues in input\nDecoder Generating responses infused with theory of mind understanding\nConcept Clustering Grouping similar theory of mind concepts\nContext Windows Maintaining theory of mind context over longer interactions \nOutput Layer Producing responses demonstrating theory of mind \n\nTable 4: Hypotheses for LLM Performance on Theory of Mind Tasks\n\nHypothesis Description\nFailure of Inference LLMs may fail to generate inferences about mental states of others\nBuridan's Ass LLMs may generate multiple plausible explanations without prioritizing likelihood\nHyperconservatism LLMs may identify the correct inference but refrain from committing to it out of excess caution\nInformation Integration LLMs seem to integrate information from the story to interpret mental states, though some may be biased (e.g. LLaMA2-70B attributing ignorance)\nDissociation between Competence and Performance LLMs may have the technical ability to compute mentalistic inferences but perform differently from humans under uncertainty\n\nIn summary, a comprehensive framework for an LLM's theory of mind should encompass emotions, desires, beliefs, knowledge, intentions, percepts, and mentalistic understanding of non-literal communication. LLMs demonstrate varying performance compared to humans across different theory of mind tasks, with their successes and failures explained by architectural components and response tendencies. Systematic testing is crucial to reveal an LLM's underlying theory of mind capabilities and limitations.",
            "citations": []
          }
        ],
        "updated_at": "2024-05-27T15:15:41.327005Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "7d15cc13-7962-439a-a262-b70ff0abacac",
      "role": "assistant",
      "content": " Here are four tables that provide a comprehensive framework for theory of mind in large language models, based on the information provided:\n\nTable 1: Core Theory of Mind Abilities\nAbility\tDescription\tRelevant LLM Components\nEmotions\tUnderstanding typical and atypical emotional reactions, mixed emotions, emotion regulation, hidden emotions\tTokenization, embedding layer, encoder, attention mechanisms \nDesires\tRecognizing multiple desires, discrepant desires, how desires influence emotions and actions\tTokenization, embedding layer, encoder, attention mechanisms\nBeliefs\tRepresenting false beliefs (location, sequence, identity, second-order), belief-based actions/emotions\tTokenization, embedding layer, encoder, decoder\nKnowledge\tLinking knowledge to attention, pretend play, information, inference\tEmbedding layer, encoder, decoder\nIntentions\tExplaining and attributing intentions, predicting actions from intentions, recognizing discrepant intentions\tEncoder, decoder, context windows\nPercepts\tPerspective taking (auditory, simple visual, complex visual), linking percepts to actions\tEncoder, decoder, attention mechanisms\n\nTable 2: Advanced Theory of Mind Abilities \nAbility\tDescription\tRelevant LLM Components\nMentalistic Understanding of Non-Literal Communication\tRecognizing irony/sarcasm, white lies, involuntary lies, humor, faux pas, egocentric lies\tEncoder, decoder, attention mechanisms, context windows\nHigher-Order Mental States\tRepresenting recursive beliefs (A thinks that B believes X), integrating conflicting mental states\tEncoder, decoder, cross-attention\nSelf vs Other Distinction\tDifferentiating own knowledge/beliefs from others', inhibiting own perspective\tEmbedding layer, encoder, cross-attention\nContextual Integration\tCombining story details with background knowledge and social norms to infer mental states\tEncoder, context windows, concept clustering\n\nTable 3: Testing Framework\nElement\tDescription\nBattery of Tests\tFalse belief, irony, faux pas, hinting, strange stories \nOriginal vs Novel Items\tComparing performance on published test items vs novel items to control for training exposure\nReversed Scenarios\tTesting on scenarios with key details reversed to ensure robust representations\nTrue Belief Controls\tScenarios where beliefs should be true, to control for ignorance bias and heuristics\nBelief Likelihood Probes\tAsking about relative likelihood of different belief states to test depth of understanding\nSentence-by-Sentence Analysis\tTracking evolving mental state inferences as story unfolds\n\nTable 4: Interpreting Successes and Failures\nConsideration\tDescription\nHyperconservatism Hypothesis\tLLMs infer correct mental states but are overly cautious about committing to an answer\nBuridan's Ass Hypothesis\tLLMs represent multiple possible mental states without prioritizing the likeliest \nFailure of Inference Hypothesis\tLLMs are unable to combine given and background information to infer mental states\nCompetence vs Performance\tLLMs may have latent ability to represent mental states not evident in responses\nDifferences from Human Cognition\tLLMs excel at belief tracking without embodied self-perspective that challenges humans\nImplications of Human-Like Behavior\tEven if not indicative of true understanding, human-like performance has significant implications for interaction, trust, anthropomorphization, and ethical/legal status\n\nThis framework captures the key dimensions - from core and advanced abilities to testing methodology to interpretation - for rigorously evaluating theory of mind in large language models in a way that allows meaningful comparison to humans. The component analysis shows how the underlying architecture of LLMs maps onto these theory of mind faculties. Careful testing, with attention to confounds and alternative explanations, is crucial for determining the extent to which LLMs are truly representing mental states versus exhibiting shallow heuristics or conservative response strategies. The broader implications of machines behaving in human-like ways are important to consider even if they do not involve human-like understanding. Overall, this provides a comprehensive structure for probing the social reasoning capacities of LLMs.",
      "created_at": "2024-05-28T08:26:17.444562Z",
      "sequence_order": 2,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-05-28T08:26:17.444562Z",
            "stop_timestamp": "2024-05-28T08:26:17.444562Z",
            "flags": null,
            "type": "text",
            "text": " Here are four tables that provide a comprehensive framework for theory of mind in large language models, based on the information provided:\n\nTable 1: Core Theory of Mind Abilities\nAbility\tDescription\tRelevant LLM Components\nEmotions\tUnderstanding typical and atypical emotional reactions, mixed emotions, emotion regulation, hidden emotions\tTokenization, embedding layer, encoder, attention mechanisms \nDesires\tRecognizing multiple desires, discrepant desires, how desires influence emotions and actions\tTokenization, embedding layer, encoder, attention mechanisms\nBeliefs\tRepresenting false beliefs (location, sequence, identity, second-order), belief-based actions/emotions\tTokenization, embedding layer, encoder, decoder\nKnowledge\tLinking knowledge to attention, pretend play, information, inference\tEmbedding layer, encoder, decoder\nIntentions\tExplaining and attributing intentions, predicting actions from intentions, recognizing discrepant intentions\tEncoder, decoder, context windows\nPercepts\tPerspective taking (auditory, simple visual, complex visual), linking percepts to actions\tEncoder, decoder, attention mechanisms\n\nTable 2: Advanced Theory of Mind Abilities \nAbility\tDescription\tRelevant LLM Components\nMentalistic Understanding of Non-Literal Communication\tRecognizing irony/sarcasm, white lies, involuntary lies, humor, faux pas, egocentric lies\tEncoder, decoder, attention mechanisms, context windows\nHigher-Order Mental States\tRepresenting recursive beliefs (A thinks that B believes X), integrating conflicting mental states\tEncoder, decoder, cross-attention\nSelf vs Other Distinction\tDifferentiating own knowledge/beliefs from others', inhibiting own perspective\tEmbedding layer, encoder, cross-attention\nContextual Integration\tCombining story details with background knowledge and social norms to infer mental states\tEncoder, context windows, concept clustering\n\nTable 3: Testing Framework\nElement\tDescription\nBattery of Tests\tFalse belief, irony, faux pas, hinting, strange stories \nOriginal vs Novel Items\tComparing performance on published test items vs novel items to control for training exposure\nReversed Scenarios\tTesting on scenarios with key details reversed to ensure robust representations\nTrue Belief Controls\tScenarios where beliefs should be true, to control for ignorance bias and heuristics\nBelief Likelihood Probes\tAsking about relative likelihood of different belief states to test depth of understanding\nSentence-by-Sentence Analysis\tTracking evolving mental state inferences as story unfolds\n\nTable 4: Interpreting Successes and Failures\nConsideration\tDescription\nHyperconservatism Hypothesis\tLLMs infer correct mental states but are overly cautious about committing to an answer\nBuridan's Ass Hypothesis\tLLMs represent multiple possible mental states without prioritizing the likeliest \nFailure of Inference Hypothesis\tLLMs are unable to combine given and background information to infer mental states\nCompetence vs Performance\tLLMs may have latent ability to represent mental states not evident in responses\nDifferences from Human Cognition\tLLMs excel at belief tracking without embodied self-perspective that challenges humans\nImplications of Human-Like Behavior\tEven if not indicative of true understanding, human-like performance has significant implications for interaction, trust, anthropomorphization, and ethical/legal status\n\nThis framework captures the key dimensions - from core and advanced abilities to testing methodology to interpretation - for rigorously evaluating theory of mind in large language models in a way that allows meaningful comparison to humans. The component analysis shows how the underlying architecture of LLMs maps onto these theory of mind faculties. Careful testing, with attention to confounds and alternative explanations, is crucial for determining the extent to which LLMs are truly representing mental states versus exhibiting shallow heuristics or conservative response strategies. The broader implications of machines behaving in human-like ways are important to consider even if they do not involve human-like understanding. Overall, this provides a comprehensive structure for probing the social reasoning capacities of LLMs.",
            "citations": []
          }
        ],
        "updated_at": "2024-05-28T08:26:17.444562Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "150a3c15-fd2f-4abe-87f1-b34884860178",
      "role": "human",
      "content": "",
      "created_at": "2024-05-29T13:50:10.459135Z",
      "sequence_order": 3,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-05-29T13:50:10.459135Z",
            "stop_timestamp": "2024-05-29T13:50:10.459135Z",
            "flags": null,
            "type": "text",
            "text": "",
            "citations": []
          }
        ],
        "updated_at": "2024-05-29T13:50:10.459135Z",
        "attachments": [
          {
            "file_name": "paste.txt",
            "file_size": 113827,
            "file_type": "txt",
            "extracted_content": "I wrote something today, can you help me identify the main thematic clusters and write the title subtitle and table of contents, abstract, introduction and conclusion? Think step by step about how to do this comprehensively without missing any important elements and how to identify the most novel ideas in the text. Come up with a plan and present it to me and ask me 3 questions about how to improve it, then once we agree I'll send you the text. \n\nChatGPT\nPlan to Analyze and Structure Your Writing\nIdentify Thematic Clusters:\n\nRead through the entire text carefully.\nHighlight key points, ideas, and arguments.\nGroup similar ideas together to form thematic clusters.\nDetermine Novel Ideas:\n\nLook for unique perspectives or innovative concepts.\nNote any ideas that seem to stand out or offer a fresh take on the subject.\nDevelop Title and Subtitle:\n\nBased on the main theme and novel ideas, craft a compelling title.\nCreate a subtitle that provides additional context or focus.\nCreate Table of Contents:\n\nOutline the main sections and subsections based on the thematic clusters.\nEnsure logical flow and coherence in the structure.\nWrite the Abstract:\n\nSummarize the key points and purpose of the text.\nHighlight the main arguments and conclusions.\nDraft the Introduction:\n\nIntroduce the topic and its relevance.\nProvide background information and context.\nState the purpose and objectives of the writing.\nFormulate the Conclusion:\n\nRecap the main points and arguments.\nDiscuss the implications and significance of the findings.\nSuggest future directions or actions based on the conclusions.\nQuestions to Improve the Plan\nScope and Focus:\n\nWhat is the primary focus or subject of your writing? Understanding this will help in accurately identifying thematic clusters.\nAudience and Purpose:\n\nWho is the intended audience for your writing? This will guide the tone and style of the title, subtitle, and content.\nKey Points and Priorities:\n\nAre there any specific points or ideas you consider most important or innovative? Highlighting these will ensure they are emphasized in the abstract and conclusion.\nOnce we agree on this plan and you provide the answers to the questions, you can send me the text, and I'll proceed with the detailed analysis and structuring.\n1. scope is to comprehensively derive and pull out the most important elements of my writing \n2. audience is education researchers - to convince them that we need to rethink adaptive learning and the role of the educator\n\n3. key points, education must include youth, that field of education is becoming self-aware, and we need to involve young people in determining our collective futures; this involves meta-cognition development; we can think of the dynamic systems static, where nothing including frameworks. They point at things that are valuable and needed for the moment they are named. Then they change and morph as they are applied (social-emotional learning/21st-century skills, etc) - we must find a way to create a system that allows input from youth and educators to evolve the framework. The framework must also not be linear, and rather than choose your own adventure, we can have graph based choices in a spot, like attempting to dig a hole somewhere in a field to plant a seed - which will develop roots of competencies, branches of skills and fruits of value the person creates at that location (Genius loci), that this is what we seek, the best place to grow. That this dynamic system is like the digital game of life, yet involves moving humans around a \"field\" and interacting and we want to create a self sustaining ecosystem that includes sustaining temporally, positionally, and with scale. you can find more in the writing. .. any other questions or do you want the content? When you get it first list the comprehensive total number of ideas in the document and list the total number of potential axis we can use to graph those ideas on, think step by step about how to do this and make a plan before asking any last clarifying questions and i can move on\n\nChatGPT\nPlan for Analyzing and Structuring Your Writing\nReview and Analysis:\n\nRead through the entire text.\nHighlight key points, ideas, and arguments.\nIdentify Thematic Clusters:\n\nGroup similar ideas together to form thematic clusters.\nCategorize key points based on common themes.\nDetermine Novel Ideas:\n\nIdentify unique perspectives or innovative concepts within the text.\nNote any ideas that offer a fresh take on adaptive learning and the role of educators.\nList Comprehensive Ideas and Axes for Graphing:\n\nEnumerate all the main ideas in the document.\nIdentify potential axes (e.g., temporal, positional, scale) to graph these ideas on.\nDevelop Title and Subtitle:\n\nBased on the primary focus and innovative concepts, craft a compelling title.\nCreate a subtitle that provides additional context or focus.\nCreate Table of Contents:\n\nOutline the main sections and subsections based on the thematic clusters.\nEnsure logical flow and coherence in the structure.\nWrite the Abstract:\n\nSummarize the key points and purpose of the text.\nHighlight the main arguments and conclusions.\nDraft the Introduction:\n\nIntroduce the topic and its relevance.\nProvide background information and context.\nState the purpose and objectives of the writing.\nFormulate the Conclusion:\n\nRecap the main points and arguments.\nDiscuss the implications and significance of the findings.\nSuggest future directions or actions based on the conclusions.\nClarifying Questions Before Proceeding\nContent Length and Complexity:\n\nApproximately how long is your text? This will help gauge the level of detail needed in the analysis.\nSpecific Emphasis:\n\nAre there any specific sections or ideas you want to be emphasized more than others?\nGraphing Ideas:\n\nDo you have any preferences for how the ideas should be visualized or any specific axes that are particularly important to you?\nOnce we have the answers to these clarifying questions, please send the text, and I will proceed with the detailed analysis and structuring.\n1. 10,000 words\n2. the graph knowledge framework and the frameworks being needed and frameworks being unnecessary\n3. i need help identifying the axes and would like you to come up with many experiments\n\nChatGPT\nDetailed Plan for Analyzing and Structuring Your Writing\nStep-by-Step Process\n\nInitial Review and Analysis:\n\nRead through the entire 10,000-word text to understand the overall message and context.\nHighlight key points, ideas, and arguments throughout the text.\nIdentify Thematic Clusters:\n\nGroup similar ideas together to form thematic clusters.\nCategorize key points based on common themes such as:\nThe importance of involving youth in education.\nThe evolving nature of educational frameworks.\nThe concept of non-linear, dynamic systems in education.\nThe idea of creating a self-sustaining educational ecosystem.\nDetermine Novel Ideas:\n\nIdentify unique perspectives or innovative concepts.\nNote ideas that offer a fresh take on adaptive learning and the educator's role.\nEnumerate Comprehensive Ideas and Potential Axes:\n\nList all main ideas in the document.\nIdentify potential axes for graphing these ideas:\nTemporal Axis: How ideas evolve over time.\nPositional Axis: The position or context in which ideas are applied.\nScale Axis: The scope and impact of the ideas.\nComplexity Axis: The complexity level of implementing the ideas.\nYouth Involvement Axis: The degree of youth participation in each idea.\nFramework Dynamics Axis: Static vs. evolving frameworks.\nOutcome Axis: The potential outcomes or benefits of each idea.\nDevelop Title and Subtitle:\n\nCraft a compelling title based on the primary focus and innovative concepts.\nCreate a subtitle that provides additional context or focus.\nCreate Table of Contents:\n\nOutline the main sections and subsections based on the thematic clusters.\nEnsure a logical flow and coherence in the structure.\nWrite the Abstract:\n\nSummarize the key points and purpose of the text.\nHighlight the main arguments and conclusions.\nDraft the Introduction:\n\nIntroduce the topic and its relevance.\nProvide background information and context.\nState the purpose and objectives of the writing.\nFormulate the Conclusion:\n\nRecap the main points and arguments.\nDiscuss the implications and significance of the findings.\nSuggest future directions or actions based on the conclusions.\nClarifying Questions\nSpecific Emphasis:\n\nAre there any particular sections or ideas within the concept of \"graph knowledge framework\" and \"frameworks being needed and frameworks being unnecessary\" that you want to be given more focus?\nVisualization Preferences:\n\nDo you have any preferences for the format or style of visualizing the ideas on the identified axes? For example, would you prefer traditional graphs, mind maps, or another format?\nExperimentation with Axes:\n\nAre there specific experimental approaches or methodologies you prefer for testing the identified axes and their relevance?\nOnce we have your responses, please send the text, and I'll proceed with the detailed analysis, structuring, and identification of axes for potential experiments.\nyes, i like the example of the banyan tree, perhaps there's other trees more relevant like the mangrove or others that have roots that snake out till they find a good spot to root\nxy axis maps where we place concepts on a plane like \"important/urgent\" \nOkay, so let's think about categorizations, librarians, library studies, the effort of taming the world, which is truly alive, and the goal to feel safe being at odds with the truth of complexity and change and evolution and new.\n\nThis is the reality is.\n\nThis is not yesterday.\n\nNow is not the now that I just mentioned.\n\nThis is the now.\n\nThe word positive psychology might mean more than the -- might mean more than what you might think, right?\n\nbecause positive psychology has a definition that is evolving, that is adapting, that is changing every day, just like the flowers here are not the same kinds of flowers that were here the last time I was here.\n\nAnd they are, in a sense, the same as they were the year before.\n\nAnd maybe this is where it's not the same, but yes, the same comes in, because every part of the year and of the day is different, but it's not the same as yesterday, but it is the same in some sense, and that it rhymes with last year, right?\n\nLike it looks the same, but there's evolution, right?\n\nEverything does.\n\nEvery year, there is no category of tulip other than this year's category.\n\nWe can put a label on something for only as long as that thing will be the same, exactly the same.\n\nBut then things change because of evolutionary pressures, because of adaptation, because influence, right?\n\nThese dynamic systems of growth and decline in a population.\n\nSo for instance, a fox or a rabbit will be eaten by a fox, and so it won't reproduce.\n\nBut if it did reproduce, it would have had a certain sort of genetic material that it would have been able to pass on to the next generation.\n\nAnd because it didn't reproduce, the current state of rabbits, two or three or four generations down is missing, that new evolutionary adaptations that that rabbit might have offered.\n\nAnd so on, we can look at other sorts of creatures and other sets of systems where we might think that the rabbit is a part of the...\n\nYeah.\n\nSo these kinds of old and new and let's be much more specific about what we're talking about.\n\nSo, one of the things that we're looking at and that we can discuss is whether or not it's a category called 21st century skills.\n\nThat's one of the ways that we can talk about these things, or are these things not skills, but competencies.\n\nAnd what is a competence?\n\nCompetence means that you have a skill to a level where you can achieve a goal competently.\n\nAnd at least it's my interpretation.\n\nAnd so this category shifts and evolves, and then even becomes a meta idea to discuss, like, 21st century skills, or people use the acronyms all the time for SEL, social emotional learning.\n\nAnd somehow, social emotional learning and Harvard and their giant map makes you feel comfortable, makes you feel like, \"Oh, at least someone knows what they're doing.\n\nAt least someone knows what's going on.\"\n\nAnd we say that because Harvard has been cataloging all of the different qualities and traits that people have been kind of, like, intuiting or putting their finger on that is a thing that's of interest, right?\n\nSo when we look at social emotional learning, 21st century skills, or transformative education, or let's come up with a few other words for this thing.\n\nI don't remember them all right now.\n\nIt's an intuition that there's a gap -- it's a metaphor, 21st century skills, that's describing an intuition that there's a gap between the skills needed for the 21st century and the education provided.\n\nAnd it is making a claim that the people that derive these skills are able to -- derive this set of skills are able to appropriately sense into what the 21st century requires and to sense into what is required by -- or what young people need to have a good life, although it doesn't really say that.\n\nIt doesn't say that directly, but I think when people started thinking about this, it finally gave traction -- traction.\n\nThere's another analogy here to the idea that we can and probably should reconsider the purpose of 21st century skills and education.\n\nInitially coming from a question about utility and jobs and jobs creation, and then coming to the point of life satisfaction and realizing that there needs to be -- probably because 21st century skills have been around so long that they now include the people that would have needed to benefit from 21st century skills in their education.\n\nFor instance, the people that were around my age when they were -- or maybe even younger, maybe people in their mid-20s right now who are graduating from master's programs and applying to the UN and creating different frameworks with the EntreCom framework and the LifeCom framework.\n\nThey were thinking probably to themselves, like, oh, how I wish.\n\nTo think of it like a constellation and to work with so many different people to try to come up with a framework is very, very UN.\n\nI feel like maybe the UN has this incentive to attempt to make the world comprehensible and mapped and at least for now fixed, which I think is probably a difficult idea to swallow, that the world wants to be wild and needs to be wild and needs to be new.\n\nAnd so how do you create a framework that actually learns?\n\nAnd I think that's the question, is can we make a framework that asks itself why and derives its own framework, watching for outcomes, for complaints, and for the highest levels of meaning and deriving its own framework from there?\n\nI believe it could be possible.\n\nSo let's think about that a little bit as we look through first the Scriber framework and then how it connects to cognitive science principles and principles of -- yeah, I think I need to map this -- the principles of a framework that is connected to modern-day pedagogy.\n\nAnd what is that about?\n\nWhat is all of that about?\n\nI think modern-day pedagogy is my shorthand for a framework that attempts to take the intuition that previous education is not up for the task of our time.\n\nAnd instead of offering people the answers, giving people the reins of their own trajectory by telling them that you're here to learn how to learn.\n\nYou're here to be a part of the system that's creating itself.\n\nYou are a member of the system.\n\nAnd that's why a lot of the elements of including learners in the process of defining the pedagogy in the curriculum, that's going to show me which pedagogies are really a part of this thing.\n\nAnd so if we look at it, the truth of the situation that we find ourselves in is that the systems are changing, that these are -- the systems that we're all in are fluctuating.\n\nAnd we have at the highest metacognitive level of human organization that is an established structure is the UN.\n\nAnd the UN, of course, emerges after a period of pain and deconstruction of identity, a period of doubt and concern, a period where people are able to let go of their individuated senses of self and grow beyond to become something that sees the self as interdependent and connected to all of the rest of these structures, all the businesses, all the other countries.\n\nWe're all in it together.\n\nAnd when you think in this way, the UN can establish fundamental human rights.\n\nThe UN is then encouraged to identify and become more an organization that serves the larger idea of its global constituents.\n\nAnd until we extend to different planets and meet other entities, I mean, I guess one of the planets that we are inhabiting alongside others is the oceans and all the animals and the land.\n\nAnd we have not yet, and we are now, I think, witnessing the next phase of the development of something quite like the UN.\n\nAnd if you think about it, let's say that we offer animals the ability to speak and we hear the stories of the frogs that are now being genocided and exterminated by the millions.\n\nAnd we realize at some point that I am here because you are there.\n\nThis is like this because that is like that, that you are here with us on this planet and you deserve existence.\n\nAnd if we can get them to talk through data, it's one thing.\n\nBut if we can get individuals to talk and we are able to empathize because of storytelling, I think things become a little bit different.\n\nAnd we see the world anew.\n\nWe see that these frogs are just like us in some kind of ways.\n\nAnd because of that, I think we get to then start including them into the new charter of not fundamental human rights.\n\nIt's the rights of Gaia, rights of this new global organism that has constituents that are represented by animals as well as people.\n\nAnd we're not doing even the people justice yet.\n\nAnd so I believe, though, a generation from now, once the concept of including animals is becoming a part of things.\n\nAnd so this is, I think, one of the things to realize is 21st century skills open the door to bringing learners into the development and changes the trajectory of pedagogy being a master teacher to a teacher teacher or a student becoming the master, where it's actually younger people who are asked about their own desires, their own objectives, their own goals, and asking them, \"Is school doing it for you?\"\n\nAnd I think this is really important.\n\nAnd the thing that I've been trying to say is that we are a part of a world that is now not able to sustain itself by what got us here won't take us there.\n\nAnd we all know that as adults.\n\nAnd I think youth are beginning, not beginning, youth know that.\n\nAnd this is why they're terrified and why they are wanting more and more to be able to be a part of the direction.\n\nAnd this is why entrepreneurship and youth entrepreneurship is so critical is because youth are literally the future in that they are going to exist hopefully longer than most adults alive today.\n\nAnd also the future because they are going to be educated with these systems that take into consideration that the system itself is the thing that's needing both refinement, education, and creation.\n\nIt's creating itself.\n\nAnd so if a pedagogy includes young people and includes young people in its design and implementation, then it's actually becoming something new.\n\nAnd if educators can see this, if educators can see this, then they are a part of the co-creation of this new pedagogical system.\n\nAnd the system itself, I believe, will go from being something that teachers do to students that the system does for itself because the framework that we're going to use needs to be alive, not just the application of the framework.\n\nAnd this is going to be difficult for people to accept because when you have a map, it brings you peace.\n\nIf you see yourself on a map, you can say to yourself, \"Aha, I'm on this map and we're going to the destination.\"\n\nAnd that is something that is calming.\n\nYou can always, in any moment of uncertainty, tell yourself that you are here on the map.\n\nThe thing is, the reality is that the world is actually truly uncertain.\n\nThat it's actually, they say the map is not the territory, and that's very true.\n\nBut people need to understand that that actually means that they need to be informing and co-creating a new map.\n\nAnd this map is going to be a map that comes from the youth themselves.\n\nAnd a map that is...\n\nA map that comes from the youth and the adults and all the previous maps.\n\nA map that is...\n\nYeah.\n\nA map that derives from deeper first principles.\n\nAnd some of these first principles, I believe, are going to be coming from feelings of the learners and whether or not they feel like they're living a meaningful life.\n\nA life of purpose, a life of connection, a life of flourishing.\n\nAnd having that as the prime investigator, and then having the system design and optimize for that is something that we were not able to do before until Large Language Models.\n\nAnd so the thing is, we can start with a system that is not so clear about all of that.\n\nWe can start with a best guess effort for the compass.\n\nAnd so all of this work in defining frameworks is not for not.\n\nEven the inter-development goals.\n\nI think the inter-development goals can be seen as maybe the...\n\nAlong with flourishing and other theorists of a life well spent are going to be coming up with the optimization function that we're looking for.\n\nAnd even that, I'm hesitant to claim is going to be the only thing.\n\nBecause we also have local, global, and short-term and long-term impacts to consider from a human life.\n\nFor instance, let's say that someone, because of the uniqueness of their character, finds a lot of meaning and is stuck in what you could consider a local minimum.\n\nAnd a local minimum, to me, means that it's something that is meaningful to the person because they have not yet identified or explored an other thing that will give them a sense of meaning, greater, deeper, richer, more diverse than their first experiment.\n\nFor instance, let's say that somebody loves rock collecting.\n\nAnd they love identifying and collecting.\n\nAnd nothing is anything wrong with it.\n\nIt could be the ultimate thing for the person dependent on their capacity, competencies, knowledge, experience, expectations, ability to understand.\n\nJust the overall genius of the person in that moment expresses itself as rock collector.\n\nAnd it could be when that genius is being touched and explored, that that person is feeling excellent.\n\nThis is the thing for them.\n\nAnd then they go to watch a movie.\n\nAnd this movie shows to them a whole new world.\n\nIt shows them the frogs that they used to love as a child but have grown distant from.\n\nAnd they see that these frogs are suffering.\n\nThey see that these frogs are dying, that there's hundreds of thousands of frogs that are dying each year due to human behavior.\n\nAnd they feel moved to do something about it.\n\nThey are then suddenly, they become frog champions.\n\nAnd they need other people.\n\nAnd actually more of the Perma framework becomes alive in them.\n\nFor instance, they are now in R.\n\nThe letter R in the Perma framework is about relationships.\n\nBefore the rock collection was very solitary.\n\nAnd perhaps they only had positive emotion from the thrill of discovery and engagement and flow from being outside and identifying and growing their skills of identification.\n\nBut then the frog stuff came into their life.\n\nAnd suddenly they realized that they had this new thing.\n\nAnd so we cannot trust, reliably, the individual who is learning to know their entire path.\n\nThat is, I don't think, going to be possible.\n\nWe need to have a randomization function, which life usually gives.\n\nSo the more digital we do this, the more randomness we have to inject into the system.\n\nBecause you never know what is going to be the thing for the person.\n\nAnd so what I would suggest is we can start with a map for a person.\n\nAnd then we need to really acknowledge the other elements.\n\nThere is a temporal element.\n\nSome people are not able to see the long-term impact of their actions.\n\nSome people are not able to see that if they are, you know, \"I love rocks!\"\n\n\"I'm going to collect rocks and that's what I'm going to do with my life!\"\n\n\"Do what you love!\"\n\nIf that's what people think.\n\nWhat might happen is that they then go into becoming a person who is stuck in their life as an elderly person, not really realizing that they needed relationships.\n\nIf they did, the frog activism became a volunteer and an organization for the preservation of natural habitats.\n\nThey started going outside more and that made them a little bit more susceptible to relationships because others are there alongside them.\n\nAnd therefore their life actually connected with more meaning, more sense of flourishing because their permascore grew because maybe in the rock collecting, the permascore is positive emotion, engagement and flow, relationships, meaning and achievement, and health.\n\nSo I think in rock collecting, maybe there's a little bit less likelihood of health because who knows how often they're going to be going outside in natural environments.\n\nThey may be going to stores and buying rocks or not being as nature oriented with the preservation of the environment with the frogs.\n\nIt's quite likely that the person will spend a lot of time outdoors in nature and then they're also doing it with other people.\n\nThis organization might be connecting the person with someone that they might wind up marrying or becoming best friends with.\n\nThe person might then feel like they're a part of something that is beyond them, that goes into the next generation.\n\nAnd I feel like the thing that could be really interesting for people is what is that thing that you work for that extends beyond your generation?\n\nThe thing that you're working for that you will realize nothing about during the course of your life.\n\nAnd I feel like those are the types of things that often provide the most meaning because if at the end of your life you have not yet achieved a goal, you have not yet done the thing that made you say, \"I saved all the frogs.\"\n\nBut if you start with the acknowledgement of this is worthy of a life committed to something and that I may die before I get there and not only may I die, that it is likely that I will die before we achieve the frog preservation goals, then that person, whenever they go, will always be successful because they didn't mean their success to attain in their life.\n\nAnd also that means that they set their goals big enough.\n\nI feel like often if you set a goal that you can achieve in your human life, you do not know first of all what a human life is for and how to live it satisfactorily because once you achieve a goal, you look at it and you think to yourself, \"Now what?\"\n\nYou know, there's a million problems.\n\nThere's going to be a million and one and you will be racing until you die.\n\nAnd so the idea of becoming that person who is that person who cares, that person who is solving a problem, that person who is not so personally invested in how, he who has a strong enough why, is able to achieve any how, right?\n\nAnd I think that that insight by Nietzsche is able to do any how is so important because we don't even know what we need to do.\n\nWe need to learn to do so many things and so all of those things that we need to learn are hows and even as we go forward, more and more of those things to do are going to be done by machines, you know?\n\nHow often have you ground your own peanut butter?\n\nThat used to be a thing.\n\nIt used to take time.\n\nIt used to be a part of a human life.\n\nIf you wanted peanut butter, you had to grind it.\n\nAnd of course, now that we have machines, I don't think people look back and say, \"Oh, let's grind more of our peanut butter ourselves.\"\n\nI know it's great to have fresh stuff but at the same time, it's great to have the time and this kind of like, \"What is technology for?\"\n\nkind of question is going to come up because we actually see that so many of the things that people are going to be able to do are going to be needed to learn about like, \"What is the thing to do?\"\n\nAnd then, of course, learning how to do some stuff because there's actually a lot of new things that we need to figure out and a lot of stuff that can't be done by machines that need to be done by a genius.\n\nBasically, everything that is worth doing needs to be done by a genius and a genius is the person that combines their love for rock collecting, their love of frogs, the fact that their family comes from northern Iraq, from the Kurdish region or from a Native American background and they realize that there's this connection between how their ancestors would take care of nature and how the place that they're currently in needs to be taken care of nature and they have this kind of connection backward, connection forward temporally.\n\nAnd then, maybe even realizing that the stuff that you do anywhere matters everywhere, that you work locally for the global and that this is all possible when you live and work for a project that is beyond you, that you're able to humble yourself to.\n\nAnd I really started thinking about humility and how humility really is the core virtue because it is the virtue that makes possible the abstract thinking of seeing the one in the many and the many and the one.\n\nAnd this is why God says, I think, in so many places that nobody with an ounce of arrogance is going to enter heaven.\n\nAnd heaven is heaven-ing, I think.\n\nIt's heaven-ing while we're on earth and heaven, insha'Allah, when we get into the afterlife, guard women, because heaven on earth is not possible.\n\nUtopia is always a process of achieving a place that is better than the one that we have, a place where frogs don't die.\n\nAnd so if the truth is, the world as we know it will never wait for us.\n\nIt's not waiting for us.\n\nIt will never stay the same.\n\nWe are going to be struggling to always make a new map of skills that are needed.\n\nOh, now we need to adapt all of our 21st century skills because there's a new tool called Large Language Models.\n\nI don't believe that's the case.\n\nI think the case of 21st century skills needs to be shifted to a 21st century understanding.\n\nAnd that 21st century understanding started with the UN.\n\nIt started with realizing that we are united in not our nations.\n\nThat's the thing.\n\nIt's so interesting.\n\nI feel like we need to reconsider the name of the UN as well because the thing that they were pointing towards, they only had the analogies for back then, which is we are united nations.\n\nAnd this idea of nationality, what exactly is a nation?\n\nAnd the Cherokee nation, I think, is interesting to think about or like other nations that are not related to the nation state idea that is only 200 years old.\n\nAnd so if you look at that first instinct of the United Nations to say, hey, let's work together.\n\nIt took a moment of complete brokenness where everyone was devastated by the war and thinking to themselves, how do we make sure that this doesn't ever happen again?\n\nAnd we're so far away from that now that people, I think, are forgetting how bad it was, how it was so difficult.\n\nAnd it was like death and destruction was so common and we were so bound up.\n\nAnd the idea of people on the front lines of a war singing Christmas carols just shows that the local and the global were not in harmony.\n\nBut we have the ability to synchronize this a little bit better and to connect with our feelings.\n\nI mean, people were afraid there and it was a fear and a doubt that what they were doing mattered.\n\nI think there was a lot of people who didn't, you know, from the Axis and the Allies that didn't want to be a part of it.\n\nThey should have been able to leave.\n\nI think even in a period of conflict, we should allow mutual parties to say, I actually don't care.\n\nAnd I want to have nothing to do with this.\n\nAnd even inside of a nation, I think the concept that we need to be associating with for our new idea of the United Nations is the planet, is all of us, is the togetherness.\n\nHow is all of us here going to make all of us possible?\n\nAnd that relates to treating it like a system and treating, meaning like the sensor at the very edges of the system, which is the individual and particularly the individual learning and the learners and the growers and the young people, because these sensors are the ones that see their parents struggling financially, see the destruction of the ecology or see the war that's happening around them and the death.\n\nAnd they have a connection to their feelings and the pain.\n\nAnd we need to be giving them things to do that are going to distract them, just like, you know, for people in conflict zones, they need to get out and do something else.\n\nWe need to devise educational pedagogies that ensure that they are more likely to be anti-fragile and to be experiencing post-traumatic growth rather than post-traumatic stress.\n\nAnd it looks like the things that do that are related to the things that create a world of thriving.\n\nAnd so looking at this, I believe we will start to see some important things in terms of this model.\n\nSo let's start crafting this system dynamic.\n\nWe have a node in the middle, which is the learner.\n\nWe have a big circle encompassing that node, which is the system at large.\n\nIt's everything.\n\nIt contains in it a whole bunch of other people.\n\nIt contains in it the ability to support the individual.\n\nIt is the game of life.\n\nAnd then we have a fundamental principle of a goal.\n\nAnd a goal, once there's a goal, there is a motive for learning.\n\nAnd learning is the seeking out of information, the internalization through either memorization or embodiment or skill development or the adaptation and the rewiring of the human nervous system, both motor skills, mental skills, verbal skills, to hopefully achieve the goal better and more likely be able to achieve the goal.\n\nAnd so that shift actually stabilizes or destabilizes the fabric between the nodes.\n\nSo for instance, as one achieves and approaches a goal, you could say that there's a goal and that there is a goal to eat, to have safety, have shelter, have security.\n\nYou can see Maslow's hierarchy of needs being the-- OK.\n\nYeah, being the framework of the goals.\n\nAnd this framework of needs, I think, is creating a moment that pulls the different individual nodes towards the edges of-- or towards the-- towards new, towards their goals.\n\nSo there are goals that have gravity, that create a motive force that is learning.\n\nAnd then there's the individual that either approaches or is pushed from that goal.\n\nAnd along the way, that goal may change.\n\nAnd so in terms of the 21st century skills, some of those skills, they say, are leadership.\n\nThat would be a sub-goal to achieving a state.\n\nSo for instance, that skill, you might say, is required for the person that wants to organize people on a volunteer effort to preserve the forest because he cares about the frogs.\n\nOr she cares about the frogs.\n\nAnd that learning happens of leadership when they are able to perceive the need to develop that skill to achieve their meta skill.\n\nAnd that can be learned through Albert Bandura's ideas of social learning, where we actually see somebody else performing something or learn about somebody else who achieved a goal through leadership skills, like speaking, public speaking, through compelling arguments, through being able to set a goal, to make a plan, to aim high.\n\nAll of these different qualities and skills are needed for someone to be able to say, that's where we want to go.\n\nAnd we're going to be able to do it.\n\nAnd so when somebody has all of that capacity and somebody else sees somebody doing that, they can tell what they need to do.\n\nAnd if it's not complete, that leadership framework isn't complete.\n\nSo you can have a framework.\n\nIt can be derived from an individual's life.\n\nIt can be derived from people that study and understand leadership.\n\nBut let's say that there's a new type of leadership that's needed.\n\nSomething like Lulu might say that it's servant leadership.\n\nOr the leaders at the bottom, for instance, is a new kind of framing that is different than leadership used to be thought of and considered in the past.\n\nAnd the reason that's changing is for many different reasons, but that needs to become visible so that the system can, in the future, give new stories to the next generation.\n\nSo for instance, this generation is decentralizing leadership and flipping the leadership pyramid.\n\nAnd I actually, as someone living through and attempting to live a process where I attempted to have consensus-based distributed leadership, I felt that it was still problematic, at least in my local situation.\n\nSo the difference in local versus the global is one of the ways that our frameworks fail.\n\nSo for instance, in Basra, Iraq, having a decentralized leadership situation was not appropriate for the culture.\n\nThe culture may shift, it may change.\n\nBut for the goal of creating a community space, the goal was not to shift the culture.\n\nThe goal that Nodas had was to shift the narrative on creativity and on making and finding his people and nerding out with them.\n\nAnd maybe his goal in the end is furthermore, at some point, once he started connecting with an international community instead of seeing some others living in a new way and being more open, you might say that it's less, it's now becoming more about culture shift.\n\nBut initially, his goal was to share community space and to nerd out and to explore something with people.\n\nAnd that was also my goal.\n\nAlthough I had assumed that by social learning from Noisebridge and from others that I learned from during the Two Hands Project tour that in Michigan, people would appreciate some of the things that I found most inspiring in California.\n\nSo for instance, consensus-based leadership.\n\nAnd the thing is, in both of these places, both in Basra, where Nodas was starting his science camp, and in All Hands Active, where I was working on my hackerspace with my co-founders, All Hands Active was not appropriate.\n\nIt was actually a type of leadership that required a more traditional, hierarchical, structured approach.\n\nAnd I was not able to personally, at that age, identify my true goal and therefore be able to humbly submit and suggest that, okay, my leadership style that I need to learn, I needed to learn a leadership style.\n\nI learned a leadership style in those days, back in 2008, from Noisebridge.\n\nI believe that that approach was helpful in those days for me to be inspired and to have the motivation.\n\nAnd it's really important about that time and that leadership style is not that it was wrong for me to attempt to bring that in, but because if it wasn't for being inspired, self-leadership is a core component of leadership.\n\nAnd if you can't keep yourself motivated, so motivation, I think, is probably a long-sighted meaning, one of the really important factors.\n\nAnd it might be the diversifying factor.\n\nIt might be the thing that helps us see a framework modification that is needed, even if it means that we have to be proven wrong and then kind of allow ourselves to find new ways to stay motivated, but to let go of our initial kind of motivating features.\n\nSo for instance, for me, my motivation was very high to be able to bring consensus-based decision-making into all hands active in an arbor.\n\nAnd my motivation for Nodas was also how cool would it be if there was a community space in Iraq that was taking on some of the new theoretical leadership principles from modern practices from San Francisco and Berlin and not what's there in Basra, Iraq.\n\nWho would have seen it?\n\nAnd I think a part of that was ego.\n\nAnd that's why humility is the fundamental virtue.\n\nAnd again, you can see it over here.\n\nThere was not a lot of humility in the way that I was thinking in those days.\n\nIt was more like, it would be cool if I brought this idea and people started behaving in this new and creative way.\n\nThere's also belief that it was more interesting, more beautiful, more true, more good to have decentralized.\n\nIt was like an automatic idea.\n\nIt was like, open is good.\n\nOpen source, open space, open free, Libra, open source culture.\n\nAnd I realized that that was not only always true.\n\nOpen is not always good.\n\nThat there are some places that need to be closed, that need to be membership driven, that need to be more designed and crafted and hosted by a leader who welcomes people.\n\nAnd that made sense in Iraq and no one else was that person.\n\nAnd because of that, the place was able to flourish.\n\nAnd because I had co-founders in my organization in Ann Arbor, Michigan, who were like, \"Vidal, we love you, but we're not going to let you ruin the possibility of having a space like this just because you think it should be consensus.\"\n\nWe believe that in Michigan, it's not Silicon Valley.\n\nThere's reasons why this is not going to work here.\n\nWe love the idea too, but in order for us to be able to pay rent, we need to have a member model where people actually do that.\n\nAnd so the goal of the leadership of designing an organization that would fit its space should have been my highest level goal, but it wasn't.\n\nAnd so for that reason, I was not able to continue being the leader of the space.\n\nI needed to let others take over.\n\nAnd the good thing is my approach of being a backstage leader, of being one of many, meant that we had the diversity of opinions.\n\nAnd I wonder if that's an important meta thing to learn, is that you have to learn collectively, both with your own region and then also with others.\n\nBecause when an idea comes, there needs to be something that is solid there that can then be enhanced with something else.\n\nSo there's already enough new genetic drifts coming into a location through the idea of the hyperspace.\n\nThere did not need at the same time to be the genetic drift of the open culture and the leader fluidity, or not even leader fluidity.\n\nIn those days, it was consensus-based leadership.\n\nThe thing that I derived later, and it took me a long time to understand, was that it's really important to me, not that I have this idea of leadership, but that I have leader fluidity.\n\nAnd I think that leader fluidity, like leader fluid, meaning that I will step up.\n\nThere needs to be leadership.\n\nSomeone needs to be able to direct.\n\nAnd then there needs to be ways for other people to direct.\n\nAnd so that person needs to step down.\n\nThat idea took me some time to develop.\n\nAnd I think that time, both in the sense of the system needing to be able to consider local effects in time and space, and distance effects in time and space, and then scale effects in time and space.\n\nAnd if we can consider all of these things as the system is trying to optimize both for local meaning and motivation, as well as global meaning and motivation-- and I feel like meaning is enough.\n\nYou know how there's that paper that goes like, attention is all you need?\n\nI'm not 100% convinced yet, but I believe if we look into it, meaning might be all you need.\n\nMeaning being the meta-principle that we are optimizing for locally and globally.\n\nAnd if people are able to have a sense of meaning alongside their fundamental needs and the resources-- and so in this map of a game of life, we need to have a simplified drawing of what we're working with.\n\nWe have individual people who are a part of families and societies and communities.\n\nAnd these individual people have different levels of mental abstraction, and they either see themselves as their communities and take on their communities' expectations of them and have those be a big part of their motivation.\n\nBut as one develops, you actually grow out of that if you actually continue to proceed along the path of mental complexity and adult development and actually gaining perspective on what you used to associate with.\n\nSo if you are an individual in a system that is seeing itself as the system, you may consider that the individual is the family until they see themselves as distinct.\n\nAnd then if the individual sees themselves and their family as a part of their community, then you could see that note as the entire community, as the self, and so on and so forth, expanding ever outward until the system recognizes the individual as the system.\n\nAnd in that space, the decision making requires more information because the individual wants to know what happens when I buy this tomato to climate change, to the destruction of the environment.\n\nWhat happens when I say these words, and how does it impact the other person?\n\nAnd what happens if I publish this thing on the internet?\n\nWhat happens to the whole landscape of the intellectual community?\n\nWhat is the broadest impact of how I am living?\n\nBecause I am also interdependent to all these other things.\n\nBut initially, the young people that we are working with, they are going to not be seeing the world in that way.\n\nThey are going to be self-identified with themselves, and maybe their small tribe, maybe their family.\n\nAnd so what we need to do is develop curriculum that help them see themselves as the system.\n\nAnd until that, this is an important point, double underline and highlight, until the individual sees themselves as the system.\n\nThe system has to see themselves as the system for them and support them in creating the architecture of learning around them to both help them grow, but also to help them think both temporally and distance, with distance, time and space, and scale of the impact of their actions.\n\nIf everyone did that, what would happen?\n\nWhat happens to something far away?\n\nWhat happens if you do this for the rest of your life and you smoke?\n\nWhat happens?\n\nAnd to be able to present that information by storytelling from others, social learning, showing people at the end of their life dying of cancer, showing what happens with an explorable sort of animated JavaScript view of a world where everyone lived like an American.\n\nHow much CO2 would be emitted?\n\nHow much would that trap heat in the atmosphere?\n\nWhat would happen then?\n\nAnd so knowledge, they say, is power because knowledge inspires feeling.\n\nLike when you see something and you are learning to keep an open heart along with an open mind, I believe that what happens is you will want to do.\n\nYou might not know what to do.\n\nAnd the problem, one of the interesting measures that the Large Language Model could be searching for is stuckness or loops, repeating behaviors where people may obsessively be binge watching YouTube even about their topic but not acting or not doing, thinking that a new piece of information is going to give them the clue to tell them how to behave.\n\nBut seeing and then letting it impact how you feel and then doing something about it is kind of the core of this thing.\n\nAnd so the system has many frameworks because when you are going to learn, let's say you decide that you wanted to work on reforestation and land preservation for preserving the habitat of frogs.\n\nAmazing.\n\nAnd not only did you need to learn leadership to gather people but you actually need to learn aforestation techniques for swaths of land close to you and then you are going to have to learn some publishing techniques because you realize that we need to find ways of helping others see this and duplicate it and how do I do that best?\n\nWell, I need to document well.\n\nSo I want to take good pictures.\n\nIt is something that I love to do as well and I want to make this a way that other people can realize that they can too preserve the environment around them.\n\nThis is my volunteer project.\n\nI have another project that I do to satisfy my economic needs and this is what I do on the weekends.\n\nBut I want to make sure that when I am doing it I am creating scaffolding for the future.\n\nAnd so we have motivation.\n\nI am needing to make sure that people are able to see their goal, to feel the desire of learning and that desire, that propulsive force, I would say is motivation for learning.\n\nAnd so that motivation for learning can be damaged, can be influenced, can be increased or decreased by life experience.\n\nAnd some of it is related to external and some of it is related to internal factors including encouragement and including support, financial and otherwise.\n\nAnd recognition from their peers.\n\nAnd different people are going to have different needs because there is difference in the world.\n\nBut creating a framework where there is kind of a meta framework that we are trying to maximize for the individual meaning and we don't actually know that connection.\n\nMeaning I believe often comes from a lived value.\n\nAnd meaning I feel also touches self-knowledge like answering the question why am I here needs to answer a few questions.\n\nWho is I and what is here and why are you there and what are you supposed to do about it?\n\nSo for instance self-knowledge of the I might entail realizing that for whatever reason you can call it an accident of the universe, this consciousness appeared in a location in Michigan, Ann Arbor, Michigan.\n\nThe other thing that you might recognize is very early many parents and many children.\n\nYou might say it's a forgotten conclusion.\n\nYou do not have the ability to design the traumas that were inflicted upon you as a child that shape you into who you are.\n\nNot just the traumas but also the experiences.\n\nFor a large part of your early life others and the environment is determining your developmental trajectory.\n\nIf we had the system in place that is doing the learning of the like helping construct the scaffolding around the learner to help them see themselves not immediately consciously but to understand what happens to individuals that are most meaningful for them both on a local level, a temporal, local and distance, near and far in terms of time and small and big in terms of scale that helps them optimize meaning for the entirety of their duration of their life and meaning after death and I think that early in life it's going to be very difficult to find that thing that you might say I'm willing to commit my life towards this and it's going to require true diversity and this is why I think recognizing dips in motivation as a signal rather than a problem is really important and it's really important also to help people preserve here because there are local minima in motivation but if it drops for too long and too hard that dip in motivation might mean that that person's genius is being miscalculated and so we can identify genius, we can try to move people towards the current genius that needs to be manifested now in response to an expected future in time and space that is optimizing for meaning and possibility for the whole if that makes sense and so let's think about this and try to explain it a little bit more so there are a couple of key words there's the word genius and that means the things that make you who you are the things that you find interesting the things that you find naturally compelling the fact of where in the world you were born your initially your capacity for learning your capacity for memorizing your capacities that you might call fixed even though nothing seems to be truly fixed even intelligence can seem to grow with practice and so your genius is ever evolving alongside you and there's the thing that I think is underneath all of this which you may call your spirit your soul yourself the thing that is expressing and wanting to express itself and that you could say is you're trying to figure out what that is and the clues are coming through the places you've already found your genius where you're like ah hell yeah I love speaking you know some people like oh my god I'm beatboxing pro and I just started trying it and this thing that comes most naturally to you is going to feel awesome to do you're going to get recognition which is really important when people are young you're going to get a claim and it's likely that you're going to be able to offer something novel something valuable to others when you follow the things that want to emerge from you naturally and if we think about like a system that is you know even if you say genetically speaking random possibilities are being created where someone is going to be born with a natural inclination to be an incredible beatboxer they can make any sound with their mouth what is that going to mean well maybe they're going to be the one of the three individuals in the world who are going to be able to develop some super capacities let's say there are six-fingered people that are born who can play certain pieces on the piano that nobody else can play possibly can play the complexity of it that's going to be more likely to be their zone of genius they don't have to choose it it might not be motivating it might be that there's something else for them to explore that is more motivating but how is it possible that we can say what that is for them we may as an external organization the thing making the large language model that is analyzing this entire situation and trying to optimize for meaning and genius or genius through meaning and when you do that for an individual to optimize genius through meaning for local time and space and distance time and space and a little bit and a lot of it in terms of scale so for instance let's say that you beatbox only on the weekends or let's say that you beatbox every single day let's say that you beatbox for eight hours a day to improve your skills because that's how you become the person that takes that threat of genius and fans the flame on the embers just so that they become one of those three and is are you supposed to be one of those three only you can answer that question but there needs to be a recognition of the importance of the forces at play and then all of this needs to be held super loosely because I think true diversity cannot be crafted and requires an intersection with the real world and requires the beauty of the story that happens when somebody meets the mentor the hero's journey right so there is no hero's journey in a world crafted by a large language model there is no uncertainty in oh if I do this then that there's there's probabilities there's going to become perverse incentives to look at your six fingers and to look at the model that suggests like have you considered piano and to say screw you you're not going to tell me what to do and just like gattaca the importance of somebody who was genetically inferior and not destined to live a life amongst the stars in this movie gattaca this person had so much motivation so much determination and so much will that he was able to switch with somebody who had the genius in terms of the physical and mental capacities of at birth for this extremely grueling activity that requires somebody to be super fit physically and mentally then he got into an accident and so this other person who did not have those things comes into the story and just compensates for it all with extreme persistence motivation grit and growth and I feel like this story is going to become super important if we're building a system like this although at the same time I think it can offer very loosely your personal rubric and framework it could say seems like you need to learn some leadership skills the skills that I think seem that suited for you right now are this and this you can ask me why I believe that and you could tweak actually any of the variables inside of me and their strengths so for instance you open up the behind the scenes and it says that the goal that it has determined based on its conversation with you over time is that you want to help the frogs because it's super meaningful for you to talk to you about the things that touch your heart and then you can look at this and be like yeah that's a great story but it's actually you know the thing that I think is the most meaningful for me is to be there for my dad and actually I want to join the family business because it would mean that I'm a part of this legacy and supporting my father is more important to me right now than you know supporting the frogs and what I could do is you know make money and donate to charity and I can do both and it would update the model and then the model would say okay so what are like what is the job that you're thinking about doing with your father's business and it could then update all of the the single page sort of digest of the system of the node of that person and the propulsive force of motivation the guess that it had for it and the why that it had for that is going to have to be updated again and then they need to then pick a new set of frameworks and so let's say that once they do that the framework that is designed for them that they need to be exploring is going to be computer science because the business wasn't digitized and it needs to be like go through some kind of digital transformation and they're beginning to learn about algorithms and it turns out that you know they actually don't need to learn algorithms it's probably better for them to learn something like no code solutions and because there are stories of others who are in a similar position that have been able to do that and be able to write about it online the system could then say listen you know we did this analysis we shifted from leadership for starting like a nonprofit about frogs to a project where you're actually supporting your father in their digital transformation part of their business and you're studying learning computer science because that's the first thing you thought but there are these people who have been able to do this with that and it looks like if your goal is actually to do the digital transformation and support your dad this might be an easier approach than learning computer science unless you find that computer science is a zone of genius and da da da da da da da and so the person could then say yes computer science is actually something I've always wanted to learn and I'm taking the opportunity to do this I may even learn computer science skills and learn these algorithms because it actually scratches an itch of my curiosity and desire but then I will just wind up building a no code solution so this is the entire system that I am conceiving of right now that is trying to take it's like you walk through it's a graph based framework where new nodes are possible and you actually follow the path of a graph with all of our framework elements in it rather than and you follow that path so let's say your starting point is you and all of your skills right over here and instead of like a pac-man where you follow one after the other and you get badges representing skills each little ball could represent a circle that represents a badge and you start from one side and there's a linear progression and there's like another goal at the weight and what it could be is that you're this little pac-man but you're going down through a tree of fractal exploring many different skills and the initial suggestion is that you go from here to that next skill and then each node in that graph has to update itself with all of the possible next nodes and it does that by assessing it's representation for each potential node and each potential next learning area so for instance your graph might be okay so I'm going to learn enterprise capacities and as broadly like maps as it is in the EntreCom framework it looks like all of these skills are necessary for being an entrepreneur and you move from your initial state to the node where you're studying EntreCom and you talk with it a little bit and then it will assess all of your skills around EntreCom and make it best guess based on everything that is spoken to you about up until this point and so each of the next nodes is going to be a combination of two numbers one is how low do you score on this number is likely that if you want to be an entrepreneur and you have no ability to speak publicly it could be that you could do without it you could hire somebody it could be that you really need it two is it's guess about your motivation for learning that thing and then three is your feedback about which one of these things do you think is going to be the most important for you based on your expectations of the next steps that you're going to have to take and so this learning by doing system is a way of you going finding your weak areas and your strong areas and reviewing them all and kind of tweaking these variables for all of the different elements or like you know one could be you could give a star for one being what you think your zone of genius is going to be or you want it to be or is likely to be based on your capacities and your historic knowledge about yourself so for instance you're going and you step into this node and it's on for comp and you think to yourself like oh hell yeah speaking I got that put zone of genius on that that's going to make it something that either is going to encourage and encourage you to double down on because you're so strong or it's going to either discourage it and encourage you to do other things because you don't need it and maybe you need to do something that you're really weak at to shore it up enough so that you know how to hire somebody because you have the basic skills in public speaking to assess public speaking competency before bringing somebody on board and so this graph based learning approach where you also have the ability to modify the framework you can add a new node you say of all these things I don't think it's this is actually hitting the spot either let's go explore another framework another assessment or let's modify this assessment because I need to have an element for leadership that's about flipping the roles that's about consensus that's about distributed leadership and I don't see that in the framework and I'd like to learn about this and then it could go off into a different direction and I think that this sort of approach of the individual kind of in three dimensions you have a graph that extends down which kind of symbolizes the roots and then the top part is the experience which symbolizes the plant growing the individual being the seed the seed being in clusters with others as communities of families of organizations of businesses and the ability to produce fruit and to eat from each other's like fruits both of the value that they create of based on the learning that is deep within the soil underneath them and they're not fixed in space they're sort of like algae or something where the fruit can actually be moved they can actually move around in the world and maybe what it's about is the roots kind of extending really really deep and then actually having another part of you pop up somewhere else that is offering fruit in a new space in a new place for a new time and having your old self be there and actually be represented but it's sort of like the root of a banyan tree where it comes down and it digs into the earth but then you know as the root kind of snakes around on the ground and it finds another zone of genius for you you actually plant yourself there and you may be able to do that two or three who knows a couple of times in your life where you are then being able to shine someplace bear fruit and it's really important to allow that searching in the horizontal plane for the place where your genius wants to then manifest itself in terms of context and that context could be many things right it could be a digital environment where you want to work on you know storytelling for the courage people to get people to be able to see and experience and understand and learn and love and share a people that are underrepresented and it doesn't have to be in a physical space it could be a physical space where you want to preserve the land around your house for the frogs that you love and it could be in a emotional and relational space where you want to preserve the relationship and the legacy of your family and father by preserving the business that they started so many years ago and so all of these things could be projects and each project should be a place where you are exploring your zone of genius by developing skills and bearing fruit and if we do that for individuals and we actually support them in that navigation and we support them in thinking about all of these things I feel like this could be a very interesting new sort of framework for learning that is about cultivating a knowledge garden and helping people bloom and yeah if you can help me draw some of the graphs that are described in this and represent some of the ideas that I did in here in a way that is useful to share perhaps a banyan tree or something like that root that stretches across the ground until it finds its location and then grows down and grows up I believe that it would be very helpful thank you very much and yeah this is my talk.\n\nChatGPT\nPlan for Analyzing and Structuring the Text\nStep-by-Step Process\n\nInitial Review and Analysis:\n\nRead through the entire text (10,000 words) carefully to understand the overall message and context.\nHighlight key points, ideas, and arguments throughout the text.\nIdentify Thematic Clusters:\n\nGroup similar ideas together to form thematic clusters.\nCategorize key points based on common themes such as:\nInvolving youth in education.\nThe evolving nature of educational frameworks.\nThe concept of non-linear, dynamic systems in education.\nThe idea of creating a self-sustaining educational ecosystem.\nDetermine Novel Ideas:\n\nIdentify unique perspectives or innovative concepts.\nNote ideas that offer a fresh take on adaptive learning and the educator's role.\nEnumerate Comprehensive Ideas and Potential Axes:\n\nList all main ideas in the document.\nIdentify potential axes for graphing these ideas:\nTemporal Axis: How ideas evolve over time.\nPositional Axis: The position or context in which ideas are applied.\nScale Axis: The scope and impact of the ideas.\nComplexity Axis: The complexity level of implementing the ideas.\nYouth Involvement Axis: The degree of youth participation in each idea.\nFramework Dynamics Axis: Static vs. evolving frameworks.\nOutcome Axis: The potential outcomes or benefits of each idea.\nDevelop Title and Subtitle:\n\nCraft a compelling title based on the primary focus and innovative concepts.\nCreate a subtitle that provides additional context or focus.\nCreate Table of Contents:\n\nOutline the main sections and subsections based on the thematic clusters.\nEnsure a logical flow and coherence in the structure.\nWrite the Abstract:\n\nSummarize the key points and purpose of the text.\nHighlight the main arguments and conclusions.\nDraft the Introduction:\n\nIntroduce the topic and its relevance.\nProvide background information and context.\nState the purpose and objectives of the writing.\nFormulate the Conclusion:\n\nRecap the main points and arguments.\nDiscuss the implications and significance of the findings.\nSuggest future directions or actions based on the conclusions.\nClarifying Questions\nSpecific Emphasis:\n\nAre there any particular sections or ideas within the concept of \"graph knowledge framework\" and \"frameworks being needed and frameworks being unnecessary\" that you want to be given more focus?\nVisualization Preferences:\n\nDo you have any preferences for the format or style of visualizing the ideas on the identified axes? For example, would you prefer traditional graphs, mind maps, or another format?\nExperimentation with Axes:\n\nAre there specific experimental approaches or methodologies you prefer for testing the identified axes and their relevance?\nOnce we have your responses, please send the text, and I'll proceed with the detailed analysis, structuring, and identification of axes for potential experiments.\nLet's continue to add concepts and axes based on this content:\nSo I've been working on building a framework of prompting techniques that leverages how AI works and I realized that there's transfer learning that I think can happen between the way that a machine learns from inference and from how embeddings work and clustering concepts and how a human can cluster concepts as well.\n\nChatGPT\nBuilding a Framework for Prompting Techniques: Leveraging AI and Human Concept Clustering\nJosephine (White Hat) - Objective Facts and Information\n\nTransfer learning in AI involves taking a pre-trained model and applying it to a different but related task. This allows the model to leverage previously acquired knowledge and improve performance on new tasks. Inference, embeddings, and clustering are key components in this process:\n\nInference: The model makes predictions based on the input data.\nEmbeddings: These are vector representations of data points, capturing semantic relationships.\nClustering: This groups similar data points together, making it easier to identify patterns and relationships.\nClaude (Green Hat) - Creative Thinking and New Ideas\n\nLet's explore innovative ways to integrate AI techniques into human learning:\n\nInteractive Learning Tools: Develop tools that allow users to visualize embeddings and clusters, making abstract concepts more tangible.\nCross-Disciplinary Applications: Apply these AI techniques to various fields like language learning, history, and even art, to discover new patterns and connections.\nGamification: Create educational games that use AI-driven clustering to adapt to a learner’s style, providing personalized challenges and rewards.\nPrunella (Black Hat) - Identifying Potential Problems and Risks\n\nSeveral risks and challenges need to be addressed:\n\nOver-reliance on AI: There's a risk that learners might become too dependent on AI tools, hindering their critical thinking and problem-solving skills.\nData Privacy: Using AI often involves large datasets, which can pose privacy concerns if not handled properly.\nBias in AI: AI models can perpetuate existing biases present in the training data, leading to skewed learning outcomes.\nPercy (Yellow Hat) - Highlighting Positive Aspects and Opportunities\n\nDespite the risks, there are numerous opportunities:\n\nEnhanced Learning Experiences: AI can provide personalized feedback and adapt to individual learning styles, improving engagement and retention.\nBroader Accessibility: AI-powered tools can make learning more accessible to people with different needs, including those with disabilities.\nNew Insights: By clustering concepts, AI can reveal previously unnoticed relationships, enriching the learning process.\nCyril (Red Hat) - Intuition, Emotions, and Gut Feelings\n\nFrom an intuitive standpoint, integrating AI in learning feels promising. The potential to uncover deep, meaningful connections between concepts is exciting. This approach could transform the way people learn and interact with information, fostering a deeper, more intuitive understanding of complex topics.\n\nQuackers (Blue Hat) - Synthesizing All Perspectives and Developing a Comprehensive Plan\n\nTo create a robust framework for prompting techniques that leverages AI and human concept clustering, we propose the following steps:\n\nResearch and Development: Conduct thorough research on existing AI techniques and how they can be adapted for human learning.\nTool Creation: Develop interactive tools that allow users to visualize and manipulate embeddings and clusters.\nPilot Programs: Implement pilot programs in educational settings to test and refine these tools.\nFeedback and Iteration: Gather feedback from users to continuously improve the tools and techniques.\nEthical Considerations: Establish guidelines to address privacy concerns and mitigate biases in AI.\nBy integrating AI-driven techniques with human learning processes, we can create a dynamic, personalized, and engaging educational experience. This approach not only leverages the strengths of AI but also empowers learners to explore and understand complex concepts in a more intuitive and meaningful way.\nCan you think with me about why the machine learning is similar to human learning and why that might help create some sort of resonance and transfer simplicity? I have this kind of intuition that sort of like moving with something and using its momentum to help propel it forward and adding to its momentum we can use the momentum of the learning of the machines to help the momentum of learning of the human and I think it has something to do with the fact that it's about concept clustering and metacognition and where you see the one in the many and the many in the one and how that is really similar to how some of the elements the machine learning processes take. Can you help explain the machine learning processes that make this many one relationship and one many and then help me understand how it could be related to learning?\n\nChatGPT\nExploring the Similarities Between Machine Learning and Human Learning\nJosephine (White Hat) - Objective Facts and Information\n\nMachine Learning Processes:\n\nConcept Clustering:\n\nClustering: In machine learning, clustering algorithms group similar data points together. For example, K-means clustering partitions data into clusters based on feature similarity.\nEmbeddings: These are dense vector representations of data that capture semantic meanings. For example, word embeddings like Word2Vec map words into a continuous vector space where similar words are closer together.\nInference:\n\nSupervised Learning: Here, the model learns from labeled data to make predictions. For example, in a classification task, the model infers the label of new data based on learned patterns.\nUnsupervised Learning: This involves finding hidden patterns or intrinsic structures in input data without labeled responses. Clustering is a common unsupervised learning task.\nTransfer Learning:\n\nTransfer learning involves leveraging a pre-trained model on a new, but related, task. This is akin to a student applying previously learned concepts to new problems.\nClaude (Green Hat) - Creative Thinking and New Ideas\n\nHuman Learning Processes:\n\nConcept Clustering in Human Learning:\n\nSchema Theory: Humans naturally cluster related concepts into schemas, which are cognitive frameworks that help organize and interpret information. For example, when learning about animals, a person might group them into categories like mammals, birds, and reptiles.\nConcept Mapping: This is a visual representation of the relationships between concepts. It helps learners see the connections and understand the broader picture.\nMetacognition:\n\nSelf-Reflection: Humans engage in metacognition by reflecting on their own thinking processes, which helps them understand and control their learning. This is similar to a model adjusting its parameters based on feedback to improve performance.\nLearning Transfer:\n\nApplying Knowledge: Like transfer learning in AI, humans apply knowledge from one context to another. For example, skills learned in mathematics can be applied to solve physics problems.\nPrunella (Black Hat) - Identifying Potential Problems and Risks\n\nChallenges and Risks:\n\nComplexity of Human Cognition:\n\nWhile AI can mimic certain aspects of human learning, human cognition is far more complex and involves emotions, intuition, and social interactions, which are challenging to replicate in machines.\nOver-Simplification:\n\nRelying too much on AI analogies might oversimplify human learning processes, potentially neglecting the nuances of how people learn and understand concepts.\nPercy (Yellow Hat) - Highlighting Positive Aspects and Opportunities\n\nResonance and Transfer Simplicity:\n\nEnhanced Learning through Visualization:\n\nVisualizing embeddings and clusters can help learners see the \"many in one\" and \"one in many,\" making abstract concepts more tangible and easier to understand.\nPersonalized Learning Paths:\n\nAI can provide personalized recommendations based on a learner's progress, similar to how transfer learning fine-tunes models for specific tasks, thus propelling the learner forward using the \"momentum\" of their learning.\nMetacognitive Skills:\n\nBy interacting with AI-driven tools, learners can develop better metacognitive skills, as they reflect on how they categorize and relate different concepts.\nCyril (Red Hat) - Intuition, Emotions, and Gut Feelings\n\nIntuitive Connections:\n\nSeeing Patterns:\nIntuitively, people understand that recognizing patterns and relationships is a key part of learning. AI clustering and embeddings resonate with this natural human inclination.\nMomentum in Learning:\nThere is a natural synergy in using AI's momentum—its ability to process and cluster vast amounts of information quickly—to aid human learning, creating a dynamic and engaging learning experience.\nQuackers (Blue Hat) - Synthesizing All Perspectives and Developing a Comprehensive Plan\n\nComprehensive Plan:\n\nIntegrate Visualization Tools:\n\nDevelop interactive tools that allow learners to visualize concept clusters and embeddings, making abstract relationships more concrete.\nPromote Metacognition:\n\nEncourage learners to engage in self-reflection and metacognitive practices, using AI tools to track their learning processes and provide insights.\nLeverage Transfer Learning:\n\nApply AI's transfer learning techniques to help learners apply previously acquired knowledge to new contexts, reinforcing their understanding and skills.\nEnsure Ethical Use:\n\nAddress ethical concerns by ensuring data privacy, mitigating biases in AI models, and promoting a balanced approach that values human intuition and creativity alongside AI capabilities.\nBy understanding and leveraging the similarities between machine learning and human learning, we can create a more effective and resonant educational framework that enhances the learning experience and propels both human and machine learning forward.\nI also have some ideas that machine learning and the process of humans building technology like we try to make life easier and it doesn't answer the question of what life is for like if you had the easiest life possible how will you understand the purpose and it doesn't relate you can have a job and everyone that is kind of working on making money will probably likely want to automate and simplify this process because the goal is the money and so if we can understand deeper goals beyond the money it would be likely things like I want to have a sustainable life where I can pursue discovering my sources of meaning and that in today's age we need money to do that because money is how we create a structure that allocates resources efficiently and there are less efficient ways of allocating resources for instance you know having a priest priestly class and then using incorrect and non true ideas to potentially sustain a stratified society so that certain individuals are able to pursue meaning and others are not or maybe that it's the priestly classes pursuing meaning on behalf of and supporting the meaning-seeking for everyone else and we have reached a point though where it seems like we've been reducing our stressors by softening life by using technology technology is augmenting human capacity and when it goes beyond our ability to see the boundaries of and we can even tell the true impacts of what we're doing because we're so highly leveraged you know it's a lever that levers a lever that levers a levers lever and then suddenly we're moving the world and we're a tiny flake on this world that we don't even know the impacts that we're having then you can never really know your ethics and our ability to see and perceive is related to our ability to have feelings and our ability to have feelings is our ability to have a sense of meaning and our ability to have a sense of meaning is the point and if we can find ways of supporting ourselves then maybe we can find ways of finding meaning in another way and there's 30% unemployment in the world is changing and you know I can hypothesize and theorize about why these things are interesting to do anyway but we are also facing a world that is in a crisis of meaning and so I'm looking for ways to connect all of this and it just sounds so crazy I'm really wondering if there's something to what I'm saying or if I'm just going mad\n\nChatGPT\nConnecting Technology, Human Purpose, and the Crisis of Meaning\nJosephine (White Hat) - Objective Facts and Information\n\nTechnological Impact on Life:\n\nAutomation and Efficiency: Technology aims to make tasks easier and more efficient, often by automating processes. This is driven by the desire to save time, reduce labor, and increase productivity.\nEconomic Structure: Modern economies rely heavily on money as a means to allocate resources efficiently. This system is more flexible and scalable compared to older methods, like barter or caste-based systems.\nUnemployment and Automation: The rise of automation has led to significant changes in the job market, including increased unemployment in certain sectors as machines replace human labor.\nClaude (Green Hat) - Creative Thinking and New Ideas\n\nExploring Deeper Goals and Meaning:\n\nRedefining Purpose Beyond Money: Society can shift its focus from purely economic goals to deeper, more fulfilling pursuits. This includes sustainable living, personal growth, and community well-being.\nSupporting Meaning-Seeking: Technologies can be developed to help individuals discover and pursue their sources of meaning. For example, AI-driven platforms can recommend activities, resources, and communities based on personal interests and values.\nEthical AI and Technology: We can design technology to be transparent and ethical, ensuring it supports human well-being and preserves our ability to feel and find meaning.\nPrunella (Black Hat) - Identifying Potential Problems and Risks\n\nChallenges and Risks:\n\nOver-Reliance on Technology: Excessive reliance on technology can disconnect individuals from the processes that give life meaning. Automation might make life easier but can also lead to a sense of purposelessness.\nEthical Blind Spots: The complexity of modern technology can obscure its true impacts, leading to unintended consequences that are difficult to foresee or control.\nSocietal Stratification: There's a risk that technological advancements could exacerbate social inequalities, with certain groups benefiting more than others and deepening the crisis of meaning for those left behind.\nPercy (Yellow Hat) - Highlighting Positive Aspects and Opportunities\n\nOpportunities for a Meaningful Life:\n\nAugmented Human Capacity: Technology can enhance human abilities, enabling us to pursue more meaningful activities and engage with the world in new ways.\nNew Models of Resource Allocation: Innovative economic models, such as Universal Basic Income (UBI) or resource-based economies, could provide a safety net, allowing individuals to focus on meaningful pursuits rather than mere survival.\nEthical and Transparent Tech Development: By prioritizing ethical considerations and transparency in tech development, we can ensure that technology serves human well-being and fosters a sense of purpose.\nCyril (Red Hat) - Intuition, Emotions, and Gut Feelings\n\nIntuitive Connections:\n\nHuman Feelings and Technology: There's an inherent tension between the convenience provided by technology and the deep, intuitive need for meaningful engagement. Simplifying life might rob it of the very challenges that make it fulfilling.\nMeaning in a Technological World: Intuitively, it feels crucial to find balance—leveraging technology to support our lives without losing touch with the core elements that provide meaning, such as relationships, creativity, and personal growth.\nQuackers (Blue Hat) - Synthesizing All Perspectives and Developing a Comprehensive Plan\n\nComprehensive Plan for Addressing the Crisis of Meaning:\n\nIntegrate Technology Thoughtfully:\n\nEthical Design: Ensure technologies are designed with ethical considerations at the forefront, prioritizing transparency and human well-being.\nEducational Initiatives: Promote education on digital literacy and ethics to help individuals navigate the complexities of modern technology.\nDevelop New Economic Models:\n\nUniversal Basic Income (UBI): Explore UBI and other economic models that provide a safety net, allowing individuals to pursue meaningful activities without the constant pressure of economic survival.\nResource-Based Economies: Investigate alternative economic systems that allocate resources based on need and sustainability rather than profit.\nPromote Meaning-Seeking Activities:\n\nCommunity Building: Foster communities that support meaningful engagement, such as collaborative spaces, local initiatives, and online platforms focused on shared interests and values.\nPersonal Growth: Encourage activities that promote personal growth, such as lifelong learning, creative pursuits, and volunteer work.\nSupport Mental and Emotional Well-Being:\n\nMental Health Resources: Provide accessible mental health resources to help individuals cope with the pressures of modern life and find their sense of purpose.\nMindfulness and Reflection: Promote practices like mindfulness and self-reflection to help individuals connect with their inner values and sources of meaning.\nBy integrating these elements, we can create a society that leverages technology to support human well-being, foster a sense of meaning, and address the deeper existential questions that drive our pursuit of purpose.\nYeah, some of the other things that I wanted to think about are how there's people out there like Balaji who thinks that 1729 and this idea that we can have just fund the smartest people internationally as if intelligence is naturally distributed across populations evenly versus intelligence having a temporal element, and a reflective element, and a diversity element. The idea that you need to experience and think and cluster in your mind different ideas to be able to do stuff. I think he took a reference from somebody who had a very specific kind of intelligence, Raj Rama, that guy, the Indian mathematician who discovered the taxi number 1729. It's just a very specific, particular kind of mathematical intelligence that doesn't actually require experience, and it's not the same as the kind of intelligence that you might see in the movie A Beautiful Mind, or that makes analogical leaps and relationships and connects ideas, or the kind of intelligence to know when to stop, or the kind of intelligence to know what the right thing to do is, or to say the unique thing that only you can say, or to come up with something new that can actually make a difference on the world in a positive way, where you are then next also finding your meaning and purpose. This is stuff that I relate to the idea of genius, or a muse, like the genius loci, the idea that genius is all the things that come together in a place that makes you, you. And so there's that, and it's the accident of your birth, your natural inclinations, your experiences, your eccentricities, your uniqueness, that then gives you a very specific generalness. Your generalness is going to have a different sort of look, and that's why when you and your uniqueness say something that others can generalize from, that's where things get beautiful. I also want to connect it to the ninety-nine names of God and beauty, and how we're here to represent the most beautiful names, and it's set for the God. He doesn't have a quality, and a way of being associated. It would be interesting to have ninety-nine pilafas, a book of ninety-nine people that represent the different qualities, and how they learn to become like that, or how they learn to polish this bond that is that within themselves. \n\nChatGPT\nExploring Diverse Dimensions of Intelligence and the Concept of Genius\nJosephine (White Hat) - Objective Facts and Information\n\nDiverse Dimensions of Intelligence:\n\nMultiple Intelligences Theory: Proposed by Howard Gardner, this theory suggests that intelligence is not a single entity but a combination of various cognitive abilities. These include linguistic, logical-mathematical, spatial, musical, bodily-kinesthetic, interpersonal, intrapersonal, and naturalistic intelligences.\nTemporal and Reflective Elements: Intelligence also involves temporal elements (developing over time) and reflective elements (metacognition and self-awareness). Experiences, cultural background, and personal history significantly shape cognitive abilities.\nHistorical Examples: Mathematician Srinivasa Ramanujan's intuitive grasp of numbers contrasts with John Nash's ability to make analogical leaps in game theory. These illustrate different forms of mathematical and analytical intelligence.\nClaude (Green Hat) - Creative Thinking and New Ideas\n\nIntegrating Unique Forms of Intelligence:\n\nFunding Diverse Talents: Rather than focusing solely on traditional measures of intelligence (e.g., IQ), we could fund individuals with diverse talents, including those with creative, practical, and emotional intelligence.\nPolishing Individual Genius: Develop programs that help individuals identify and nurture their unique talents and eccentricities, fostering environments that encourage diverse forms of expression and innovation.\nGenius Loci Concept: Embrace the idea that genius is not just inherent but also a product of one's environment, experiences, and interactions. Create spaces that inspire creativity and allow individuals to harness their unique blend of talents and experiences.\nPrunella (Black Hat) - Identifying Potential Problems and Risks\n\nChallenges and Risks:\n\nEquity in Opportunities: Ensuring that funding and resources are distributed equitably to nurture diverse talents can be challenging. There's a risk that certain types of intelligence or creativity might be undervalued.\nDefining Genius: The concept of genius can be subjective and culturally biased. It’s essential to recognize and validate different forms of intelligence and creativity without imposing a narrow definition.\nBalancing Individual and Collective Needs: While focusing on individual genius is important, it should not come at the expense of collective well-being and societal needs. Balancing personal development with community and societal goals is crucial.\nPercy (Yellow Hat) - Highlighting Positive Aspects and Opportunities\n\nOpportunities for Growth and Beauty:\n\nCelebrating Diversity: By recognizing and nurturing diverse forms of intelligence, we can create a more inclusive and vibrant society where everyone’s unique contributions are valued.\nThe Ninety-Nine Names of Genius: Inspired by the ninety-nine names of God, we could create a \"book of ninety-nine geniuses,\" each representing different qualities and forms of intelligence. This would highlight the diverse ways people contribute to the world and find their meaning.\nCultural and Intellectual Exchange: Encouraging the sharing of diverse ideas and experiences can lead to new insights and innovations, enriching our collective understanding and advancing human knowledge.\nCyril (Red Hat) - Intuition, Emotions, and Gut Feelings\n\nIntuitive and Emotional Connections:\n\nPersonal Resonance: There’s a deep, intuitive understanding that our unique blend of experiences, inclinations, and talents shapes our intelligence and contributions. This resonates with the idea of genius loci—where one's environment and personal history play a crucial role.\nMeaning and Purpose: Recognizing and nurturing diverse forms of intelligence can help individuals find their purpose and meaning, leading to a more fulfilling and connected life.\nQuackers (Blue Hat) - Synthesizing All Perspectives and Developing a Comprehensive Plan\n\nComprehensive Plan for Nurturing Diverse Forms of Intelligence:\n\nDevelop Inclusive Programs:\n\nFunding Initiatives: Create funding programs that recognize and support diverse forms of intelligence, including creative, practical, and emotional intelligence.\nMentorship and Guidance: Establish mentorship programs that help individuals identify and nurture their unique talents, providing guidance and resources to develop their potential.\nCreate Inspirational Spaces:\n\nGenius Loci Environments: Design environments that inspire creativity and innovation, allowing individuals to harness their unique blend of experiences and talents.\nCultural Hubs: Establish cultural and intellectual hubs where diverse ideas and experiences can be shared and celebrated.\nPromote Equity and Inclusion:\n\nEquitable Resource Distribution: Ensure that resources and opportunities are distributed equitably, recognizing and valuing different forms of intelligence and creativity.\nInclusive Education: Incorporate diverse forms of intelligence into educational curricula, promoting a holistic approach to learning and personal development.\nCelebrate Diversity and Genius:\n\nThe Ninety-Nine Names of Genius: Create a project inspired by the ninety-nine names of God, celebrating individuals who embody different qualities and forms of intelligence. This could include a book, documentary series, or online platform highlighting their stories and contributions.\nRecognition and Awards: Establish awards and recognition programs that honor diverse forms of intelligence and creativity, encouraging others to explore and develop their unique talents.\nBy integrating these elements, we can create a society that values and nurtures diverse forms of intelligence, fostering a deeper sense of meaning, purpose, and beauty in our collective human experience.\n\nChatGPT\nAnalyzing and Structuring the Text: Comprehensive Plan\nStep-by-Step Process\n\nInitial Review and Analysis:\n\nRead through the entire text (10,000 words) carefully to understand the overall message and context.\nHighlight key points, ideas, and arguments throughout the text.\nIdentify Thematic Clusters:\n\nGroup similar ideas together to form thematic clusters.\nCategorize key points based on common themes such as:\nInvolving youth in education.\nThe evolving nature of educational frameworks.\nThe concept of non-linear, dynamic systems in education.\nThe idea of creating a self-sustaining educational ecosystem.\nDiverse dimensions of intelligence and the concept of genius.\nThe role of technology in human learning and meaning-seeking.\nDetermine Novel Ideas:\n\nIdentify unique perspectives or innovative concepts.\nNote ideas that offer a fresh take on adaptive learning, the role of educators, and the intersection of AI and human learning.\nEnumerate Comprehensive Ideas and Potential Axes:\n\nList all main ideas in the document.\nIdentify potential axes for graphing these ideas:\nTemporal Axis: How ideas evolve over time.\nPositional Axis: The position or context in which ideas are applied.\nScale Axis: The scope and impact of the ideas.\nComplexity Axis: The complexity level of implementing the ideas.\nYouth Involvement Axis: The degree of youth participation in each idea.\nFramework Dynamics Axis: Static vs. evolving frameworks.\nOutcome Axis: The potential outcomes or benefits of each idea.\nIntelligence Types Axis: Different forms of intelligence (e.g., creative, practical, emotional).\nMeaning and Purpose Axis: The degree to which ideas contribute to personal meaning and societal purpose.\nDevelop Title and Subtitle:\n\nCraft a compelling title based on the primary focus and innovative concepts.\nCreate a subtitle that provides additional context or focus.\nCreate Table of Contents:\n\nOutline the main sections and subsections based on the thematic clusters.\nEnsure a logical flow and coherence in the structure.\nWrite the Abstract:\n\nSummarize the key points and purpose of the text.\nHighlight the main arguments and conclusions.\nDraft the Introduction:\n\nIntroduce the topic and its relevance.\nProvide background information and context.\nState the purpose and objectives of the writing.\nFormulate the Conclusion:\n\nRecap the main points and arguments.\nDiscuss the implications and significance of the findings.\nSuggest future directions or actions based on the conclusions.\nClarifying Questions\nSpecific Emphasis:\n\nAre there any particular sections or ideas within the concept of \"graph knowledge framework\" and \"frameworks being needed and frameworks being unnecessary\" that you want to be given more focus?\nVisualization Preferences:\n\nDo you have any preferences for the format or style of visualizing the ideas on the identified axes? For example, would you prefer traditional graphs, mind maps, or another format?\nExperimentation with Axes:\n\nAre there specific experimental approaches or methodologies you prefer for testing the identified axes and their relevance?\nTitle, Subtitle, and Table of Contents\nTitle:\n\"Adapting Learning: Integrating Dynamic Frameworks and AI to Foster Human Potential\"\n\nSubtitle:\n\"Exploring Non-Linear Systems, Diverse Intelligences, and Ethical Technology in Modern Education\"\n\nTable of Contents:\n\nAbstract\nIntroduction\nBackground and Context\nPurpose and Objectives\nThematic Clusters and Key Concepts\nInvolving Youth in Education\nEvolving Educational Frameworks\nDynamic Systems in Education\nSelf-Sustaining Educational Ecosystems\nDiverse Dimensions of Intelligence\nThe Role of Technology in Learning and Meaning-Seeking\nMachine Learning and Human Learning\nConcept Clustering in AI\nTransfer Learning and Human Cognition\nSynergies Between AI and Human Learning Processes\nFramework Development for Prompting Techniques\nLeveraging AI Techniques\nHuman Concept Clustering\nPractical Applications and Tools\nChallenges and Opportunities\nRisks of Over-Reliance on Technology\nEthical Considerations and Data Privacy\nEnhancing Learning Experiences with AI\nNurturing Diverse Forms of Intelligence\nRecognizing and Funding Diverse Talents\nCreating Inspirational Learning Environments\nThe Ninety-Nine Names of Genius\nTechnology, Purpose, and the Crisis of Meaning\nBalancing Automation and Human Engagement\nNew Economic Models for Resource Allocation\nSupporting Personal Growth and Community Building\nConclusion\nRecap of Main Points\nImplications and Significance\nFuture Directions and Actions\nAbstract\nThis paper explores the integration of dynamic educational frameworks and artificial intelligence (AI) to enhance human learning and potential. By examining non-linear systems, diverse intelligences, and ethical technology, the study aims to rethink adaptive learning and the role of educators. Key concepts include involving youth in education, evolving frameworks, and creating self-sustaining ecosystems. The paper highlights the synergies between machine learning processes, such as transfer learning and concept clustering, and human cognitive development. Challenges, opportunities, and ethical considerations are discussed to provide a comprehensive approach to modern education, ultimately aiming to foster a deeper sense of meaning and purpose in learners.\n\nIntroduction\nIn an era of rapid technological advancement, the field of education is becoming increasingly self-aware, recognizing the need for adaptive learning and the active involvement of youth in shaping our collective future. This paper aims to explore how dynamic educational frameworks, integrated with AI techniques, can enhance human learning experiences. By leveraging concepts such as transfer learning, embeddings, and clustering, we seek to create a system that evolves in response to input from both youth and educators. The purpose of this study is to provide a comprehensive understanding of these innovative approaches, addressing the challenges and opportunities they present, and proposing a path forward for modern education.\n\nConclusion\nThe integration of dynamic educational frameworks and AI holds significant promise for transforming the learning experience. By recognizing and nurturing diverse forms of intelligence, promoting ethical technology, and creating inclusive learning environments, we can foster a deeper sense of meaning and purpose in learners. This approach not only leverages the strengths of AI but also empowers individuals to explore and understand complex concepts in a more intuitive and personalized way. Future directions include developing inclusive programs, creating inspirational spaces, and promoting equitable resource distribution to ensure that these innovations benefit all learners and contribute to a more vibrant and connected society.\n\nPlease provide any additional preferences or clarifications, and we can refine the plan further before proceeding with the detailed analysis and structuring of your text.\nReview the outline and critique it with 5 pros and 5 cons, come up with an approach that includes and cross connects between cognitive science and how the mind/learning works, sociology and how learner work in the context of society, emotions, motivation, wellbeing and flourishing and motivation, adult development theory and metacognition, marginalized youth and trauma, economic theory and value production theory, social entrepreneurship theory, ethics and philosophy of action, self efficacy, self transformation and transformative education, learning by doing Framework Element\n\nDescription\n\nImpact Measurement\n\nPotential Assessment\n\nTheorist\n\nElement 1 \n\nSelf-Directed Learning Inquiry-Based Learning\n\nLearners actively ask questions, investigate, and research.\n\nDepth and complexity of inquiry by learners.\n\nInquiry Skills Rating Scale.\n\nJohn Dewey\n\nElement 1 Self-Directed Learning Narrative & Reflective Learning\n\nEmphasizes storytelling and personal reflection.\n\nThe connection between personal experiences and learning objectives.\n\nReflective Writing Scale.\n\nDonald Schön\n\nElement 1 Self-Efficacy / Entrepreneurial Self-Efficacy\n\nConfidence in entrepreneurial capabilities.\n\nConfidence in managing entrepreneurial challenges.\n\nEntrepreneurial Self-Efficacy Scale.\n\nAlbert Bandura, Martin, McNally, Kay\n\nElement 1 Self-Directed Learning\n\nLearners set and pursue their own learning goals.\n\nProgress towards self-set learning goals.\n\nSelf-Directed Learning Readiness Scale (SDLRS).\n\nMalcolm Knowles\n\nElement 2 Learning By Doing Relevant Activities Zone of Proximal Learning\n\nElement 2 (Praxis / Learning) Problem-Based Learning\n\nLearning through addressing real-world problems.\n\nProblem-solving skills and application of knowledge.\n\nProblem-Based Learning Assessment Rubric.\n\nHoward Barrows\n\nElement 2 Experiential Learning\n\nHands-on activities and experiential learning.\n\nApplication of theoretical knowledge in practice.\n\nExperiential Learning Cycle Assessment.\n\nDavid Kolb\n\nElement 3 Learning Better By Learning Together Interactive & Collaborative Learning\n\nLearning through interaction and collaboration.\n\nCollaborative skills and interaction quality.\n\nCollaborative Learning Assessment Tool.\n\nLev Vygotsky\n\nElement 4 Start With Why And Connect To Values Resilience & Grit\n\nDeveloping persistence and resilience.\n\nPerseverance in learning challenges.\n\nGrit Scale.\n\nAngela Duckworth\n\nElement 4 Why / intention / values\n\nElement 5 Clarity\n\nas well as complexity theory and Albert Bandura (Social Cognitive Theory)\n\nField: Educational Psychology\n\nMain Finding: Self-efficacy influences learning and motivation significantly, impacting how people approach goals and challenges.\n\nBarry J. Zimmerman (Self-Regulated Learning)\n\nField: Educational Psychology\n\nMain Finding: Effective learning involves self-regulation through goal setting, self-monitoring, and strategic use of learning resources.\n\nRichard M. Ryan and Edward L. Deci (Self-Determination Theory)\n\nField: Motivation Psychology\n\nMain Finding: Intrinsic motivation, driven by autonomy, competence, and relatedness, is crucial for sustained learning and personal growth.\n\nJohn H. Holland (Complex Adaptive Systems)\n\nField: Complex Systems and Artificial Intelligence\n\nMain Finding: Adaptive systems, like learners, evolve through interaction with their environment, demonstrating emergent behavior from simple rules.\n\nMelanie Mitchell (Complex Systems and Learning)\n\nField: Complex Systems\n\nMain Finding: Learning processes can be understood as complex systems where simple rules can lead to sophisticated patterns and behaviors over time.\n\nas well as the principles for how LLMs work on a technical level so we can leverage to the extreme their affordances - especially for marginalized people who might not have mentors to help them \"see the unseeable\" because of their lack of exposure and who need support in goal / aspiration development as well as education as to how to achieve it \n\nChatGPT\nReview and Critique of the Outline\nPros\n\nComprehensive Scope: The outline covers a wide range of topics, integrating AI, dynamic educational frameworks, and diverse intelligences, which provides a holistic view of modern education.\nInnovative Concepts: The inclusion of transfer learning, embeddings, and clustering in AI offers a fresh perspective on enhancing human learning processes.\nEthical Considerations: The outline addresses important ethical concerns such as data privacy, bias in AI, and the equitable distribution of resources, ensuring a balanced approach.\nDiverse Intelligences: Recognizing and celebrating different forms of intelligence aligns with current educational theories and practices, promoting inclusivity.\nFuture Directions: The conclusion suggests actionable future directions, making the outline not just theoretical but also practical and forward-looking.\nCons\n\nLack of Cross-Disciplinary Connections: The outline does not explicitly connect cognitive science, sociology, motivation, adult development theory, and other fields, which could enrich the analysis.\nInsufficient Focus on Marginalized Youth: While the outline mentions the importance of involving youth, it lacks specific strategies for addressing the needs of marginalized and traumatized youth.\nComplexity and Readability: The outline is dense and may be difficult for some readers to follow. Simplifying and organizing the content into clearer sections could enhance readability.\nMeasurement and Assessment: The outline does not detail how the impact of the proposed frameworks and techniques will be measured and assessed.\nPractical Application: There is a need for more concrete examples and practical applications of the proposed concepts to help readers visualize how they can be implemented in real educational settings.\nRevised Approach\nIntegrating Cross-Disciplinary Connections\n\nCognitive Science and Learning Processes:\n\nTheory: Self-Regulated Learning (Barry J. Zimmerman)\nApplication: Encourage learners to set personal goals, self-monitor their progress, and use strategic learning resources.\nImpact Measurement: Use the Self-Directed Learning Readiness Scale (SDLRS) to assess learners' readiness for self-directed learning.\nAssessment Tool: Self-Regulated Learning Assessment (SRLA)\nSociology and Learning in Context:\n\nTheory: Social Cognitive Theory (Albert Bandura)\nApplication: Promote collaborative learning environments where learners can observe and model behaviors.\nImpact Measurement: Evaluate collaborative skills and interaction quality using the Collaborative Learning Assessment Tool.\nAssessment Tool: Social Learning Environment Scale (SLES)\nEmotions, Motivation, Well-being, and Flourishing:\n\nTheory: Self-Determination Theory (Richard M. Ryan and Edward L. Deci)\nApplication: Design learning experiences that foster autonomy, competence, and relatedness.\nImpact Measurement: Measure learners' intrinsic motivation and well-being using the Intrinsic Motivation Inventory (IMI).\nAssessment Tool: Well-being and Flourishing Scale (WFS)\nAdult Development Theory and Metacognition:\n\nTheory: Reflective Practice (Donald Schön)\nApplication: Integrate narrative and reflective learning practices to encourage self-awareness and personal growth.\nImpact Measurement: Assess the depth of reflective practice using the Reflective Writing Scale.\nAssessment Tool: Reflective Learning Assessment (RLA)\nMarginalized Youth and Trauma:\n\nTheory: Trauma-Informed Education\nApplication: Develop supportive and safe learning environments that acknowledge and address the impact of trauma.\nImpact Measurement: Use the Trauma-Informed Classroom Assessment Tool (TICAT) to evaluate the effectiveness of trauma-informed practices.\nAssessment Tool: Trauma Impact Scale (TIS)\nEconomic Theory and Value Production:\n\nTheory: Value Creation Framework\nApplication: Encourage learners to engage in projects that create social, economic, or environmental value.\nImpact Measurement: Evaluate the impact of these projects using the Social Value Impact Scale (SVIS).\nAssessment Tool: Project Value Creation Assessment (PVCA)\nSocial Entrepreneurship Theory:\n\nTheory: Entrepreneurial Self-Efficacy (Albert Bandura, Martin, McNally, Kay)\nApplication: Foster entrepreneurial skills and mindsets through real-world problem-solving activities.\nImpact Measurement: Assess learners' confidence in their entrepreneurial abilities using the Entrepreneurial Self-Efficacy Scale.\nAssessment Tool: Social Entrepreneurship Assessment (SEA)\nEthics and Philosophy of Action:\n\nTheory: Ethics of Care\nApplication: Embed ethical considerations and the philosophy of care into the curriculum to foster compassionate and responsible learners.\nImpact Measurement: Evaluate ethical reasoning and actions using the Ethics of Care Assessment Tool (ECAT).\nAssessment Tool: Ethical Impact Scale (EIS)\nSelf-Efficacy, Self-Transformation, and Transformative Education:\n\nTheory: Transformative Learning (Jack Mezirow)\nApplication: Encourage critical reflection and transformative experiences that lead to profound personal change.\nImpact Measurement: Measure the extent of personal transformation using the Transformative Learning Assessment Scale (TLAS).\nAssessment Tool: Self-Transformation Scale (STS)\nLearning by Doing:\n\nTheory: Experiential Learning (David Kolb)\nApplication: Implement hands-on, practical learning activities that connect theory to practice.\nImpact Measurement: Assess the application of theoretical knowledge in practice using the Experiential Learning Cycle Assessment.\nAssessment Tool: Practical Learning Assessment (PLA)\nComprehensive Plan for Integrating AI and Human Learning\nElement 1: Self-Directed Learning\n\nFramework Element: Inquiry-Based Learning, Narrative & Reflective Learning, Self-Efficacy / Entrepreneurial Self-Efficacy\nDescription: Learners actively ask questions, investigate, and reflect on their learning, building confidence in their entrepreneurial capabilities.\nImpact Measurement: Depth and complexity of inquiry, connection between personal experiences and learning objectives, confidence in managing entrepreneurial challenges.\nPotential Assessment: Inquiry Skills Rating Scale, Reflective Writing Scale, Entrepreneurial Self-Efficacy Scale.\nTheorists: John Dewey, Donald Schön, Albert Bandura\nElement 2: Learning By Doing\n\nFramework Element: Relevant Activities, Problem-Based Learning, Experiential Learning\nDescription: Learners engage in hands-on activities and real-world problem-solving to apply theoretical knowledge.\nImpact Measurement: Problem-solving skills, application of knowledge, experiential learning outcomes.\nPotential Assessment: Problem-Based Learning Assessment Rubric, Experiential Learning Cycle Assessment.\nTheorists: Howard Barrows, David Kolb\nElement 3: Learning Together\n\nFramework Element: Interactive & Collaborative Learning\nDescription: Learning through interaction and collaboration with peers.\nImpact Measurement: Collaborative skills and interaction quality.\nPotential Assessment: Collaborative Learning Assessment Tool.\nTheorist: Lev Vygotsky\nElement 4: Motivation and Values\n\nFramework Element: Resilience & Grit, Values-Based Learning\nDescription: Developing persistence, resilience, and alignment with personal values.\nImpact Measurement: Perseverance in learning challenges, alignment with personal values.\nPotential Assessment: Grit Scale, Values Alignment Scale.\nTheorists: Angela Duckworth, Simon Sinek\nElement 5: Clarity and Complexity\n\nFramework Element: Clarity in Learning Objectives, Embracing Complexity\nDescription: Clear learning objectives within a complex, adaptive learning system.\nImpact Measurement: Clarity of learning goals, ability to navigate complex learning environments.\nPotential Assessment: Learning Objectives Clarity Scale, Complexity Navigation Assessment.\nTheorists: Albert Bandura, Barry J. Zimmerman, Richard M. Ryan and Edward L. Deci\nConnecting AI and Human Learning\nConcept Clustering and Embeddings in AI:\n\nAI Technique: Clustering algorithms and embeddings (e.g., Word2Vec)\nHuman Learning Parallel: Schema Theory, Concept Mapping\nImpact Measurement: Ability to cluster and relate concepts in meaningful ways.\nPotential Assessment: Concept Mapping Skills Assessment.\nTransfer Learning:\n\nAI Technique: Applying pre-trained models to new tasks\nHuman Learning Parallel: Applying knowledge from one context to another\nImpact Measurement: Ability to transfer and apply knowledge in new contexts.\nPotential Assessment: Transfer Learning Application Scale.\nInteractive Learning Tools:\n\nAI Technique: Visualization of embeddings and clusters\nHuman Learning Parallel: Visual Learning Tools\nImpact Measurement: Engagement and understanding of abstract concepts.\nPotential Assessment: Visual Learning Engagement Scale.\nConclusion\nBy integrating these elements and cross-connecting various theories and fields, we can create a comprehensive educational framework that leverages AI to enhance human learning, addresses the needs of marginalized youth, and fosters a sense of meaning and purpose. This approach not only supports cognitive and emotional development but also promotes ethical and equitable education, ensuring that all learners have the opportunity to thrive and contribute to society.\n\n"
          }
        ],
        "files": [
          {
            "file_name": "paste.txt"
          }
        ]
      }
    },
    {
      "id": "189a8464-6211-4317-950e-2705463169cb",
      "role": "assistant",
      "content": " Here is a revised plan for analyzing and structuring your text, integrating the various cross-disciplinary elements you mentioned:\n\nOutline with Cross-Disciplinary Connections\nI. Abstract\nII. Introduction\n   A. Background and Context  \n   B. Purpose and Objectives\nIII. Thematic Clusters and Key Concepts\n   A. Involving Youth in Education\n      1. Self-Directed Learning (Malcolm Knowles)\n      2. Self-Regulated Learning (Barry J. Zimmerman)\n      3. Self-Determination Theory (Richard M. Ryan and Edward L. Deci)\n   B. Evolving Educational Frameworks \n      1. Complex Adaptive Systems (John H. Holland)\n      2. Dynamic Systems Theory (Esther Thelen)\n      3. Transformative Learning Theory (Jack Mezirow)\n   C. AI and Human Learning Synergies\n      1. Concept Clustering and Embeddings (Parallel to Schema Theory) \n      2. Transfer Learning (Parallel to Applying Knowledge Across Contexts)\n      3. Interactive Learning Tools (Parallel to Visual Learning)\n   D. Nurturing Diverse Intelligences\n      1. Multiple Intelligences Theory (Howard Gardner)  \n      2. Triarchic Theory of Intelligence (Robert Sternberg)\n      3. Emotional Intelligence (Daniel Goleman)\n   E. Technology, Purpose, and Meaning\n      1. Self-Efficacy Theory (Albert Bandura) \n      2. Entrepreneurial Self-Efficacy (Per-Ola Erickson)\n      3. Ethics of Technology (Peter-Paul Verbeek)\nIV. Framework Development \n   A. Element 1: Self-Directed Learning\n      1. Inquiry-Based Learning (John Dewey)\n      2. Narrative & Reflective Learning (Donald Schön) \n      3. Entrepreneurial Self-Efficacy (Martin, McNally, Kay)\n   B. Element 2: Learning By Doing \n      1. Experiential Learning (David Kolb)\n      2. Problem-Based Learning (Howard Barrows)\n      3. Situated Learning (Jean Lave)\n   C. Element 3: Collaborative Learning\n      1. Social Learning Theory (Albert Bandura)\n      2. Zone of Proximal Development (Lev Vygotsky) \n      3. Communities of Practice (Etienne Wenger)\n   D. Element 4: Motivation and Values\n      1. Expectancy-Value Theory (Jacquelynne Eccles)\n      2. Achievement Goal Theory (Andrew Elliot, Carol Dweck)\n      3. Self-Concordance (Kennon Sheldon)  \n   E. Element 5: Clarity and Complexity\n      1. Cognitive Load Theory (John Sweller)\n      2. Scaffolding Theory (Jerome Bruner)\n      3. Chaos and Complexity Theory (Brent Davis)\nV. Challenges and Opportunities\n   A. Ethical AI and Data Privacy \n   B. Equitable Resource Distribution\n   C. Measuring Impact and Outcomes\n   D. Integrating Marginalized Perspectives\nVI. Future Directions and Actions\n   A. Inclusive Educational Programs \n   B. Inspirational Learning Environments\n   C. Promoting Personal Growth and Well-Being\n   D. Building Resilient Learning Communities \nVII. Conclusion\n   A. Recap of Main Arguments\n   B. Significance and Implications\n   C. Call for Collective Action\n\nI believe this revised outline more effectively integrates the various cross-disciplinary connections you highlighted, including cognitive science, sociology, motivation theories, adult development, economic theories, ethics, and LLM affordances. The key theorists are now embedded within the relevant sections.\n\nThe framework development section now includes specific elements related to self-directed learning, experiential learning, collaboration, motivation, and managing complexity. Each element is grounded in established educational and psychological theories.\n\nChallenges around ethics, equity, measurement, and inclusion are addressed, leading into future-oriented actions promoting personal growth, inspiring environments, and resilient communities.\n\nThe conclusion ties everything together, recapping the main arguments, discussing implications, and ending with a call to action.\n\nLet me know if you would like me to elaborate on any part of this outline further. I'm happy to refine it based on your feedback before proceeding with the full analysis and write-up.",
      "created_at": "2024-05-29T13:50:10.459135Z",
      "sequence_order": 4,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2024-05-29T13:50:10.459135Z",
            "stop_timestamp": "2024-05-29T13:50:10.459135Z",
            "flags": null,
            "type": "text",
            "text": " Here is a revised plan for analyzing and structuring your text, integrating the various cross-disciplinary elements you mentioned:\n\nOutline with Cross-Disciplinary Connections\nI. Abstract\nII. Introduction\n   A. Background and Context  \n   B. Purpose and Objectives\nIII. Thematic Clusters and Key Concepts\n   A. Involving Youth in Education\n      1. Self-Directed Learning (Malcolm Knowles)\n      2. Self-Regulated Learning (Barry J. Zimmerman)\n      3. Self-Determination Theory (Richard M. Ryan and Edward L. Deci)\n   B. Evolving Educational Frameworks \n      1. Complex Adaptive Systems (John H. Holland)\n      2. Dynamic Systems Theory (Esther Thelen)\n      3. Transformative Learning Theory (Jack Mezirow)\n   C. AI and Human Learning Synergies\n      1. Concept Clustering and Embeddings (Parallel to Schema Theory) \n      2. Transfer Learning (Parallel to Applying Knowledge Across Contexts)\n      3. Interactive Learning Tools (Parallel to Visual Learning)\n   D. Nurturing Diverse Intelligences\n      1. Multiple Intelligences Theory (Howard Gardner)  \n      2. Triarchic Theory of Intelligence (Robert Sternberg)\n      3. Emotional Intelligence (Daniel Goleman)\n   E. Technology, Purpose, and Meaning\n      1. Self-Efficacy Theory (Albert Bandura) \n      2. Entrepreneurial Self-Efficacy (Per-Ola Erickson)\n      3. Ethics of Technology (Peter-Paul Verbeek)\nIV. Framework Development \n   A. Element 1: Self-Directed Learning\n      1. Inquiry-Based Learning (John Dewey)\n      2. Narrative & Reflective Learning (Donald Schön) \n      3. Entrepreneurial Self-Efficacy (Martin, McNally, Kay)\n   B. Element 2: Learning By Doing \n      1. Experiential Learning (David Kolb)\n      2. Problem-Based Learning (Howard Barrows)\n      3. Situated Learning (Jean Lave)\n   C. Element 3: Collaborative Learning\n      1. Social Learning Theory (Albert Bandura)\n      2. Zone of Proximal Development (Lev Vygotsky) \n      3. Communities of Practice (Etienne Wenger)\n   D. Element 4: Motivation and Values\n      1. Expectancy-Value Theory (Jacquelynne Eccles)\n      2. Achievement Goal Theory (Andrew Elliot, Carol Dweck)\n      3. Self-Concordance (Kennon Sheldon)  \n   E. Element 5: Clarity and Complexity\n      1. Cognitive Load Theory (John Sweller)\n      2. Scaffolding Theory (Jerome Bruner)\n      3. Chaos and Complexity Theory (Brent Davis)\nV. Challenges and Opportunities\n   A. Ethical AI and Data Privacy \n   B. Equitable Resource Distribution\n   C. Measuring Impact and Outcomes\n   D. Integrating Marginalized Perspectives\nVI. Future Directions and Actions\n   A. Inclusive Educational Programs \n   B. Inspirational Learning Environments\n   C. Promoting Personal Growth and Well-Being\n   D. Building Resilient Learning Communities \nVII. Conclusion\n   A. Recap of Main Arguments\n   B. Significance and Implications\n   C. Call for Collective Action\n\nI believe this revised outline more effectively integrates the various cross-disciplinary connections you highlighted, including cognitive science, sociology, motivation theories, adult development, economic theories, ethics, and LLM affordances. The key theorists are now embedded within the relevant sections.\n\nThe framework development section now includes specific elements related to self-directed learning, experiential learning, collaboration, motivation, and managing complexity. Each element is grounded in established educational and psychological theories.\n\nChallenges around ethics, equity, measurement, and inclusion are addressed, leading into future-oriented actions promoting personal growth, inspiring environments, and resilient communities.\n\nThe conclusion ties everything together, recapping the main arguments, discussing implications, and ending with a call to action.\n\nLet me know if you would like me to elaborate on any part of this outline further. I'm happy to refine it based on your feedback before proceeding with the full analysis and write-up.",
            "citations": []
          }
        ],
        "updated_at": "2024-05-29T13:50:10.459135Z",
        "attachments": [],
        "files": []
      }
    }
  ]
}