{
  "id": "20c9af50-0b84-4db5-95ce-413d3c33aad3",
  "source": "chatgpt",
  "title": "Whisper Diarization Transcription",
  "created_at": "2023-07-25T16:24:17.630481",
  "updated_at": "2023-07-25T16:24:42",
  "summary": null,
  "message_count": 2,
  "messages": [
    {
      "id": "aaa2dc68-a490-406d-ab49-418e8550fad2",
      "role": "user",
      "content": "can you give me one python script that takes all the content and workflow here and makes it into a functional pythonic program that will accept an argument of a file and output a website html file that shows the audio / text annotated:\nWhisper transcription and diarization (speaker-identification)\nHow to use OpenAIs Whisper to transcribe and diarize audio files\n\nWhat is Whisper?\nWhisper is an State-of-the-Art speech recognition system from OpenAI that has been trained on 680,000 hours of multilingual and multitask supervised data collected from the web. This large and diverse dataset leads to improved robustness to accents, background noise and technical language. In addition, it enables transcription in multiple languages, as well as translation from those languages into English. OpenAI released the models and code to serve as a foundation for building useful applications that leverage speech recognition.\n\nOne big downside of Whisper is though, that it can not tell you who is speaking in a conversation. That's a problem when analyzing conversations. This is where diarization comes in. Diarization is the process of identifying who is speaking in a conversation.\n\nIn this tutorial you will learn how to identify the speakers, and then match them with the transcriptions of Whisper. We will use pyannote-audio to accomplish this. Let's get started!\n\nPreparing the audio\nFirst, we need to prepare the audio file. We will use the first 20 minutes of Lex Fridmans podcast with Yann download. To download the video and extract the audio, we will use yt-dlp package.\n\n!pip install -U yt-dlp\nWe will also need ffmpeg installed\n\n!wget -O - -q  https://github.com/yt-dlp/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-linux64-gpl.tar.xz | xz -qdc| tar -x\nNow we can do the actual download and audio extraction via the command line.\n\n!yt-dlp -xv --ffmpeg-location ffmpeg-master-latest-linux64-gpl/bin --audio-format wav  -o download.wav -- https://youtu.be/SGzMElJ11Cc\nNow we have the download.wav file in our working directory. Let's cut the first 20 minutes of the audio. We can use the pydub package for this with just a few lines of code.\n\n!pip install pydub\nfrom pydub import AudioSegment\n\nt1 = 0 * 1000 # works in milliseconds\nt2 = 20 * 60 * 1000\n\nnewAudio = AudioSegment.from_wav(\"download.wav\")\na = newAudio[t1:t2]\na.export(\"audio.wav\", format=\"wav\") \naudio.wav is now the first 20 minutes of the audio file.\n\nPyannote's Diarization\npyannote.audio is an open-source toolkit written in Python for speaker diarization. Based on PyTorch machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines. pyannote.audio also comes with pretrained models and pipelines covering a wide range of domains for voice activity detection, speaker segmentation, overlapped speech detection, speaker embedding reaching state-of-the-art performance for most of them.\n\nInstalling Pyannote and running it on the video audio to generate the diarizations.\n\n!pip install pyannote.audio\nfrom pyannote.audio import Pipeline\n\npipeline = Pipeline.from_pretrained('pyannote/speaker-diarization')\nDEMO_FILE = {'uri': 'blabal', 'audio': 'audio.wav'}\ndz = pipeline(DEMO_FILE)  \n\nwith open(\"diarization.txt\", \"w\") as text_file:\n    text_file.write(str(dz))\nLets print this out to see what it looks like.\n\nprint(*list(dz.itertracks(yield_label = True))[:10], sep=\"\\n\")\nThe output:\n\n(<Segment(2.03344, 36.8128)>, 0, 'SPEAKER_00')\n(<Segment(38.1122, 51.3759)>, 0, 'SPEAKER_00')\n(<Segment(51.8653, 90.2053)>, 1, 'SPEAKER_01')\n(<Segment(91.2853, 92.9391)>, 1, 'SPEAKER_01')\n(<Segment(94.8628, 116.497)>, 0, 'SPEAKER_00')\n(<Segment(116.497, 124.124)>, 1, 'SPEAKER_01')\n(<Segment(124.192, 151.597)>, 1, 'SPEAKER_01')\n(<Segment(152.018, 179.12)>, 1, 'SPEAKER_01')\n(<Segment(180.318, 194.037)>, 1, 'SPEAKER_01')\n(<Segment(195.016, 207.385)>, 0, 'SPEAKER_00')\nThis looks pretty good already, but let's clean the data a little bit:\n\ndef millisec(timeStr):\n  spl = timeStr.split(\":\")\n  s = (int)((int(spl[0]) * 60 * 60 + int(spl[1]) * 60 + float(spl[2]) )* 1000)\n  return s\n\nimport re\ndz = open('diarization.txt').read().splitlines()\ndzList = []\nfor l in dz:\n  start, end =  tuple(re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=l))\n  start = millisec(start) - spacermilli\n  end = millisec(end)  - spacermilli\n  lex = not re.findall('SPEAKER_01', string=l)\n  dzList.append([start, end, lex])\n\nprint(*dzList[:10], sep='\\n')\n[33, 34812, True]\n[36112, 49375, True]\n[49865, 88205, False]\n[89285, 90939, False]\n[92862, 114496, True]\n[114496, 122124, False]\n[122191, 149596, False]\n[150018, 177119, False]\n[178317, 192037, False]\n[193015, 205385, True]\nNow we have the diarization data in a list. The first two numbers are the start and end time of the speaker segment in milliseconds. The third number is a boolean that tells us if the speaker is Lex or not.\n\nPreparing audio file from the diarization\nNext, we will attach the audio segements according to the diarization, with a spacer as the delimiter.\n\nfrom pydub import AudioSegment\nimport re \n\nsounds = spacer\nsegments = []\n\ndz = open('diarization.txt').read().splitlines()\nfor l in dz:\n  start, end =  tuple(re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=l))\n  start = int(millisec(start)) #milliseconds\n  end = int(millisec(end))  #milliseconds\n  \n  segments.append(len(sounds))\n  sounds = sounds.append(audio[start:end], crossfade=0)\n  sounds = sounds.append(spacer, crossfade=0)\n\nsounds.export(\"dz.wav\", format=\"wav\") #Exports to a wav file in the current path.\nprint(segments[:8])\n[2000, 38779, 54042, 94382, 98036, 121670, 131297, 160702]\nTranscription with Whisper\nNext, we will use Whisper to transcribe the different segments of the audio file. Important: There is a version conflict with pyannote.audio resulting in an error. Our workaround is to first run Pyannote and then whisper. You can safely ignore the error.\n\nInstalling Open AI Whisper.\n\n!pip install git+https://github.com/openai/whisper.git \nRunning Open AI whisper on the prepared audio file. It writes the transcription into a file. You can adjust the model size to your needs. You can find all models on the model card on Github.\n\n!whisper dz.wav --language en --model base\n[00:00.000 --> 00:04.720]  The following is a conversation with Yann LeCun,\n[00:04.720 --> 00:06.560]  his second time on the podcast.\n[00:06.560 --> 00:11.160]  He is the chief AI scientist at Meta, formerly Facebook,\n[00:11.160 --> 00:15.040]  professor at NYU, touring award winner,\n[00:15.040 --> 00:17.600]  one of the seminal figures in the history\n[00:17.600 --> 00:20.460]  of machine learning and artificial intelligence,\n...\nIn order to work with .vtt files, we need to install the webvtt-py library.\n\n!pip install -U webvtt-py\nLets take a look at the data:\n\nimport webvtt\n\ncaptions = [[(int)(millisec(caption.start)), (int)(millisec(caption.end)),  caption.text] for caption in webvtt.read('dz.wav.vtt')]\nprint(*captions[:8], sep='\\n')\n[0, 4720, 'The following is a conversation with Yann LeCun,']\n[4720, 6560, 'his second time on the podcast.']\n[6560, 11160, 'He is the chief AI scientist at Meta, formerly Facebook,']\n[11160, 15040, 'professor at NYU, touring award winner,']\n[15040, 17600, 'one of the seminal figures in the history']\n[17600, 20460, 'of machine learning and artificial intelligence,']\n[20460, 23940, 'and someone who is brilliant and opinionated']\n[23940, 25400, 'in the best kind of way,']\n...\nMatching the Transcriptions and the Diarizations\nNext, we will match each transcribtion line to some diarizations, and display everything by generating a HTML file. To get the correct timing, we should take care of the parts in original audio that were in no diarization segment. We append a new div for each segment in our audio.\n\n# we need this fore our HTML file (basicly just some styling)\npreS = '<!DOCTYPE html>\\n<html lang=\"en\">\\n  <head>\\n    <meta charset=\"UTF-8\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\\n    <title>Lexicap</title>\\n    <style>\\n        body {\\n            font-family: sans-serif;\\n            font-size: 18px;\\n            color: #111;\\n            padding: 0 0 1em 0;\\n        }\\n        .l {\\n          color: #050;\\n        }\\n        .s {\\n            display: inline-block;\\n        }\\n        .e {\\n            display: inline-block;\\n        }\\n        .t {\\n            display: inline-block;\\n        }\\n        #player {\\n\\t\\tposition: sticky;\\n\\t\\ttop: 20px;\\n\\t\\tfloat: right;\\n\\t}\\n    </style>\\n  </head>\\n  <body>\\n    <h2>Yann LeCun: Dark Matter of Intelligence and Self-Supervised Learning | Lex Fridman Podcast #258</h2>\\n  <div  id=\"player\"></div>\\n    <script>\\n      var tag = document.createElement(\\'script\\');\\n      tag.src = \"https://www.youtube.com/iframe_api\";\\n      var firstScriptTag = document.getElementsByTagName(\\'script\\')[0];\\n      firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\\n      var player;\\n      function onYouTubeIframeAPIReady() {\\n        player = new YT.Player(\\'player\\', {\\n          height: \\'210\\',\\n          width: \\'340\\',\\n          videoId: \\'SGzMElJ11Cc\\',\\n        });\\n      }\\n      function setCurrentTime(timepoint) {\\n        player.seekTo(timepoint);\\n   player.playVideo();\\n   }\\n    </script><br>\\n'\npostS = '\\t</body>\\n</html>'\n\nfrom datetime import timedelta\n\nhtml = list(preS)\n\nfor i in range(len(segments)):\n  idx = 0\n  for idx in range(len(captions)):\n    if captions[idx][0] >= (segments[i] - spacermilli):\n      break;\n  \n  while (idx < (len(captions))) and ((i == len(segments) - 1) or (captions[idx][1] < segments[i+1])):\n    c = captions[idx]  \n    \n    start = dzList[i][0] + (c[0] -segments[i])\n\n    if start < 0: \n      start = 0\n    idx += 1\n\n    start = start / 1000.0\n    startStr = '{0:02d}:{1:02d}:{2:02.2f}'.format((int)(start // 3600), \n                                            (int)(start % 3600 // 60), \n                                            start % 60)\n    \n    html.append('\\t\\t\\t<div class=\"c\">\\n')\n    html.append(f'\\t\\t\\t\\t<a class=\"l\" href=\"#{startStr}\" id=\"{startStr}\">link</a> |\\n')\n    html.append(f'\\t\\t\\t\\t<div class=\"s\"><a href=\"javascript:void(0);\" onclick=setCurrentTime({int(start)})>{startStr}</a></div>\\n')\n    html.append(f'\\t\\t\\t\\t<div class=\"t\">{\"[Lex]\" if dzList[i][2] else \"[Yann]\"} {c[2]}</div>\\n')\n    html.append('\\t\\t\\t</div>\\n\\n')\n\nhtml.append(postS)\ns = \"\".join(html)\n\nwith open(\"lexicap.html\", \"w\") as text_file:\n    text_file.write(s)\nprint(s)\n",
      "created_at": "2023-07-25T16:24:17.714587",
      "sequence_order": 0,
      "parent_id": "3442e6c8-137f-4ef4-8503-1f04a5a67a22",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "01970411-c7c9-450c-83a3-36d68588a8c6",
      "role": "assistant",
      "content": "Sure! Here's a Python script that takes the content and workflow described above and turns it into a functional Pythonic program:\n\n```python\nimport os\nimport re\nimport webvtt\nfrom pydub import AudioSegment\nfrom datetime import timedelta\nfrom subprocess import run\n\ndef millisec(timeStr):\n    spl = timeStr.split(\":\")\n    s = (int)((int(spl[0]) * 60 * 60 + int(spl[1]) * 60 + float(spl[2])) * 1000)\n    return s\n\ndef download_and_extract_audio():\n    run([\"pip\", \"install\", \"-U\", \"yt-dlp\"])\n    run([\"pip\", \"install\", \"pydub\"])\n    run([\"wget\", \"-O\", \"-\", \"-q\", \"https://github.com/yt-dlp/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-linux64-gpl.tar.xz\",\n         \"|\", \"xz\", \"-qdc\", \"|\", \"tar\", \"-x\"])\n    run([\"yt-dlp\", \"-xv\", \"--ffmpeg-location\", \"ffmpeg-master-latest-linux64-gpl/bin\",\n         \"--audio-format\", \"wav\", \"-o\", \"download.wav\", \"--\", \"https://youtu.be/SGzMElJ11Cc\"])\n\n    t1 = 0  # works in seconds\n    t2 = 20 * 60  # 20 minutes\n\n    newAudio = AudioSegment.from_wav(\"download.wav\")\n    a = newAudio[t1 * 1000:t2 * 1000]\n    a.export(\"audio.wav\", format=\"wav\")\n    print(\"Downloaded and extracted the audio successfully!\")\n\ndef diarization():\n    run([\"pip\", \"install\", \"pyannote.audio\"])\n    from pyannote.audio import Pipeline\n\n    pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization')\n    DEMO_FILE = {'uri': 'blabal', 'audio': 'audio.wav'}\n    dz = pipeline(DEMO_FILE)\n\n    with open(\"diarization.txt\", \"w\") as text_file:\n        text_file.write(str(dz))\n\n    dzList = []\n    for l in open('diarization.txt').read().splitlines():\n        start, end = tuple(re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=l))\n        start = millisec(start) - spacermilli\n        end = millisec(end) - spacermilli\n        lex = not re.findall('SPEAKER_01', string=l)\n        dzList.append([start, end, lex])\n\n    print(\"Diarization completed!\")\n    return dzList\n\ndef prepare_audio_file(dzList):\n    from pydub import AudioSegment\n    import re\n\n    sounds = spacer\n    segments = []\n\n    for l in open('diarization.txt').read().splitlines():\n        start, end = tuple(re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=l))\n        start = int(millisec(start))  # milliseconds\n        end = int(millisec(end))  # milliseconds\n\n        segments.append(len(sounds))\n        sounds = sounds.append(audio[start:end], crossfade=0)\n        sounds = sounds.append(spacer, crossfade=0)\n\n    sounds.export(\"dz.wav\", format=\"wav\")  # Exports to a wav file in the current path.\n    print(\"Prepared audio file with diarization segments!\")\n\ndef transcription_with_whisper():\n    run([\"pip\", \"install\", \"git+https://github.com/openai/whisper.git\"])\n    run([\"whisper\", \"dz.wav\", \"--language\", \"en\", \"--model\", \"base\"])\n\n    print(\"Whisper transcription completed!\")\n\ndef match_transcriptions_and_diarizations(dzList):\n    spacermilli = 500\n\n    captions = [[(int)(millisec(caption.start)), (int)(millisec(caption.end)), caption.text]\n                for caption in webvtt.read('dz.wav.vtt')]\n\n    html = []\n    for i in range(len(segments)):\n        idx = 0\n        for idx in range(len(captions)):\n            if captions[idx][0] >= (segments[i] - spacermilli):\n                break\n\n        while (idx < (len(captions))) and ((i == len(segments) - 1) or (captions[idx][1] < segments[i + 1])):\n            c = captions[idx]\n\n            start = dzList[i][0] + (c[0] - segments[i])\n\n            if start < 0:\n                start = 0\n            idx += 1\n\n            start = start / 1000.0\n            startStr = '{0:02d}:{1:02d}:{2:02.2f}'.format((int)(start // 3600),\n                                                          (int)(start % 3600 // 60),\n                                                          start % 60)\n\n            html.append('\\t\\t\\t<div class=\"c\">\\n')\n            html.append(f'\\t\\t\\t\\t<a class=\"l\" href=\"#{startStr}\" id=\"{startStr}\">link</a> |\\n')\n            html.append(f'\\t\\t\\t\\t<div class=\"s\"><a href=\"javascript:void(0);\" onclick=setCurrentTime({int(start)})>{startStr}</a></div>\\n')\n            html.append(f'\\t\\t\\t\\t<div class=\"t\">{\"[Lex]\" if dzList[i][2] else \"[Yann]\"} {c[2]}</div>\\n')\n            html.append('\\t\\t\\t</div>\\n\\n')\n\n    return html\n\n\nif __name__ == \"__main__\":\n    spacer = AudioSegment.silent(duration=500)  # Milliseconds of silence to be inserted between segments\n    spacermilli = 500\n\n    # Step 1: Download and extract audio\n    download_and_extract_audio()\n\n    # Step 2: Diarization\n    dzList = diarization()\n\n    # Step 3: Prepare audio file with diarization segments\n    prepare_audio_file(dzList)\n\n    # Step 4: Transcription with Whisper\n    transcription_with_whisper()\n\n    # Step 5: Match transcriptions and diarizations and generate HTML file\n    html = match_transcriptions_and_diarizations(dzList)\n\n    preS = '''<!DOCTYPE html>\\n<html lang=\"en\">\\n  <head>\\n    <meta charset=\"UTF-8\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\\n    <title>Lexicap</title>\\n    <style>\\n        body {\\n            font-family: sans-serif;\\n            font-size: 18px;\\n            color: #111;\\n            padding: 0 0 1em 0;\\n        }\\n        .l {\\n          color: #050;\\n        }\\n        .s {\\n            display: inline-block;\\n        }\\n        .e {\\n            display: inline-block;\\n        }\\n        .t {\\n            display: inline-block;\\n        }\\n        #player {\\n\\t\\tposition:",
      "created_at": "2023-07-25T16:24:42.972356",
      "sequence_order": 1,
      "parent_id": "aaa2dc68-a490-406d-ab49-418e8550fad2",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": false
      }
    }
  ]
}