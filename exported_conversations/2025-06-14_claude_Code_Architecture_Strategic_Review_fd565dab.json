{
  "id": "fd565dab-d31f-410e-9247-b022033744f8",
  "source": "claude",
  "title": "Code Architecture Strategic Review",
  "created_at": "2025-06-14T22:13:50.425994Z",
  "updated_at": "2025-06-14T22:18:46.217644Z",
  "summary": "",
  "message_count": 2,
  "messages": [
    {
      "id": "ee3ca7ac-47a4-4cd4-ba7c-0cff2d5ec4c4",
      "role": "human",
      "content": "Review the code for five pros and five cons and if it helps us achieve our goal and where there are duplications and redundant implmentations. make a strategic plan to fix it and then implement your fixes with returning any changed files in full and then end with opportunities for improvement, what you see that is interesting and how we can leverage and make the program better",
      "created_at": "2025-06-14T22:13:52.518858Z",
      "sequence_order": 0,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2025-06-14T22:13:52.514951Z",
            "stop_timestamp": "2025-06-14T22:13:52.514951Z",
            "flags": null,
            "type": "text",
            "text": "Review the code for five pros and five cons and if it helps us achieve our goal and where there are duplications and redundant implmentations. make a strategic plan to fix it and then implement your fixes with returning any changed files in full and then end with opportunities for improvement, what you see that is interesting and how we can leverage and make the program better",
            "citations": []
          }
        ],
        "updated_at": "2025-06-14T22:13:52.518858Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "55dbb6d0-49cf-4e6d-9838-9b187984f45b",
      "role": "assistant",
      "content": "This is a large Knowledge Garden codebase with multiple services. Let me analyze it thoroughly to identify pros, cons, duplications, and create a strategic improvement plan.\n\n## Analysis Overview\n\nThe codebase appears to be a microservices architecture for a \"Knowledge Garden\" system that:\n1. Extracts patterns, insights, and concepts from text\n2. Uses embeddings for semantic analysis\n3. Has a Parliament service for decision-making\n4. Can generate various artifacts\n5. Has both regex-based and LLM-based extraction approaches\n\nLet me go through the code systematically:\n\n### Key Components:\n1. **Live Demo** - A Flask web app for demonstrating extraction capabilities\n2. **Extraction Service** - FastAPI service for extracting knowledge from text\n3. **Parliament Service** - Decision-making service using LLMs\n4. **Shared Models** - Common data models across services\n5. **Configuration** - Various config files for patterns, prompts, etc.\n\n### Major Issues I'm Seeing:\n\n1. **Massive Duplication**: \n   - Multiple implementations of extraction pipelines\n   - Duplicate LLM client implementations\n   - Duplicate embedding generators\n   - Multiple pattern extraction approaches\n\n2. **Inconsistent Architecture**:\n   - Some services use shared models, others don't\n   - Mixed async/sync patterns\n   - Different configuration approaches\n\n3. **Circular Dependencies**:\n   - Live demo imports from extraction service\n   - Services import from each other\n\n4. **Over-Engineering**:\n   - Too many abstraction layers\n   - Complex pipeline stages that could be simplified\n   - Multiple ways to do the same thing\n\n5. **Technical Debt**:\n   - Deprecated code still present\n   - Multiple TODO comments\n   - Incomplete implementations\n\nLet me provide a detailed analysis with pros, cons, and a strategic plan.# Knowledge Garden Code Review\n\n## Five Pros\n\n1. **Comprehensive Pattern Extraction**: The system supports multiple pattern types (CAPs, insights, tensions, questions, values) with both regex-based and LLM-based approaches, providing flexibility and depth in knowledge extraction.\n\n2. **Tiered LLM Architecture**: Smart use of different LLM tiers (Gemma/Haiku/Sonnet/Opus) for tasks of varying complexity, optimizing cost and performance.\n\n3. **Embedding-Based Semantic Analysis**: Good integration of embeddings for concept similarity, clustering, and semantic search capabilities.\n\n4. **Modular Service Architecture**: Clear separation between extraction, parliament (decision-making), and artifact generation services.\n\n5. **Rich Ontology Support**: Well-defined ontology with RDF/SPARQL integration for knowledge graph representation.\n\n## Five Cons\n\n1. **Severe Code Duplication**:\n   - Multiple LLM client implementations (`live_demo/llm_client.py` vs shared services)\n   - Duplicate extraction pipelines (at least 3 different implementations)\n   - Redundant embedding generators across services\n\n2. **Architectural Inconsistency**:\n   - Mix of Flask (live_demo) and FastAPI (services)\n   - Inconsistent async patterns\n   - Different configuration management approaches\n\n3. **Circular Dependencies**:\n   - Live demo imports from extraction service\n   - Services depend on each other in complex ways\n   - Shared modules not properly utilized\n\n4. **Over-Engineering**:\n   - Too many abstraction layers in pipeline stages\n   - Complex inheritance hierarchies that add little value\n   - Multiple ways to achieve the same goal\n\n5. **Incomplete Implementation**:\n   - Parliament service is mostly a placeholder\n   - Many TODO comments and stub methods\n   - Missing error handling in critical paths\n\n## Strategic Plan to Fix\n\n### Phase 1: Consolidate Core Components\n\n1. **Unify LLM Client**\n   - Keep only the shared LLM service implementation\n   - Remove duplicate implementations\n   - Standardize on async patterns\n\n2. **Consolidate Extraction Pipeline**\n   - Create one unified extraction pipeline\n   - Remove redundant implementations\n   - Simplify stage architecture\n\n3. **Centralize Configuration**\n   - Move all configuration to shared module\n   - Use consistent environment variable patterns\n   - Remove duplicate config files\n\n### Phase 2: Fix Architecture\n\n1. **Resolve Circular Dependencies**\n   - Move shared functionality to shared modules\n   - Ensure services only depend on shared, not each other\n   - Separate live demo from core services\n\n2. **Standardize Service Patterns**\n   - All services use FastAPI\n   - Consistent async/await patterns\n   - Unified error handling\n\n3. **Simplify Abstractions**\n   - Reduce pipeline stage complexity\n   - Remove unnecessary inheritance\n   - Focus on composition over inheritance\n\n### Phase 3: Complete Implementation\n\n1. **Finish Parliament Service**\n   - Implement actual decision-making logic\n   - Add proper caching\n   - Complete all endpoints\n\n2. **Add Missing Features**\n   - Error recovery\n   - Monitoring/logging\n   - Health checks\n\n3. **Improve Testing**\n   - Add unit tests\n   - Integration tests\n   - Performance benchmarks\n\n## Implementation\n\nHere are the key files that need to be changed:\n\n<artifacts>\n<artifact identifier=\"unified-llm-service\" type=\"application/vnd.ant.code\" language=\"python\" title=\"Unified LLM Service\">\n\"\"\"\nUnified LLM Service for Knowledge Garden.\n\nThis replaces all duplicate LLM client implementations with a single,\nconsistent service that supports multiple providers and tiers.\n\"\"\"\nimport os\nimport json\nimport logging\nimport asyncio\nfrom typing import Dict, List, Any, Optional, Union\nfrom enum import Enum\nimport aiohttp\n\nlogger = logging.getLogger(__name__)\n\nclass LLMProvider(Enum):\n    \"\"\"Supported LLM providers\"\"\"\n    ANTHROPIC = \"anthropic\"\n    OLLAMA = \"ollama\"\n    OPENAI = \"openai\"\n\nclass LLMTier(Enum):\n    \"\"\"LLM complexity tiers\"\"\"\n    BASIC = \"basic\"      # Simple tasks (Ollama/Gemma)\n    STANDARD = \"standard\" # Standard tasks (Haiku)\n    ADVANCED = \"advanced\" # Complex tasks (Sonnet)\n    EXTENDED = \"extended\" # Deep analysis (Opus)\n\nclass LLMConfig:\n    \"\"\"LLM tier configurations\"\"\"\n    TIERS = {\n        LLMTier.BASIC: {\n            \"provider\": LLMProvider.OLLAMA,\n            \"model\": os.getenv(\"BASIC_LLM_MODEL\", \"gemma:2b\"),\n            \"api_base\": os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\") + \"/api\",\n            \"timeout\": 10,\n            \"max_tokens\": 1024,\n            \"temperature\": 0.2\n        },\n        LLMTier.STANDARD: {\n            \"provider\": LLMProvider.ANTHROPIC,\n            \"model\": os.getenv(\"STANDARD_LLM_MODEL\", \"claude-3-haiku-20240307\"),\n            \"timeout\": 15,\n            \"max_tokens\": 2048,\n            \"temperature\": 0.3\n        },\n        LLMTier.ADVANCED: {\n            \"provider\": LLMProvider.ANTHROPIC,\n            \"model\": os.getenv(\"ADVANCED_LLM_MODEL\", \"claude-3-sonnet-20240229\"),\n            \"timeout\": 30,\n            \"max_tokens\": 4096,\n            \"temperature\": 0.4\n        },\n        LLMTier.EXTENDED: {\n            \"provider\": LLMProvider.ANTHROPIC,\n            \"model\": os.getenv(\"EXTENDED_LLM_MODEL\", \"claude-3-opus-20240229\"),\n            \"timeout\": 60,\n            \"max_tokens\": 8192,\n            \"temperature\": 0.5\n        }\n    }\n\nclass UnifiedLLMService:\n    \"\"\"Unified LLM service supporting multiple providers and tiers.\"\"\"\n    \n    def __init__(self, default_tier: LLMTier = LLMTier.STANDARD):\n        \"\"\"Initialize the LLM service with a default tier.\"\"\"\n        self.default_tier = default_tier\n        self._validate_api_keys()\n        \n    def _validate_api_keys(self):\n        \"\"\"Validate required API keys are present.\"\"\"\n        if not os.getenv(\"ANTHROPIC_API_KEY\"):\n            logger.warning(\"ANTHROPIC_API_KEY not found. Anthropic models will not work.\")\n    \n    async def generate(\n        self, \n        prompt: str,\n        tier: Optional[LLMTier] = None,\n        system_prompt: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        response_format: Optional[str] = None\n    ) -> Union[str, Dict[str, Any]]:\n        \"\"\"\n        Generate a response using the specified tier.\n        \n        Args:\n            prompt: The user prompt\n            tier: LLM tier to use (defaults to service default)\n            system_prompt: Optional system prompt\n            temperature: Optional temperature override\n            max_tokens: Optional max tokens override\n            response_format: Optional response format (\"json\" for JSON responses)\n            \n        Returns:\n            Generated response (string or parsed JSON)\n        \"\"\"\n        tier = tier or self.default_tier\n        config = LLMConfig.TIERS[tier]\n        \n        # Override config values if provided\n        temperature = temperature or config[\"temperature\"]\n        max_tokens = max_tokens or config[\"max_tokens\"]\n        \n        provider = config[\"provider\"]\n        \n        if provider == LLMProvider.ANTHROPIC:\n            response = await self._generate_anthropic(\n                prompt, system_prompt, temperature, max_tokens, config\n            )\n        elif provider == LLMProvider.OLLAMA:\n            response = await self._generate_ollama(\n                prompt, system_prompt, temperature, max_tokens, config\n            )\n        else:\n            raise ValueError(f\"Unsupported provider: {provider}\")\n        \n        # Parse JSON if requested\n        if response_format == \"json\":\n            return self._parse_json_response(response)\n        \n        return response\n    \n    async def _generate_anthropic(\n        self,\n        prompt: str,\n        system_prompt: Optional[str],\n        temperature: float,\n        max_tokens: int,\n        config: Dict[str, Any]\n    ) -> str:\n        \"\"\"Generate response using Anthropic API.\"\"\"\n        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n        if not api_key:\n            raise ValueError(\"ANTHROPIC_API_KEY not found\")\n        \n        headers = {\n            \"x-api-key\": api_key,\n            \"anthropic-version\": \"2023-06-01\",\n            \"content-type\": \"application/json\"\n        }\n        \n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        \n        request_body = {\n            \"model\": config[\"model\"],\n            \"messages\": messages,\n            \"max_tokens\": max_tokens,\n            \"temperature\": temperature\n        }\n        \n        if system_prompt:\n            request_body[\"system\"] = system_prompt\n        \n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                \"https://api.anthropic.com/v1/messages\",\n                headers=headers,\n                json=request_body,\n                timeout=aiohttp.ClientTimeout(total=config[\"timeout\"])\n            ) as response:\n                if response.status != 200:\n                    error_text = await response.text()\n                    raise Exception(f\"Anthropic API error: {response.status} - {error_text}\")\n                \n                result = await response.json()\n                return result.get(\"content\", [{\"text\": \"\"}])[0].get(\"text\", \"\")\n    \n    async def _generate_ollama(\n        self,\n        prompt: str,\n        system_prompt: Optional[str],\n        temperature: float,\n        max_tokens: int,\n        config: Dict[str, Any]\n    ) -> str:\n        \"\"\"Generate response using Ollama API.\"\"\"\n        api_base = config.get(\"api_base\", \"http://localhost:11434/api\")\n        \n        request_body = {\n            \"model\": config[\"model\"],\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": {\n                \"temperature\": temperature,\n                \"num_predict\": max_tokens\n            }\n        }\n        \n        if system_prompt:\n            request_body[\"system\"] = system_prompt\n        \n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{api_base}/generate\",\n                json=request_body,\n                timeout=aiohttp.ClientTimeout(total=config[\"timeout\"])\n            ) as response:\n                if response.status != 200:\n                    error_text = await response.text()\n                    raise Exception(f\"Ollama API error: {response.status} - {error_text}\")\n                \n                result = await response.json()\n                return result.get(\"response\", \"\")\n    \n    def _parse_json_response(self, response: str) -> Dict[str, Any]:\n        \"\"\"Extract and parse JSON from response.\"\"\"\n        import re\n        \n        # Try direct parsing first\n        try:\n            return json.loads(response)\n        except:\n            pass\n        \n        # Try to extract JSON from markdown code blocks\n        json_match = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", response)\n        if json_match:\n            try:\n                return json.loads(json_match.group(1))\n            except:\n                pass\n        \n        # Try to find JSON-like structures\n        json_match = re.search(r\"\\{[\\s\\S]*\\}\", response)\n        if json_match:\n            try:\n                return json.loads(json_match.group(0))\n            except:\n                pass\n        \n        # Return empty dict if parsing fails\n        logger.warning(f\"Failed to parse JSON from response: {response[:200]}...\")\n        return {}\n    \n    def get_tier_for_task(self, task_type: str) -> LLMTier:\n        \"\"\"Get recommended tier for a specific task type.\"\"\"\n        task_tiers = {\n            # Basic tasks\n            \"entity_extraction\": LLMTier.BASIC,\n            \"keyword_extraction\": LLMTier.BASIC,\n            \"simple_classification\": LLMTier.BASIC,\n            \n            # Standard tasks\n            \"cap_extraction\": LLMTier.STANDARD,\n            \"insight_extraction\": LLMTier.STANDARD,\n            \"question_extraction\": LLMTier.STANDARD,\n            \"pattern_validation\": LLMTier.STANDARD,\n            \n            # Advanced tasks\n            \"tension_extraction\": LLMTier.ADVANCED,\n            \"relationship_inference\": LLMTier.ADVANCED,\n            \"entity_resolution\": LLMTier.ADVANCED,\n            \"concept_clustering\": LLMTier.ADVANCED,\n            \n            # Extended tasks\n            \"value_extraction\": LLMTier.EXTENDED,\n            \"deep_analysis\": LLMTier.EXTENDED,\n            \"cross_document_synthesis\": LLMTier.EXTENDED\n        }\n        \n        return task_tiers.get(task_type, self.default_tier)\n\n# Global service instance\n_llm_service = None\n\ndef get_llm_service(default_tier: LLMTier = LLMTier.STANDARD) -> UnifiedLLMService:\n    \"\"\"Get or create the global LLM service instance.\"\"\"\n    global _llm_service\n    if _llm_service is None:\n        _llm_service = UnifiedLLMService(default_tier)\n    return _llm_service\n</artifact>\n\n<artifact identifier=\"simplified-extraction-pipeline\" type=\"application/vnd.ant.code\" language=\"python\" title=\"Simplified Extraction Pipeline\">\n\"\"\"\nSimplified Extraction Pipeline for Knowledge Garden.\n\nThis replaces the overly complex multi-stage pipeline with a simpler,\nmore maintainable approach that still supports all features.\n\"\"\"\nimport logging\nimport asyncio\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom datetime import datetime\nfrom dataclasses import dataclass, field\nfrom enum import Enum\n\nfrom shared.services.llm import get_llm_service, LLMTier\nfrom shared.services.embeddings import get_embedding_service\nfrom shared.services.fuseki import get_fuseki_service\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ExtractionConfig:\n    \"\"\"Configuration for extraction pipeline.\"\"\"\n    extract_entities: bool = True\n    extract_caps: bool = True\n    extract_insights: bool = True\n    extract_tensions: bool = True\n    extract_questions: bool = True\n    extract_values: bool = True\n    generate_embeddings: bool = True\n    enable_clustering: bool = False\n    min_cluster_size: int = 5\n    use_parliament: bool = False\n\n@dataclass\nclass ExtractedConcept:\n    \"\"\"A concept extracted from text.\"\"\"\n    text: str\n    type: str\n    confidence: float\n    source: str\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass ExtractionResult:\n    \"\"\"Result of extraction pipeline.\"\"\"\n    source_uri: str\n    timestamp: str\n    duration_seconds: float\n    concepts: List[ExtractedConcept]\n    clusters: List[Dict[str, Any]]\n    stats: Dict[str, int]\n    errors: List[str] = field(default_factory=list)\n\nclass SimplifiedExtractionPipeline:\n    \"\"\"\n    Simplified extraction pipeline that:\n    1. Extracts concepts using tiered LLMs\n    2. Generates embeddings\n    3. Optionally clusters concepts\n    4. Tracks evidence\n    \"\"\"\n    \n    def __init__(self, config: ExtractionConfig = None):\n        \"\"\"Initialize the pipeline with configuration.\"\"\"\n        self.config = config or ExtractionConfig()\n        self.llm_service = get_llm_service()\n        self.embedding_service = get_embedding_service()\n        self.fuseki_service = get_fuseki_service()\n        \n    async def extract(\n        self,\n        text: str,\n        source_uri: str = \"unknown\",\n        context: Optional[str] = None\n    ) -> ExtractionResult:\n        \"\"\"\n        Extract concepts from text.\n        \n        Args:\n            text: Text to extract from\n            source_uri: Source identifier\n            context: Optional context\n            \n        Returns:\n            Extraction result with concepts and statistics\n        \"\"\"\n        start_time = datetime.now()\n        errors = []\n        \n        try:\n            # 1. Extract concepts in parallel\n            extraction_tasks = []\n            \n            if self.config.extract_entities:\n                extraction_tasks.append(self._extract_entities(text))\n            if self.config.extract_caps:\n                extraction_tasks.append(self._extract_caps(text, context))\n            if self.config.extract_insights:\n                extraction_tasks.append(self._extract_insights(text, context))\n            if self.config.extract_tensions:\n                extraction_tasks.append(self._extract_tensions(text, context))\n            if self.config.extract_questions:\n                extraction_tasks.append(self._extract_questions(text))\n            \n            # Run extractions in parallel\n            results = await asyncio.gather(*extraction_tasks, return_exceptions=True)\n            \n            # Collect concepts\n            concepts = []\n            for result in results:\n                if isinstance(result, Exception):\n                    errors.append(str(result))\n                    logger.error(f\"Extraction error: {result}\")\n                else:\n                    concepts.extend(result)\n            \n            # 2. Extract values if we have other concepts\n            if self.config.extract_values and concepts:\n                try:\n                    values = await self._extract_values(text, concepts)\n                    concepts.extend(values)\n                except Exception as e:\n                    errors.append(f\"Value extraction error: {e}\")\n            \n            # 3. Generate embeddings\n            if self.config.generate_embeddings and concepts:\n                await self._generate_embeddings(concepts)\n            \n            # 4. Cluster if enabled\n            clusters = []\n            if self.config.enable_clustering and len(concepts) >= self.config.min_cluster_size:\n                try:\n                    clusters = await self._cluster_concepts(concepts)\n                except Exception as e:\n                    errors.append(f\"Clustering error: {e}\")\n            \n            # 5. Store in knowledge graph (optional)\n            if self.fuseki_service:\n                await self._store_concepts(concepts, source_uri)\n            \n            # Calculate statistics\n            stats = self._calculate_stats(concepts)\n            duration = (datetime.now() - start_time).total_seconds()\n            \n            return ExtractionResult(\n                source_uri=source_uri,\n                timestamp=start_time.isoformat(),\n                duration_seconds=duration,\n                concepts=concepts,\n                clusters=clusters,\n                stats=stats,\n                errors=errors\n            )\n            \n        except Exception as e:\n            logger.error(f\"Pipeline error: {e}\")\n            duration = (datetime.now() - start_time).total_seconds()\n            return ExtractionResult(\n                source_uri=source_uri,\n                timestamp=start_time.isoformat(),\n                duration_seconds=duration,\n                concepts=[],\n                clusters=[],\n                stats={},\n                errors=[str(e)]\n            )\n    \n    async def _extract_entities(self, text: str) -> List[ExtractedConcept]:\n        \"\"\"Extract entities using basic tier.\"\"\"\n        prompt = f\"\"\"\n        Extract named entities from this text. Return only a JSON list of entities.\n        \n        Text: {text}\n        \n        Format each entity as:\n        {{\"text\": \"entity name\", \"type\": \"Person/Organization/Location/Other\"}}\n        \n        Return only the JSON array, no explanation.\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.BASIC,\n            response_format=\"json\"\n        )\n        \n        concepts = []\n        for item in response:\n            if isinstance(item, dict) and \"text\" in item:\n                concepts.append(ExtractedConcept(\n                    text=item[\"text\"],\n                    type=\"entity\",\n                    confidence=0.8,\n                    source=text,\n                    metadata={\"entity_type\": item.get(\"type\", \"Other\")}\n                ))\n        \n        return concepts\n    \n    async def _extract_caps(self, text: str, context: Optional[str]) -> List[ExtractedConcept]:\n        \"\"\"Extract CAPs using standard tier.\"\"\"\n        prompt = f\"\"\"\n        Extract Constitutive Attentional Policies (CAPs) from this text.\n        \n        CAPs are patterns of attention, often in these forms:\n        - \"MOMENTS when [situation] are OPPORTUNITIES for [action]\"\n        - \"CIRCUMSTANCES where [condition]\"\n        - \"CONTEXTS that [enable something]\"\n        \n        Text: {text}\n        {f\"Context: {context}\" if context else \"\"}\n        \n        Return a JSON list where each CAP has:\n        {{\"pattern\": \"full CAP text\", \"context\": \"the situation\", \"action\": \"the opportunity\"}}\n        \n        Only return the JSON array.\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.STANDARD,\n            response_format=\"json\"\n        )\n        \n        concepts = []\n        for item in response:\n            if isinstance(item, dict) and \"pattern\" in item:\n                concepts.append(ExtractedConcept(\n                    text=item[\"pattern\"],\n                    type=\"cap\",\n                    confidence=0.85,\n                    source=text,\n                    metadata={\n                        \"context\": item.get(\"context\", \"\"),\n                        \"action\": item.get(\"action\", \"\")\n                    }\n                ))\n        \n        return concepts\n    \n    async def _extract_insights(self, text: str, context: Optional[str]) -> List[ExtractedConcept]:\n        \"\"\"Extract insights using standard tier.\"\"\"\n        prompt = f\"\"\"\n        Extract insights and realizations from this text.\n        \n        Look for:\n        - \"I realized that...\"\n        - \"I discovered that...\"\n        - \"The key insight is...\"\n        - Learning statements\n        - Aha moments\n        \n        Text: {text}\n        {f\"Context: {context}\" if context else \"\"}\n        \n        Return a JSON list where each insight is:\n        {{\"insight\": \"the realization text\", \"trigger\": \"what led to this insight\"}}\n        \n        Only return the JSON array.\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.STANDARD,\n            response_format=\"json\"\n        )\n        \n        concepts = []\n        for item in response:\n            if isinstance(item, dict) and \"insight\" in item:\n                concepts.append(ExtractedConcept(\n                    text=item[\"insight\"],\n                    type=\"insight\",\n                    confidence=0.85,\n                    source=text,\n                    metadata={\"trigger\": item.get(\"trigger\", \"\")}\n                ))\n        \n        return concepts\n    \n    async def _extract_tensions(self, text: str, context: Optional[str]) -> List[ExtractedConcept]:\n        \"\"\"Extract tensions using advanced tier.\"\"\"\n        prompt = f\"\"\"\n        Extract tensions, trade-offs, and polarities from this text.\n        \n        Look for:\n        - \"tension between X and Y\"\n        - \"balancing X with Y\"\n        - \"X versus Y\"\n        - Trade-offs and dilemmas\n        - Opposing forces\n        \n        Text: {text}\n        {f\"Context: {context}\" if context else \"\"}\n        \n        Return a JSON list where each tension is:\n        {{\"pole1\": \"first side\", \"pole2\": \"second side\", \"description\": \"nature of the tension\"}}\n        \n        Only return the JSON array.\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.ADVANCED,\n            response_format=\"json\"\n        )\n        \n        concepts = []\n        for item in response:\n            if isinstance(item, dict) and \"pole1\" in item and \"pole2\" in item:\n                concepts.append(ExtractedConcept(\n                    text=f\"{item['pole1']} vs {item['pole2']}\",\n                    type=\"tension\",\n                    confidence=0.9,\n                    source=text,\n                    metadata={\n                        \"pole1\": item[\"pole1\"],\n                        \"pole2\": item[\"pole2\"],\n                        \"description\": item.get(\"description\", \"\")\n                    }\n                ))\n        \n        return concepts\n    \n    async def _extract_questions(self, text: str) -> List[ExtractedConcept]:\n        \"\"\"Extract questions using basic tier.\"\"\"\n        prompt = f\"\"\"\n        Extract meaningful questions from this text.\n        Ignore rhetorical questions. Focus on genuine inquiries.\n        \n        Text: {text}\n        \n        Return a JSON list where each item is:\n        {{\"question\": \"the question text\"}}\n        \n        Only return the JSON array.\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.BASIC,\n            response_format=\"json\"\n        )\n        \n        concepts = []\n        for item in response:\n            if isinstance(item, dict) and \"question\" in item:\n                concepts.append(ExtractedConcept(\n                    text=item[\"question\"],\n                    type=\"question\",\n                    confidence=0.8,\n                    source=text\n                ))\n        \n        return concepts\n    \n    async def _extract_values(self, text: str, concepts: List[ExtractedConcept]) -> List[ExtractedConcept]:\n        \"\"\"Extract values using extended tier based on other concepts.\"\"\"\n        # Prepare concept summary\n        caps = [c for c in concepts if c.type == \"cap\"]\n        insights = [c for c in concepts if c.type == \"insight\"]\n        tensions = [c for c in concepts if c.type == \"tension\"]\n        \n        prompt = f\"\"\"\n        Based on this text and the patterns found, identify the underlying values and sources of meaning.\n        \n        Original text: {text}\n        \n        Patterns found:\n        - CAPs: {[c.text for c in caps[:5]]}\n        - Insights: {[c.text for c in insights[:5]]}\n        - Tensions: {[c.text for c in tensions[:5]]}\n        \n        What core values or sources of meaning emerge from these patterns?\n        Look for what matters most to the person based on their attention patterns.\n        \n        Return a JSON list where each value is:\n        {{\"value\": \"name of value\", \"evidence\": \"what reveals this value\", \"strength\": 0.0-1.0}}\n        \n        Only return the JSON array.\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.EXTENDED,\n            response_format=\"json\"\n        )\n        \n        value_concepts = []\n        for item in response:\n            if isinstance(item, dict) and \"value\" in item:\n                value_concepts.append(ExtractedConcept(\n                    text=item[\"value\"],\n                    type=\"value\",\n                    confidence=item.get(\"strength\", 0.9),\n                    source=text,\n                    metadata={\"evidence\": item.get(\"evidence\", \"\")}\n                ))\n        \n        return value_concepts\n    \n    async def _generate_embeddings(self, concepts: List[ExtractedConcept]) -> None:\n        \"\"\"Generate embeddings for concepts.\"\"\"\n        texts = [c.text for c in concepts]\n        embeddings = await self.embedding_service.batch_generate(texts)\n        \n        for concept, embedding in zip(concepts, embeddings):\n            concept.embedding = embedding.tolist()\n    \n    async def _cluster_concepts(self, concepts: List[ExtractedConcept]) -> List[Dict[str, Any]]:\n        \"\"\"Cluster concepts based on embeddings.\"\"\"\n        if not all(c.embedding for c in concepts):\n            return []\n        \n        try:\n            import numpy as np\n            from sklearn.cluster import DBSCAN\n            \n            # Create embedding matrix\n            embeddings = np.array([c.embedding for c in concepts])\n            \n            # Simple DBSCAN clustering\n            clustering = DBSCAN(eps=0.3, min_samples=2, metric='cosine')\n            labels = clustering.fit_predict(embeddings)\n            \n            # Group by cluster\n            clusters = {}\n            for concept, label in zip(concepts, labels):\n                if label != -1:  # Skip noise\n                    if label not in clusters:\n                        clusters[label] = []\n                    clusters[label].append(concept)\n            \n            # Format clusters\n            cluster_list = []\n            for label, cluster_concepts in clusters.items():\n                cluster_list.append({\n                    \"id\": label,\n                    \"size\": len(cluster_concepts),\n                    \"concepts\": [c.text for c in cluster_concepts],\n                    \"types\": list(set(c.type for c in cluster_concepts))\n                })\n            \n            return cluster_list\n            \n        except Exception as e:\n            logger.error(f\"Clustering error: {e}\")\n            return []\n    \n    async def _store_concepts(self, concepts: List[ExtractedConcept], source_uri: str) -> None:\n        \"\"\"Store concepts in knowledge graph.\"\"\"\n        # TODO: Implement storage logic\n        pass\n    \n    def _calculate_stats(self, concepts: List[ExtractedConcept]) -> Dict[str, int]:\n        \"\"\"Calculate extraction statistics.\"\"\"\n        stats = {\n            \"total_concepts\": len(concepts),\n            \"entities\": len([c for c in concepts if c.type == \"entity\"]),\n            \"caps\": len([c for c in concepts if c.type == \"cap\"]),\n            \"insights\": len([c for c in concepts if c.type == \"insight\"]),\n            \"tensions\": len([c for c in concepts if c.type == \"tension\"]),\n            \"questions\": len([c for c in concepts if c.type == \"question\"]),\n            \"values\": len([c for c in concepts if c.type == \"value\"])\n        }\n        return stats\n\n# Factory function\ndef create_extraction_pipeline(config: Optional[ExtractionConfig] = None) -> SimplifiedExtractionPipeline:\n    \"\"\"Create an extraction pipeline with the given configuration.\"\"\"\n    return SimplifiedExtractionPipeline(config)\n</artifact>\n\n<artifact identifier=\"enhanced-parliament-service\" type=\"application/vnd.ant.code\" language=\"python\" title=\"Enhanced Parliament Service\">\n\"\"\"\nEnhanced Parliament Service for Knowledge Garden.\n\nThis replaces the placeholder Parliament service with actual\ndecision-making capabilities using LLMs.\n\"\"\"\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport logging\nfrom typing import Dict, List, Any, Optional, Union\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\nimport hashlib\nimport json\n\nfrom shared.services.llm import get_llm_service, LLMTier\nfrom shared.services.cache import get_cache_service\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize app\napp = FastAPI(\n    title=\"Knowledge Garden Parliament Service\",\n    version=\"3.0.0\",\n    description=\"Intelligent decision-making service for Knowledge Garden\"\n)\n\n# Add CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Services\nllm_service = get_llm_service()\ncache_service = get_cache_service()\n\n# Request/Response Models\nclass DecisionRequest(BaseModel):\n    \"\"\"Base request for decisions.\"\"\"\n    decision_type: str\n    context: Dict[str, Any]\n    options: Optional[Dict[str, Any]] = None\n    use_cache: bool = True\n\nclass DecisionResponse(BaseModel):\n    \"\"\"Base response for decisions.\"\"\"\n    decision_id: str\n    decision_type: str\n    result: Dict[str, Any]\n    confidence: float\n    reasoning: str\n    cached: bool = False\n    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())\n\nclass EntityResolutionRequest(BaseModel):\n    \"\"\"Request for entity resolution.\"\"\"\n    entity: Dict[str, Any]\n    candidates: List[Dict[str, Any]]\n    context: Optional[str] = None\n\nclass ClusterLabelingRequest(BaseModel):\n    \"\"\"Request for cluster labeling.\"\"\"\n    clusters: List[Dict[str, Any]]\n    \nclass ArtifactTypeRequest(BaseModel):\n    \"\"\"Request for artifact type selection.\"\"\"\n    content: str\n    available_types: List[str]\n    user_preference: Optional[str] = None\n    metadata: Optional[Dict[str, Any]] = None\n\nclass PatternValidationRequest(BaseModel):\n    \"\"\"Request for pattern validation.\"\"\"\n    pattern: Dict[str, Any]\n    pattern_type: str\n    context: Optional[str] = None\n\n# Decision Making Functions\nclass ParliamentDecisionMaker:\n    \"\"\"Core decision-making logic for Parliament.\"\"\"\n    \n    def __init__(self):\n        self.llm_service = llm_service\n        self.cache_service = cache_service\n        \n    async def make_decision(\n        self,\n        decision_type: str,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None,\n        use_cache: bool = True\n    ) -> DecisionResponse:\n        \"\"\"Make a decision based on type and context.\"\"\"\n        # Generate cache key\n        cache_key = self._generate_cache_key(decision_type, context, options)\n        \n        # Check cache\n        if use_cache:\n            cached_result = await self.cache_service.get(cache_key)\n            if cached_result:\n                return DecisionResponse(\n                    decision_id=cache_key,\n                    decision_type=decision_type,\n                    result=cached_result[\"result\"],\n                    confidence=cached_result[\"confidence\"],\n                    reasoning=cached_result[\"reasoning\"],\n                    cached=True\n                )\n        \n        # Route to specific decision handler\n        handlers = {\n            \"entity_resolution\": self._resolve_entity,\n            \"cluster_labeling\": self._label_clusters,\n            \"artifact_type\": self._select_artifact_type,\n            \"pattern_validation\": self._validate_pattern,\n            \"relationship_inference\": self._infer_relationships,\n            \"concept_classification\": self._classify_concept\n        }\n        \n        handler = handlers.get(decision_type)\n        if not handler:\n            raise ValueError(f\"Unknown decision type: {decision_type}\")\n        \n        # Make decision\n        result, confidence, reasoning = await handler(context, options)\n        \n        # Create response\n        response = DecisionResponse(\n            decision_id=cache_key,\n            decision_type=decision_type,\n            result=result,\n            confidence=confidence,\n            reasoning=reasoning\n        )\n        \n        # Cache result\n        if use_cache:\n            await self.cache_service.set(\n                cache_key,\n                {\n                    \"result\": result,\n                    \"confidence\": confidence,\n                    \"reasoning\": reasoning\n                },\n                ttl=3600  # 1 hour\n            )\n        \n        return response\n    \n    async def _resolve_entity(\n        self,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> tuple[Dict[str, Any], float, str]:\n        \"\"\"Resolve if an entity matches any candidates.\"\"\"\n        entity = context[\"entity\"]\n        candidates = context.get(\"candidates\", [])\n        \n        if not candidates:\n            return {\n                \"resolved\": False,\n                \"match_id\": None,\n                \"should_merge\": False\n            }, 1.0, \"No candidates to match against\"\n        \n        # Create prompt\n        prompt = f\"\"\"\n        Determine if this entity matches any of the candidate entities.\n        \n        Entity to resolve:\n        - Label: {entity.get('label', '')}\n        - Type: {entity.get('type', '')}\n        - Context: {entity.get('context', '')}\n        \n        Candidate entities:\n        {json.dumps(candidates, indent=2)}\n        \n        Consider:\n        1. Name variations (abbreviations, aliases)\n        2. Contextual clues\n        3. Entity types\n        4. Associated attributes\n        \n        Return JSON:\n        {{\n            \"matches_candidate\": true/false,\n            \"match_index\": index of matching candidate or -1,\n            \"confidence\": 0.0-1.0,\n            \"reasoning\": \"brief explanation\"\n        }}\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.ADVANCED,\n            response_format=\"json\"\n        )\n        \n        match_index = response.get(\"match_index\", -1)\n        confidence = response.get(\"confidence\", 0.0)\n        reasoning = response.get(\"reasoning\", \"\")\n        \n        result = {\n            \"resolved\": match_index >= 0,\n            \"match_id\": candidates[match_index][\"id\"] if match_index >= 0 else None,\n            \"should_merge\": match_index >= 0 and confidence > 0.8\n        }\n        \n        return result, confidence, reasoning\n    \n    async def _label_clusters(\n        self,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> tuple[Dict[str, Any], float, str]:\n        \"\"\"Generate meaningful labels for clusters.\"\"\"\n        clusters = context.get(\"clusters\", [])\n        \n        if not clusters:\n            return {\"clusters\": []}, 1.0, \"No clusters to label\"\n        \n        # Process each cluster\n        labeled_clusters = []\n        \n        for cluster in clusters:\n            cluster_concepts = cluster.get(\"concepts\", [])[:10]  # Limit to first 10\n            \n            prompt = f\"\"\"\n            Generate a descriptive label for this cluster of concepts:\n            \n            Concepts:\n            {json.dumps(cluster_concepts, indent=2)}\n            \n            The label should:\n            1. Capture the main theme\n            2. Be 2-5 words\n            3. Be specific and meaningful\n            \n            Return JSON:\n            {{\n                \"label\": \"cluster label\",\n                \"keywords\": [\"key\", \"words\"],\n                \"confidence\": 0.0-1.0\n            }}\n            \"\"\"\n            \n            response = await self.llm_service.generate(\n                prompt,\n                tier=LLMTier.STANDARD,\n                response_format=\"json\"\n            )\n            \n            labeled_clusters.append({\n                \"id\": cluster.get(\"id\"),\n                \"label\": response.get(\"label\", f\"Cluster {cluster.get('id')}\"),\n                \"keywords\": response.get(\"keywords\", []),\n                \"size\": cluster.get(\"size\", len(cluster_concepts)),\n                \"confidence\": response.get(\"confidence\", 0.7)\n            })\n        \n        avg_confidence = sum(c[\"confidence\"] for c in labeled_clusters) / len(labeled_clusters)\n        \n        return {\n            \"clusters\": labeled_clusters\n        }, avg_confidence, f\"Labeled {len(clusters)} clusters\"\n    \n    async def _select_artifact_type(\n        self,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> tuple[Dict[str, Any], float, str]:\n        \"\"\"Select the best artifact type for content.\"\"\"\n        content = context.get(\"content\", \"\")\n        available_types = context.get(\"available_types\", [])\n        user_preference = context.get(\"user_preference\")\n        \n        prompt = f\"\"\"\n        Select the best artifact type for this content:\n        \n        Content: {content[:500]}...\n        \n        Available types: {available_types}\n        User preference: {user_preference or \"None\"}\n        \n        Consider:\n        1. Content structure and purpose\n        2. Information density\n        3. Target audience\n        4. User preference (if any)\n        \n        Return JSON:\n        {{\n            \"selected_type\": \"chosen type\",\n            \"alternatives\": [\"other viable options\"],\n            \"confidence\": 0.0-1.0,\n            \"key_factors\": [\"reasons for selection\"]\n        }}\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.STANDARD,\n            response_format=\"json\"\n        )\n        \n        return {\n            \"selected_type\": response.get(\"selected_type\", available_types[0]),\n            \"alternatives\": response.get(\"alternatives\", []),\n            \"key_factors\": response.get(\"key_factors\", [])\n        }, response.get(\"confidence\", 0.8), \"Type selected based on content analysis\"\n    \n    async def _validate_pattern(\n        self,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> tuple[Dict[str, Any], float, str]:\n        \"\"\"Validate if a pattern is high quality.\"\"\"\n        pattern = context.get(\"pattern\", {})\n        pattern_type = context.get(\"pattern_type\", \"\")\n        \n        prompt = f\"\"\"\n        Validate this {pattern_type} pattern:\n        \n        Pattern: {json.dumps(pattern, indent=2)}\n        \n        Criteria:\n        1. Clarity and specificity\n        2. Meaningful content\n        3. Proper structure for pattern type\n        4. Actionable or insightful\n        \n        Return JSON:\n        {{\n            \"is_valid\": true/false,\n            \"quality_score\": 0.0-1.0,\n            \"issues\": [\"list of issues if any\"],\n            \"suggestions\": [\"improvements if needed\"]\n        }}\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.STANDARD,\n            response_format=\"json\"\n        )\n        \n        is_valid = response.get(\"is_valid\", False)\n        quality_score = response.get(\"quality_score\", 0.0)\n        \n        return {\n            \"is_valid\": is_valid,\n            \"quality_score\": quality_score,\n            \"issues\": response.get(\"issues\", []),\n            \"suggestions\": response.get(\"suggestions\", [])\n        }, quality_score, f\"Pattern {'validated' if is_valid else 'rejected'}\"\n    \n    async def _infer_relationships(\n        self,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> tuple[Dict[str, Any], float, str]:\n        \"\"\"Infer relationships between concepts.\"\"\"\n        concepts = context.get(\"concepts\", [])\n        \n        if len(concepts) < 2:\n            return {\"relationships\": []}, 1.0, \"Need at least 2 concepts\"\n        \n        # Limit to first 10 concepts for efficiency\n        concepts = concepts[:10]\n        \n        prompt = f\"\"\"\n        Infer relationships between these concepts:\n        \n        Concepts:\n        {json.dumps(concepts, indent=2)}\n        \n        Common relationship types:\n        - causes/caused_by\n        - enables/enabled_by\n        - contains/part_of\n        - contradicts/supports\n        - transforms_into\n        - depends_on\n        \n        Return JSON:\n        {{\n            \"relationships\": [\n                {{\n                    \"source\": \"concept1\",\n                    \"target\": \"concept2\",\n                    \"type\": \"relationship_type\",\n                    \"confidence\": 0.0-1.0\n                }}\n            ]\n        }}\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.ADVANCED,\n            response_format=\"json\"\n        )\n        \n        relationships = response.get(\"relationships\", [])\n        avg_confidence = sum(r.get(\"confidence\", 0) for r in relationships) / len(relationships) if relationships else 0\n        \n        return {\n            \"relationships\": relationships\n        }, avg_confidence, f\"Inferred {len(relationships)} relationships\"\n    \n    async def _classify_concept(\n        self,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> tuple[Dict[str, Any], float, str]:\n        \"\"\"Classify a concept into predefined categories.\"\"\"\n        concept = context.get(\"concept\", {})\n        categories = context.get(\"categories\", [])\n        \n        prompt = f\"\"\"\n        Classify this concept into the most appropriate category:\n        \n        Concept: {json.dumps(concept, indent=2)}\n        \n        Available categories: {categories}\n        \n        Return JSON:\n        {{\n            \"category\": \"selected category\",\n            \"confidence\": 0.0-1.0,\n            \"secondary_categories\": [\"other applicable categories\"]\n        }}\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.STANDARD,\n            response_format=\"json\"\n        )\n        \n        return {\n            \"category\": response.get(\"category\", categories[0] if categories else \"unknown\"),\n            \"secondary_categories\": response.get(\"secondary_categories\", [])\n        }, response.get(\"confidence\", 0.8), \"Concept classified\"\n    \n    def _generate_cache_key(\n        self,\n        decision_type: str,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> str:\n        \"\"\"Generate a cache key for a decision.\"\"\"\n        key_data = {\n            \"type\": decision_type,\n            \"context\": context,\n            \"options\": options or {}\n        }\n        key_str = json.dumps(key_data, sort_keys=True)\n        return hashlib.md5(key_str.encode()).hexdigest()\n\n# Initialize decision maker\ndecision_maker = ParliamentDecisionMaker()\n\n# API Endpoints\n@app.get(\"/health\")\nasync def health():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"service\": \"parliament\",\n        \"version\": \"3.0.0\"\n    }\n\n@app.post(\"/decisions/generic\", response_model=DecisionResponse)\nasync def make_generic_decision(request: DecisionRequest):\n    \"\"\"Make a generic decision.\"\"\"\n    try:\n        response = await decision_maker.make_decision(\n            request.decision_type,\n            request.context,\n            request.options,\n            request.use_cache\n        )\n        return response\n    except Exception as e:\n        logger.error(f\"Decision error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/decisions/entity\")\nasync def resolve_entity(request: EntityResolutionRequest):\n    \"\"\"Resolve entity against candidates.\"\"\"\n    try:\n        response = await decision_maker.make_decision(\n            \"entity_resolution\",\n            {\n                \"entity\": request.entity,\n                \"candidates\": request.candidates,\n                \"context\": request.context\n            }\n        )\n        return response.result\n    except Exception as e:\n        logger.error(f\"Entity resolution error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/decisions/clusters\")\nasync def label_clusters(request: ClusterLabelingRequest):\n    \"\"\"Generate labels for clusters.\"\"\"\n    try:\n        response = await decision_maker.make_decision(\n            \"cluster_labeling\",\n            {\"clusters\": request.clusters}\n        )\n        return response.result\n    except Exception as e:\n        logger.error(f\"Cluster labeling error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/decisions/artifact_type\")\nasync def select_artifact_type(request: ArtifactTypeRequest):\n    \"\"\"Select best artifact type for content.\"\"\"\n    try:\n        response = await decision_maker.make_decision(\n            \"artifact_type\",\n            {\n                \"content\": request.content,\n                \"available_types\": request.available_types,\n                \"user_preference\": request.user_preference,\n                \"metadata\": request.metadata\n            }\n        )\n        return response.result\n    except Exception as e:\n        logger.error(f\"Artifact type selection error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/decisions/pattern\")\nasync def validate_pattern(request: PatternValidationRequest):\n    \"\"\"Validate a pattern.\"\"\"\n    try:\n        response = await decision_maker.make_decision(\n            \"pattern_validation\",\n            {\n                \"pattern\": request.pattern,\n                \"pattern_type\": request.pattern_type,\n                \"context\": request.context\n            }\n        )\n        return response.result\n    except Exception as e:\n        logger.error(f\"Pattern validation error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/cache/stats\")\nasync def get_cache_stats():\n    \"\"\"Get cache statistics.\"\"\"\n    stats = await cache_service.get_stats()\n    return stats\n\n@app.post(\"/cache/clear\")\nasync def clear_cache():\n    \"\"\"Clear the decision cache.\"\"\"\n    await cache_service.clear()\n    return {\"status\": \"cache cleared\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8003)\n</artifact>\n\n<artifact identifier=\"shared-services-init\" type=\"application/vnd.ant.code\" language=\"python\" title=\"Shared Services Package\">\n\"\"\"\nShared Services Package for Knowledge Garden.\n\nThis consolidates all shared functionality into a single, well-organized package.\n\"\"\"\n# shared/services/__init__.py\n\nfrom .llm import get_llm_service, LLMTier, UnifiedLLMService\nfrom .embeddings import get_embedding_service, EmbeddingService\nfrom .fuseki import get_fuseki_service, FusekiService\nfrom .parliament import get_parliament_client, ParliamentClient\nfrom .cache import get_cache_service, CacheService\n\n__all__ = [\n    # LLM\n    \"get_llm_service\",\n    \"LLMTier\",\n    \"UnifiedLLMService\",\n    \n    # Embeddings\n    \"get_embedding_service\",\n    \"EmbeddingService\",\n    \n    # Fuseki\n    \"get_fuseki_service\", \n    \"FusekiService\",\n    \n    # Parliament\n    \"get_parliament_client\",\n    \"ParliamentClient\",\n    \n    # Cache\n    \"get_cache_service\",\n    \"CacheService\"\n]\n</artifact>\n\n<artifact identifier=\"simplified-config\" type=\"application/vnd.ant.code\" language=\"python\" title=\"Simplified Configuration\">\n\"\"\"\nSimplified Configuration for Knowledge Garden.\n\nThis replaces multiple configuration approaches with a single,\nconsistent configuration system.\n\"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport yaml\nimport json\n\n@dataclass\nclass ServiceConfig:\n    \"\"\"Configuration for a single service.\"\"\"\n    name: str\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    debug: bool = False\n    data_dir: Path = None\n    \n    def __post_init__(self):\n        if self.data_dir is None:\n            self.data_dir = Path(f\"data/{self.name}\")\n        self.data_dir.mkdir(exist_ok=True, parents=True)\n\n@dataclass\nclass LLMConfig:\n    \"\"\"LLM configuration.\"\"\"\n    tiers: Dict[str, Dict[str, Any]] = field(default_factory=dict)\n    default_tier: str = \"standard\"\n    \n    def __post_init__(self):\n        if not self.tiers:\n            self.tiers = {\n                \"basic\": {\n                    \"provider\": os.getenv(\"BASIC_LLM_PROVIDER\", \"ollama\"),\n                    \"model\": os.getenv(\"BASIC_LLM_MODEL\", \"gemma:2b\"),\n                    \"api_base\": os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\") + \"/api\",\n                    \"timeout\": 10,\n                    \"max_tokens\": 1024,\n                    \"temperature\": 0.2\n                },\n                \"standard\": {\n                    \"provider\": os.getenv(\"STANDARD_LLM_PROVIDER\", \"anthropic\"),\n                    \"model\": os.getenv(\"STANDARD_LLM_MODEL\", \"claude-3-haiku-20240307\"),\n                    \"timeout\": 15,\n                    \"max_tokens\": 2048,\n                    \"temperature\": 0.3\n                },\n                \"advanced\": {\n                    \"provider\": os.getenv(\"ADVANCED_LLM_PROVIDER\", \"anthropic\"),\n                    \"model\": os.getenv(\"ADVANCED_LLM_MODEL\", \"claude-3-sonnet-20240229\"),\n                    \"timeout\": 30,\n                    \"max_tokens\": 4096,\n                    \"temperature\": 0.4\n                },\n                \"extended\": {\n                    \"provider\": os.getenv(\"EXTENDED_LLM_PROVIDER\", \"anthropic\"),\n                    \"model\": os.getenv(\"EXTENDED_LLM_MODEL\", \"claude-3-opus-20240229\"),\n                    \"timeout\": 60,\n                    \"max_tokens\": 8192,\n                    \"temperature\": 0.5\n                }\n            }\n\n@dataclass\nclass EmbeddingConfig:\n    \"\"\"Embedding configuration.\"\"\"\n    model: str = \"jinaai/jina-embeddings-v2-base-en\"\n    batch_size: int = 64\n    cache_enabled: bool = True\n    cache_dir: Path = Path(\"data/embeddings\")\n    \n    def __post_init__(self):\n        self.cache_dir.mkdir(exist_ok=True, parents=True)\n\n@dataclass\nclass FusekiConfig:\n    \"\"\"Fuseki configuration.\"\"\"\n    url: str = os.getenv(\"FUSEKI_URL\", \"http://localhost:3030\")\n    dataset: str = os.getenv(\"FUSEKI_DATASET\", \"kg\")\n    user: str = os.getenv(\"FUSEKI_USER\", \"admin\")\n    password: str = os.getenv(\"FUSEKI_PASSWORD\", \"admin\")\n\n@dataclass\nclass KnowledgeGardenConfig:\n    \"\"\"Main configuration for Knowledge Garden.\"\"\"\n    # Service configs\n    extraction: ServiceConfig = field(default_factory=lambda: ServiceConfig(\"extraction\", port=8001))\n    artifacts: ServiceConfig = field(default_factory=lambda: ServiceConfig(\"artifacts\", port=8002))\n    parliament: ServiceConfig = field(default_factory=lambda: ServiceConfig(\"parliament\", port=8003))\n    \n    # Component configs\n    llm: LLMConfig = field(default_factory=LLMConfig)\n    embedding: EmbeddingConfig = field(default_factory=EmbeddingConfig)\n    fuseki: FusekiConfig = field(default_factory=FusekiConfig)\n    \n    # General settings\n    environment: str = os.getenv(\"ENVIRONMENT\", \"development\")\n    log_level: str = os.getenv(\"LOG_LEVEL\", \"INFO\")\n    \n    @classmethod\n    def from_file(cls, config_file: Path) -> \"KnowledgeGardenConfig\":\n        \"\"\"Load configuration from file.\"\"\"\n        if not config_file.exists():\n            return cls()\n        \n        with open(config_file) as f:\n            if config_file.suffix == \".yaml\":\n                data = yaml.safe_load(f)\n            else:\n                data = json.load(f)\n        \n        return cls(**data)\n    \n    def to_file(self, config_file: Path):\n        \"\"\"Save configuration to file.\"\"\"\n        data = {\n            \"extraction\": {\n                \"name\": self.extraction.name,\n                \"host\": self.extraction.host,\n                \"port\": self.extraction.port,\n                \"debug\": self.extraction.debug\n            },\n            \"artifacts\": {\n                \"name\": self.artifacts.name,\n                \"host\": self.artifacts.host,\n                \"port\": self.artifacts.port,\n                \"debug\": self.artifacts.debug\n            },\n            \"parliament\": {\n                \"name\": self.parliament.name,\n                \"host\": self.parliament.host,\n                \"port\": self.parliament.port,\n                \"debug\": self.parliament.debug\n            },\n            \"llm\": {\n                \"tiers\": self.llm.tiers,\n                \"default_tier\": self.llm.default_tier\n            },\n            \"embedding\": {\n                \"model\": self.embedding.model,\n                \"batch_size\": self.embedding.batch_size,\n                \"cache_enabled\": self.embedding.cache_enabled\n            },\n            \"fuseki\": {\n                \"url\": self.fuseki.url,\n                \"dataset\": self.fuseki.dataset,\n                \"user\": self.fuseki.user,\n                \"password\": self.fuseki.password\n            },\n            \"environment\": self.environment,\n            \"log_level\": self.log_level\n        }\n        \n        with open(config_file, \"w\") as f:\n            if config_file.suffix == \".yaml\":\n                yaml.dump(data, f)\n            else:\n                json.dump(data, f, indent=2)\n\n# Global configuration instance\n_config = None\n\ndef get_config() -> KnowledgeGardenConfig:\n    \"\"\"Get or create the global configuration.\"\"\"\n    global _config\n    if _config is None:\n        config_file = Path(os.getenv(\"KG_CONFIG_FILE\", \"config/knowledge_garden.yaml\"))\n        _config = KnowledgeGardenConfig.from_file(config_file)\n    return _config\n\ndef set_config(config: KnowledgeGardenConfig):\n    \"\"\"Set the global configuration.\"\"\"\n    global _config\n    _config = config\n</artifact>\n</artifacts>\n\n## Opportunities for Improvement\n\n### 1. **Event-Driven Architecture**\n- Implement event streaming (Kafka/RabbitMQ) for real-time processing\n- Enable services to react to extraction events\n- Support workflow orchestration\n\n### 2. **Graph Learning Integration**\n- Add graph neural networks for relationship prediction\n- Implement knowledge graph embeddings\n- Support link prediction and anomaly detection\n\n### 3. **Active Learning Loop**\n- User feedback integration for pattern validation\n- Continuous model improvement\n- Uncertainty-based sampling for human review\n\n### 4. **Multi-Modal Support**\n- Add image analysis capabilities\n- Support audio transcription and analysis\n- Enable video content extraction\n\n### 5. **Advanced Reasoning**\n- Implement causal reasoning chains\n- Add temporal reasoning for evolution tracking\n- Support counterfactual analysis\n\n## Interesting Observations\n\n### 1. **Innovative Pattern Types**\nThe CAPs (Constitutive Attentional Policies) concept is fascinating - it captures attention patterns that reveal values. This could be extended to track attention evolution over time.\n\n### 2. **Parliament Metaphor**\nThe Parliament service concept is clever but underutilized. It could become a powerful multi-agent system where different \"members\" specialize in different types of reasoning.\n\n### 3. **Embedding-First Architecture**\nThe focus on embeddings throughout is good. This could be leveraged further with:\n- Semantic search across all extracted knowledge\n- Concept drift detection\n- Cross-lingual knowledge extraction\n\n### 4. **Rich Ontology**\nThe RDF/SPARQL integration provides a solid foundation for complex queries. This could support:\n- Reasoning over extracted knowledge\n- Consistency checking\n- Knowledge fusion from multiple sources\n\n## How to Make the Program Better\n\n### 1. **Implement Streaming Pipeline**\nReplace batch processing with streaming for real-time extraction and immediate insights.\n\n### 2. **Add Explainability**\n- Show why patterns were extracted\n- Visualize decision paths in Parliament\n- Provide extraction confidence breakdowns\n\n### 3. **Enhance Collaboration**\n- Multi-user support with different expertise levels\n- Collaborative pattern validation\n- Shared knowledge gardens with access control\n\n### 4. **Optimize Performance**\n- Implement proper caching strategies\n- Use vector databases for embedding search\n- Add horizontal scaling capabilities\n\n### 5. **Improve User Experience**\n- Create a unified web interface (not just demo)\n- Add interactive visualizations\n- Implement progressive disclosure of complexity\n\nThe Knowledge Garden has a solid foundation with innovative concepts. By addressing the architectural issues and building on the interesting patterns already present, it could become a powerful system for collaborative knowledge extraction and sense-making.",
      "created_at": "2025-06-14T22:18:32.571031Z",
      "sequence_order": 1,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2025-06-14T22:13:59.326916Z",
            "stop_timestamp": "2025-06-14T22:14:12.762787Z",
            "flags": null,
            "type": "thinking",
            "thinking": "This is a large Knowledge Garden codebase with multiple services. Let me analyze it thoroughly to identify pros, cons, duplications, and create a strategic improvement plan.\n\n## Analysis Overview\n\nThe codebase appears to be a microservices architecture for a \"Knowledge Garden\" system that:\n1. Extracts patterns, insights, and concepts from text\n2. Uses embeddings for semantic analysis\n3. Has a Parliament service for decision-making\n4. Can generate various artifacts\n5. Has both regex-based and LLM-based extraction approaches\n\nLet me go through the code systematically:\n\n### Key Components:\n1. **Live Demo** - A Flask web app for demonstrating extraction capabilities\n2. **Extraction Service** - FastAPI service for extracting knowledge from text\n3. **Parliament Service** - Decision-making service using LLMs\n4. **Shared Models** - Common data models across services\n5. **Configuration** - Various config files for patterns, prompts, etc.\n\n### Major Issues I'm Seeing:\n\n1. **Massive Duplication**: \n   - Multiple implementations of extraction pipelines\n   - Duplicate LLM client implementations\n   - Duplicate embedding generators\n   - Multiple pattern extraction approaches\n\n2. **Inconsistent Architecture**:\n   - Some services use shared models, others don't\n   - Mixed async/sync patterns\n   - Different configuration approaches\n\n3. **Circular Dependencies**:\n   - Live demo imports from extraction service\n   - Services import from each other\n\n4. **Over-Engineering**:\n   - Too many abstraction layers\n   - Complex pipeline stages that could be simplified\n   - Multiple ways to do the same thing\n\n5. **Technical Debt**:\n   - Deprecated code still present\n   - Multiple TODO comments\n   - Incomplete implementations\n\nLet me provide a detailed analysis with pros, cons, and a strategic plan.",
            "summaries": [
              {
                "summary": "Thinking about analyzing code quality, identifying improvements, and strategic refactoring"
              },
              {
                "summary": "Analyzing microservices architecture for strategic improvements"
              },
              {
                "summary": "Dissected complex codebase, identified architectural challenges and improvements"
              }
            ],
            "cut_off": false
          },
          {
            "start_timestamp": "2025-06-14T22:14:13.095663Z",
            "stop_timestamp": "2025-06-14T22:18:32.358177Z",
            "flags": null,
            "type": "text",
            "text": "# Knowledge Garden Code Review\n\n## Five Pros\n\n1. **Comprehensive Pattern Extraction**: The system supports multiple pattern types (CAPs, insights, tensions, questions, values) with both regex-based and LLM-based approaches, providing flexibility and depth in knowledge extraction.\n\n2. **Tiered LLM Architecture**: Smart use of different LLM tiers (Gemma/Haiku/Sonnet/Opus) for tasks of varying complexity, optimizing cost and performance.\n\n3. **Embedding-Based Semantic Analysis**: Good integration of embeddings for concept similarity, clustering, and semantic search capabilities.\n\n4. **Modular Service Architecture**: Clear separation between extraction, parliament (decision-making), and artifact generation services.\n\n5. **Rich Ontology Support**: Well-defined ontology with RDF/SPARQL integration for knowledge graph representation.\n\n## Five Cons\n\n1. **Severe Code Duplication**:\n   - Multiple LLM client implementations (`live_demo/llm_client.py` vs shared services)\n   - Duplicate extraction pipelines (at least 3 different implementations)\n   - Redundant embedding generators across services\n\n2. **Architectural Inconsistency**:\n   - Mix of Flask (live_demo) and FastAPI (services)\n   - Inconsistent async patterns\n   - Different configuration management approaches\n\n3. **Circular Dependencies**:\n   - Live demo imports from extraction service\n   - Services depend on each other in complex ways\n   - Shared modules not properly utilized\n\n4. **Over-Engineering**:\n   - Too many abstraction layers in pipeline stages\n   - Complex inheritance hierarchies that add little value\n   - Multiple ways to achieve the same goal\n\n5. **Incomplete Implementation**:\n   - Parliament service is mostly a placeholder\n   - Many TODO comments and stub methods\n   - Missing error handling in critical paths\n\n## Strategic Plan to Fix\n\n### Phase 1: Consolidate Core Components\n\n1. **Unify LLM Client**\n   - Keep only the shared LLM service implementation\n   - Remove duplicate implementations\n   - Standardize on async patterns\n\n2. **Consolidate Extraction Pipeline**\n   - Create one unified extraction pipeline\n   - Remove redundant implementations\n   - Simplify stage architecture\n\n3. **Centralize Configuration**\n   - Move all configuration to shared module\n   - Use consistent environment variable patterns\n   - Remove duplicate config files\n\n### Phase 2: Fix Architecture\n\n1. **Resolve Circular Dependencies**\n   - Move shared functionality to shared modules\n   - Ensure services only depend on shared, not each other\n   - Separate live demo from core services\n\n2. **Standardize Service Patterns**\n   - All services use FastAPI\n   - Consistent async/await patterns\n   - Unified error handling\n\n3. **Simplify Abstractions**\n   - Reduce pipeline stage complexity\n   - Remove unnecessary inheritance\n   - Focus on composition over inheritance\n\n### Phase 3: Complete Implementation\n\n1. **Finish Parliament Service**\n   - Implement actual decision-making logic\n   - Add proper caching\n   - Complete all endpoints\n\n2. **Add Missing Features**\n   - Error recovery\n   - Monitoring/logging\n   - Health checks\n\n3. **Improve Testing**\n   - Add unit tests\n   - Integration tests\n   - Performance benchmarks\n\n## Implementation\n\nHere are the key files that need to be changed:\n\n<artifacts>\n<artifact identifier=\"unified-llm-service\" type=\"application/vnd.ant.code\" language=\"python\" title=\"Unified LLM Service\">\n\"\"\"\nUnified LLM Service for Knowledge Garden.\n\nThis replaces all duplicate LLM client implementations with a single,\nconsistent service that supports multiple providers and tiers.\n\"\"\"\nimport os\nimport json\nimport logging\nimport asyncio\nfrom typing import Dict, List, Any, Optional, Union\nfrom enum import Enum\nimport aiohttp\n\nlogger = logging.getLogger(__name__)\n\nclass LLMProvider(Enum):\n    \"\"\"Supported LLM providers\"\"\"\n    ANTHROPIC = \"anthropic\"\n    OLLAMA = \"ollama\"\n    OPENAI = \"openai\"\n\nclass LLMTier(Enum):\n    \"\"\"LLM complexity tiers\"\"\"\n    BASIC = \"basic\"      # Simple tasks (Ollama/Gemma)\n    STANDARD = \"standard\" # Standard tasks (Haiku)\n    ADVANCED = \"advanced\" # Complex tasks (Sonnet)\n    EXTENDED = \"extended\" # Deep analysis (Opus)\n\nclass LLMConfig:\n    \"\"\"LLM tier configurations\"\"\"\n    TIERS = {\n        LLMTier.BASIC: {\n            \"provider\": LLMProvider.OLLAMA,\n            \"model\": os.getenv(\"BASIC_LLM_MODEL\", \"gemma:2b\"),\n            \"api_base\": os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\") + \"/api\",\n            \"timeout\": 10,\n            \"max_tokens\": 1024,\n            \"temperature\": 0.2\n        },\n        LLMTier.STANDARD: {\n            \"provider\": LLMProvider.ANTHROPIC,\n            \"model\": os.getenv(\"STANDARD_LLM_MODEL\", \"claude-3-haiku-20240307\"),\n            \"timeout\": 15,\n            \"max_tokens\": 2048,\n            \"temperature\": 0.3\n        },\n        LLMTier.ADVANCED: {\n            \"provider\": LLMProvider.ANTHROPIC,\n            \"model\": os.getenv(\"ADVANCED_LLM_MODEL\", \"claude-3-sonnet-20240229\"),\n            \"timeout\": 30,\n            \"max_tokens\": 4096,\n            \"temperature\": 0.4\n        },\n        LLMTier.EXTENDED: {\n            \"provider\": LLMProvider.ANTHROPIC,\n            \"model\": os.getenv(\"EXTENDED_LLM_MODEL\", \"claude-3-opus-20240229\"),\n            \"timeout\": 60,\n            \"max_tokens\": 8192,\n            \"temperature\": 0.5\n        }\n    }\n\nclass UnifiedLLMService:\n    \"\"\"Unified LLM service supporting multiple providers and tiers.\"\"\"\n    \n    def __init__(self, default_tier: LLMTier = LLMTier.STANDARD):\n        \"\"\"Initialize the LLM service with a default tier.\"\"\"\n        self.default_tier = default_tier\n        self._validate_api_keys()\n        \n    def _validate_api_keys(self):\n        \"\"\"Validate required API keys are present.\"\"\"\n        if not os.getenv(\"ANTHROPIC_API_KEY\"):\n            logger.warning(\"ANTHROPIC_API_KEY not found. Anthropic models will not work.\")\n    \n    async def generate(\n        self, \n        prompt: str,\n        tier: Optional[LLMTier] = None,\n        system_prompt: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        response_format: Optional[str] = None\n    ) -> Union[str, Dict[str, Any]]:\n        \"\"\"\n        Generate a response using the specified tier.\n        \n        Args:\n            prompt: The user prompt\n            tier: LLM tier to use (defaults to service default)\n            system_prompt: Optional system prompt\n            temperature: Optional temperature override\n            max_tokens: Optional max tokens override\n            response_format: Optional response format (\"json\" for JSON responses)\n            \n        Returns:\n            Generated response (string or parsed JSON)\n        \"\"\"\n        tier = tier or self.default_tier\n        config = LLMConfig.TIERS[tier]\n        \n        # Override config values if provided\n        temperature = temperature or config[\"temperature\"]\n        max_tokens = max_tokens or config[\"max_tokens\"]\n        \n        provider = config[\"provider\"]\n        \n        if provider == LLMProvider.ANTHROPIC:\n            response = await self._generate_anthropic(\n                prompt, system_prompt, temperature, max_tokens, config\n            )\n        elif provider == LLMProvider.OLLAMA:\n            response = await self._generate_ollama(\n                prompt, system_prompt, temperature, max_tokens, config\n            )\n        else:\n            raise ValueError(f\"Unsupported provider: {provider}\")\n        \n        # Parse JSON if requested\n        if response_format == \"json\":\n            return self._parse_json_response(response)\n        \n        return response\n    \n    async def _generate_anthropic(\n        self,\n        prompt: str,\n        system_prompt: Optional[str],\n        temperature: float,\n        max_tokens: int,\n        config: Dict[str, Any]\n    ) -> str:\n        \"\"\"Generate response using Anthropic API.\"\"\"\n        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n        if not api_key:\n            raise ValueError(\"ANTHROPIC_API_KEY not found\")\n        \n        headers = {\n            \"x-api-key\": api_key,\n            \"anthropic-version\": \"2023-06-01\",\n            \"content-type\": \"application/json\"\n        }\n        \n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        \n        request_body = {\n            \"model\": config[\"model\"],\n            \"messages\": messages,\n            \"max_tokens\": max_tokens,\n            \"temperature\": temperature\n        }\n        \n        if system_prompt:\n            request_body[\"system\"] = system_prompt\n        \n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                \"https://api.anthropic.com/v1/messages\",\n                headers=headers,\n                json=request_body,\n                timeout=aiohttp.ClientTimeout(total=config[\"timeout\"])\n            ) as response:\n                if response.status != 200:\n                    error_text = await response.text()\n                    raise Exception(f\"Anthropic API error: {response.status} - {error_text}\")\n                \n                result = await response.json()\n                return result.get(\"content\", [{\"text\": \"\"}])[0].get(\"text\", \"\")\n    \n    async def _generate_ollama(\n        self,\n        prompt: str,\n        system_prompt: Optional[str],\n        temperature: float,\n        max_tokens: int,\n        config: Dict[str, Any]\n    ) -> str:\n        \"\"\"Generate response using Ollama API.\"\"\"\n        api_base = config.get(\"api_base\", \"http://localhost:11434/api\")\n        \n        request_body = {\n            \"model\": config[\"model\"],\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": {\n                \"temperature\": temperature,\n                \"num_predict\": max_tokens\n            }\n        }\n        \n        if system_prompt:\n            request_body[\"system\"] = system_prompt\n        \n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{api_base}/generate\",\n                json=request_body,\n                timeout=aiohttp.ClientTimeout(total=config[\"timeout\"])\n            ) as response:\n                if response.status != 200:\n                    error_text = await response.text()\n                    raise Exception(f\"Ollama API error: {response.status} - {error_text}\")\n                \n                result = await response.json()\n                return result.get(\"response\", \"\")\n    \n    def _parse_json_response(self, response: str) -> Dict[str, Any]:\n        \"\"\"Extract and parse JSON from response.\"\"\"\n        import re\n        \n        # Try direct parsing first\n        try:\n            return json.loads(response)\n        except:\n            pass\n        \n        # Try to extract JSON from markdown code blocks\n        json_match = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", response)\n        if json_match:\n            try:\n                return json.loads(json_match.group(1))\n            except:\n                pass\n        \n        # Try to find JSON-like structures\n        json_match = re.search(r\"\\{[\\s\\S]*\\}\", response)\n        if json_match:\n            try:\n                return json.loads(json_match.group(0))\n            except:\n                pass\n        \n        # Return empty dict if parsing fails\n        logger.warning(f\"Failed to parse JSON from response: {response[:200]}...\")\n        return {}\n    \n    def get_tier_for_task(self, task_type: str) -> LLMTier:\n        \"\"\"Get recommended tier for a specific task type.\"\"\"\n        task_tiers = {\n            # Basic tasks\n            \"entity_extraction\": LLMTier.BASIC,\n            \"keyword_extraction\": LLMTier.BASIC,\n            \"simple_classification\": LLMTier.BASIC,\n            \n            # Standard tasks\n            \"cap_extraction\": LLMTier.STANDARD,\n            \"insight_extraction\": LLMTier.STANDARD,\n            \"question_extraction\": LLMTier.STANDARD,\n            \"pattern_validation\": LLMTier.STANDARD,\n            \n            # Advanced tasks\n            \"tension_extraction\": LLMTier.ADVANCED,\n            \"relationship_inference\": LLMTier.ADVANCED,\n            \"entity_resolution\": LLMTier.ADVANCED,\n            \"concept_clustering\": LLMTier.ADVANCED,\n            \n            # Extended tasks\n            \"value_extraction\": LLMTier.EXTENDED,\n            \"deep_analysis\": LLMTier.EXTENDED,\n            \"cross_document_synthesis\": LLMTier.EXTENDED\n        }\n        \n        return task_tiers.get(task_type, self.default_tier)\n\n# Global service instance\n_llm_service = None\n\ndef get_llm_service(default_tier: LLMTier = LLMTier.STANDARD) -> UnifiedLLMService:\n    \"\"\"Get or create the global LLM service instance.\"\"\"\n    global _llm_service\n    if _llm_service is None:\n        _llm_service = UnifiedLLMService(default_tier)\n    return _llm_service\n</artifact>\n\n<artifact identifier=\"simplified-extraction-pipeline\" type=\"application/vnd.ant.code\" language=\"python\" title=\"Simplified Extraction Pipeline\">\n\"\"\"\nSimplified Extraction Pipeline for Knowledge Garden.\n\nThis replaces the overly complex multi-stage pipeline with a simpler,\nmore maintainable approach that still supports all features.\n\"\"\"\nimport logging\nimport asyncio\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom datetime import datetime\nfrom dataclasses import dataclass, field\nfrom enum import Enum\n\nfrom shared.services.llm import get_llm_service, LLMTier\nfrom shared.services.embeddings import get_embedding_service\nfrom shared.services.fuseki import get_fuseki_service\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ExtractionConfig:\n    \"\"\"Configuration for extraction pipeline.\"\"\"\n    extract_entities: bool = True\n    extract_caps: bool = True\n    extract_insights: bool = True\n    extract_tensions: bool = True\n    extract_questions: bool = True\n    extract_values: bool = True\n    generate_embeddings: bool = True\n    enable_clustering: bool = False\n    min_cluster_size: int = 5\n    use_parliament: bool = False\n\n@dataclass\nclass ExtractedConcept:\n    \"\"\"A concept extracted from text.\"\"\"\n    text: str\n    type: str\n    confidence: float\n    source: str\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass ExtractionResult:\n    \"\"\"Result of extraction pipeline.\"\"\"\n    source_uri: str\n    timestamp: str\n    duration_seconds: float\n    concepts: List[ExtractedConcept]\n    clusters: List[Dict[str, Any]]\n    stats: Dict[str, int]\n    errors: List[str] = field(default_factory=list)\n\nclass SimplifiedExtractionPipeline:\n    \"\"\"\n    Simplified extraction pipeline that:\n    1. Extracts concepts using tiered LLMs\n    2. Generates embeddings\n    3. Optionally clusters concepts\n    4. Tracks evidence\n    \"\"\"\n    \n    def __init__(self, config: ExtractionConfig = None):\n        \"\"\"Initialize the pipeline with configuration.\"\"\"\n        self.config = config or ExtractionConfig()\n        self.llm_service = get_llm_service()\n        self.embedding_service = get_embedding_service()\n        self.fuseki_service = get_fuseki_service()\n        \n    async def extract(\n        self,\n        text: str,\n        source_uri: str = \"unknown\",\n        context: Optional[str] = None\n    ) -> ExtractionResult:\n        \"\"\"\n        Extract concepts from text.\n        \n        Args:\n            text: Text to extract from\n            source_uri: Source identifier\n            context: Optional context\n            \n        Returns:\n            Extraction result with concepts and statistics\n        \"\"\"\n        start_time = datetime.now()\n        errors = []\n        \n        try:\n            # 1. Extract concepts in parallel\n            extraction_tasks = []\n            \n            if self.config.extract_entities:\n                extraction_tasks.append(self._extract_entities(text))\n            if self.config.extract_caps:\n                extraction_tasks.append(self._extract_caps(text, context))\n            if self.config.extract_insights:\n                extraction_tasks.append(self._extract_insights(text, context))\n            if self.config.extract_tensions:\n                extraction_tasks.append(self._extract_tensions(text, context))\n            if self.config.extract_questions:\n                extraction_tasks.append(self._extract_questions(text))\n            \n            # Run extractions in parallel\n            results = await asyncio.gather(*extraction_tasks, return_exceptions=True)\n            \n            # Collect concepts\n            concepts = []\n            for result in results:\n                if isinstance(result, Exception):\n                    errors.append(str(result))\n                    logger.error(f\"Extraction error: {result}\")\n                else:\n                    concepts.extend(result)\n            \n            # 2. Extract values if we have other concepts\n            if self.config.extract_values and concepts:\n                try:\n                    values = await self._extract_values(text, concepts)\n                    concepts.extend(values)\n                except Exception as e:\n                    errors.append(f\"Value extraction error: {e}\")\n            \n            # 3. Generate embeddings\n            if self.config.generate_embeddings and concepts:\n                await self._generate_embeddings(concepts)\n            \n            # 4. Cluster if enabled\n            clusters = []\n            if self.config.enable_clustering and len(concepts) >= self.config.min_cluster_size:\n                try:\n                    clusters = await self._cluster_concepts(concepts)\n                except Exception as e:\n                    errors.append(f\"Clustering error: {e}\")\n            \n            # 5. Store in knowledge graph (optional)\n            if self.fuseki_service:\n                await self._store_concepts(concepts, source_uri)\n            \n            # Calculate statistics\n            stats = self._calculate_stats(concepts)\n            duration = (datetime.now() - start_time).total_seconds()\n            \n            return ExtractionResult(\n                source_uri=source_uri,\n                timestamp=start_time.isoformat(),\n                duration_seconds=duration,\n                concepts=concepts,\n                clusters=clusters,\n                stats=stats,\n                errors=errors\n            )\n            \n        except Exception as e:\n            logger.error(f\"Pipeline error: {e}\")\n            duration = (datetime.now() - start_time).total_seconds()\n            return ExtractionResult(\n                source_uri=source_uri,\n                timestamp=start_time.isoformat(),\n                duration_seconds=duration,\n                concepts=[],\n                clusters=[],\n                stats={},\n                errors=[str(e)]\n            )\n    \n    async def _extract_entities(self, text: str) -> List[ExtractedConcept]:\n        \"\"\"Extract entities using basic tier.\"\"\"\n        prompt = f\"\"\"\n        Extract named entities from this text. Return only a JSON list of entities.\n        \n        Text: {text}\n        \n        Format each entity as:\n        {{\"text\": \"entity name\", \"type\": \"Person/Organization/Location/Other\"}}\n        \n        Return only the JSON array, no explanation.\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.BASIC,\n            response_format=\"json\"\n        )\n        \n        concepts = []\n        for item in response:\n            if isinstance(item, dict) and \"text\" in item:\n                concepts.append(ExtractedConcept(\n                    text=item[\"text\"],\n                    type=\"entity\",\n                    confidence=0.8,\n                    source=text,\n                    metadata={\"entity_type\": item.get(\"type\", \"Other\")}\n                ))\n        \n        return concepts\n    \n    async def _extract_caps(self, text: str, context: Optional[str]) -> List[ExtractedConcept]:\n        \"\"\"Extract CAPs using standard tier.\"\"\"\n        prompt = f\"\"\"\n        Extract Constitutive Attentional Policies (CAPs) from this text.\n        \n        CAPs are patterns of attention, often in these forms:\n        - \"MOMENTS when [situation] are OPPORTUNITIES for [action]\"\n        - \"CIRCUMSTANCES where [condition]\"\n        - \"CONTEXTS that [enable something]\"\n        \n        Text: {text}\n        {f\"Context: {context}\" if context else \"\"}\n        \n        Return a JSON list where each CAP has:\n        {{\"pattern\": \"full CAP text\", \"context\": \"the situation\", \"action\": \"the opportunity\"}}\n        \n        Only return the JSON array.\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.STANDARD,\n            response_format=\"json\"\n        )\n        \n        concepts = []\n        for item in response:\n            if isinstance(item, dict) and \"pattern\" in item:\n                concepts.append(ExtractedConcept(\n                    text=item[\"pattern\"],\n                    type=\"cap\",\n                    confidence=0.85,\n                    source=text,\n                    metadata={\n                        \"context\": item.get(\"context\", \"\"),\n                        \"action\": item.get(\"action\", \"\")\n                    }\n                ))\n        \n        return concepts\n    \n    async def _extract_insights(self, text: str, context: Optional[str]) -> List[ExtractedConcept]:\n        \"\"\"Extract insights using standard tier.\"\"\"\n        prompt = f\"\"\"\n        Extract insights and realizations from this text.\n        \n        Look for:\n        - \"I realized that...\"\n        - \"I discovered that...\"\n        - \"The key insight is...\"\n        - Learning statements\n        - Aha moments\n        \n        Text: {text}\n        {f\"Context: {context}\" if context else \"\"}\n        \n        Return a JSON list where each insight is:\n        {{\"insight\": \"the realization text\", \"trigger\": \"what led to this insight\"}}\n        \n        Only return the JSON array.\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.STANDARD,\n            response_format=\"json\"\n        )\n        \n        concepts = []\n        for item in response:\n            if isinstance(item, dict) and \"insight\" in item:\n                concepts.append(ExtractedConcept(\n                    text=item[\"insight\"],\n                    type=\"insight\",\n                    confidence=0.85,\n                    source=text,\n                    metadata={\"trigger\": item.get(\"trigger\", \"\")}\n                ))\n        \n        return concepts\n    \n    async def _extract_tensions(self, text: str, context: Optional[str]) -> List[ExtractedConcept]:\n        \"\"\"Extract tensions using advanced tier.\"\"\"\n        prompt = f\"\"\"\n        Extract tensions, trade-offs, and polarities from this text.\n        \n        Look for:\n        - \"tension between X and Y\"\n        - \"balancing X with Y\"\n        - \"X versus Y\"\n        - Trade-offs and dilemmas\n        - Opposing forces\n        \n        Text: {text}\n        {f\"Context: {context}\" if context else \"\"}\n        \n        Return a JSON list where each tension is:\n        {{\"pole1\": \"first side\", \"pole2\": \"second side\", \"description\": \"nature of the tension\"}}\n        \n        Only return the JSON array.\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.ADVANCED,\n            response_format=\"json\"\n        )\n        \n        concepts = []\n        for item in response:\n            if isinstance(item, dict) and \"pole1\" in item and \"pole2\" in item:\n                concepts.append(ExtractedConcept(\n                    text=f\"{item['pole1']} vs {item['pole2']}\",\n                    type=\"tension\",\n                    confidence=0.9,\n                    source=text,\n                    metadata={\n                        \"pole1\": item[\"pole1\"],\n                        \"pole2\": item[\"pole2\"],\n                        \"description\": item.get(\"description\", \"\")\n                    }\n                ))\n        \n        return concepts\n    \n    async def _extract_questions(self, text: str) -> List[ExtractedConcept]:\n        \"\"\"Extract questions using basic tier.\"\"\"\n        prompt = f\"\"\"\n        Extract meaningful questions from this text.\n        Ignore rhetorical questions. Focus on genuine inquiries.\n        \n        Text: {text}\n        \n        Return a JSON list where each item is:\n        {{\"question\": \"the question text\"}}\n        \n        Only return the JSON array.\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.BASIC,\n            response_format=\"json\"\n        )\n        \n        concepts = []\n        for item in response:\n            if isinstance(item, dict) and \"question\" in item:\n                concepts.append(ExtractedConcept(\n                    text=item[\"question\"],\n                    type=\"question\",\n                    confidence=0.8,\n                    source=text\n                ))\n        \n        return concepts\n    \n    async def _extract_values(self, text: str, concepts: List[ExtractedConcept]) -> List[ExtractedConcept]:\n        \"\"\"Extract values using extended tier based on other concepts.\"\"\"\n        # Prepare concept summary\n        caps = [c for c in concepts if c.type == \"cap\"]\n        insights = [c for c in concepts if c.type == \"insight\"]\n        tensions = [c for c in concepts if c.type == \"tension\"]\n        \n        prompt = f\"\"\"\n        Based on this text and the patterns found, identify the underlying values and sources of meaning.\n        \n        Original text: {text}\n        \n        Patterns found:\n        - CAPs: {[c.text for c in caps[:5]]}\n        - Insights: {[c.text for c in insights[:5]]}\n        - Tensions: {[c.text for c in tensions[:5]]}\n        \n        What core values or sources of meaning emerge from these patterns?\n        Look for what matters most to the person based on their attention patterns.\n        \n        Return a JSON list where each value is:\n        {{\"value\": \"name of value\", \"evidence\": \"what reveals this value\", \"strength\": 0.0-1.0}}\n        \n        Only return the JSON array.\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.EXTENDED,\n            response_format=\"json\"\n        )\n        \n        value_concepts = []\n        for item in response:\n            if isinstance(item, dict) and \"value\" in item:\n                value_concepts.append(ExtractedConcept(\n                    text=item[\"value\"],\n                    type=\"value\",\n                    confidence=item.get(\"strength\", 0.9),\n                    source=text,\n                    metadata={\"evidence\": item.get(\"evidence\", \"\")}\n                ))\n        \n        return value_concepts\n    \n    async def _generate_embeddings(self, concepts: List[ExtractedConcept]) -> None:\n        \"\"\"Generate embeddings for concepts.\"\"\"\n        texts = [c.text for c in concepts]\n        embeddings = await self.embedding_service.batch_generate(texts)\n        \n        for concept, embedding in zip(concepts, embeddings):\n            concept.embedding = embedding.tolist()\n    \n    async def _cluster_concepts(self, concepts: List[ExtractedConcept]) -> List[Dict[str, Any]]:\n        \"\"\"Cluster concepts based on embeddings.\"\"\"\n        if not all(c.embedding for c in concepts):\n            return []\n        \n        try:\n            import numpy as np\n            from sklearn.cluster import DBSCAN\n            \n            # Create embedding matrix\n            embeddings = np.array([c.embedding for c in concepts])\n            \n            # Simple DBSCAN clustering\n            clustering = DBSCAN(eps=0.3, min_samples=2, metric='cosine')\n            labels = clustering.fit_predict(embeddings)\n            \n            # Group by cluster\n            clusters = {}\n            for concept, label in zip(concepts, labels):\n                if label != -1:  # Skip noise\n                    if label not in clusters:\n                        clusters[label] = []\n                    clusters[label].append(concept)\n            \n            # Format clusters\n            cluster_list = []\n            for label, cluster_concepts in clusters.items():\n                cluster_list.append({\n                    \"id\": label,\n                    \"size\": len(cluster_concepts),\n                    \"concepts\": [c.text for c in cluster_concepts],\n                    \"types\": list(set(c.type for c in cluster_concepts))\n                })\n            \n            return cluster_list\n            \n        except Exception as e:\n            logger.error(f\"Clustering error: {e}\")\n            return []\n    \n    async def _store_concepts(self, concepts: List[ExtractedConcept], source_uri: str) -> None:\n        \"\"\"Store concepts in knowledge graph.\"\"\"\n        # TODO: Implement storage logic\n        pass\n    \n    def _calculate_stats(self, concepts: List[ExtractedConcept]) -> Dict[str, int]:\n        \"\"\"Calculate extraction statistics.\"\"\"\n        stats = {\n            \"total_concepts\": len(concepts),\n            \"entities\": len([c for c in concepts if c.type == \"entity\"]),\n            \"caps\": len([c for c in concepts if c.type == \"cap\"]),\n            \"insights\": len([c for c in concepts if c.type == \"insight\"]),\n            \"tensions\": len([c for c in concepts if c.type == \"tension\"]),\n            \"questions\": len([c for c in concepts if c.type == \"question\"]),\n            \"values\": len([c for c in concepts if c.type == \"value\"])\n        }\n        return stats\n\n# Factory function\ndef create_extraction_pipeline(config: Optional[ExtractionConfig] = None) -> SimplifiedExtractionPipeline:\n    \"\"\"Create an extraction pipeline with the given configuration.\"\"\"\n    return SimplifiedExtractionPipeline(config)\n</artifact>\n\n<artifact identifier=\"enhanced-parliament-service\" type=\"application/vnd.ant.code\" language=\"python\" title=\"Enhanced Parliament Service\">\n\"\"\"\nEnhanced Parliament Service for Knowledge Garden.\n\nThis replaces the placeholder Parliament service with actual\ndecision-making capabilities using LLMs.\n\"\"\"\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport logging\nfrom typing import Dict, List, Any, Optional, Union\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\nimport hashlib\nimport json\n\nfrom shared.services.llm import get_llm_service, LLMTier\nfrom shared.services.cache import get_cache_service\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize app\napp = FastAPI(\n    title=\"Knowledge Garden Parliament Service\",\n    version=\"3.0.0\",\n    description=\"Intelligent decision-making service for Knowledge Garden\"\n)\n\n# Add CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Services\nllm_service = get_llm_service()\ncache_service = get_cache_service()\n\n# Request/Response Models\nclass DecisionRequest(BaseModel):\n    \"\"\"Base request for decisions.\"\"\"\n    decision_type: str\n    context: Dict[str, Any]\n    options: Optional[Dict[str, Any]] = None\n    use_cache: bool = True\n\nclass DecisionResponse(BaseModel):\n    \"\"\"Base response for decisions.\"\"\"\n    decision_id: str\n    decision_type: str\n    result: Dict[str, Any]\n    confidence: float\n    reasoning: str\n    cached: bool = False\n    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())\n\nclass EntityResolutionRequest(BaseModel):\n    \"\"\"Request for entity resolution.\"\"\"\n    entity: Dict[str, Any]\n    candidates: List[Dict[str, Any]]\n    context: Optional[str] = None\n\nclass ClusterLabelingRequest(BaseModel):\n    \"\"\"Request for cluster labeling.\"\"\"\n    clusters: List[Dict[str, Any]]\n    \nclass ArtifactTypeRequest(BaseModel):\n    \"\"\"Request for artifact type selection.\"\"\"\n    content: str\n    available_types: List[str]\n    user_preference: Optional[str] = None\n    metadata: Optional[Dict[str, Any]] = None\n\nclass PatternValidationRequest(BaseModel):\n    \"\"\"Request for pattern validation.\"\"\"\n    pattern: Dict[str, Any]\n    pattern_type: str\n    context: Optional[str] = None\n\n# Decision Making Functions\nclass ParliamentDecisionMaker:\n    \"\"\"Core decision-making logic for Parliament.\"\"\"\n    \n    def __init__(self):\n        self.llm_service = llm_service\n        self.cache_service = cache_service\n        \n    async def make_decision(\n        self,\n        decision_type: str,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None,\n        use_cache: bool = True\n    ) -> DecisionResponse:\n        \"\"\"Make a decision based on type and context.\"\"\"\n        # Generate cache key\n        cache_key = self._generate_cache_key(decision_type, context, options)\n        \n        # Check cache\n        if use_cache:\n            cached_result = await self.cache_service.get(cache_key)\n            if cached_result:\n                return DecisionResponse(\n                    decision_id=cache_key,\n                    decision_type=decision_type,\n                    result=cached_result[\"result\"],\n                    confidence=cached_result[\"confidence\"],\n                    reasoning=cached_result[\"reasoning\"],\n                    cached=True\n                )\n        \n        # Route to specific decision handler\n        handlers = {\n            \"entity_resolution\": self._resolve_entity,\n            \"cluster_labeling\": self._label_clusters,\n            \"artifact_type\": self._select_artifact_type,\n            \"pattern_validation\": self._validate_pattern,\n            \"relationship_inference\": self._infer_relationships,\n            \"concept_classification\": self._classify_concept\n        }\n        \n        handler = handlers.get(decision_type)\n        if not handler:\n            raise ValueError(f\"Unknown decision type: {decision_type}\")\n        \n        # Make decision\n        result, confidence, reasoning = await handler(context, options)\n        \n        # Create response\n        response = DecisionResponse(\n            decision_id=cache_key,\n            decision_type=decision_type,\n            result=result,\n            confidence=confidence,\n            reasoning=reasoning\n        )\n        \n        # Cache result\n        if use_cache:\n            await self.cache_service.set(\n                cache_key,\n                {\n                    \"result\": result,\n                    \"confidence\": confidence,\n                    \"reasoning\": reasoning\n                },\n                ttl=3600  # 1 hour\n            )\n        \n        return response\n    \n    async def _resolve_entity(\n        self,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> tuple[Dict[str, Any], float, str]:\n        \"\"\"Resolve if an entity matches any candidates.\"\"\"\n        entity = context[\"entity\"]\n        candidates = context.get(\"candidates\", [])\n        \n        if not candidates:\n            return {\n                \"resolved\": False,\n                \"match_id\": None,\n                \"should_merge\": False\n            }, 1.0, \"No candidates to match against\"\n        \n        # Create prompt\n        prompt = f\"\"\"\n        Determine if this entity matches any of the candidate entities.\n        \n        Entity to resolve:\n        - Label: {entity.get('label', '')}\n        - Type: {entity.get('type', '')}\n        - Context: {entity.get('context', '')}\n        \n        Candidate entities:\n        {json.dumps(candidates, indent=2)}\n        \n        Consider:\n        1. Name variations (abbreviations, aliases)\n        2. Contextual clues\n        3. Entity types\n        4. Associated attributes\n        \n        Return JSON:\n        {{\n            \"matches_candidate\": true/false,\n            \"match_index\": index of matching candidate or -1,\n            \"confidence\": 0.0-1.0,\n            \"reasoning\": \"brief explanation\"\n        }}\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.ADVANCED,\n            response_format=\"json\"\n        )\n        \n        match_index = response.get(\"match_index\", -1)\n        confidence = response.get(\"confidence\", 0.0)\n        reasoning = response.get(\"reasoning\", \"\")\n        \n        result = {\n            \"resolved\": match_index >= 0,\n            \"match_id\": candidates[match_index][\"id\"] if match_index >= 0 else None,\n            \"should_merge\": match_index >= 0 and confidence > 0.8\n        }\n        \n        return result, confidence, reasoning\n    \n    async def _label_clusters(\n        self,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> tuple[Dict[str, Any], float, str]:\n        \"\"\"Generate meaningful labels for clusters.\"\"\"\n        clusters = context.get(\"clusters\", [])\n        \n        if not clusters:\n            return {\"clusters\": []}, 1.0, \"No clusters to label\"\n        \n        # Process each cluster\n        labeled_clusters = []\n        \n        for cluster in clusters:\n            cluster_concepts = cluster.get(\"concepts\", [])[:10]  # Limit to first 10\n            \n            prompt = f\"\"\"\n            Generate a descriptive label for this cluster of concepts:\n            \n            Concepts:\n            {json.dumps(cluster_concepts, indent=2)}\n            \n            The label should:\n            1. Capture the main theme\n            2. Be 2-5 words\n            3. Be specific and meaningful\n            \n            Return JSON:\n            {{\n                \"label\": \"cluster label\",\n                \"keywords\": [\"key\", \"words\"],\n                \"confidence\": 0.0-1.0\n            }}\n            \"\"\"\n            \n            response = await self.llm_service.generate(\n                prompt,\n                tier=LLMTier.STANDARD,\n                response_format=\"json\"\n            )\n            \n            labeled_clusters.append({\n                \"id\": cluster.get(\"id\"),\n                \"label\": response.get(\"label\", f\"Cluster {cluster.get('id')}\"),\n                \"keywords\": response.get(\"keywords\", []),\n                \"size\": cluster.get(\"size\", len(cluster_concepts)),\n                \"confidence\": response.get(\"confidence\", 0.7)\n            })\n        \n        avg_confidence = sum(c[\"confidence\"] for c in labeled_clusters) / len(labeled_clusters)\n        \n        return {\n            \"clusters\": labeled_clusters\n        }, avg_confidence, f\"Labeled {len(clusters)} clusters\"\n    \n    async def _select_artifact_type(\n        self,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> tuple[Dict[str, Any], float, str]:\n        \"\"\"Select the best artifact type for content.\"\"\"\n        content = context.get(\"content\", \"\")\n        available_types = context.get(\"available_types\", [])\n        user_preference = context.get(\"user_preference\")\n        \n        prompt = f\"\"\"\n        Select the best artifact type for this content:\n        \n        Content: {content[:500]}...\n        \n        Available types: {available_types}\n        User preference: {user_preference or \"None\"}\n        \n        Consider:\n        1. Content structure and purpose\n        2. Information density\n        3. Target audience\n        4. User preference (if any)\n        \n        Return JSON:\n        {{\n            \"selected_type\": \"chosen type\",\n            \"alternatives\": [\"other viable options\"],\n            \"confidence\": 0.0-1.0,\n            \"key_factors\": [\"reasons for selection\"]\n        }}\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.STANDARD,\n            response_format=\"json\"\n        )\n        \n        return {\n            \"selected_type\": response.get(\"selected_type\", available_types[0]),\n            \"alternatives\": response.get(\"alternatives\", []),\n            \"key_factors\": response.get(\"key_factors\", [])\n        }, response.get(\"confidence\", 0.8), \"Type selected based on content analysis\"\n    \n    async def _validate_pattern(\n        self,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> tuple[Dict[str, Any], float, str]:\n        \"\"\"Validate if a pattern is high quality.\"\"\"\n        pattern = context.get(\"pattern\", {})\n        pattern_type = context.get(\"pattern_type\", \"\")\n        \n        prompt = f\"\"\"\n        Validate this {pattern_type} pattern:\n        \n        Pattern: {json.dumps(pattern, indent=2)}\n        \n        Criteria:\n        1. Clarity and specificity\n        2. Meaningful content\n        3. Proper structure for pattern type\n        4. Actionable or insightful\n        \n        Return JSON:\n        {{\n            \"is_valid\": true/false,\n            \"quality_score\": 0.0-1.0,\n            \"issues\": [\"list of issues if any\"],\n            \"suggestions\": [\"improvements if needed\"]\n        }}\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.STANDARD,\n            response_format=\"json\"\n        )\n        \n        is_valid = response.get(\"is_valid\", False)\n        quality_score = response.get(\"quality_score\", 0.0)\n        \n        return {\n            \"is_valid\": is_valid,\n            \"quality_score\": quality_score,\n            \"issues\": response.get(\"issues\", []),\n            \"suggestions\": response.get(\"suggestions\", [])\n        }, quality_score, f\"Pattern {'validated' if is_valid else 'rejected'}\"\n    \n    async def _infer_relationships(\n        self,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> tuple[Dict[str, Any], float, str]:\n        \"\"\"Infer relationships between concepts.\"\"\"\n        concepts = context.get(\"concepts\", [])\n        \n        if len(concepts) < 2:\n            return {\"relationships\": []}, 1.0, \"Need at least 2 concepts\"\n        \n        # Limit to first 10 concepts for efficiency\n        concepts = concepts[:10]\n        \n        prompt = f\"\"\"\n        Infer relationships between these concepts:\n        \n        Concepts:\n        {json.dumps(concepts, indent=2)}\n        \n        Common relationship types:\n        - causes/caused_by\n        - enables/enabled_by\n        - contains/part_of\n        - contradicts/supports\n        - transforms_into\n        - depends_on\n        \n        Return JSON:\n        {{\n            \"relationships\": [\n                {{\n                    \"source\": \"concept1\",\n                    \"target\": \"concept2\",\n                    \"type\": \"relationship_type\",\n                    \"confidence\": 0.0-1.0\n                }}\n            ]\n        }}\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.ADVANCED,\n            response_format=\"json\"\n        )\n        \n        relationships = response.get(\"relationships\", [])\n        avg_confidence = sum(r.get(\"confidence\", 0) for r in relationships) / len(relationships) if relationships else 0\n        \n        return {\n            \"relationships\": relationships\n        }, avg_confidence, f\"Inferred {len(relationships)} relationships\"\n    \n    async def _classify_concept(\n        self,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> tuple[Dict[str, Any], float, str]:\n        \"\"\"Classify a concept into predefined categories.\"\"\"\n        concept = context.get(\"concept\", {})\n        categories = context.get(\"categories\", [])\n        \n        prompt = f\"\"\"\n        Classify this concept into the most appropriate category:\n        \n        Concept: {json.dumps(concept, indent=2)}\n        \n        Available categories: {categories}\n        \n        Return JSON:\n        {{\n            \"category\": \"selected category\",\n            \"confidence\": 0.0-1.0,\n            \"secondary_categories\": [\"other applicable categories\"]\n        }}\n        \"\"\"\n        \n        response = await self.llm_service.generate(\n            prompt,\n            tier=LLMTier.STANDARD,\n            response_format=\"json\"\n        )\n        \n        return {\n            \"category\": response.get(\"category\", categories[0] if categories else \"unknown\"),\n            \"secondary_categories\": response.get(\"secondary_categories\", [])\n        }, response.get(\"confidence\", 0.8), \"Concept classified\"\n    \n    def _generate_cache_key(\n        self,\n        decision_type: str,\n        context: Dict[str, Any],\n        options: Optional[Dict[str, Any]] = None\n    ) -> str:\n        \"\"\"Generate a cache key for a decision.\"\"\"\n        key_data = {\n            \"type\": decision_type,\n            \"context\": context,\n            \"options\": options or {}\n        }\n        key_str = json.dumps(key_data, sort_keys=True)\n        return hashlib.md5(key_str.encode()).hexdigest()\n\n# Initialize decision maker\ndecision_maker = ParliamentDecisionMaker()\n\n# API Endpoints\n@app.get(\"/health\")\nasync def health():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"service\": \"parliament\",\n        \"version\": \"3.0.0\"\n    }\n\n@app.post(\"/decisions/generic\", response_model=DecisionResponse)\nasync def make_generic_decision(request: DecisionRequest):\n    \"\"\"Make a generic decision.\"\"\"\n    try:\n        response = await decision_maker.make_decision(\n            request.decision_type,\n            request.context,\n            request.options,\n            request.use_cache\n        )\n        return response\n    except Exception as e:\n        logger.error(f\"Decision error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/decisions/entity\")\nasync def resolve_entity(request: EntityResolutionRequest):\n    \"\"\"Resolve entity against candidates.\"\"\"\n    try:\n        response = await decision_maker.make_decision(\n            \"entity_resolution\",\n            {\n                \"entity\": request.entity,\n                \"candidates\": request.candidates,\n                \"context\": request.context\n            }\n        )\n        return response.result\n    except Exception as e:\n        logger.error(f\"Entity resolution error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/decisions/clusters\")\nasync def label_clusters(request: ClusterLabelingRequest):\n    \"\"\"Generate labels for clusters.\"\"\"\n    try:\n        response = await decision_maker.make_decision(\n            \"cluster_labeling\",\n            {\"clusters\": request.clusters}\n        )\n        return response.result\n    except Exception as e:\n        logger.error(f\"Cluster labeling error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/decisions/artifact_type\")\nasync def select_artifact_type(request: ArtifactTypeRequest):\n    \"\"\"Select best artifact type for content.\"\"\"\n    try:\n        response = await decision_maker.make_decision(\n            \"artifact_type\",\n            {\n                \"content\": request.content,\n                \"available_types\": request.available_types,\n                \"user_preference\": request.user_preference,\n                \"metadata\": request.metadata\n            }\n        )\n        return response.result\n    except Exception as e:\n        logger.error(f\"Artifact type selection error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/decisions/pattern\")\nasync def validate_pattern(request: PatternValidationRequest):\n    \"\"\"Validate a pattern.\"\"\"\n    try:\n        response = await decision_maker.make_decision(\n            \"pattern_validation\",\n            {\n                \"pattern\": request.pattern,\n                \"pattern_type\": request.pattern_type,\n                \"context\": request.context\n            }\n        )\n        return response.result\n    except Exception as e:\n        logger.error(f\"Pattern validation error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/cache/stats\")\nasync def get_cache_stats():\n    \"\"\"Get cache statistics.\"\"\"\n    stats = await cache_service.get_stats()\n    return stats\n\n@app.post(\"/cache/clear\")\nasync def clear_cache():\n    \"\"\"Clear the decision cache.\"\"\"\n    await cache_service.clear()\n    return {\"status\": \"cache cleared\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8003)\n</artifact>\n\n<artifact identifier=\"shared-services-init\" type=\"application/vnd.ant.code\" language=\"python\" title=\"Shared Services Package\">\n\"\"\"\nShared Services Package for Knowledge Garden.\n\nThis consolidates all shared functionality into a single, well-organized package.\n\"\"\"\n# shared/services/__init__.py\n\nfrom .llm import get_llm_service, LLMTier, UnifiedLLMService\nfrom .embeddings import get_embedding_service, EmbeddingService\nfrom .fuseki import get_fuseki_service, FusekiService\nfrom .parliament import get_parliament_client, ParliamentClient\nfrom .cache import get_cache_service, CacheService\n\n__all__ = [\n    # LLM\n    \"get_llm_service\",\n    \"LLMTier\",\n    \"UnifiedLLMService\",\n    \n    # Embeddings\n    \"get_embedding_service\",\n    \"EmbeddingService\",\n    \n    # Fuseki\n    \"get_fuseki_service\", \n    \"FusekiService\",\n    \n    # Parliament\n    \"get_parliament_client\",\n    \"ParliamentClient\",\n    \n    # Cache\n    \"get_cache_service\",\n    \"CacheService\"\n]\n</artifact>\n\n<artifact identifier=\"simplified-config\" type=\"application/vnd.ant.code\" language=\"python\" title=\"Simplified Configuration\">\n\"\"\"\nSimplified Configuration for Knowledge Garden.\n\nThis replaces multiple configuration approaches with a single,\nconsistent configuration system.\n\"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nimport yaml\nimport json\n\n@dataclass\nclass ServiceConfig:\n    \"\"\"Configuration for a single service.\"\"\"\n    name: str\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    debug: bool = False\n    data_dir: Path = None\n    \n    def __post_init__(self):\n        if self.data_dir is None:\n            self.data_dir = Path(f\"data/{self.name}\")\n        self.data_dir.mkdir(exist_ok=True, parents=True)\n\n@dataclass\nclass LLMConfig:\n    \"\"\"LLM configuration.\"\"\"\n    tiers: Dict[str, Dict[str, Any]] = field(default_factory=dict)\n    default_tier: str = \"standard\"\n    \n    def __post_init__(self):\n        if not self.tiers:\n            self.tiers = {\n                \"basic\": {\n                    \"provider\": os.getenv(\"BASIC_LLM_PROVIDER\", \"ollama\"),\n                    \"model\": os.getenv(\"BASIC_LLM_MODEL\", \"gemma:2b\"),\n                    \"api_base\": os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\") + \"/api\",\n                    \"timeout\": 10,\n                    \"max_tokens\": 1024,\n                    \"temperature\": 0.2\n                },\n                \"standard\": {\n                    \"provider\": os.getenv(\"STANDARD_LLM_PROVIDER\", \"anthropic\"),\n                    \"model\": os.getenv(\"STANDARD_LLM_MODEL\", \"claude-3-haiku-20240307\"),\n                    \"timeout\": 15,\n                    \"max_tokens\": 2048,\n                    \"temperature\": 0.3\n                },\n                \"advanced\": {\n                    \"provider\": os.getenv(\"ADVANCED_LLM_PROVIDER\", \"anthropic\"),\n                    \"model\": os.getenv(\"ADVANCED_LLM_MODEL\", \"claude-3-sonnet-20240229\"),\n                    \"timeout\": 30,\n                    \"max_tokens\": 4096,\n                    \"temperature\": 0.4\n                },\n                \"extended\": {\n                    \"provider\": os.getenv(\"EXTENDED_LLM_PROVIDER\", \"anthropic\"),\n                    \"model\": os.getenv(\"EXTENDED_LLM_MODEL\", \"claude-3-opus-20240229\"),\n                    \"timeout\": 60,\n                    \"max_tokens\": 8192,\n                    \"temperature\": 0.5\n                }\n            }\n\n@dataclass\nclass EmbeddingConfig:\n    \"\"\"Embedding configuration.\"\"\"\n    model: str = \"jinaai/jina-embeddings-v2-base-en\"\n    batch_size: int = 64\n    cache_enabled: bool = True\n    cache_dir: Path = Path(\"data/embeddings\")\n    \n    def __post_init__(self):\n        self.cache_dir.mkdir(exist_ok=True, parents=True)\n\n@dataclass\nclass FusekiConfig:\n    \"\"\"Fuseki configuration.\"\"\"\n    url: str = os.getenv(\"FUSEKI_URL\", \"http://localhost:3030\")\n    dataset: str = os.getenv(\"FUSEKI_DATASET\", \"kg\")\n    user: str = os.getenv(\"FUSEKI_USER\", \"admin\")\n    password: str = os.getenv(\"FUSEKI_PASSWORD\", \"admin\")\n\n@dataclass\nclass KnowledgeGardenConfig:\n    \"\"\"Main configuration for Knowledge Garden.\"\"\"\n    # Service configs\n    extraction: ServiceConfig = field(default_factory=lambda: ServiceConfig(\"extraction\", port=8001))\n    artifacts: ServiceConfig = field(default_factory=lambda: ServiceConfig(\"artifacts\", port=8002))\n    parliament: ServiceConfig = field(default_factory=lambda: ServiceConfig(\"parliament\", port=8003))\n    \n    # Component configs\n    llm: LLMConfig = field(default_factory=LLMConfig)\n    embedding: EmbeddingConfig = field(default_factory=EmbeddingConfig)\n    fuseki: FusekiConfig = field(default_factory=FusekiConfig)\n    \n    # General settings\n    environment: str = os.getenv(\"ENVIRONMENT\", \"development\")\n    log_level: str = os.getenv(\"LOG_LEVEL\", \"INFO\")\n    \n    @classmethod\n    def from_file(cls, config_file: Path) -> \"KnowledgeGardenConfig\":\n        \"\"\"Load configuration from file.\"\"\"\n        if not config_file.exists():\n            return cls()\n        \n        with open(config_file) as f:\n            if config_file.suffix == \".yaml\":\n                data = yaml.safe_load(f)\n            else:\n                data = json.load(f)\n        \n        return cls(**data)\n    \n    def to_file(self, config_file: Path):\n        \"\"\"Save configuration to file.\"\"\"\n        data = {\n            \"extraction\": {\n                \"name\": self.extraction.name,\n                \"host\": self.extraction.host,\n                \"port\": self.extraction.port,\n                \"debug\": self.extraction.debug\n            },\n            \"artifacts\": {\n                \"name\": self.artifacts.name,\n                \"host\": self.artifacts.host,\n                \"port\": self.artifacts.port,\n                \"debug\": self.artifacts.debug\n            },\n            \"parliament\": {\n                \"name\": self.parliament.name,\n                \"host\": self.parliament.host,\n                \"port\": self.parliament.port,\n                \"debug\": self.parliament.debug\n            },\n            \"llm\": {\n                \"tiers\": self.llm.tiers,\n                \"default_tier\": self.llm.default_tier\n            },\n            \"embedding\": {\n                \"model\": self.embedding.model,\n                \"batch_size\": self.embedding.batch_size,\n                \"cache_enabled\": self.embedding.cache_enabled\n            },\n            \"fuseki\": {\n                \"url\": self.fuseki.url,\n                \"dataset\": self.fuseki.dataset,\n                \"user\": self.fuseki.user,\n                \"password\": self.fuseki.password\n            },\n            \"environment\": self.environment,\n            \"log_level\": self.log_level\n        }\n        \n        with open(config_file, \"w\") as f:\n            if config_file.suffix == \".yaml\":\n                yaml.dump(data, f)\n            else:\n                json.dump(data, f, indent=2)\n\n# Global configuration instance\n_config = None\n\ndef get_config() -> KnowledgeGardenConfig:\n    \"\"\"Get or create the global configuration.\"\"\"\n    global _config\n    if _config is None:\n        config_file = Path(os.getenv(\"KG_CONFIG_FILE\", \"config/knowledge_garden.yaml\"))\n        _config = KnowledgeGardenConfig.from_file(config_file)\n    return _config\n\ndef set_config(config: KnowledgeGardenConfig):\n    \"\"\"Set the global configuration.\"\"\"\n    global _config\n    _config = config\n</artifact>\n</artifacts>\n\n## Opportunities for Improvement\n\n### 1. **Event-Driven Architecture**\n- Implement event streaming (Kafka/RabbitMQ) for real-time processing\n- Enable services to react to extraction events\n- Support workflow orchestration\n\n### 2. **Graph Learning Integration**\n- Add graph neural networks for relationship prediction\n- Implement knowledge graph embeddings\n- Support link prediction and anomaly detection\n\n### 3. **Active Learning Loop**\n- User feedback integration for pattern validation\n- Continuous model improvement\n- Uncertainty-based sampling for human review\n\n### 4. **Multi-Modal Support**\n- Add image analysis capabilities\n- Support audio transcription and analysis\n- Enable video content extraction\n\n### 5. **Advanced Reasoning**\n- Implement causal reasoning chains\n- Add temporal reasoning for evolution tracking\n- Support counterfactual analysis\n\n## Interesting Observations\n\n### 1. **Innovative Pattern Types**\nThe CAPs (Constitutive Attentional Policies) concept is fascinating - it captures attention patterns that reveal values. This could be extended to track attention evolution over time.\n\n### 2. **Parliament Metaphor**\nThe Parliament service concept is clever but underutilized. It could become a powerful multi-agent system where different \"members\" specialize in different types of reasoning.\n\n### 3. **Embedding-First Architecture**\nThe focus on embeddings throughout is good. This could be leveraged further with:\n- Semantic search across all extracted knowledge\n- Concept drift detection\n- Cross-lingual knowledge extraction\n\n### 4. **Rich Ontology**\nThe RDF/SPARQL integration provides a solid foundation for complex queries. This could support:\n- Reasoning over extracted knowledge\n- Consistency checking\n- Knowledge fusion from multiple sources\n\n## How to Make the Program Better\n\n### 1. **Implement Streaming Pipeline**\nReplace batch processing with streaming for real-time extraction and immediate insights.\n\n### 2. **Add Explainability**\n- Show why patterns were extracted\n- Visualize decision paths in Parliament\n- Provide extraction confidence breakdowns\n\n### 3. **Enhance Collaboration**\n- Multi-user support with different expertise levels\n- Collaborative pattern validation\n- Shared knowledge gardens with access control\n\n### 4. **Optimize Performance**\n- Implement proper caching strategies\n- Use vector databases for embedding search\n- Add horizontal scaling capabilities\n\n### 5. **Improve User Experience**\n- Create a unified web interface (not just demo)\n- Add interactive visualizations\n- Implement progressive disclosure of complexity\n\nThe Knowledge Garden has a solid foundation with innovative concepts. By addressing the architectural issues and building on the interesting patterns already present, it could become a powerful system for collaborative knowledge extraction and sense-making.",
            "citations": []
          }
        ],
        "updated_at": "2025-06-14T22:18:32.571031Z",
        "attachments": [],
        "files": []
      }
    }
  ]
}