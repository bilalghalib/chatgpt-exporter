{
  "id": "4ddbf0a0-ad4b-4e8d-8a1c-25350e138945",
  "source": "claude",
  "title": "Code Extraction Service Refactoring",
  "created_at": "2025-06-12T05:50:40.071692Z",
  "updated_at": "2025-06-12T05:54:41.247356Z",
  "summary": "",
  "message_count": 2,
  "messages": [
    {
      "id": "f9f2fd96-6eb9-4865-92da-e7af7c6284dd",
      "role": "human",
      "content": "Review all files related to the extraction, why do we have it happening in multiple places, can we unify that endpoint? do we have the micro service set up? can we organize our files better and imports? you can ignore reverse compatibility and be bold and help streamline. Please return scripts and terminal commands that will update the files in full with all code needed and will also delete and remove files and folders that aren't needed when done",
      "created_at": "2025-06-12T05:50:42.509993Z",
      "sequence_order": 0,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2025-06-12T05:50:42.500300Z",
            "stop_timestamp": "2025-06-12T05:50:42.500300Z",
            "flags": null,
            "type": "text",
            "text": "Review all files related to the extraction, why do we have it happening in multiple places, can we unify that endpoint? do we have the micro service set up? can we organize our files better and imports? you can ignore reverse compatibility and be bold and help streamline. Please return scripts and terminal commands that will update the files in full with all code needed and will also delete and remove files and folders that aren't needed when done",
            "citations": []
          }
        ],
        "updated_at": "2025-06-12T05:50:42.509993Z",
        "attachments": [],
        "files": []
      }
    },
    {
      "id": "d77eb8b2-6fe3-4e31-8157-6a789f5915ed",
      "role": "assistant",
      "content": "Looking at the Knowledge Garden codebase, I can see there are indeed multiple extraction-related components scattered across different places:\n\n1. **Extraction Module** (`knowledge_garden/extraction/`):\n   - `pipeline.py` - Main extraction pipeline\n   - `sentence_extractor.py` - Sentence-level extraction\n   - `pattern_extractor.py` - Pattern extraction (CAPs, questions, insights, tensions)\n   - `entity_resolver.py` - Entity resolution\n   - `llm_extractor.py` - LLM-based extraction\n   - Other supporting files\n\n2. **Apps/Extractor** (`knowledge_garden/apps/extractor/`):\n   - `extract.py` - Appears to be another extraction implementation\n   - Has its own README\n\n3. **Pipeline Module** (`knowledge_garden/pipeline/`):\n   - Another pipeline implementation with stages\n   - `runner.py` - ExtractionPipeline class\n   - Various stages in `stages/` directory\n\n4. **Test Extraction** (`knowledge_garden/test_extraction.py`):\n   - Standalone test extraction script\n\n5. **CLI** (`knowledge_garden/cli.py`):\n   - Has an `extract` command\n\nThis is clearly duplicated functionality that needs to be unified. Let me analyze what each does:\n\n1. The `extraction/` module seems to be the most comprehensive with all the components\n2. The `pipeline/` module seems to be a different architecture using a staged approach\n3. The `apps/extractor/` seems to be a standalone app\n4. `test_extraction.py` appears to be a test/example script\n\nFor a unified approach, I would:\n1. Keep the core extraction logic in `knowledge_garden/extraction/`\n2. Create a single extraction service/API endpoint\n3. Remove duplicate implementations\n4. Update the CLI to use the unified extraction\n5. Create a proper microservice setup if needed\n\nLet me create scripts to reorganize this.Looking at the codebase, I can see there are multiple extraction implementations scattered across different modules. This needs to be unified into a single, well-organized extraction service. Here's my analysis:\n\n## Current Issues:\n1. **Multiple extraction implementations**:\n   - `knowledge_garden/extraction/` - Main extraction module\n   - `knowledge_garden/pipeline/` - Alternative pipeline implementation\n   - `knowledge_garden/apps/extractor/` - Standalone extractor app\n   - `knowledge_garden/test_extraction.py` - Test extraction script\n\n2. **Duplicate functionality** across modules\n3. **Inconsistent imports** and module organization\n4. **No clear microservice architecture**\n\n## Solution: Unified Extraction Service\n\nI'll create scripts to:\n1. Consolidate all extraction logic into a single service\n2. Remove duplicate code\n3. Create a clean API endpoint\n4. Organize imports properly\n5. Set up a proper microservice architecture\n\nHere are the scripts:\n\n### 1. Main Reorganization Script\n\n```bash\n#!/bin/bash\n# save as: reorganize_extraction.sh\n\necho \"ðŸ”§ Reorganizing Knowledge Garden Extraction System...\"\n\n# Create new unified extraction structure\nmkdir -p knowledge_garden/services/extraction\nmkdir -p knowledge_garden/api\nmkdir -p knowledge_garden/api/endpoints\nmkdir -p knowledge_garden/tests/extraction\n\n# Create backup of current state\necho \"ðŸ“¦ Creating backup...\"\ntar -czf extraction_backup_$(date +%Y%m%d_%H%M%S).tar.gz \\\n    knowledge_garden/extraction \\\n    knowledge_garden/pipeline \\\n    knowledge_garden/apps/extractor \\\n    knowledge_garden/test_extraction.py 2>/dev/null || true\n\necho \"âœ… Backup created\"\n```\n\n### 2. Create Unified Extraction Service\n\n```python\n#!/usr/bin/env python3\n# save as: create_unified_extraction.py\n\nimport os\nimport shutil\nfrom pathlib import Path\n\n# Create the unified extraction service\nservice_content = '''\"\"\"\nUnified Extraction Service for Knowledge Garden\n\nThis module provides a single, consolidated extraction service that combines\nall extraction functionality into one coherent API.\n\"\"\"\nimport asyncio\nimport logging\nfrom typing import Dict, List, Any, Optional, Union\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom knowledge_garden.extraction.embedding_generator import EmbeddingGenerator\nfrom knowledge_garden.extraction.sentence_extractor import SentenceExtractor\nfrom knowledge_garden.extraction.pattern_extractor import PatternExtractor\nfrom knowledge_garden.extraction.entity_resolver import EntityResolver\nfrom knowledge_garden.extraction.evidence_tracker import EvidenceTracker\nfrom knowledge_garden.extraction.llm_extractor import LLMExtractor\nfrom knowledge_garden.extraction.prompt_manager import PromptManager\nfrom knowledge_garden.infrastructure.services.llm_service import LLMService\nfrom knowledge_garden.core.fuseki_client import FusekiClient\nfrom knowledge_garden.core.config import config\nfrom knowledge_garden.models.evidence import Evidence\nfrom knowledge_garden.models.extracted_sentence import ExtractedSentence\nfrom knowledge_garden.parliament.decision_engine import ParliamentEngine\n\nlogger = logging.getLogger(__name__)\n\n\nclass UnifiedExtractionService:\n    \"\"\"Unified service for all extraction operations\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the unified extraction service\"\"\"\n        # Initialize services\n        self.embedding_generator = EmbeddingGenerator(\n            model_name=config.get(\"models.embedding\"),\n            cache_dir=config.get(\"project.data_dir\") / \"embeddings\"\n        )\n        \n        self.llm_service = LLMService(provider=\"anthropic\")\n        self.fuseki_client = FusekiClient()\n        \n        # Initialize parliament if configured\n        self.parliament = None\n        if config.get(\"parliament.core.enabled\", False):\n            self.parliament = ParliamentEngine(\n                llm_service=self.llm_service,\n                fuseki_client=self.fuseki_client\n            )\n        \n        # Initialize extraction components\n        self.prompt_manager = PromptManager(\n            llm_service=self.llm_service,\n            cache_enabled=True\n        )\n        \n        self.evidence_tracker = EvidenceTracker(\n            fuseki_client=self.fuseki_client\n        )\n        \n        self.sentence_extractor = SentenceExtractor(\n            embedder=self.embedding_generator,\n            llm=self.llm_service,\n            prompt_manager=self.prompt_manager,\n            evidence_tracker=self.evidence_tracker\n        )\n        \n        self.pattern_extractor = PatternExtractor()\n        \n        self.llm_extractor = LLMExtractor(\n            api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n        )\n        \n        self.entity_resolver = EntityResolver(\n            embedding_generator=self.embedding_generator,\n            llm_service=self.llm_service,\n            parliament_engine=self.parliament\n        )\n        \n        # Statistics\n        self.stats = {\n            \"total_extractions\": 0,\n            \"total_sentences\": 0,\n            \"total_entities\": 0,\n            \"total_triples\": 0,\n            \"total_patterns\": 0\n        }\n    \n    async def extract_from_text(\n        self, \n        text: str, \n        source_uri: str,\n        metadata: Optional[Dict[str, Any]] = None,\n        options: Optional[Dict[str, Any]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Extract knowledge from text\n        \n        Args:\n            text: Text to extract from\n            source_uri: URI of the source document/conversation\n            metadata: Optional metadata about the source\n            options: Extraction options (use_llm, skip_patterns, etc.)\n            \n        Returns:\n            Extraction results dictionary\n        \"\"\"\n        options = options or {}\n        metadata = metadata or {}\n        \n        results = {\n            \"source_uri\": source_uri,\n            \"timestamp\": datetime.now().isoformat(),\n            \"sentences\": [],\n            \"entities\": [],\n            \"triples\": [],\n            \"patterns\": {\n                \"caps\": [],\n                \"questions\": [],\n                \"insights\": [],\n                \"tensions\": []\n            },\n            \"evidence\": [],\n            \"metadata\": metadata,\n            \"stats\": {}\n        }\n        \n        try:\n            # Step 1: Extract sentences with embeddings\n            sentences, sentence_evidence = await self.sentence_extractor.process_text(\n                text, source_uri\n            )\n            results[\"sentences\"] = [s.to_dict() for s in sentences]\n            results[\"evidence\"].extend([e.to_dict() for e in sentence_evidence])\n            \n            # Step 2: Extract patterns (unless skipped)\n            if not options.get(\"skip_patterns\", False):\n                from knowledge_garden.models.elements import ExtractedElements\n                elements = ExtractedElements()\n                \n                # Use pattern extractor\n                self.pattern_extractor.extract(text, elements)\n                \n                # Use LLM extractor if enabled\n                if options.get(\"use_llm\", True) and self.llm_extractor.client:\n                    self.llm_extractor.extract(text, elements)\n                \n                # Convert to results format\n                results[\"patterns\"][\"caps\"] = [\n                    {\"pattern\": cap[\"pattern\"], \"confidence\": cap.get(\"confidence\", 0.8)}\n                    for cap in elements.caps\n                ]\n                results[\"patterns\"][\"questions\"] = [\n                    {\"text\": q[\"text\"], \"confidence\": q.get(\"confidence\", 0.8)}\n                    for q in elements.questions\n                ]\n                results[\"patterns\"][\"insights\"] = [\n                    {\"realization\": i[\"realization\"], \"confidence\": i.get(\"confidence\", 0.8)}\n                    for i in elements.insights\n                ]\n                results[\"patterns\"][\"tensions\"] = [\n                    {\"pole1\": t[\"pole1\"], \"pole2\": t[\"pole2\"], \"confidence\": t.get(\"confidence\", 0.8)}\n                    for t in elements.tensions\n                ]\n            \n            # Step 3: Extract entities and resolve\n            entities = {}\n            for sentence in sentences:\n                for triple in sentence.triples:\n                    # Extract subject entity\n                    subject = triple.get(\"subject\", \"\")\n                    if subject and subject not in entities:\n                        entity = await self._create_entity(subject, triple.get(\"subject_type\"))\n                        entities[subject] = entity\n                    \n                    # Extract object entity (if not literal)\n                    object_val = triple.get(\"object\", \"\")\n                    object_type = triple.get(\"object_type\", \"\")\n                    if object_val and object_type.lower() not in [\"value\", \"literal\", \"string\"]:\n                        if object_val not in entities:\n                            entity = await self._create_entity(object_val, object_type)\n                            entities[object_val] = entity\n            \n            # Resolve entities\n            resolved_entities = []\n            entity_map = {}  # Map old labels to resolved entities\n            \n            for label, entity in entities.items():\n                # Check if we already resolved a similar entity\n                resolved = False\n                for resolved_entity in resolved_entities:\n                    resolution = await self.entity_resolver.resolve_entity(\n                        entity, {resolved_entity.entity_id: resolved_entity}\n                    )\n                    if resolution.action == \"merge\":\n                        entity_map[label] = resolved_entity\n                        resolved = True\n                        break\n                \n                if not resolved:\n                    resolved_entities.append(entity)\n                    entity_map[label] = entity\n            \n            results[\"entities\"] = [e.to_dict() for e in resolved_entities]\n            \n            # Step 4: Extract triples with resolved entities\n            for sentence in sentences:\n                for triple_data in sentence.triples:\n                    subject = triple_data.get(\"subject\", \"\")\n                    predicate = triple_data.get(\"predicate\", \"\")\n                    object_val = triple_data.get(\"object\", \"\")\n                    \n                    # Map to resolved entities\n                    subject_entity = entity_map.get(subject)\n                    object_entity = entity_map.get(object_val)\n                    \n                    triple = {\n                        \"subject\": subject_entity.entity_id if subject_entity else subject,\n                        \"predicate\": predicate,\n                        \"object\": object_entity.entity_id if object_entity else object_val,\n                        \"is_literal\": object_entity is None,\n                        \"confidence\": triple_data.get(\"confidence\", 0.7),\n                        \"source_sentence\": sentence.sentence_id\n                    }\n                    results[\"triples\"].append(triple)\n            \n            # Update statistics\n            results[\"stats\"] = {\n                \"sentences\": len(results[\"sentences\"]),\n                \"entities\": len(results[\"entities\"]),\n                \"triples\": len(results[\"triples\"]),\n                \"caps\": len(results[\"patterns\"][\"caps\"]),\n                \"questions\": len(results[\"patterns\"][\"questions\"]),\n                \"insights\": len(results[\"patterns\"][\"insights\"]),\n                \"tensions\": len(results[\"patterns\"][\"tensions\"]),\n                \"evidence\": len(results[\"evidence\"])\n            }\n            \n            # Update global stats\n            self.stats[\"total_extractions\"] += 1\n            self.stats[\"total_sentences\"] += results[\"stats\"][\"sentences\"]\n            self.stats[\"total_entities\"] += results[\"stats\"][\"entities\"]\n            self.stats[\"total_triples\"] += results[\"stats\"][\"triples\"]\n            self.stats[\"total_patterns\"] += sum([\n                results[\"stats\"][\"caps\"],\n                results[\"stats\"][\"questions\"],\n                results[\"stats\"][\"insights\"],\n                results[\"stats\"][\"tensions\"]\n            ])\n            \n            return results\n            \n        except Exception as e:\n            logger.error(f\"Extraction failed: {e}\")\n            results[\"error\"] = str(e)\n            return results\n    \n    async def extract_from_conversations(\n        self,\n        conversations: List[Dict[str, Any]],\n        options: Optional[Dict[str, Any]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Extract knowledge from multiple conversations\n        \n        Args:\n            conversations: List of conversation dictionaries\n            options: Extraction options\n            \n        Returns:\n            Aggregated extraction results\n        \"\"\"\n        all_results = {\n            \"conversations\": [],\n            \"aggregated\": {\n                \"entities\": {},\n                \"triples\": [],\n                \"patterns\": {\n                    \"caps\": [],\n                    \"questions\": [],\n                    \"insights\": [],\n                    \"tensions\": []\n                }\n            },\n            \"stats\": {\n                \"total_conversations\": len(conversations),\n                \"total_messages\": 0\n            }\n        }\n        \n        # Process each conversation\n        for conv in conversations:\n            conv_id = conv.get(\"id\", f\"conv_{datetime.now().timestamp()}\")\n            conv_results = {\n                \"id\": conv_id,\n                \"title\": conv.get(\"title\", \"Untitled\"),\n                \"messages\": []\n            }\n            \n            # Process each message\n            messages = conv.get(\"messages\", [])\n            for msg in messages:\n                if msg.get(\"role\") == \"user\":\n                    text = msg.get(\"content\", \"\")\n                    if text:\n                        source_uri = f\"conversation:{conv_id}:message:{msg.get('id', '')}\"\n                        \n                        # Extract from message\n                        extraction = await self.extract_from_text(\n                            text=text,\n                            source_uri=source_uri,\n                            metadata={\n                                \"conversation_id\": conv_id,\n                                \"message_id\": msg.get(\"id\"),\n                                \"role\": msg.get(\"role\")\n                            },\n                            options=options\n                        )\n                        \n                        conv_results[\"messages\"].append(extraction)\n                        all_results[\"stats\"][\"total_messages\"] += 1\n                        \n                        # Aggregate results\n                        for entity in extraction.get(\"entities\", []):\n                            entity_id = entity[\"entity_id\"]\n                            if entity_id not in all_results[\"aggregated\"][\"entities\"]:\n                                all_results[\"aggregated\"][\"entities\"][entity_id] = entity\n                        \n                        all_results[\"aggregated\"][\"triples\"].extend(\n                            extraction.get(\"triples\", [])\n                        )\n                        \n                        # Aggregate patterns\n                        for pattern_type in [\"caps\", \"questions\", \"insights\", \"tensions\"]:\n                            all_results[\"aggregated\"][\"patterns\"][pattern_type].extend(\n                                extraction.get(\"patterns\", {}).get(pattern_type, [])\n                            )\n            \n            all_results[\"conversations\"].append(conv_results)\n        \n        # Deduplicate patterns\n        for pattern_type in [\"caps\", \"questions\", \"insights\", \"tensions\"]:\n            patterns = all_results[\"aggregated\"][\"patterns\"][pattern_type]\n            # Simple deduplication based on text content\n            seen = set()\n            unique_patterns = []\n            for pattern in patterns:\n                # Get the text key based on pattern type\n                if pattern_type == \"caps\":\n                    key = pattern.get(\"pattern\", \"\")\n                elif pattern_type == \"questions\":\n                    key = pattern.get(\"text\", \"\")\n                elif pattern_type == \"insights\":\n                    key = pattern.get(\"realization\", \"\")\n                elif pattern_type == \"tensions\":\n                    key = f\"{pattern.get('pole1', '')}:{pattern.get('pole2', '')}\"\n                \n                if key and key not in seen:\n                    seen.add(key)\n                    unique_patterns.append(pattern)\n            \n            all_results[\"aggregated\"][\"patterns\"][pattern_type] = unique_patterns\n        \n        return all_results\n    \n    async def _create_entity(self, label: str, entity_type: Optional[str] = None):\n        \"\"\"Create an entity object\"\"\"\n        from knowledge_garden.extraction.entity_resolver import Entity\n        from knowledge_garden.core.uri_generator import uri_generator\n        \n        entity_id = uri_generator.generate_entity_uri(label, entity_type or \"Entity\")\n        \n        entity = Entity(\n            entity_id=entity_id,\n            label=label,\n            entity_type=entity_type\n        )\n        \n        # Generate embedding\n        if self.embedding_generator:\n            entity.embedding = await self.embedding_generator.encode(label)\n        \n        return entity\n    \n    async def store_results(self, results: Dict[str, Any]) -> bool:\n        \"\"\"\n        Store extraction results in Fuseki\n        \n        Args:\n            results: Extraction results to store\n            \n        Returns:\n            Success status\n        \"\"\"\n        if not self.fuseki_client:\n            logger.warning(\"No Fuseki client available, skipping storage\")\n            return False\n        \n        try:\n            # Store entities\n            for entity in results.get(\"entities\", []):\n                entity_triples = self._entity_to_triples(entity)\n                await self.fuseki_client.insert_triples(entity_triples)\n            \n            # Store triples\n            for triple in results.get(\"triples\", []):\n                triple_triples = self._triple_to_triples(triple)\n                await self.fuseki_client.insert_triples(triple_triples)\n            \n            # Store evidence\n            if self.evidence_tracker:\n                for evidence_dict in results.get(\"evidence\", []):\n                    evidence = Evidence.from_dict(evidence_dict)\n                    # You'll need to determine the target URI based on what the evidence supports\n                    target_uri = evidence_dict.get(\"target_uri\", results[\"source_uri\"])\n                    await self.evidence_tracker.store_evidence(evidence, target_uri)\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to store results: {e}\")\n            return False\n    \n    def _entity_to_triples(self, entity: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Convert entity to RDF triples\"\"\"\n        entity_uri = entity[\"entity_id\"]\n        \n        triples = [\n            {\n                \"subject\": entity_uri,\n                \"predicate\": \"rdf:type\",\n                \"object\": f\"kg:{entity.get('entity_type', 'Entity')}\",\n                \"is_literal\": False\n            },\n            {\n                \"subject\": entity_uri,\n                \"predicate\": \"rdfs:label\",\n                \"object\": entity[\"label\"],\n                \"is_literal\": True\n            }\n        ]\n        \n        # Add attributes\n        for key, value in entity.get(\"attributes\", {}).items():\n            triples.append({\n                \"subject\": entity_uri,\n                \"predicate\": f\"kg:{key}\",\n                \"object\": str(value),\n                \"is_literal\": True\n            })\n        \n        return triples\n    \n    def _triple_to_triples(self, triple: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Convert knowledge triple to RDF triples\"\"\"\n        # For now, just return the triple as-is\n        # In a full implementation, you might add metadata, provenance, etc.\n        return [{\n            \"subject\": triple[\"subject\"],\n            \"predicate\": triple[\"predicate\"],\n            \"object\": triple[\"object\"],\n            \"is_literal\": triple.get(\"is_literal\", False)\n        }]\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get extraction statistics\"\"\"\n        return self.stats.copy()\n\n\n# Global instance\nextraction_service = UnifiedExtractionService()\n'''\n\n# Write the unified extraction service\nPath(\"knowledge_garden/services/extraction/__init__.py\").parent.mkdir(parents=True, exist_ok=True)\nwith open(\"knowledge_garden/services/extraction/__init__.py\", \"w\") as f:\n    f.write(service_content)\n\nprint(\"âœ… Created unified extraction service\")\n```\n\n### 3. Create API Endpoint\n\n```python\n#!/usr/bin/env python3\n# save as: create_extraction_api.py\n\nimport os\nfrom pathlib import Path\n\n# Create FastAPI endpoint for extraction\napi_content = '''\"\"\"\nExtraction API endpoints for Knowledge Garden\n\"\"\"\nfrom fastapi import APIRouter, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, List, Any, Optional\nimport logging\n\nfrom knowledge_garden.services.extraction import extraction_service\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(prefix=\"/api/v1/extraction\", tags=[\"extraction\"])\n\n\nclass ExtractTextRequest(BaseModel):\n    \"\"\"Request model for text extraction\"\"\"\n    text: str = Field(..., description=\"Text to extract knowledge from\")\n    source_uri: str = Field(..., description=\"URI identifying the source\")\n    metadata: Optional[Dict[str, Any]] = Field(default={}, description=\"Optional metadata\")\n    options: Optional[Dict[str, Any]] = Field(\n        default={},\n        description=\"Extraction options (use_llm, skip_patterns, etc.)\"\n    )\n\n\nclass ExtractConversationsRequest(BaseModel):\n    \"\"\"Request model for conversation extraction\"\"\"\n    conversations: List[Dict[str, Any]] = Field(..., description=\"List of conversations\")\n    options: Optional[Dict[str, Any]] = Field(\n        default={},\n        description=\"Extraction options\"\n    )\n\n\nclass ExtractionResponse(BaseModel):\n    \"\"\"Response model for extraction results\"\"\"\n    success: bool\n    results: Optional[Dict[str, Any]] = None\n    error: Optional[str] = None\n\n\n@router.post(\"/text\", response_model=ExtractionResponse)\nasync def extract_from_text(request: ExtractTextRequest):\n    \"\"\"\n    Extract knowledge from text\n    \n    This endpoint processes raw text and extracts:\n    - Sentences with embeddings\n    - Entities (people, concepts, etc.)\n    - Relationships (triples)\n    - Patterns (CAPs, questions, insights, tensions)\n    - Evidence for provenance\n    \"\"\"\n    try:\n        results = await extraction_service.extract_from_text(\n            text=request.text,\n            source_uri=request.source_uri,\n            metadata=request.metadata,\n            options=request.options\n        )\n        \n        return ExtractionResponse(\n            success=True,\n            results=results\n        )\n        \n    except Exception as e:\n        logger.error(f\"Extraction failed: {e}\")\n        return ExtractionResponse(\n            success=False,\n            error=str(e)\n        )\n\n\n@router.post(\"/conversations\", response_model=ExtractionResponse)\nasync def extract_from_conversations(request: ExtractConversationsRequest):\n    \"\"\"\n    Extract knowledge from multiple conversations\n    \n    This endpoint processes conversation data (e.g., from ChatGPT exports)\n    and extracts aggregated knowledge across all conversations.\n    \"\"\"\n    try:\n        results = await extraction_service.extract_from_conversations(\n            conversations=request.conversations,\n            options=request.options\n        )\n        \n        return ExtractionResponse(\n            success=True,\n            results=results\n        )\n        \n    except Exception as e:\n        logger.error(f\"Conversation extraction failed: {e}\")\n        return ExtractionResponse(\n            success=False,\n            error=str(e)\n        )\n\n\n@router.post(\"/text/store\", response_model=ExtractionResponse)\nasync def extract_and_store(\n    request: ExtractTextRequest,\n    background_tasks: BackgroundTasks\n):\n    \"\"\"\n    Extract knowledge from text and store in knowledge graph\n    \n    This endpoint extracts knowledge and immediately stores it in Fuseki.\n    Storage happens in the background to avoid blocking the response.\n    \"\"\"\n    try:\n        # Extract knowledge\n        results = await extraction_service.extract_from_text(\n            text=request.text,\n            source_uri=request.source_uri,\n            metadata=request.metadata,\n            options=request.options\n        )\n        \n        # Store in background\n        background_tasks.add_task(\n            extraction_service.store_results,\n            results\n        )\n        \n        return ExtractionResponse(\n            success=True,\n            results=results\n        )\n        \n    except Exception as e:\n        logger.error(f\"Extract and store failed: {e}\")\n        return ExtractionResponse(\n            success=False,\n            error=str(e)\n        )\n\n\n@router.get(\"/stats\")\nasync def get_extraction_stats():\n    \"\"\"Get extraction service statistics\"\"\"\n    return extraction_service.get_stats()\n\n\n@router.get(\"/health\")\nasync def health_check():\n    \"\"\"Check extraction service health\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"service\": \"extraction\",\n        \"version\": \"1.0.0\"\n    }\n'''\n\n# Write the API endpoint\nPath(\"knowledge_garden/api/endpoints/extraction.py\").parent.mkdir(parents=True, exist_ok=True)\nwith open(\"knowledge_garden/api/endpoints/extraction.py\", \"w\") as f:\n    f.write(api_content)\n\n# Create main API app\napi_app_content = '''\"\"\"\nKnowledge Garden API Application\n\"\"\"\nfrom fastapi import FastAPI, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nimport logging\n\nfrom knowledge_garden.api.endpoints import extraction\n\nlogger = logging.getLogger(__name__)\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"Knowledge Garden API\",\n    description=\"Unified API for Knowledge Garden extraction and management\",\n    version=\"1.0.0\"\n)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Include routers\napp.include_router(extraction.router)\n\n\n@app.exception_handler(Exception)\nasync def global_exception_handler(request: Request, exc: Exception):\n    \"\"\"Global exception handler\"\"\"\n    logger.error(f\"Unhandled exception: {exc}\")\n    return JSONResponse(\n        status_code=500,\n        content={\"error\": \"Internal server error\", \"detail\": str(exc)}\n    )\n\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Root endpoint\"\"\"\n    return {\n        \"name\": \"Knowledge Garden API\",\n        \"version\": \"1.0.0\",\n        \"endpoints\": {\n            \"extraction\": \"/api/v1/extraction\",\n            \"docs\": \"/docs\",\n            \"redoc\": \"/redoc\"\n        }\n    }\n\n\n@app.get(\"/health\")\nasync def health():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"healthy\"}\n'''\n\nwith open(\"knowledge_garden/api/__init__.py\", \"w\") as f:\n    f.write('\"\"\"Knowledge Garden API\"\"\"')\n\nwith open(\"knowledge_garden/api/app.py\", \"w\") as f:\n    f.write(api_app_content)\n\n# Create run script for the API\nrun_api_content = '''#!/usr/bin/env python3\n\"\"\"\nRun the Knowledge Garden API server\n\"\"\"\nimport uvicorn\nimport argparse\nimport sys\nfrom pathlib import Path\n\n# Add parent directory to path\nsys.path.insert(0, str(Path(__file__).parent))\n\nfrom knowledge_garden.api.app import app\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run Knowledge Garden API\")\n    parser.add_argument(\"--host\", default=\"0.0.0.0\", help=\"Host to bind to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind to\")\n    parser.add_argument(\"--reload\", action=\"store_true\", help=\"Enable auto-reload\")\n    args = parser.parse_args()\n    \n    uvicorn.run(\n        \"knowledge_garden.api.app:app\",\n        host=args.host,\n        port=args.port,\n        reload=args.reload,\n        log_level=\"info\"\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n'''\n\nwith open(\"run_api.py\", \"w\") as f:\n    f.write(run_api_content)\n\nos.chmod(\"run_api.py\", 0o755)\n\nprint(\"âœ… Created API endpoints and run script\")\n```\n\n### 4. Update CLI to Use Unified Service\n\n```python\n#!/usr/bin/env python3\n# save as: update_cli_extraction.py\n\nimport re\nfrom pathlib import Path\n\n# Read current CLI\ncli_path = Path(\"knowledge_garden/cli.py\")\nwith open(cli_path, \"r\") as f:\n    cli_content = f.read()\n\n# Find and replace the extract command\nnew_extract_command = '''@cli.command()\n@click.argument('input_file', type=click.Path(exists=True))\n@click.option('--output', type=click.Path(), help='Output file path')\n@click.option('--source-uri', help='Source URI (defaults to file path)')\n@click.option('--use-llm/--no-llm', default=True, help='Use LLM for enhanced extraction')\n@click.option('--skip-patterns', is_flag=True, help='Skip pattern extraction')\n@click.option('--store', is_flag=True, help='Store results in Fuseki')\n@click.option('--format', type=click.Choice(['json', 'turtle', 'summary']), default='json', help='Output format')\nasync def extract(input_file, output, source_uri, use_llm, skip_patterns, store, format):\n    \"\"\"Extract knowledge from text or conversations\"\"\"\n    from knowledge_garden.services.extraction import extraction_service\n    import json\n    import asyncio\n    \n    # Determine source URI\n    if not source_uri:\n        source_uri = f\"file://{Path(input_file).absolute()}\"\n    \n    click.echo(f\"ðŸ“Š Extracting knowledge from {input_file}\")\n    \n    # Read input file\n    with open(input_file, 'r') as f:\n        content = f.read()\n    \n    # Check if it's JSON (conversations) or plain text\n    try:\n        data = json.loads(content)\n        if isinstance(data, list) and all('messages' in item for item in data[:5]):\n            # It's a conversation export\n            click.echo(\"ðŸ“± Detected conversation format\")\n            results = await extraction_service.extract_from_conversations(\n                conversations=data,\n                options={\n                    'use_llm': use_llm,\n                    'skip_patterns': skip_patterns\n                }\n            )\n        else:\n            # Treat as plain text\n            click.echo(\"ðŸ“ Processing as plain text\")\n            results = await extraction_service.extract_from_text(\n                text=content,\n                source_uri=source_uri,\n                options={\n                    'use_llm': use_llm,\n                    'skip_patterns': skip_patterns\n                }\n            )\n    except json.JSONDecodeError:\n        # Plain text\n        click.echo(\"ðŸ“ Processing as plain text\")\n        results = await extraction_service.extract_from_text(\n            text=content,\n            source_uri=source_uri,\n            options={\n                'use_llm': use_llm,\n                'skip_patterns': skip_patterns\n            }\n        )\n    \n    # Store if requested\n    if store:\n        click.echo(\"ðŸ’¾ Storing results in Fuseki...\")\n        success = await extraction_service.store_results(results)\n        if success:\n            click.echo(\"âœ… Results stored successfully\")\n        else:\n            click.echo(\"âŒ Failed to store results\")\n    \n    # Output results\n    if output:\n        output_path = Path(output)\n    else:\n        output_path = Path(input_file).with_suffix(f'.extraction.{format}')\n    \n    if format == 'json':\n        with open(output_path, 'w') as f:\n            json.dump(results, f, indent=2)\n    elif format == 'turtle':\n        # Convert to Turtle format\n        click.echo(\"ðŸ¢ Turtle format not yet implemented\")\n    elif format == 'summary':\n        # Create human-readable summary\n        summary = f\"\"\"Knowledge Extraction Summary\n==========================\nSource: {source_uri}\nTimestamp: {results.get('timestamp', 'N/A')}\n\nStatistics:\n-----------\nSentences: {results.get('stats', {}).get('sentences', 0)}\nEntities: {results.get('stats', {}).get('entities', 0)}\nTriples: {results.get('stats', {}).get('triples', 0)}\nCAPs: {results.get('stats', {}).get('caps', 0)}\nQuestions: {results.get('stats', {}).get('questions', 0)}\nInsights: {results.get('stats', {}).get('insights', 0)}\nTensions: {results.get('stats', {}).get('tensions', 0)}\n\nTop Entities:\n------------\n\"\"\"\n        for entity in results.get('entities', [])[:10]:\n            summary += f\"- {entity['label']} ({entity.get('entity_type', 'Unknown')})\"\n        \n        with open(output_path, 'w') as f:\n            f.write(summary)\n    \n    click.echo(f\"âœ… Extraction complete! Results saved to {output_path}\")\n    \n    # Print summary\n    stats = results.get('stats', {})\n    click.echo(f\"\"\"\nðŸ“Š Extraction Summary:\n  - Sentences: {stats.get('sentences', 0)}\n  - Entities: {stats.get('entities', 0)}\n  - Triples: {stats.get('triples', 0)}\n  - Patterns: {stats.get('caps', 0) + stats.get('questions', 0) + stats.get('insights', 0) + stats.get('tensions', 0)}\n    \"\"\")'''\n\n# Replace the extract command in CLI\n# This is a simplified replacement - in practice you'd use proper AST parsing\ncli_content = re.sub(\n    r'@cli\\.command\\(\\)\\s*@click\\.argument.*?def extract.*?click\\.echo\\(f\"âœ… Knowledge extraction complete!\"\\)',\n    new_extract_command,\n    cli_content,\n    flags=re.DOTALL\n)\n\n# Write updated CLI\nwith open(cli_path, \"w\") as f:\n    f.write(cli_content)\n\nprint(\"âœ… Updated CLI to use unified extraction service\")\n```\n\n### 5. Clean Up Duplicate Files\n\n```bash\n#!/bin/bash\n# save as: cleanup_extraction.sh\n\necho \"ðŸ§¹ Cleaning up duplicate extraction files...\"\n\n# Remove duplicate extraction implementations\necho \"Removing pipeline module (duplicate implementation)...\"\nrm -rf knowledge_garden/pipeline/\n\necho \"Removing apps/extractor (duplicate implementation)...\"\nrm -rf knowledge_garden/apps/extractor/\n\necho \"Removing test_extraction.py (moved to tests/)...\"\nrm -f knowledge_garden/test_extraction.py\n\n# Move any useful tests to tests directory\nmkdir -p knowledge_garden/tests/extraction\nif [ -f \"knowledge_garden/test_extraction.py\" ]; then\n    mv knowledge_garden/test_extraction.py knowledge_garden/tests/extraction/test_basic_extraction.py\nfi\n\n# Create proper test structure\ncat > knowledge_garden/tests/extraction/__init__.py << 'EOF'\n\"\"\"Extraction tests for Knowledge Garden\"\"\"\nEOF\n\ncat > knowledge_garden/tests/extraction/test_unified_extraction.py << 'EOF'\n\"\"\"Tests for unified extraction service\"\"\"\nimport pytest\nimport asyncio\nfrom knowledge_garden.services.extraction import extraction_service\n\n\n@pytest.mark.asyncio\nasync def test_extract_from_text():\n    \"\"\"Test basic text extraction\"\"\"\n    text = \"John works at OpenAI. He is developing GPT models.\"\n    source_uri = \"test:123\"\n    \n    results = await extraction_service.extract_from_text(\n        text=text,\n        source_uri=source_uri\n    )\n    \n    assert results[\"source_uri\"] == source_uri\n    assert len(results[\"sentences\"]) > 0\n    assert \"stats\" in results\n\n\n@pytest.mark.asyncio\nasync def test_extract_patterns():\n    \"\"\"Test pattern extraction\"\"\"\n    text = \"\"\"\n    MOMENTS when I feel stuck are OPPORTUNITIES for creative thinking.\n    I realized that taking breaks improves productivity.\n    How might we balance work and life better?\n    There's a tension between speed and quality.\n    \"\"\"\n    \n    results = await extraction_service.extract_from_text(\n        text=text,\n        source_uri=\"test:patterns\"\n    )\n    \n    patterns = results[\"patterns\"]\n    assert len(patterns[\"caps\"]) > 0\n    assert len(patterns[\"insights\"]) > 0\n    assert len(patterns[\"questions\"]) > 0\n    assert len(patterns[\"tensions\"]) > 0\nEOF\n\necho \"âœ… Cleanup complete\"\n```\n\n### 6. Update Imports Script\n\n```python\n#!/usr/bin/env python3\n# save as: fix_extraction_imports.py\n\nimport os\nimport re\nfrom pathlib import Path\n\ndef update_imports_in_file(file_path):\n    \"\"\"Update imports in a single file\"\"\"\n    with open(file_path, 'r') as f:\n        content = f.read()\n    \n    # Map old imports to new ones\n    replacements = [\n        # Pipeline imports -> extraction imports\n        (r'from knowledge_garden\\.pipeline\\.pipeline import ExtractionPipeline',\n         'from knowledge_garden.services.extraction import extraction_service'),\n        (r'from knowledge_garden\\.pipeline\\.runner import extraction_pipeline',\n         'from knowledge_garden.services.extraction import extraction_service'),\n        (r'from knowledge_garden\\.pipeline\\..*? import',\n         'from knowledge_garden.extraction import'),\n        \n        # Apps extractor imports -> service imports\n        (r'from knowledge_garden\\.apps\\.extractor\\..*? import',\n         'from knowledge_garden.services.extraction import'),\n        \n        # Direct extraction module imports remain the same\n        # Just ensure they're properly namespaced\n        (r'from extraction\\.',\n         'from knowledge_garden.extraction.'),\n    ]\n    \n    modified = False\n    for old_pattern, new_pattern in replacements:\n        new_content = re.sub(old_pattern, new_pattern, content)\n        if new_content != content:\n            content = new_content\n            modified = True\n    \n    if modified:\n        with open(file_path, 'w') as f:\n            f.write(content)\n        print(f\"Updated imports in {file_path}\")\n    \n    return modified\n\n# Find all Python files\nupdated_count = 0\nfor py_file in Path(\"knowledge_garden\").rglob(\"*.py\"):\n    if update_imports_in_file(py_file):\n        updated_count += 1\n\nprint(f\"âœ… Updated imports in {updated_count} files\")\n```\n\n### 7. Create Microservice Docker Setup\n\n```dockerfile\n# save as: Dockerfile.extraction\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY knowledge_garden knowledge_garden/\nCOPY run_api.py .\n\n# Expose port\nEXPOSE 8000\n\n# Run the API\nCMD [\"python\", \"run_api.py\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n```yaml\n# save as: docker-compose.yml\nversion: '3.8'\n\nservices:\n  fuseki:\n    image: stain/jena-fuseki:latest\n    environment:\n      - ADMIN_PASSWORD=admin\n    ports:\n      - \"3030:3030\"\n    volumes:\n      - fuseki_data:/fuseki\n    \n  extraction-api:\n    build:\n      context: .\n      dockerfile: Dockerfile.extraction\n    ports:\n      - \"8000:8000\"\n    environment:\n      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - FUSEKI_URL=http://fuseki:3030\n      - FUSEKI_DATASET=kg\n    depends_on:\n      - fuseki\n    volumes:\n      - ./knowledge_garden:/app/knowledge_garden\n      - ./data:/app/data\n\nvolumes:\n  fuseki_data:\n```\n\n### 8. Master Script to Run Everything\n\n```bash\n#!/bin/bash\n# save as: reorganize_all.sh\n\necho \"ðŸš€ Knowledge Garden Extraction Reorganization\"\necho \"============================================\"\n\n# Make all scripts executable\nchmod +x reorganize_extraction.sh\nchmod +x cleanup_extraction.sh\n\n# Step 1: Initial reorganization\necho \"ðŸ“ Step 1: Creating new structure...\"\n./reorganize_extraction.sh\n\n# Step 2: Create unified service\necho \"ðŸ”§ Step 2: Creating unified extraction service...\"\npython3 create_unified_extraction.py\n\n# Step 3: Create API\necho \"ðŸŒ Step 3: Creating API endpoints...\"\npython3 create_extraction_api.py\n\n# Step 4: Update CLI\necho \"ðŸ“ Step 4: Updating CLI...\"\npython3 update_cli_extraction.py\n\n# Step 5: Fix imports\necho \"ðŸ”— Step 5: Fixing imports...\"\npython3 fix_extraction_imports.py\n\n# Step 6: Clean up duplicates\necho \"ðŸ§¹ Step 6: Cleaning up duplicate files...\"\n./cleanup_extraction.sh\n\n# Step 7: Update requirements\necho \"ðŸ“¦ Step 7: Updating requirements.txt...\"\ncat >> requirements.txt << 'EOF'\n\n# API requirements\nfastapi>=0.104.0\nuvicorn[standard]>=0.24.0\npython-multipart>=0.0.6\nEOF\n\necho \"âœ… Reorganization complete!\"\necho \"\"\necho \"Next steps:\"\necho \"1. Install new requirements: pip install -r requirements.txt\"\necho \"2. Run the API: python run_api.py\"\necho \"3. Access API docs: http://localhost:8000/docs\"\necho \"4. Use CLI: python -m knowledge_garden.cli extract <file>\"\necho \"5. Run with Docker: docker-compose up\"\n```\n\n### Terminal Commands to Execute\n\nRun these commands in order:\n\n```bash\n# 1. Save all the scripts\n# (Save each script to its designated filename)\n\n# 2. Make the master script executable and run it\nchmod +x reorganize_all.sh\n./reorganize_all.sh\n\n# 3. Install new requirements\npip install -r requirements.txt\n\n# 4. Test the unified extraction service\npython -c \"from knowledge_garden.services.extraction import extraction_service; print('âœ… Import successful')\"\n\n# 5. Run the API server\npython run_api.py\n\n# 6. In another terminal, test the API\ncurl http://localhost:8000/health\ncurl http://localhost:8000/api/v1/extraction/health\n\n# 7. Test extraction via CLI\necho \"This is a test. MOMENTS when we test are OPPORTUNITIES for learning.\" > test.txt\npython -m knowledge_garden.cli extract test.txt --format summary\n\n# 8. Optional: Run with Docker\ndocker-compose up -d\n```\n\nThis reorganization:\n1. **Unifies all extraction logic** into a single service\n2. **Removes duplicate implementations** from pipeline/, apps/extractor/, etc.\n3. **Creates a clean API** with FastAPI\n4. **Updates the CLI** to use the unified service\n5. **Provides Docker setup** for microservice deployment\n6. **Maintains all core functionality** while simplifying the structure\n\nThe extraction is now available through:\n- **API**: `http://localhost:8000/api/v1/extraction`\n- **CLI**: `python -m knowledge_garden.cli extract`\n- **Python**: `from knowledge_garden.services.extraction import extraction_service`",
      "created_at": "2025-06-12T05:54:19.519129Z",
      "sequence_order": 1,
      "metadata": {
        "content": [
          {
            "start_timestamp": "2025-06-12T05:50:49.198272Z",
            "stop_timestamp": "2025-06-12T05:51:02.472913Z",
            "flags": null,
            "type": "thinking",
            "thinking": "Looking at the Knowledge Garden codebase, I can see there are indeed multiple extraction-related components scattered across different places:\n\n1. **Extraction Module** (`knowledge_garden/extraction/`):\n   - `pipeline.py` - Main extraction pipeline\n   - `sentence_extractor.py` - Sentence-level extraction\n   - `pattern_extractor.py` - Pattern extraction (CAPs, questions, insights, tensions)\n   - `entity_resolver.py` - Entity resolution\n   - `llm_extractor.py` - LLM-based extraction\n   - Other supporting files\n\n2. **Apps/Extractor** (`knowledge_garden/apps/extractor/`):\n   - `extract.py` - Appears to be another extraction implementation\n   - Has its own README\n\n3. **Pipeline Module** (`knowledge_garden/pipeline/`):\n   - Another pipeline implementation with stages\n   - `runner.py` - ExtractionPipeline class\n   - Various stages in `stages/` directory\n\n4. **Test Extraction** (`knowledge_garden/test_extraction.py`):\n   - Standalone test extraction script\n\n5. **CLI** (`knowledge_garden/cli.py`):\n   - Has an `extract` command\n\nThis is clearly duplicated functionality that needs to be unified. Let me analyze what each does:\n\n1. The `extraction/` module seems to be the most comprehensive with all the components\n2. The `pipeline/` module seems to be a different architecture using a staged approach\n3. The `apps/extractor/` seems to be a standalone app\n4. `test_extraction.py` appears to be a test/example script\n\nFor a unified approach, I would:\n1. Keep the core extraction logic in `knowledge_garden/extraction/`\n2. Create a single extraction service/API endpoint\n3. Remove duplicate implementations\n4. Update the CLI to use the unified extraction\n5. Create a proper microservice setup if needed\n\nLet me create scripts to reorganize this.",
            "summaries": [
              {
                "summary": "Thinking about consolidating and streamlining code extraction processes"
              },
              {
                "summary": "Investigating extraction module's scattered components and organization"
              },
              {
                "summary": "Dissected extraction code, proposed unified microservice architecture"
              }
            ],
            "cut_off": false
          },
          {
            "start_timestamp": "2025-06-12T05:51:02.808172Z",
            "stop_timestamp": "2025-06-12T05:54:18.902777Z",
            "flags": null,
            "type": "text",
            "text": "Looking at the codebase, I can see there are multiple extraction implementations scattered across different modules. This needs to be unified into a single, well-organized extraction service. Here's my analysis:\n\n## Current Issues:\n1. **Multiple extraction implementations**:\n   - `knowledge_garden/extraction/` - Main extraction module\n   - `knowledge_garden/pipeline/` - Alternative pipeline implementation\n   - `knowledge_garden/apps/extractor/` - Standalone extractor app\n   - `knowledge_garden/test_extraction.py` - Test extraction script\n\n2. **Duplicate functionality** across modules\n3. **Inconsistent imports** and module organization\n4. **No clear microservice architecture**\n\n## Solution: Unified Extraction Service\n\nI'll create scripts to:\n1. Consolidate all extraction logic into a single service\n2. Remove duplicate code\n3. Create a clean API endpoint\n4. Organize imports properly\n5. Set up a proper microservice architecture\n\nHere are the scripts:\n\n### 1. Main Reorganization Script\n\n```bash\n#!/bin/bash\n# save as: reorganize_extraction.sh\n\necho \"ðŸ”§ Reorganizing Knowledge Garden Extraction System...\"\n\n# Create new unified extraction structure\nmkdir -p knowledge_garden/services/extraction\nmkdir -p knowledge_garden/api\nmkdir -p knowledge_garden/api/endpoints\nmkdir -p knowledge_garden/tests/extraction\n\n# Create backup of current state\necho \"ðŸ“¦ Creating backup...\"\ntar -czf extraction_backup_$(date +%Y%m%d_%H%M%S).tar.gz \\\n    knowledge_garden/extraction \\\n    knowledge_garden/pipeline \\\n    knowledge_garden/apps/extractor \\\n    knowledge_garden/test_extraction.py 2>/dev/null || true\n\necho \"âœ… Backup created\"\n```\n\n### 2. Create Unified Extraction Service\n\n```python\n#!/usr/bin/env python3\n# save as: create_unified_extraction.py\n\nimport os\nimport shutil\nfrom pathlib import Path\n\n# Create the unified extraction service\nservice_content = '''\"\"\"\nUnified Extraction Service for Knowledge Garden\n\nThis module provides a single, consolidated extraction service that combines\nall extraction functionality into one coherent API.\n\"\"\"\nimport asyncio\nimport logging\nfrom typing import Dict, List, Any, Optional, Union\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom knowledge_garden.extraction.embedding_generator import EmbeddingGenerator\nfrom knowledge_garden.extraction.sentence_extractor import SentenceExtractor\nfrom knowledge_garden.extraction.pattern_extractor import PatternExtractor\nfrom knowledge_garden.extraction.entity_resolver import EntityResolver\nfrom knowledge_garden.extraction.evidence_tracker import EvidenceTracker\nfrom knowledge_garden.extraction.llm_extractor import LLMExtractor\nfrom knowledge_garden.extraction.prompt_manager import PromptManager\nfrom knowledge_garden.infrastructure.services.llm_service import LLMService\nfrom knowledge_garden.core.fuseki_client import FusekiClient\nfrom knowledge_garden.core.config import config\nfrom knowledge_garden.models.evidence import Evidence\nfrom knowledge_garden.models.extracted_sentence import ExtractedSentence\nfrom knowledge_garden.parliament.decision_engine import ParliamentEngine\n\nlogger = logging.getLogger(__name__)\n\n\nclass UnifiedExtractionService:\n    \"\"\"Unified service for all extraction operations\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the unified extraction service\"\"\"\n        # Initialize services\n        self.embedding_generator = EmbeddingGenerator(\n            model_name=config.get(\"models.embedding\"),\n            cache_dir=config.get(\"project.data_dir\") / \"embeddings\"\n        )\n        \n        self.llm_service = LLMService(provider=\"anthropic\")\n        self.fuseki_client = FusekiClient()\n        \n        # Initialize parliament if configured\n        self.parliament = None\n        if config.get(\"parliament.core.enabled\", False):\n            self.parliament = ParliamentEngine(\n                llm_service=self.llm_service,\n                fuseki_client=self.fuseki_client\n            )\n        \n        # Initialize extraction components\n        self.prompt_manager = PromptManager(\n            llm_service=self.llm_service,\n            cache_enabled=True\n        )\n        \n        self.evidence_tracker = EvidenceTracker(\n            fuseki_client=self.fuseki_client\n        )\n        \n        self.sentence_extractor = SentenceExtractor(\n            embedder=self.embedding_generator,\n            llm=self.llm_service,\n            prompt_manager=self.prompt_manager,\n            evidence_tracker=self.evidence_tracker\n        )\n        \n        self.pattern_extractor = PatternExtractor()\n        \n        self.llm_extractor = LLMExtractor(\n            api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n        )\n        \n        self.entity_resolver = EntityResolver(\n            embedding_generator=self.embedding_generator,\n            llm_service=self.llm_service,\n            parliament_engine=self.parliament\n        )\n        \n        # Statistics\n        self.stats = {\n            \"total_extractions\": 0,\n            \"total_sentences\": 0,\n            \"total_entities\": 0,\n            \"total_triples\": 0,\n            \"total_patterns\": 0\n        }\n    \n    async def extract_from_text(\n        self, \n        text: str, \n        source_uri: str,\n        metadata: Optional[Dict[str, Any]] = None,\n        options: Optional[Dict[str, Any]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Extract knowledge from text\n        \n        Args:\n            text: Text to extract from\n            source_uri: URI of the source document/conversation\n            metadata: Optional metadata about the source\n            options: Extraction options (use_llm, skip_patterns, etc.)\n            \n        Returns:\n            Extraction results dictionary\n        \"\"\"\n        options = options or {}\n        metadata = metadata or {}\n        \n        results = {\n            \"source_uri\": source_uri,\n            \"timestamp\": datetime.now().isoformat(),\n            \"sentences\": [],\n            \"entities\": [],\n            \"triples\": [],\n            \"patterns\": {\n                \"caps\": [],\n                \"questions\": [],\n                \"insights\": [],\n                \"tensions\": []\n            },\n            \"evidence\": [],\n            \"metadata\": metadata,\n            \"stats\": {}\n        }\n        \n        try:\n            # Step 1: Extract sentences with embeddings\n            sentences, sentence_evidence = await self.sentence_extractor.process_text(\n                text, source_uri\n            )\n            results[\"sentences\"] = [s.to_dict() for s in sentences]\n            results[\"evidence\"].extend([e.to_dict() for e in sentence_evidence])\n            \n            # Step 2: Extract patterns (unless skipped)\n            if not options.get(\"skip_patterns\", False):\n                from knowledge_garden.models.elements import ExtractedElements\n                elements = ExtractedElements()\n                \n                # Use pattern extractor\n                self.pattern_extractor.extract(text, elements)\n                \n                # Use LLM extractor if enabled\n                if options.get(\"use_llm\", True) and self.llm_extractor.client:\n                    self.llm_extractor.extract(text, elements)\n                \n                # Convert to results format\n                results[\"patterns\"][\"caps\"] = [\n                    {\"pattern\": cap[\"pattern\"], \"confidence\": cap.get(\"confidence\", 0.8)}\n                    for cap in elements.caps\n                ]\n                results[\"patterns\"][\"questions\"] = [\n                    {\"text\": q[\"text\"], \"confidence\": q.get(\"confidence\", 0.8)}\n                    for q in elements.questions\n                ]\n                results[\"patterns\"][\"insights\"] = [\n                    {\"realization\": i[\"realization\"], \"confidence\": i.get(\"confidence\", 0.8)}\n                    for i in elements.insights\n                ]\n                results[\"patterns\"][\"tensions\"] = [\n                    {\"pole1\": t[\"pole1\"], \"pole2\": t[\"pole2\"], \"confidence\": t.get(\"confidence\", 0.8)}\n                    for t in elements.tensions\n                ]\n            \n            # Step 3: Extract entities and resolve\n            entities = {}\n            for sentence in sentences:\n                for triple in sentence.triples:\n                    # Extract subject entity\n                    subject = triple.get(\"subject\", \"\")\n                    if subject and subject not in entities:\n                        entity = await self._create_entity(subject, triple.get(\"subject_type\"))\n                        entities[subject] = entity\n                    \n                    # Extract object entity (if not literal)\n                    object_val = triple.get(\"object\", \"\")\n                    object_type = triple.get(\"object_type\", \"\")\n                    if object_val and object_type.lower() not in [\"value\", \"literal\", \"string\"]:\n                        if object_val not in entities:\n                            entity = await self._create_entity(object_val, object_type)\n                            entities[object_val] = entity\n            \n            # Resolve entities\n            resolved_entities = []\n            entity_map = {}  # Map old labels to resolved entities\n            \n            for label, entity in entities.items():\n                # Check if we already resolved a similar entity\n                resolved = False\n                for resolved_entity in resolved_entities:\n                    resolution = await self.entity_resolver.resolve_entity(\n                        entity, {resolved_entity.entity_id: resolved_entity}\n                    )\n                    if resolution.action == \"merge\":\n                        entity_map[label] = resolved_entity\n                        resolved = True\n                        break\n                \n                if not resolved:\n                    resolved_entities.append(entity)\n                    entity_map[label] = entity\n            \n            results[\"entities\"] = [e.to_dict() for e in resolved_entities]\n            \n            # Step 4: Extract triples with resolved entities\n            for sentence in sentences:\n                for triple_data in sentence.triples:\n                    subject = triple_data.get(\"subject\", \"\")\n                    predicate = triple_data.get(\"predicate\", \"\")\n                    object_val = triple_data.get(\"object\", \"\")\n                    \n                    # Map to resolved entities\n                    subject_entity = entity_map.get(subject)\n                    object_entity = entity_map.get(object_val)\n                    \n                    triple = {\n                        \"subject\": subject_entity.entity_id if subject_entity else subject,\n                        \"predicate\": predicate,\n                        \"object\": object_entity.entity_id if object_entity else object_val,\n                        \"is_literal\": object_entity is None,\n                        \"confidence\": triple_data.get(\"confidence\", 0.7),\n                        \"source_sentence\": sentence.sentence_id\n                    }\n                    results[\"triples\"].append(triple)\n            \n            # Update statistics\n            results[\"stats\"] = {\n                \"sentences\": len(results[\"sentences\"]),\n                \"entities\": len(results[\"entities\"]),\n                \"triples\": len(results[\"triples\"]),\n                \"caps\": len(results[\"patterns\"][\"caps\"]),\n                \"questions\": len(results[\"patterns\"][\"questions\"]),\n                \"insights\": len(results[\"patterns\"][\"insights\"]),\n                \"tensions\": len(results[\"patterns\"][\"tensions\"]),\n                \"evidence\": len(results[\"evidence\"])\n            }\n            \n            # Update global stats\n            self.stats[\"total_extractions\"] += 1\n            self.stats[\"total_sentences\"] += results[\"stats\"][\"sentences\"]\n            self.stats[\"total_entities\"] += results[\"stats\"][\"entities\"]\n            self.stats[\"total_triples\"] += results[\"stats\"][\"triples\"]\n            self.stats[\"total_patterns\"] += sum([\n                results[\"stats\"][\"caps\"],\n                results[\"stats\"][\"questions\"],\n                results[\"stats\"][\"insights\"],\n                results[\"stats\"][\"tensions\"]\n            ])\n            \n            return results\n            \n        except Exception as e:\n            logger.error(f\"Extraction failed: {e}\")\n            results[\"error\"] = str(e)\n            return results\n    \n    async def extract_from_conversations(\n        self,\n        conversations: List[Dict[str, Any]],\n        options: Optional[Dict[str, Any]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Extract knowledge from multiple conversations\n        \n        Args:\n            conversations: List of conversation dictionaries\n            options: Extraction options\n            \n        Returns:\n            Aggregated extraction results\n        \"\"\"\n        all_results = {\n            \"conversations\": [],\n            \"aggregated\": {\n                \"entities\": {},\n                \"triples\": [],\n                \"patterns\": {\n                    \"caps\": [],\n                    \"questions\": [],\n                    \"insights\": [],\n                    \"tensions\": []\n                }\n            },\n            \"stats\": {\n                \"total_conversations\": len(conversations),\n                \"total_messages\": 0\n            }\n        }\n        \n        # Process each conversation\n        for conv in conversations:\n            conv_id = conv.get(\"id\", f\"conv_{datetime.now().timestamp()}\")\n            conv_results = {\n                \"id\": conv_id,\n                \"title\": conv.get(\"title\", \"Untitled\"),\n                \"messages\": []\n            }\n            \n            # Process each message\n            messages = conv.get(\"messages\", [])\n            for msg in messages:\n                if msg.get(\"role\") == \"user\":\n                    text = msg.get(\"content\", \"\")\n                    if text:\n                        source_uri = f\"conversation:{conv_id}:message:{msg.get('id', '')}\"\n                        \n                        # Extract from message\n                        extraction = await self.extract_from_text(\n                            text=text,\n                            source_uri=source_uri,\n                            metadata={\n                                \"conversation_id\": conv_id,\n                                \"message_id\": msg.get(\"id\"),\n                                \"role\": msg.get(\"role\")\n                            },\n                            options=options\n                        )\n                        \n                        conv_results[\"messages\"].append(extraction)\n                        all_results[\"stats\"][\"total_messages\"] += 1\n                        \n                        # Aggregate results\n                        for entity in extraction.get(\"entities\", []):\n                            entity_id = entity[\"entity_id\"]\n                            if entity_id not in all_results[\"aggregated\"][\"entities\"]:\n                                all_results[\"aggregated\"][\"entities\"][entity_id] = entity\n                        \n                        all_results[\"aggregated\"][\"triples\"].extend(\n                            extraction.get(\"triples\", [])\n                        )\n                        \n                        # Aggregate patterns\n                        for pattern_type in [\"caps\", \"questions\", \"insights\", \"tensions\"]:\n                            all_results[\"aggregated\"][\"patterns\"][pattern_type].extend(\n                                extraction.get(\"patterns\", {}).get(pattern_type, [])\n                            )\n            \n            all_results[\"conversations\"].append(conv_results)\n        \n        # Deduplicate patterns\n        for pattern_type in [\"caps\", \"questions\", \"insights\", \"tensions\"]:\n            patterns = all_results[\"aggregated\"][\"patterns\"][pattern_type]\n            # Simple deduplication based on text content\n            seen = set()\n            unique_patterns = []\n            for pattern in patterns:\n                # Get the text key based on pattern type\n                if pattern_type == \"caps\":\n                    key = pattern.get(\"pattern\", \"\")\n                elif pattern_type == \"questions\":\n                    key = pattern.get(\"text\", \"\")\n                elif pattern_type == \"insights\":\n                    key = pattern.get(\"realization\", \"\")\n                elif pattern_type == \"tensions\":\n                    key = f\"{pattern.get('pole1', '')}:{pattern.get('pole2', '')}\"\n                \n                if key and key not in seen:\n                    seen.add(key)\n                    unique_patterns.append(pattern)\n            \n            all_results[\"aggregated\"][\"patterns\"][pattern_type] = unique_patterns\n        \n        return all_results\n    \n    async def _create_entity(self, label: str, entity_type: Optional[str] = None):\n        \"\"\"Create an entity object\"\"\"\n        from knowledge_garden.extraction.entity_resolver import Entity\n        from knowledge_garden.core.uri_generator import uri_generator\n        \n        entity_id = uri_generator.generate_entity_uri(label, entity_type or \"Entity\")\n        \n        entity = Entity(\n            entity_id=entity_id,\n            label=label,\n            entity_type=entity_type\n        )\n        \n        # Generate embedding\n        if self.embedding_generator:\n            entity.embedding = await self.embedding_generator.encode(label)\n        \n        return entity\n    \n    async def store_results(self, results: Dict[str, Any]) -> bool:\n        \"\"\"\n        Store extraction results in Fuseki\n        \n        Args:\n            results: Extraction results to store\n            \n        Returns:\n            Success status\n        \"\"\"\n        if not self.fuseki_client:\n            logger.warning(\"No Fuseki client available, skipping storage\")\n            return False\n        \n        try:\n            # Store entities\n            for entity in results.get(\"entities\", []):\n                entity_triples = self._entity_to_triples(entity)\n                await self.fuseki_client.insert_triples(entity_triples)\n            \n            # Store triples\n            for triple in results.get(\"triples\", []):\n                triple_triples = self._triple_to_triples(triple)\n                await self.fuseki_client.insert_triples(triple_triples)\n            \n            # Store evidence\n            if self.evidence_tracker:\n                for evidence_dict in results.get(\"evidence\", []):\n                    evidence = Evidence.from_dict(evidence_dict)\n                    # You'll need to determine the target URI based on what the evidence supports\n                    target_uri = evidence_dict.get(\"target_uri\", results[\"source_uri\"])\n                    await self.evidence_tracker.store_evidence(evidence, target_uri)\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to store results: {e}\")\n            return False\n    \n    def _entity_to_triples(self, entity: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Convert entity to RDF triples\"\"\"\n        entity_uri = entity[\"entity_id\"]\n        \n        triples = [\n            {\n                \"subject\": entity_uri,\n                \"predicate\": \"rdf:type\",\n                \"object\": f\"kg:{entity.get('entity_type', 'Entity')}\",\n                \"is_literal\": False\n            },\n            {\n                \"subject\": entity_uri,\n                \"predicate\": \"rdfs:label\",\n                \"object\": entity[\"label\"],\n                \"is_literal\": True\n            }\n        ]\n        \n        # Add attributes\n        for key, value in entity.get(\"attributes\", {}).items():\n            triples.append({\n                \"subject\": entity_uri,\n                \"predicate\": f\"kg:{key}\",\n                \"object\": str(value),\n                \"is_literal\": True\n            })\n        \n        return triples\n    \n    def _triple_to_triples(self, triple: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Convert knowledge triple to RDF triples\"\"\"\n        # For now, just return the triple as-is\n        # In a full implementation, you might add metadata, provenance, etc.\n        return [{\n            \"subject\": triple[\"subject\"],\n            \"predicate\": triple[\"predicate\"],\n            \"object\": triple[\"object\"],\n            \"is_literal\": triple.get(\"is_literal\", False)\n        }]\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get extraction statistics\"\"\"\n        return self.stats.copy()\n\n\n# Global instance\nextraction_service = UnifiedExtractionService()\n'''\n\n# Write the unified extraction service\nPath(\"knowledge_garden/services/extraction/__init__.py\").parent.mkdir(parents=True, exist_ok=True)\nwith open(\"knowledge_garden/services/extraction/__init__.py\", \"w\") as f:\n    f.write(service_content)\n\nprint(\"âœ… Created unified extraction service\")\n```\n\n### 3. Create API Endpoint\n\n```python\n#!/usr/bin/env python3\n# save as: create_extraction_api.py\n\nimport os\nfrom pathlib import Path\n\n# Create FastAPI endpoint for extraction\napi_content = '''\"\"\"\nExtraction API endpoints for Knowledge Garden\n\"\"\"\nfrom fastapi import APIRouter, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, List, Any, Optional\nimport logging\n\nfrom knowledge_garden.services.extraction import extraction_service\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(prefix=\"/api/v1/extraction\", tags=[\"extraction\"])\n\n\nclass ExtractTextRequest(BaseModel):\n    \"\"\"Request model for text extraction\"\"\"\n    text: str = Field(..., description=\"Text to extract knowledge from\")\n    source_uri: str = Field(..., description=\"URI identifying the source\")\n    metadata: Optional[Dict[str, Any]] = Field(default={}, description=\"Optional metadata\")\n    options: Optional[Dict[str, Any]] = Field(\n        default={},\n        description=\"Extraction options (use_llm, skip_patterns, etc.)\"\n    )\n\n\nclass ExtractConversationsRequest(BaseModel):\n    \"\"\"Request model for conversation extraction\"\"\"\n    conversations: List[Dict[str, Any]] = Field(..., description=\"List of conversations\")\n    options: Optional[Dict[str, Any]] = Field(\n        default={},\n        description=\"Extraction options\"\n    )\n\n\nclass ExtractionResponse(BaseModel):\n    \"\"\"Response model for extraction results\"\"\"\n    success: bool\n    results: Optional[Dict[str, Any]] = None\n    error: Optional[str] = None\n\n\n@router.post(\"/text\", response_model=ExtractionResponse)\nasync def extract_from_text(request: ExtractTextRequest):\n    \"\"\"\n    Extract knowledge from text\n    \n    This endpoint processes raw text and extracts:\n    - Sentences with embeddings\n    - Entities (people, concepts, etc.)\n    - Relationships (triples)\n    - Patterns (CAPs, questions, insights, tensions)\n    - Evidence for provenance\n    \"\"\"\n    try:\n        results = await extraction_service.extract_from_text(\n            text=request.text,\n            source_uri=request.source_uri,\n            metadata=request.metadata,\n            options=request.options\n        )\n        \n        return ExtractionResponse(\n            success=True,\n            results=results\n        )\n        \n    except Exception as e:\n        logger.error(f\"Extraction failed: {e}\")\n        return ExtractionResponse(\n            success=False,\n            error=str(e)\n        )\n\n\n@router.post(\"/conversations\", response_model=ExtractionResponse)\nasync def extract_from_conversations(request: ExtractConversationsRequest):\n    \"\"\"\n    Extract knowledge from multiple conversations\n    \n    This endpoint processes conversation data (e.g., from ChatGPT exports)\n    and extracts aggregated knowledge across all conversations.\n    \"\"\"\n    try:\n        results = await extraction_service.extract_from_conversations(\n            conversations=request.conversations,\n            options=request.options\n        )\n        \n        return ExtractionResponse(\n            success=True,\n            results=results\n        )\n        \n    except Exception as e:\n        logger.error(f\"Conversation extraction failed: {e}\")\n        return ExtractionResponse(\n            success=False,\n            error=str(e)\n        )\n\n\n@router.post(\"/text/store\", response_model=ExtractionResponse)\nasync def extract_and_store(\n    request: ExtractTextRequest,\n    background_tasks: BackgroundTasks\n):\n    \"\"\"\n    Extract knowledge from text and store in knowledge graph\n    \n    This endpoint extracts knowledge and immediately stores it in Fuseki.\n    Storage happens in the background to avoid blocking the response.\n    \"\"\"\n    try:\n        # Extract knowledge\n        results = await extraction_service.extract_from_text(\n            text=request.text,\n            source_uri=request.source_uri,\n            metadata=request.metadata,\n            options=request.options\n        )\n        \n        # Store in background\n        background_tasks.add_task(\n            extraction_service.store_results,\n            results\n        )\n        \n        return ExtractionResponse(\n            success=True,\n            results=results\n        )\n        \n    except Exception as e:\n        logger.error(f\"Extract and store failed: {e}\")\n        return ExtractionResponse(\n            success=False,\n            error=str(e)\n        )\n\n\n@router.get(\"/stats\")\nasync def get_extraction_stats():\n    \"\"\"Get extraction service statistics\"\"\"\n    return extraction_service.get_stats()\n\n\n@router.get(\"/health\")\nasync def health_check():\n    \"\"\"Check extraction service health\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"service\": \"extraction\",\n        \"version\": \"1.0.0\"\n    }\n'''\n\n# Write the API endpoint\nPath(\"knowledge_garden/api/endpoints/extraction.py\").parent.mkdir(parents=True, exist_ok=True)\nwith open(\"knowledge_garden/api/endpoints/extraction.py\", \"w\") as f:\n    f.write(api_content)\n\n# Create main API app\napi_app_content = '''\"\"\"\nKnowledge Garden API Application\n\"\"\"\nfrom fastapi import FastAPI, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nimport logging\n\nfrom knowledge_garden.api.endpoints import extraction\n\nlogger = logging.getLogger(__name__)\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"Knowledge Garden API\",\n    description=\"Unified API for Knowledge Garden extraction and management\",\n    version=\"1.0.0\"\n)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Include routers\napp.include_router(extraction.router)\n\n\n@app.exception_handler(Exception)\nasync def global_exception_handler(request: Request, exc: Exception):\n    \"\"\"Global exception handler\"\"\"\n    logger.error(f\"Unhandled exception: {exc}\")\n    return JSONResponse(\n        status_code=500,\n        content={\"error\": \"Internal server error\", \"detail\": str(exc)}\n    )\n\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Root endpoint\"\"\"\n    return {\n        \"name\": \"Knowledge Garden API\",\n        \"version\": \"1.0.0\",\n        \"endpoints\": {\n            \"extraction\": \"/api/v1/extraction\",\n            \"docs\": \"/docs\",\n            \"redoc\": \"/redoc\"\n        }\n    }\n\n\n@app.get(\"/health\")\nasync def health():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"healthy\"}\n'''\n\nwith open(\"knowledge_garden/api/__init__.py\", \"w\") as f:\n    f.write('\"\"\"Knowledge Garden API\"\"\"')\n\nwith open(\"knowledge_garden/api/app.py\", \"w\") as f:\n    f.write(api_app_content)\n\n# Create run script for the API\nrun_api_content = '''#!/usr/bin/env python3\n\"\"\"\nRun the Knowledge Garden API server\n\"\"\"\nimport uvicorn\nimport argparse\nimport sys\nfrom pathlib import Path\n\n# Add parent directory to path\nsys.path.insert(0, str(Path(__file__).parent))\n\nfrom knowledge_garden.api.app import app\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run Knowledge Garden API\")\n    parser.add_argument(\"--host\", default=\"0.0.0.0\", help=\"Host to bind to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind to\")\n    parser.add_argument(\"--reload\", action=\"store_true\", help=\"Enable auto-reload\")\n    args = parser.parse_args()\n    \n    uvicorn.run(\n        \"knowledge_garden.api.app:app\",\n        host=args.host,\n        port=args.port,\n        reload=args.reload,\n        log_level=\"info\"\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n'''\n\nwith open(\"run_api.py\", \"w\") as f:\n    f.write(run_api_content)\n\nos.chmod(\"run_api.py\", 0o755)\n\nprint(\"âœ… Created API endpoints and run script\")\n```\n\n### 4. Update CLI to Use Unified Service\n\n```python\n#!/usr/bin/env python3\n# save as: update_cli_extraction.py\n\nimport re\nfrom pathlib import Path\n\n# Read current CLI\ncli_path = Path(\"knowledge_garden/cli.py\")\nwith open(cli_path, \"r\") as f:\n    cli_content = f.read()\n\n# Find and replace the extract command\nnew_extract_command = '''@cli.command()\n@click.argument('input_file', type=click.Path(exists=True))\n@click.option('--output', type=click.Path(), help='Output file path')\n@click.option('--source-uri', help='Source URI (defaults to file path)')\n@click.option('--use-llm/--no-llm', default=True, help='Use LLM for enhanced extraction')\n@click.option('--skip-patterns', is_flag=True, help='Skip pattern extraction')\n@click.option('--store', is_flag=True, help='Store results in Fuseki')\n@click.option('--format', type=click.Choice(['json', 'turtle', 'summary']), default='json', help='Output format')\nasync def extract(input_file, output, source_uri, use_llm, skip_patterns, store, format):\n    \"\"\"Extract knowledge from text or conversations\"\"\"\n    from knowledge_garden.services.extraction import extraction_service\n    import json\n    import asyncio\n    \n    # Determine source URI\n    if not source_uri:\n        source_uri = f\"file://{Path(input_file).absolute()}\"\n    \n    click.echo(f\"ðŸ“Š Extracting knowledge from {input_file}\")\n    \n    # Read input file\n    with open(input_file, 'r') as f:\n        content = f.read()\n    \n    # Check if it's JSON (conversations) or plain text\n    try:\n        data = json.loads(content)\n        if isinstance(data, list) and all('messages' in item for item in data[:5]):\n            # It's a conversation export\n            click.echo(\"ðŸ“± Detected conversation format\")\n            results = await extraction_service.extract_from_conversations(\n                conversations=data,\n                options={\n                    'use_llm': use_llm,\n                    'skip_patterns': skip_patterns\n                }\n            )\n        else:\n            # Treat as plain text\n            click.echo(\"ðŸ“ Processing as plain text\")\n            results = await extraction_service.extract_from_text(\n                text=content,\n                source_uri=source_uri,\n                options={\n                    'use_llm': use_llm,\n                    'skip_patterns': skip_patterns\n                }\n            )\n    except json.JSONDecodeError:\n        # Plain text\n        click.echo(\"ðŸ“ Processing as plain text\")\n        results = await extraction_service.extract_from_text(\n            text=content,\n            source_uri=source_uri,\n            options={\n                'use_llm': use_llm,\n                'skip_patterns': skip_patterns\n            }\n        )\n    \n    # Store if requested\n    if store:\n        click.echo(\"ðŸ’¾ Storing results in Fuseki...\")\n        success = await extraction_service.store_results(results)\n        if success:\n            click.echo(\"âœ… Results stored successfully\")\n        else:\n            click.echo(\"âŒ Failed to store results\")\n    \n    # Output results\n    if output:\n        output_path = Path(output)\n    else:\n        output_path = Path(input_file).with_suffix(f'.extraction.{format}')\n    \n    if format == 'json':\n        with open(output_path, 'w') as f:\n            json.dump(results, f, indent=2)\n    elif format == 'turtle':\n        # Convert to Turtle format\n        click.echo(\"ðŸ¢ Turtle format not yet implemented\")\n    elif format == 'summary':\n        # Create human-readable summary\n        summary = f\"\"\"Knowledge Extraction Summary\n==========================\nSource: {source_uri}\nTimestamp: {results.get('timestamp', 'N/A')}\n\nStatistics:\n-----------\nSentences: {results.get('stats', {}).get('sentences', 0)}\nEntities: {results.get('stats', {}).get('entities', 0)}\nTriples: {results.get('stats', {}).get('triples', 0)}\nCAPs: {results.get('stats', {}).get('caps', 0)}\nQuestions: {results.get('stats', {}).get('questions', 0)}\nInsights: {results.get('stats', {}).get('insights', 0)}\nTensions: {results.get('stats', {}).get('tensions', 0)}\n\nTop Entities:\n------------\n\"\"\"\n        for entity in results.get('entities', [])[:10]:\n            summary += f\"- {entity['label']} ({entity.get('entity_type', 'Unknown')})\"\n        \n        with open(output_path, 'w') as f:\n            f.write(summary)\n    \n    click.echo(f\"âœ… Extraction complete! Results saved to {output_path}\")\n    \n    # Print summary\n    stats = results.get('stats', {})\n    click.echo(f\"\"\"\nðŸ“Š Extraction Summary:\n  - Sentences: {stats.get('sentences', 0)}\n  - Entities: {stats.get('entities', 0)}\n  - Triples: {stats.get('triples', 0)}\n  - Patterns: {stats.get('caps', 0) + stats.get('questions', 0) + stats.get('insights', 0) + stats.get('tensions', 0)}\n    \"\"\")'''\n\n# Replace the extract command in CLI\n# This is a simplified replacement - in practice you'd use proper AST parsing\ncli_content = re.sub(\n    r'@cli\\.command\\(\\)\\s*@click\\.argument.*?def extract.*?click\\.echo\\(f\"âœ… Knowledge extraction complete!\"\\)',\n    new_extract_command,\n    cli_content,\n    flags=re.DOTALL\n)\n\n# Write updated CLI\nwith open(cli_path, \"w\") as f:\n    f.write(cli_content)\n\nprint(\"âœ… Updated CLI to use unified extraction service\")\n```\n\n### 5. Clean Up Duplicate Files\n\n```bash\n#!/bin/bash\n# save as: cleanup_extraction.sh\n\necho \"ðŸ§¹ Cleaning up duplicate extraction files...\"\n\n# Remove duplicate extraction implementations\necho \"Removing pipeline module (duplicate implementation)...\"\nrm -rf knowledge_garden/pipeline/\n\necho \"Removing apps/extractor (duplicate implementation)...\"\nrm -rf knowledge_garden/apps/extractor/\n\necho \"Removing test_extraction.py (moved to tests/)...\"\nrm -f knowledge_garden/test_extraction.py\n\n# Move any useful tests to tests directory\nmkdir -p knowledge_garden/tests/extraction\nif [ -f \"knowledge_garden/test_extraction.py\" ]; then\n    mv knowledge_garden/test_extraction.py knowledge_garden/tests/extraction/test_basic_extraction.py\nfi\n\n# Create proper test structure\ncat > knowledge_garden/tests/extraction/__init__.py << 'EOF'\n\"\"\"Extraction tests for Knowledge Garden\"\"\"\nEOF\n\ncat > knowledge_garden/tests/extraction/test_unified_extraction.py << 'EOF'\n\"\"\"Tests for unified extraction service\"\"\"\nimport pytest\nimport asyncio\nfrom knowledge_garden.services.extraction import extraction_service\n\n\n@pytest.mark.asyncio\nasync def test_extract_from_text():\n    \"\"\"Test basic text extraction\"\"\"\n    text = \"John works at OpenAI. He is developing GPT models.\"\n    source_uri = \"test:123\"\n    \n    results = await extraction_service.extract_from_text(\n        text=text,\n        source_uri=source_uri\n    )\n    \n    assert results[\"source_uri\"] == source_uri\n    assert len(results[\"sentences\"]) > 0\n    assert \"stats\" in results\n\n\n@pytest.mark.asyncio\nasync def test_extract_patterns():\n    \"\"\"Test pattern extraction\"\"\"\n    text = \"\"\"\n    MOMENTS when I feel stuck are OPPORTUNITIES for creative thinking.\n    I realized that taking breaks improves productivity.\n    How might we balance work and life better?\n    There's a tension between speed and quality.\n    \"\"\"\n    \n    results = await extraction_service.extract_from_text(\n        text=text,\n        source_uri=\"test:patterns\"\n    )\n    \n    patterns = results[\"patterns\"]\n    assert len(patterns[\"caps\"]) > 0\n    assert len(patterns[\"insights\"]) > 0\n    assert len(patterns[\"questions\"]) > 0\n    assert len(patterns[\"tensions\"]) > 0\nEOF\n\necho \"âœ… Cleanup complete\"\n```\n\n### 6. Update Imports Script\n\n```python\n#!/usr/bin/env python3\n# save as: fix_extraction_imports.py\n\nimport os\nimport re\nfrom pathlib import Path\n\ndef update_imports_in_file(file_path):\n    \"\"\"Update imports in a single file\"\"\"\n    with open(file_path, 'r') as f:\n        content = f.read()\n    \n    # Map old imports to new ones\n    replacements = [\n        # Pipeline imports -> extraction imports\n        (r'from knowledge_garden\\.pipeline\\.pipeline import ExtractionPipeline',\n         'from knowledge_garden.services.extraction import extraction_service'),\n        (r'from knowledge_garden\\.pipeline\\.runner import extraction_pipeline',\n         'from knowledge_garden.services.extraction import extraction_service'),\n        (r'from knowledge_garden\\.pipeline\\..*? import',\n         'from knowledge_garden.extraction import'),\n        \n        # Apps extractor imports -> service imports\n        (r'from knowledge_garden\\.apps\\.extractor\\..*? import',\n         'from knowledge_garden.services.extraction import'),\n        \n        # Direct extraction module imports remain the same\n        # Just ensure they're properly namespaced\n        (r'from extraction\\.',\n         'from knowledge_garden.extraction.'),\n    ]\n    \n    modified = False\n    for old_pattern, new_pattern in replacements:\n        new_content = re.sub(old_pattern, new_pattern, content)\n        if new_content != content:\n            content = new_content\n            modified = True\n    \n    if modified:\n        with open(file_path, 'w') as f:\n            f.write(content)\n        print(f\"Updated imports in {file_path}\")\n    \n    return modified\n\n# Find all Python files\nupdated_count = 0\nfor py_file in Path(\"knowledge_garden\").rglob(\"*.py\"):\n    if update_imports_in_file(py_file):\n        updated_count += 1\n\nprint(f\"âœ… Updated imports in {updated_count} files\")\n```\n\n### 7. Create Microservice Docker Setup\n\n```dockerfile\n# save as: Dockerfile.extraction\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY knowledge_garden knowledge_garden/\nCOPY run_api.py .\n\n# Expose port\nEXPOSE 8000\n\n# Run the API\nCMD [\"python\", \"run_api.py\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n```yaml\n# save as: docker-compose.yml\nversion: '3.8'\n\nservices:\n  fuseki:\n    image: stain/jena-fuseki:latest\n    environment:\n      - ADMIN_PASSWORD=admin\n    ports:\n      - \"3030:3030\"\n    volumes:\n      - fuseki_data:/fuseki\n    \n  extraction-api:\n    build:\n      context: .\n      dockerfile: Dockerfile.extraction\n    ports:\n      - \"8000:8000\"\n    environment:\n      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - FUSEKI_URL=http://fuseki:3030\n      - FUSEKI_DATASET=kg\n    depends_on:\n      - fuseki\n    volumes:\n      - ./knowledge_garden:/app/knowledge_garden\n      - ./data:/app/data\n\nvolumes:\n  fuseki_data:\n```\n\n### 8. Master Script to Run Everything\n\n```bash\n#!/bin/bash\n# save as: reorganize_all.sh\n\necho \"ðŸš€ Knowledge Garden Extraction Reorganization\"\necho \"============================================\"\n\n# Make all scripts executable\nchmod +x reorganize_extraction.sh\nchmod +x cleanup_extraction.sh\n\n# Step 1: Initial reorganization\necho \"ðŸ“ Step 1: Creating new structure...\"\n./reorganize_extraction.sh\n\n# Step 2: Create unified service\necho \"ðŸ”§ Step 2: Creating unified extraction service...\"\npython3 create_unified_extraction.py\n\n# Step 3: Create API\necho \"ðŸŒ Step 3: Creating API endpoints...\"\npython3 create_extraction_api.py\n\n# Step 4: Update CLI\necho \"ðŸ“ Step 4: Updating CLI...\"\npython3 update_cli_extraction.py\n\n# Step 5: Fix imports\necho \"ðŸ”— Step 5: Fixing imports...\"\npython3 fix_extraction_imports.py\n\n# Step 6: Clean up duplicates\necho \"ðŸ§¹ Step 6: Cleaning up duplicate files...\"\n./cleanup_extraction.sh\n\n# Step 7: Update requirements\necho \"ðŸ“¦ Step 7: Updating requirements.txt...\"\ncat >> requirements.txt << 'EOF'\n\n# API requirements\nfastapi>=0.104.0\nuvicorn[standard]>=0.24.0\npython-multipart>=0.0.6\nEOF\n\necho \"âœ… Reorganization complete!\"\necho \"\"\necho \"Next steps:\"\necho \"1. Install new requirements: pip install -r requirements.txt\"\necho \"2. Run the API: python run_api.py\"\necho \"3. Access API docs: http://localhost:8000/docs\"\necho \"4. Use CLI: python -m knowledge_garden.cli extract <file>\"\necho \"5. Run with Docker: docker-compose up\"\n```\n\n### Terminal Commands to Execute\n\nRun these commands in order:\n\n```bash\n# 1. Save all the scripts\n# (Save each script to its designated filename)\n\n# 2. Make the master script executable and run it\nchmod +x reorganize_all.sh\n./reorganize_all.sh\n\n# 3. Install new requirements\npip install -r requirements.txt\n\n# 4. Test the unified extraction service\npython -c \"from knowledge_garden.services.extraction import extraction_service; print('âœ… Import successful')\"\n\n# 5. Run the API server\npython run_api.py\n\n# 6. In another terminal, test the API\ncurl http://localhost:8000/health\ncurl http://localhost:8000/api/v1/extraction/health\n\n# 7. Test extraction via CLI\necho \"This is a test. MOMENTS when we test are OPPORTUNITIES for learning.\" > test.txt\npython -m knowledge_garden.cli extract test.txt --format summary\n\n# 8. Optional: Run with Docker\ndocker-compose up -d\n```\n\nThis reorganization:\n1. **Unifies all extraction logic** into a single service\n2. **Removes duplicate implementations** from pipeline/, apps/extractor/, etc.\n3. **Creates a clean API** with FastAPI\n4. **Updates the CLI** to use the unified service\n5. **Provides Docker setup** for microservice deployment\n6. **Maintains all core functionality** while simplifying the structure\n\nThe extraction is now available through:\n- **API**: `http://localhost:8000/api/v1/extraction`\n- **CLI**: `python -m knowledge_garden.cli extract`\n- **Python**: `from knowledge_garden.services.extraction import extraction_service`",
            "citations": []
          }
        ],
        "updated_at": "2025-06-12T05:54:19.519129Z",
        "attachments": [],
        "files": []
      }
    }
  ]
}