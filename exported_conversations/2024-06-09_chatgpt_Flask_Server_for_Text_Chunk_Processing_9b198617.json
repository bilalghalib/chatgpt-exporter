{
  "id": "9b198617-62f4-431f-b248-b23f2c3a1279",
  "source": "chatgpt",
  "title": "Flask Server for Text Chunk Processing",
  "created_at": "2024-06-09T10:35:24.378802",
  "updated_at": "2024-06-09T10:35:59.343249",
  "summary": null,
  "message_count": 2,
  "messages": [
    {
      "id": "aaa2398b-b4b5-4ead-b4b1-f1c0154f40aa",
      "role": "user",
      "content": "can you make this a server endpoint where someone can send a block of text in chunks and have it make the graph.csv output at the end?\n\n# %% [markdown]\n# ## Setup\n\n# %%\nimport pandas as pd\nimport numpy as np\nimport os\nfrom langchain.document_loaders import PyPDFLoader, UnstructuredPDFLoader, PyPDFium2Loader\nfrom langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom pathlib import Path\nimport random\n\n## Input data directory\ndata_dir = \"shaykh\"\ninputdirectory = Path(f\"./data_input/{data_dir}\")\n## This is where the output csv files will be written\nout_dir = data_dir\noutputdirectory = Path(f\"./data_output/{out_dir}\")\n\n# %% [markdown]\n# ## Load Documents\n\n# %%\n## Dir PDF Loader\nloader = PyPDFDirectoryLoader(inputdirectory)\n## File Loader\n#loader = PyPDFLoader(\"./data/MedicalDocuments/orf-path_health-n1.pdf\")\n# loader = DirectoryLoader(inputdirectory, show_progress=True)\ndocuments = loader.load()\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1500,\n    chunk_overlap=150,\n    length_function=len,\n    is_separator_regex=False,\n)\n\npages = splitter.split_documents(documents)\nprint(\"Number of chunks = \", len(pages))\nprint(pages[3].page_content)\n\n\n# %% [markdown]\n# ## Create a dataframe of all the chunks\n\n# %%\nfrom helpers.df_helpers import documents2Dataframe\ndf = documents2Dataframe(pages)\nprint(df.shape)\ndf.head()\n\n# %% [markdown]\n# ## Extract Concepts\n\n# %%\n## This function uses the helpers/prompt function to extract concepts from text\nfrom helpers.df_helpers import df2Graph\nfrom helpers.df_helpers import graph2Df\n\n# %% [markdown]\n# If regenerate is set to True then the dataframes are regenerated and Both the dataframes are written in the csv format so we dont have to calculate them again. \n# \n#         dfne = dataframe of edges\n# \n#         df = dataframe of chunks\n# \n# \n# Else the dataframes are read from the output directory\n\n# %%\n## To regenerate the graph with LLM, set this to True\nregenerate = False\n\nif regenerate:\n    concepts_list = df2Graph(df, model='llama3')\n    dfg1 = graph2Df(concepts_list)\n    if not os.path.exists(outputdirectory):\n        os.makedirs(outputdirectory)\n    \n    dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n    df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\nelse:\n    dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n\ndfg1.replace(\"\", np.nan, inplace=True)\ndfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\ndfg1['count'] = 4 \n## Increasing the weight of the relation to 4. \n## We will assign the weight of 1 when later the contextual proximity will be calculated.  \nprint(dfg1.shape)\ndfg1.head()\n\n# %% [markdown]\n# ## Calculating contextual proximity\n\n# %%\ndef contextual_proximity(df: pd.DataFrame) -> pd.DataFrame:\n    ## Melt the dataframe into a list of nodes\n    dfg_long = pd.melt(\n        df, id_vars=[\"chunk_id\"], value_vars=[\"node_1\", \"node_2\"], value_name=\"node\"\n    )\n    dfg_long.drop(columns=[\"variable\"], inplace=True)\n    # Self join with chunk id as the key will create a link between terms occuring in the same text chunk.\n    dfg_wide = pd.merge(dfg_long, dfg_long, on=\"chunk_id\", suffixes=(\"_1\", \"_2\"))\n    # drop self loops\n    self_loops_drop = dfg_wide[dfg_wide[\"node_1\"] == dfg_wide[\"node_2\"]].index\n    dfg2 = dfg_wide.drop(index=self_loops_drop).reset_index(drop=True)\n    ## Group and count edges.\n    dfg2 = (\n        dfg2.groupby([\"node_1\", \"node_2\"])\n        .agg({\"chunk_id\": [\",\".join, \"count\"]})\n        .reset_index()\n    )\n    dfg2.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\"]\n    dfg2.replace(\"\", np.nan, inplace=True)\n    dfg2.dropna(subset=[\"node_1\", \"node_2\"], inplace=True)\n    # Drop edges with 1 count\n    dfg2 = dfg2[dfg2[\"count\"] != 1]\n    dfg2[\"edge\"] = \"contextual proximity\"\n    return dfg2\n\n\ndfg2 = contextual_proximity(dfg1)\ndfg2.tail()\n\n# %% [markdown]\n# ### Merge both the dataframes\n\n# %%\ndfg = pd.concat([dfg1, dfg2], axis=0)\ndfg = (\n    dfg.groupby([\"node_1\", \"node_2\"])\n    .agg({\"chunk_id\": \",\".join, \"edge\": ','.join, 'count': 'sum'})\n    .reset_index()\n)\ndfg\n\n# %% [markdown]\n# ## Calculate the NetworkX Graph\n\n# %%\nnodes = pd.concat([dfg['node_1'], dfg['node_2']], axis=0).unique()\nnodes.shape\n\n# %% [markdown]\n# ### Calculate communities for coloring the nodes\n\n# %%\ncommunities_generator = nx.community.girvan_newman(G)\ntop_level_communities = next(communities_generator)\nnext_level_communities = next(communities_generator)\ncommunities = sorted(map(sorted, next_level_communities))\nprint(\"Number of Communities = \", len(communities))\nprint(communities)\ncolors = [\"\" for x in range(G.number_of_nodes())]  # initialize colors list\ncounter = 0\nfor com in communities:\n    for node in list(com):  # fill colors list with the particular color for the community nodes\n        print(com)\ncounter\n\n# %% [markdown]\n# ### Create a dataframe for community colors\n\n# %%\nimport seaborn as sns\npalette = \"hls\"\n\n## Now add these colors to communities and make another dataframe\ndef colors2Community(communities) -> pd.DataFrame:\n    ## Define a color palette\n    p = sns.color_palette(palette, len(communities)).as_hex()\n    random.shuffle(p)\n    rows = []\n    group = 0\n    for community in communities:\n        color = p.pop()\n        group += 1\n        for node in community:\n            rows += [{\"node\": node, \"color\": color, \"group\": group}]\n    df_colors = pd.DataFrame(rows)\n    return df_colors\n\n\ncolors = colors2Community(communities)\ncolors\n\n# %% [markdown]\n# ### Add colors to the graph\n\n# %%\nfor index, row in colors.iterrows():\n    G.nodes[row['node']]['group'] = row['group']\n    G.nodes[row['node']]['color'] = row['color']\n    G.nodes[row['node']]['size'] = G.degree[row['node']]\n\n# %%\nfrom pyvis.network import Network\n\ngraph_output_directory = \"./docs/index.html\"\n\nnet = Network(\n    notebook=False,\n    # bgcolor=\"#1a1a1a\",\n    cdn_resources=\"remote\",\n    height=\"900px\",\n    width=\"100%\",\n    select_menu=True,\n    # font_color=\"#cccccc\",\n    filter_menu=False,\n)\n\nnet.from_nx(G)\n# net.repulsion(node_distance=150, spring_length=400)\nnet.force_atlas_2based(central_gravity=0.015, gravity=-31)\n# net.barnes_hut(gravity=-18100, central_gravity=5.05, spring_length=380)\nnet.show_buttons(filter_=[\"physics\"])\n\nnet.show(graph_output_directory, notebook=False)\n\n# %%\n\n\n# %%\n\n\n\n",
      "created_at": "2024-06-09T10:35:24.385905",
      "sequence_order": 0,
      "parent_id": "3c496c52-03dd-4f3e-aaaf-9b524f0d2f26",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": null
      }
    },
    {
      "id": "ceea3e1c-edb7-45d0-97de-d6760788d314",
      "role": "assistant",
      "content": "Sure, let's walk through how we can turn your code into a server endpoint that processes text chunks and outputs a CSV file named `graph.csv`.\n\nWe'll use Flask, a lightweight WSGI web application framework in Python, to create the server. This endpoint will accept POST requests with text data, process the text to extract concepts, calculate contextual proximity, and finally return the `graph.csv` file.\n\nHere is how we can set it up:\n\n1. **Install Flask**:\n   First, make sure you have Flask installed in your environment. If not, you can install it using pip:\n\n   ```sh\n   pip install Flask\n   ```\n\n2. **Create the Flask Server**:\n   Below is a script that sets up a Flask server with an endpoint to accept text chunks and generate the `graph.csv`.\n\n```python\nfrom flask import Flask, request, send_file\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nfrom pathlib import Path\nimport networkx as nx\nimport seaborn as sns\nfrom langchain.document_loaders import PyPDFDirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom helpers.df_helpers import documents2Dataframe, df2Graph, graph2Df\n\napp = Flask(__name__)\n\ndata_dir = \"shaykh\"\ninputdirectory = Path(f\"./data_input/{data_dir}\")\noutputdirectory = Path(f\"./data_output/{data_dir}\")\n\n@app.route('/process_text', methods=['POST'])\ndef process_text():\n    data = request.json.get('chunks', [])\n    if not data:\n        return {\"error\": \"No text chunks provided\"}, 400\n\n    # Convert chunks to documents\n    documents = [{\"page_content\": chunk} for chunk in data]\n    \n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1500,\n        chunk_overlap=150,\n        length_function=len,\n        is_separator_regex=False,\n    )\n    pages = splitter.split_documents(documents)\n    df = documents2Dataframe(pages)\n    \n    regenerate = True  # Assuming we want to regenerate each time\n    \n    if regenerate:\n        concepts_list = df2Graph(df, model='llama3')\n        dfg1 = graph2Df(concepts_list)\n        if not os.path.exists(outputdirectory):\n            os.makedirs(outputdirectory)\n        \n        dfg1.to_csv(outputdirectory / \"graph.csv\", sep=\"|\", index=False)\n        df.to_csv(outputdirectory / \"chunks.csv\", sep=\"|\", index=False)\n    else:\n        dfg1 = pd.read_csv(outputdirectory / \"graph.csv\", sep=\"|\")\n    \n    dfg1.replace(\"\", np.nan, inplace=True)\n    dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\n    dfg1['count'] = 4 \n    \n    dfg2 = contextual_proximity(dfg1)\n    dfg = pd.concat([dfg1, dfg2], axis=0)\n    dfg = (\n        dfg.groupby([\"node_1\", \"node_2\"])\n        .agg({\"chunk_id\": \",\".join, \"edge\": ','.join, 'count': 'sum'})\n        .reset_index()\n    )\n    \n    nodes = pd.concat([dfg['node_1'], dfg['node_2']], axis=0).unique()\n    G = nx.from_pandas_edgelist(dfg, 'node_1', 'node_2', edge_attr=True)\n\n    communities_generator = nx.community.girvan_newman(G)\n    top_level_communities = next(communities_generator)\n    next_level_communities = next(communities_generator)\n    communities = sorted(map(sorted, next_level_communities))\n    \n    colors = colors2Community(communities)\n    \n    for index, row in colors.iterrows():\n        G.nodes[row['node']]['group'] = row['group']\n        G.nodes[row['node']]['color'] = row['color']\n        G.nodes[row['node']]['size'] = G.degree[row['node']]\n    \n    graph_output_directory = outputdirectory / \"index.html\"\n    net = Network(notebook=False, cdn_resources=\"remote\", height=\"900px\", width=\"100%\")\n    net.from_nx(G)\n    net.force_atlas_2based(central_gravity=0.015, gravity=-31)\n    net.show(graph_output_directory, notebook=False)\n    \n    return send_file(outputdirectory / \"graph.csv\", as_attachment=True)\n\ndef contextual_proximity(df: pd.DataFrame) -> pd.DataFrame:\n    dfg_long = pd.melt(df, id_vars=[\"chunk_id\"], value_vars=[\"node_1\", \"node_2\"], value_name=\"node\")\n    dfg_long.drop(columns=[\"variable\"], inplace=True)\n    dfg_wide = pd.merge(dfg_long, dfg_long, on=\"chunk_id\", suffixes=(\"_1\", \"_2\"))\n    self_loops_drop = dfg_wide[dfg_wide[\"node_1\"] == dfg_wide[\"node_2\"]].index\n    dfg2 = dfg_wide.drop(index=self_loops_drop).reset_index(drop=True)\n    dfg2 = dfg2.groupby([\"node_1\", \"node_2\"]).agg({\"chunk_id\": [\",\".join, \"count\"]}).reset_index()\n    dfg2.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\"]\n    dfg2.replace(\"\", np.nan, inplace=True)\n    dfg2.dropna(subset=[\"node_1\", \"node_2\"], inplace=True)\n    dfg2 = dfg2[dfg2[\"count\"] != 1]\n    dfg2[\"edge\"] = \"contextual proximity\"\n    return dfg2\n\ndef colors2Community(communities) -> pd.DataFrame:\n    palette = \"hls\"\n    p = sns.color_palette(palette, len(communities)).as_hex()\n    random.shuffle(p)\n    rows = []\n    group = 0\n    for community in communities:\n        color = p.pop()\n        group += 1\n        for node in community:\n            rows += [{\"node\": node, \"color\": color, \"group\": group}]\n    df_colors = pd.DataFrame(rows)\n    return df_colors\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\n```\n\n3. **Running the Flask Server**:\n   Save the above script as `server.py`. To run the server, use the command:\n\n   ```sh\n   python server.py\n   ```\n\n4. **Sending Requests to the Endpoint**:\n   You can use a tool like `curl` or Postman to send POST requests to your server. Here is an example using `curl`:\n\n   ```sh\n   curl -X POST http://127.0.0.1:5000/process_text -H \"Content-Type: application/json\" -d '{\"chunks\": [\"Text chunk 1\", \"Text chunk 2\", ...]}'\n   ```\n\nThis setup will process the provided text chunks, generate the `graph.csv` file, and return it as a response.",
      "created_at": "2024-06-09T10:35:58.208596",
      "sequence_order": 1,
      "parent_id": "aaa2398b-b4b5-4ead-b4b1-f1c0154f40aa",
      "metadata": {
        "status": "finished_successfully",
        "weight": 1,
        "end_turn": true
      }
    }
  ]
}